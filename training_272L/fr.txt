Pour les articles homonymes, voir Bernhardt, Bernard et Sarah Bernhardt (Lucky Luke).
Ne doit pas être confondu avec Sandra Bernhard ou Sarah Bertrand.
Répertoiremodifier Sarah Bernhardt, née le 22 octobre 1844 à Paris et morte le 26 mars 1923 à Paris 17e[2], est une actrice, peintresse et sculptrice française. Elle est considérée comme une des plus importantes actrices françaises du XIXe et du début du XXe siècle.
Appelée par Victor Hugo « la Voix d'or », mais aussi par d'autres « la Divine » ou encore « l'Impératrice du théâtre », elle est considérée comme une des plus grandes tragédiennes françaises du XIXe siècle. Première « étoile » internationale, elle est la première comédienne à avoir fait des tournées triomphales sur les cinq continents, Jean Cocteau inventant pour elle l'expression de « monstre sacré ».
La mère de Sarah, Judith-Julie Bernhardt[a] (1821-1876), modiste sans le sou et fille d'un marchand de spectacles néerlandais itinérant, était une courtisane[3] parisienne juive originaire de Hollande, connue sous le nom de « Youle » (diminutif de Youlie, prononciation allemande de Julie)[4],[5],[6]. On a longtemps ignoré qui était son père[7], Sarah ayant toujours gardé le silence sur son identité. Les noms d'Édouard Bernhardt[1] ou de Paul Morel, officier de marine, étaient les plus couramment proposés[8].
Grâce à de nouvelles recherches, on connaît désormais le nom du père, Édouard Viel, un notable du Havre, qui a fait de la prison pour malversations financières[9].
Du fait de la destruction des archives de l'état civil, la date de naissance de Sarah Bernhardt a longtemps été incertaine et débattue[10]. Si ses biographes donnaient habituellement les dates 22 ou 23 octobre 1844[11], certains proposaient juillet ou septembre 1844[10], voire 1843 ou même 1841[10].
En outre, pour faciliter les démarches d'obtention de la Légion d'honneur et prouver la nationalité française de l'actrice, un acte de naissance rétrospectif était établi par décision de justice le 23 janvier 1914[1], sur base d'un certificat de baptême produit par Sarah Bernhardt, bien que la falsification de celui-ci n'ait trompé personne, y compris les magistrats[12]. Le document[13] est ainsi daté du 25 septembre 1844 et affecté aux registres du 15e arrondissement[12].
De même, le lieu de sa naissance n'était pas plus sûrement établi : une plaque mentionnant sa naissance (le 25 octobre 1844) est apposée au 5, rue de l'École-de-Médecine (ancien 11e), on évoque également la rue Saint-Honoré — au 32 ou au 265 — ou encore le 22, rue de La Michodière (2e)[10]. Les recherches d'Hélène-Claire Richard indiquent qu'elle est née chez la sage femme Charlotte Clémence Collé, au 5 de la place de l'École-de-Médecine, disparue vers la fin du XIXe siècle lors de la construction de l'École pratique de médecine[9].
Ses prénoms — Sara Marie Henriette selon l'état civil reconstitué — sont également parfois présentés dans un ordre différent selon les sources, certaines indiquant « Henriette-Marie-Sarah » ou encore « Henriette-Rosine (Bernard) », suivant le nom qu'elle avait donné lors de son inscription au Conservatoire, « Rosine (dite Sarah) »[b].
Une certaine inclination de l'actrice à l’affabulation concernant sa vie n'a pas aidé à démêler l'écheveau[10].
Par chance, après la mort du père de Sarah, la mère, espérant un héritage, demande une copie de l'extrait de naissance (datant de 1857, soit avant l'incendie de l'Hôtel de ville) Ce document établit de manière définitive la naissance de l'actrice au 22 octobre 1844[9].
Sarah Bernhardt eut au moins trois sœurs et souffrit en particulier longtemps de la préférence de sa mère pour sa jeune sœur Jeanne-Rosine, également comédienne. Délaissée par Youle qui choisit la vie mondaine à Paris, elle passe une petite enfance solitaire chez une nourrice à Quimperlé où elle ne parle que le breton, puis en 1853 au couvent du Grand-Champs à Versailles, où elle étudie jusque 1858[14]. Elle y devient mystique catholique[15]. Elle y joue son premier rôle, un ange dans un spectacle religieux[16]. Elle y organise sa conversion à la religion catholique, reçoit le baptême chrétien en 1857 et envisage de devenir religieuse[c].
C'est alors que son nom aurait été francisé en « Bernard »[réf. nécessaire] et qu'elle quitte vers 14 ans la vie monacale et passe le concours du Conservatoire où elle est reçue. .
Elle prend aussi des leçons d'escrime, dont elle tirera profit dans ses rôles masculins comme Hamlet[18].
Elle entre en 1859 au Conservatoire d'art dramatique de Paris sur la recommandation du duc de Morny dans la classe de Jean-Baptiste Provost[19]. Sortie en 1862 avec un second prix de comédie, elle entre à la Comédie-Française mais en est renvoyée en 1866 pour avoir giflé une sociétaire, Mlle Nathalie, celle-ci ayant elle-même violemment bousculé sa sœur qui avait marché sur sa traîne[20].
À cette époque, la police des mœurs compte Sarah parmi 415 « dames galantes » soupçonnées de prostitution clandestine[21].
Elle signe un contrat avec l'Odéon[22]. Elle y est révélée en jouant Le Passant de François Coppée en 1869. En 1870, pendant le siège de Paris, elle transforme le théâtre en hôpital militaire et y soigne le futur maréchal Foch qu'elle retrouvera quarante-cinq ans plus tard sur le front de la Meuse, pendant la Première Guerre mondiale[23]. Elle triomphe dans le rôle de la Reine de Ruy Blas en 1872, ce qui la fait surnommer la « Voix d'or » par l'auteur de la pièce, Victor Hugo, à l'occasion d'un banquet organisé pour la centième représentation[24]. Ce succès lui vaut d'être rappelée par la Comédie-Française dont elle est nommée sociétaire en 1875 ; elle y joue dans Phèdre en 1874 et dans Hernani en 1877[25].
Avec le succès, les surnoms élogieux se multiplieront : « la Divine »[26], l'« Impératrice du théâtre »[27]…
En 1880, elle démissionne avec éclat du « Français », devant lui payer cent mille francs-or en dommages et intérêts pour rupture abusive de contrat. Elle crée sa propre compagnie avec laquelle elle part jouer et faire fortune à l'étranger jusqu'en 1917. Première « star » internationale, elle est la première comédienne à avoir fait des tournées triomphales sur les cinq continents, Jean Cocteau inventant pour elle l'expression de « monstre sacré »[27]. Dès 1881, à l'occasion d'une tournée de Bernhardt en Russie, Anton Tchekhov, alors chroniqueur au journal moscovite Le Spectateur[28], décrit malicieusement « celle qui a visité les deux pôles, qui de sa traîne a balayé de long en large les cinq continents, qui a traversé les océans, qui plus d'une fois s'est élevée jusqu'aux cieux »[29], brocarde l'hystérie des journalistes « qui ne boivent plus, ne mangent plus mais courent » après celle qui est devenue « une idée fixe [sic] »[30].
Elle interprète à plusieurs reprises des rôles d'homme (Hamlet, Pelléas), inspirant à Edmond Rostand sa pièce L'Aiglon en 1900. Elle se produit à Londres, à Copenhague, aux États-Unis (1880-1881) où elle affrète un train Pullman pour sa troupe et ses 8 tonnes de malles, au Pérou (1886) où tous les billets pour ses représentations se vendent en 48 heures, au Chili (1886), dont elle critique les habitants[31] et en Russie, notamment au théâtre Michel de Saint-Pétersbourg (en 1881, 1892 et 1908). Son lyrisme et sa diction emphatique enthousiasment tous les publics. Afin de promouvoir son spectacle, elle rencontre Thomas Edison à New York et y enregistre sur cylindre une lecture de Phèdre[25]. Elle devient l'un des très rares artistes français à avoir son étoile sur le Hollywood Walk of Fame à Los Angeles.
Invitée en Australie en février 1891, elle se produit à Melbourne notamment, fait la connaissance d'Adrien Loir, neveu de Pasteur, avec lequel elle a sans doute une liaison[32].
Le jour de l'arrivée de Sarah à Folkestone avec la Comédie-Française en mai 1879, Oscar Wilde est présent pour l'accueillir. L'anecdote racontant la façon dont il jette des lys à ses pieds est souvent répétée, mais la version des événements de Sarah suggère qu'il réagit plutôt avec humour à une remarque désagréable faite à ses dépens :« Un de mes camarades qui était juste à côté, et qui était loin de m'apprécier, me dit d'un ton rancunier : 
"Ils te feront bientôt un tapis de fleurs".
"En voici un !" s'écria un jeune homme en jetant par terre devant moi une brassée de lys.
Je m'arrêtai net, un peu confuse, n'osant marcher sur ces fleurs blanches, mais la foule qui se pressait derrière m'obligea à avancer, et il fallut fouler aux pieds les pauvres lys »[33].Lorsqu'Oscar voit Sarah jouer Phèdre au Gaiety Theatre le 2 juin 1879, il déclare : c'est « la création la plus splendide que j'aie jamais vue »[34]. Près d'une décennie plus tard, il écrit : « ce n'est qu'en entendant Sarah Bernhard dans Phèdre que je me suis absolument rendu compte de la douceur de la musique de Racine ». Elle lui inspire un sonnet, débutant par ce vers : « Comme ce monde commun doit sembler fade et ennuyeux à quelqu'un comme toi ». Il est publié dans The World en juin 1879 sous le titre « To Sarah Bernhardt », et réimprimé dans Poems (1881) sous le titre « Phèdre ».
Sur le pan entier d'un mur de l'appartement qu'occupent Oscar Wilde et Frank Miles au 13 Salibsury Street à Londres, juste en dessous du plafond, Sarah écrit « Sarah Bernhardt » en lettres surdimensionnées avec un gros crayon de menuisier. Oscar explique à son ami William Ward que Sarah a « essayé de voir à quelle hauteur elle pouvait sauter et écrire son nom ». Il emmène Lillie Langtry au British Museum à la recherche de statues antiques romaines qui ressemblaient à « la divine Sarah ». D'après Lillie Langtry, Oscar baptise Sarah « la divine Sarah » après l'avoir vue jouer la reine dans Ruy Blas de Victor Hugo.
Oscar songe à lui faire jouer le rôle d'Elizabeth I, déclarant : « Elle serait magnifique dans des robes monstrueuses couvertes de paons et de perles ! »[35]. Elle lui commande la pièce Salomé, dont elle interprète le rôle-titre en 1892.
En 1893, alors qu'elle joue Les Rois au Théâtre du Palais-Royal, elle se lie d'amitié avec de Max et lui propose d'intégrer la nouvelle troupe du théâtre de la Renaissance dont elle s'apprête à prendre la direction. Elle remonte quelques-uns de ses plus grands succès (Phèdre, La Dame aux camélias) et crée de nombreuses pièces : Gismonda de Victorien Sardou, La Princesse lointaine d'Edmond Rostand, Les Amants de Maurice Donnay, La Ville morte de Gabriele D'Annunzio et Lorenzaccio d'Alfred de Musset (inédit à la scène).
En 1899, elle prend la direction du théâtre des Nations qu'elle rebaptise « théâtre Sarah-Bernhardt » et où elle constitue une nouvelle troupe avec son partenaire de jeu de Max et Marguerite Moreno qui partagent avec elle une vision « corporelle » du jeu d'acteur.
En opposition à son fils, elle apporte son soutien à Émile Zola au moment de l’affaire Dreyfus[36], elle soutient Louise Michel et prend position contre la peine de mort.
Le 9 décembre 1896, une « journée Sarah Bernhardt » est organisée à la gloire de l'actrice par Catulle Mendès et d'autres sommités de l'art : Edmond Rostand, Antonio de La Gandara qui fit d'elle plusieurs portraits, Jean Dara, José-Maria de Heredia, Carolus-Duran. Le Tout-Paris s'y presse : un repas de 500 convives au Grand Hôtel précède un gala au théâtre de la Renaissance — qu'elle dirige alors — où l'actrice se rend accompagnée de 200 coupés et où l'on peut entendre entre autres hommages un Hymne à Sarah composé par Gabriel Pierné sur des paroles d'Armand Silvestre et interprété par l'orchestre Colonne[37]. Des menus sont édités pour l'occasion[38].
Ayant compris l'importance de la réclame, elle met en scène chaque minute de sa vie et n'hésite pas à associer son nom à la promotion des produits de consommation. Son style et sa silhouette inspirent la mode, les arts décoratifs mais aussi l’esthétique de l’Art nouveau. Elle fait elle-même appel au peintre Alfons Mucha pour dessiner ses affiches à partir de décembre 1894. Ces six années de collaboration donnent un second souffle à sa carrière. Tuberculeuse comme sa sœur Régina qui en meurt en 1874, elle développe une certaine morbidité en se reposant régulièrement dans un cercueil capitonné qui trône chez elle. Devant le scandale suscité, elle s'y fait photographier par un opérateur du studio Melandri pour en vendre des photos et cartes postales[39].
En 1905, lors d'une tournée au Canada, le Premier ministre Wilfrid Laurier l'accueille à Québec ; mais l’archevêque Louis-Nazaire Bégin, détestant le théâtre et reprochant à l'actrice un jeu du corps nouveau pouvant être qualifié d'érotique, demande à ses paroissiens de boycotter la représentation et l’actrice, habituée aux foules, se produit devant une salle en partie vide[40].
Après avoir joué dans plus de 120 spectacles, Sarah Bernhardt devient actrice de cinéma. Son premier film est Le Duel d'Hamlet réalisé en 1900. C'est un des premiers essais de cinéma parlant avec le procédé du Phono-Cinéma-Théâtre, où un phonographe à cylindre synchronisait plus ou moins la voix de l'actrice aux images projetées[41]. Elle tournera d'autres films — muets — dont deux œuvres autobiographiques, la dernière étant Sarah Bernhardt à Belle-Île en 1912, qui décrit sa vie quotidienne[42].
En 1914, le ministre René Viviani lui remet la croix de chevalier de la Légion d'honneur, pour avoir, en tant que comédienne,  et pour ses services d'infirmière pendant la guerre franco-prussienne de 1870-1871[43].
Sarah Bernhardt est amputée de la jambe droite en 1915[44], à l'âge de 70 ans, en raison d'une tuberculose osseuse du genou. Les premiers symptômes remontent à 1887, lorsqu’elle se blesse au genou sur le pont d'un bateau qui la ramène d'une tournée aux Amériques[44]. Cette première luxation, non soignée, s’aggrave en 1887, lors des sauts répétés du parapet dans le final de La Tosca, la comédienne ayant chuté à de nombreuses reprises sur les genoux[45], puis en 1890 à la suite d'une nouvelle blessure contractée lors d'une représentation du Procès de Jeanne d'Arc au théâtre de la Porte-Saint-Martin[réf. nécessaire]. En 1902, lors d’une tournée, un professeur de Berlin diagnostique une tuberculose ostéo-articulaire et prescrit une immobilisation de six mois que l’actrice ne peut se résoudre à suivre[46]. Elle se contente de séances d'infiltrations et, en 1914, d'une cure à Dax, d'ailleurs sans effet[47].
En septembre 1914, craignant que Sarah Bernhardt ne soit prise en otage, lors d’une éventuelle avancée allemande sur Paris, le ministère de la Guerre conseille à l’actrice de s’éloigner de la capitale. Henri Cain, un de ses proches dont la femme, Julia Guiraudon, est fille d’un ostréiculteur de Biganos, lui recommande de séjourner sur le bassin d’Arcachon, où lui et son épouse louent une villa à Andernos-les-Bains[44]. Elle arrête son choix sur la villa « Eurêka », où elle s'installe de septembre 1914 à octobre 1915[d].
Plâtré durant six mois, son genou développe une gangrène[48]. Son médecin et ancien amant, Samuel Pozzi, que Sarah surnomme « Docteur Dieu »[49], ne peut se résoudre à pratiquer lui-même l'opération et sollicite le concours du professeur Jean-Henri Maurice Denucé, désormais chirurgien à Bordeaux[50]. L'actrice est amputée au-dessus du genou le 22 février 1915 à la clinique Saint-Augustin de Bordeaux[e]. Sarah revient en convalescence à Andernos en mars 1915[51]. Elle participe à une manifestation patriotique le 10 août 1915 où elle lit deux poèmes puis quitte définitivement Andernos en octobre 1915[44]. Elle va à Reims, , le 9 septembre 1916[52] et joue le rôle d'une infirmière devant la cathédrale martyre[53].
Cela ne l'empêche pas de continuer à jouer assise — elle refuse de porter une jambe en bois ou une prothèse en celluloïd —, ni de rendre visite aux poilus au front en chaise à porteurs, lui valant le surnom de « Mère La Chaise »[54]. Elle ne s'épanche jamais sur son infirmité, sauf pour rire : [55]. Son refus des faux-semblants n'a pas été jusqu'à lui faire négliger la chirurgie esthétique. En 1912, elle demande au chirurgien américain Charles Miller un lifting, technique alors débutante, dont les résultats seront corrigés par Suzanne Noël[56].
Alors qu'elle est en train de tourner un film pour Sacha Guitry, La Voyante, elle meurt [57] le 26 mars 1923[2], à son domicile au 56, boulevard Pereire à Paris, en présence de son fils. Elle est enterrée à Paris au cimetière du Père-Lachaise (44e division)[58].
La performance théâtrale de Sarah Bernhardt, que ses contemporains acclamèrent à l'égale de celle de Mounet-Sully, est, comme cette dernière, emphatique tant dans la pantomime que dans la déclamation. Les modulations de la voix s'éloignent délibérément du naturel[59] ; les émotions sont rendues, tant par le geste que par l'intonation, plus grand que nature[60]. Ce style hérité de la déclamation baroque se démode avant la fin de sa carrière ; Alfred Kerr remarque . Les critiques modernes qui écoutent ses enregistrements de Phèdre chez Thomas Edison en 1903 sont souvent déçus[62].
Vers 1874, alors qu'elle est une comédienne au talent reconnu, mais manquant d'emplois qui l'intéressent, Sarah Bernhardt apprend le modelage[63], puis la peinture. Elle fréquente l'Académie Julian à Paris et présente au Salon de 1880 La Jeune Fille et la Mort, reçu .
Elle réalise également quelques bronzes[65], dont un buste d'Émile de Girardin et un de Louise Abbéma exposés aujourd'hui à Paris au musée d'Orsay.
Un autoportrait est exposé dans une des salles consacrées à la peinture moderne de la Fondation Bemberg à Toulouse.
Autoportrait, 1910, huile sur toile, Toulouse, Fondation Bemberg.
Nature morte aux pêches, 1922, Paris, musée Carnavalet.
Après la tempête, vers 1876, marbre, Washington, National Museum of Women in the Arts.
Portrait funéraire de Jacques Damala, vers 1889, marbre, New York, Metropolitan Museum of Art.
Buste de Victorien Sardou, 1900, bronze, Paris, Petit Palais.
Les détails de la vie privée de Sarah Bernhardt sont souvent incertains ; quand elle expliquait : , Alexandre Dumas fils — qui la détestait — ajoutait dans une discussion avec le journaliste Louis Ganderax : 
La vie privée de Sarah Bernhardt fut assez mouvementée. À l'âge de vingt ans, elle donne naissance à son seul enfant qui deviendra écrivain, Maurice Bernhardt, fruit d'une liaison avec un prince belge, Henri de Ligne (1824-1871), fils aîné d'Eugène, 8e prince de Ligne[67]. Elle a par la suite plusieurs amants, dont Charles Haas, mondain très populaire à qui elle vouait une véritable passion alors qu'il la traitait en femme légère et la trompait sans états d'âme. Après leur rupture, ils demeurèrent cependant amis jusqu'à la mort de Haas. On compte également des artistes tels que Gustave Doré et Georges Jules Victor Clairin et des acteurs tels que Mounet-Sully, Lucien Guitry et Lou Tellegen ou encore son « Docteur Dieu » Samuel Pozzi. On parle également de Victor Hugo[68] et du prince de Galles[69]. Une source lui prête également des liaisons homosexuelles, notamment avec la peintre Louise Abbéma[70][source insuffisante] qui fit d'elle plusieurs portraits[f]. Elle est également portraiturée par Gustave Doré, Giovanni Boldini et Jules Bastien-Lepage.
En 1874-1875, elle entretient des rapports intimes moyennant rétribution avec plusieurs députés dont Léon Gambetta, Henri Ducasse et le comte de Rémusat[21].
En 1882, elle se marie à Londres avec un acteur grec, Aristides Damala (en), mais celui-ci est dépendant de la morphine et leur relation ne dure guère. Elle restera cependant son épouse légitime jusqu'à la mort de l'acteur, en 1889 à l'âge de 34 ans. Mais elle perd sa  nationalité française en épousant un étranger. Donc en 1916, elle fait une demande de réintégration dans la nationalité française[71].
Elle était amie du poète Robert de Montesquiou qui lui avait dédié un poème (inédit). Ce poème manuscrit faisait partie de sa bibliothèque vendue en 1923[72].
En 1890, elle est victime d'un vol dans sa propriété du Havre pendant qu'elle réside dans sa villa de Sainte-Adresse. Plusieurs objets de valeur auraient été dérobés, notamment un diamant de grande valeur. Elle portera un temps des soupçons sur sa gouvernante Mme Guérard, qui s'occupa d'elle telle une mère adoptive. L'affaire ne se résoudra pas et l'actrice ne remettra pas la main sur son précieux diamant[73].
Dédaignant les stations balnéaires à la mode et désireuse d'établir sa retraite en un lieu qui lui soit particulier, à l'écart du monde, Sarah Bernhardt choisit de séjourner face à l'Océan, sur une pointe rocheuse déchiquetée et venteuse, éloignée du chef-lieu d'une île bretonne, Belle-île, elle-même relativement difficile d'accès et alors inconnue du grand tourisme. C'est son portraitiste attitré, Georges Clairin[74], qui la lui avait fait découvrir. Elle s'y installe progressivement avec ses animaux exotiques et sa petite cour de commensaux — qu'elle appelait indistinctement « sa ménagerie » — dans un fortin militaire désaffecté qu'elle avait acquis en 1894 au lieu-dit « La pointe des Poulains ». À côté de ce fortin elle avait fait bâtir, décorer et meubler la villa Lysiane (le prénom de sa petite-fille) et la villa Les Cinq Parties du monde, travaux importants qui lui coûtèrent plus d'un million de francs-or, somme considérable pour l'époque. Elle s'installa plus tard dans le manoir de Penhoët, un manoir de briques rouges disparu lors des bombardements de la Seconde Guerre mondiale qu'elle avait acheté car elle le jugeait trop proche de son fortin et aussi plus confortable. Pour s'y rendre elle prenait le train de Paris jusqu'à Vannes, où elle donnait à l'occasion quelques représentations, avant d'embarquer pour "son" île où elle faisait grand effet aux îliens. En 1922, infirme, malade et désargentée, elle vend ses propriétés belle-îloises. Là, un musée lui est consacré depuis 2007[75] : le fort à la pointe des Poulains et ses abords ont été aménagés pour recevoir du public.
Elle était la marraine de l'actrice franco-américaine Suzanne Caubet[76].
Selon son passeport de 1886, elle mesurait 1,54 m[77].
Sa devise était  en référence à son audace et à son mépris des conventions. Alors qu'elle est attaquée par des détracteurs sur ses origines, après la défaite de 1871, elle déclare : [78].
Elle a en partie inspiré à Marcel Proust — sans doute avec les comédiennes Rachel et Réjane — le personnage de l'actrice « la Berma » dans À la recherche du temps perdu[79]. Proust la désignait parfois dans sa correspondance par « Haras », son prénom à l'envers[80].
Sacha Guitry, dans ses Mémoires, l'évoque ainsi : « Madame Sarah jouait un grand rôle dans notre existence. Après notre père et notre mère, c'était assurément la personne la plus importante du monde à nos yeux. […] Que l'on décrive avec exactitude et drôlerie — ainsi que Jules Renard l'a fait dans son admirable Journal — sa maison, ses repas, ses accueils surprenants, ses lubies, ses excentricités, ses injustices, ses mensonges extraordinaires, certes […] mais qu'on veuille la comparer à d'autres actrices, qu'on la discute ou qu'on la blâme, cela ne m'est pas seulement odieux : il m'est impossible de le supporter. […] Ils croient qu'elle était une actrice de son époque. […] Ils ne devinent donc pas que si elle revenait, elle serait de leur époque »
— Sacha Guitry, Si j'ai bonne mémoire[81]Citation :
« Il faut haïr très peu, car c'est très fatigant. Il faut mépriser beaucoup, pardonner souvent, mais ne jamais oublier. Le pardon ne peut entrainer l'oubli ; pour moi, du moins[82]. »
On lui attribue aussi ce mot :
« Sarah Bernhardt, à qui une jeune comédienne a déclaré qu'elle avait déjà joué plusieurs fois et qu'elle n'avait même plus de trac, aurait alors répondu : « Ne vous en faites pas, le trac, cela viendra avec le talent ». »
— Maurice Thévenet, Les Talents[83]Elle aurait déclaré avoir gagné au cours de sa carrière quelque 45 millions de francs, soit 185 millions d'euros.
Dans son testament, elle déclare léguer 
À la pointe des Poulains (Belle-Île-en-Mer), le fort, la villa Lysiane et la villa Les Cinq Parties du monde sont désormais accessibles au public comme Espace muséographique Sarah-Bernhardt. Les demeures de la tragédienne sont réaménagées dans leur décor du début du XXe siècle.
Le musée Carnavalet conserve une médaille en or à l'effigie de Victor Hugo, exécutée par le graveur Jules-Clément Chaplain et qui fut offerte à Sarah Bernhardt en 1911 pour la centième représentation de Lucrèce Borgia (ND 1080).
Dans Du côté de chez Swann de Marcel Proust, le narrateur, jeune, classe [91].
Une école Sarah-Bernhardt est située à Montpellier dans l'Hérault[94] et à Nantes en Loire-Atlantique[95].
L'Espace Sarah Bernhardt est situé à Goussainville (Val-d'Oise)[96].
Pour son centenaire, un timbre a été émis, avec retard, le 16 mai 1945[98],[99]. Elle est alors en France la première actrice honorée de la sorte[100]. Existent aussi un timbre de Cuba (1989) et un (du sesquicentenaire) de Monaco (1994).
Le personnage de Sarah Bernhardt apparaît dans de nombreuses œuvres de fiction de la culture populaire contemporaine.
En 2013, un documentaire-fiction, intitulé Sarah Bernhardt, sa vie, ses folies, réalisé par Dominique Leeb, lui est consacré dans le cadre de l'émission Secrets d'Histoire[107].
Par Nadar, 1864.
Par Nadar, s.d.
Par Paul Nadar, 1878.
Par Napoléon Sarony, 1880.
Sarah Bernard en tenniswoman, 1905.
Par Henry Walter Barnett (en), 1910.
Par Gustave Doré, 1870, localisation inconnue.
Par Jules Bastien-Lepage, 1879, San Francisco, California Palace of the Legion of Honor.
Portrait présumé par Giovanni Boldini, vers 1880, localisation inconnue.
Par Alfred Stevens, vers 1882, localisation inconnue.
Illustration d'un article de Sarah Bernhardt paru dans Harper's Bazaar, 1918.
Affiche de Jules Chéret.
Affiche par Eugène Grasset.
Affiche par Alfons Mucha.
Sur les autres projets Wikimedia :
Pour les articles homonymes, voir Frank Wright (homonymie) et Wright.
Frank Lloyd Wright, né le 8 juin 1867 à Richland Center dans le Wisconsin et mort le 9 avril 1959 à Phoenix en Arizona, est un architecte et concepteur américain.
Il est l'auteur de plus de quatre cents projets réalisés, musées, stations-service, tours d’habitation, hôtels, églises, ateliers, mais principalement des maisons qui ont fait sa renommée. Il est notamment le principal protagoniste du style Prairie et le concepteur des maisons usoniennes, petites habitations en harmonie avec l’environnement où elles sont construites. En 1991, il a été reconnu par l'Institut des architectes américains comme le plus grand architecte américain de l’histoire[1].
Frank Lloyd Wright, est né en 1867 dans la petite ville de Richland Center au Wisconsin. Après le divorce de ses parents — William Carey Wright (1825-1904) et Anna Lloyd Jones (1839-1923) — en 1885, il change le nom Lincoln pour Lloyd, en l’honneur de sa mère, dont il devient le soutien financier, ainsi que de ses deux sœurs.
En 1887, Wright s'installe à Chicago[2], en quête d’un emploi. La ville se remettait alors de l’incendie dévastateur de 1871 et devait composer avec un accroissement galopant de sa population. Wright finit par se dénicher un emploi de dessinateur technique pour la firme de l’architecte Joseph Lyman Silsbee. Plusieurs dessinateurs et architectes travaillent déjà pour Silsbee, dont Cecil Corwin, George W. Maher et George Grant Elmslie. Wright se lie d'amitié avec Corwin, qui l'héberge durant un temps.
Attiré par une architecture plus moderne que celle que pratiquait Silsbee, il se joint bientôt au cabinet des architectes Adler et Sullivan, représentant l'école de Chicago. Sullivan prend le jeune apprenti sous son aile. En dépit des divergences entre eux, Wright demeurera toujours reconnaissant et attaché à celui qui lui enseigne les rudiments du métier.
En 1889, Wright épouse Catherine Lee Tobin (1871-1959), dont il aura six enfants. Louis Sullivan l’aide et lui consent un prêt de 5 000 $. Avec l'argent, Wright achète un lot à Oak Park, en banlieue de Chicago. C’est là, à l’intersection des avenues Forest et Chicago, qu'il construit sa première maison (Frank Lloyd Wright Home and Studio) dont le style, avec un grand puits central apportant vie et lumière à l'habitation, annonce ses futures réalisations.
Wright demeure employé de la firme Adler et Sullivan durant six ans et collabore principalement à des projets de maisons individuelles. C'est là qu'il acquiert l'essentiel de sa formation et une partie de son inspiration dans ce domaine. Mais une séparation brutale se produit bientôt, car Wright conçoit des maisons pour son propre compte pendant ses heures libres, afin de combler ses besoins financiers. Il contrevient ainsi aux clauses de son contrat qui lui interdisent tout projet en dehors de la firme. Frank Lloyd Wright démissionne.
En 1893, Wright découvre l'architecture japonaise à l'exposition universelle de Chicago. C'est le palais Katsura, reconstitution d'un temple shinto, qui l'avait le plus impressionné et qui influencera durablement son style. Wright fera par la suite quelques voyages au Japon où il obtiendra même des commandes, dont l'Hôtel impérial à Tokyo, construit en 1923.
Wright s'installe à son compte, toujours à Chicago, après son départ de la firme de Sullivan. Il fait alors la rencontre de jeunes architectes, dont Dwight H. Perkins, qui donneront bientôt naissance au style Prairie : constructions basses, élimination des cloisons inutiles, aires ouvertes, pivot central avec une cheminée massive maçonnée — surmontée d’un manteau large et bas, et autour de laquelle s'organise la vie de famille —, forte horizontalité (à l'image des vastes étendues des prairies), larges toitures basses qui se prolongent au-delà des murs, bandeaux de fenêtres, souvent agrémentées de vitraux. Le style, exemplifié à son meilleur par Wright, introduit notamment le principe d'aire ouverte, abondamment éclairée par des rangées de fenêtres, lien entre l'intérieur et l'environnement extérieur qui témoigne de l'influence de l'architecture japonaise sur Wright. Ces principes étaient alors très novateurs en Occident.
Wright signe bientôt une première commande indépendante : ce sera la maison Winslow, où il met déjà en application des principes du style Prairie. Durant les années 1890, il expérimente de nouveaux matériaux et de nouvelles formes, notamment dans sa propre maison à Oak Park, comme la brique et l'horizontalité des volumes. Les bâtiments qu'il dessine n'offrent toutefois pas toujours de style très personnel, Wright devant se plier aux exigences de sa clientèle afin d'établir sa pratique. Il conçoit donc des maisons dans les styles prisés de l'époque (Tudor, revêtements de bardeaux, tourelles, fenêtres en mansarde).
En 1894, il rencontre l'architecte Daniel Burnham (1846-1912) qui avait été impressionné par la maison Winslow. Wright refuse sa proposition d'aller étudier l'architecture classique pour quatre années à l'école des beaux-arts de Paris — prestigieuse école d'architecture de l'époque —, puis deux années à Rome :  Wright préfère poursuivre sa propre voie vers la modernité, au sein de l'école de Chicago.
Par la suite, en 1898, Wright ouvre sa propre agence à Oak Park afin de se rapprocher de sa famille. Il en profite pour modifier sa maison et ajouter de nouvelles chambres pour ses nombreux enfants. Il construit également un studio de deux étages où il expérimente une structure octogonale et un balcon suspendu. Sur sa cheminée, il fait graver cette inscription : Truth is life. Good friend, around these hearth-stones speak no evil word of any creature (La vérité est la vie. Bon ami, autour des pierres de ce foyer, ne médis d'aucune créature)[4].
Wright perçoit les pièces d'un bâtiment comme des organes autonomes qui constituent un corps cohérent. Il pousse l'analogie avec le monde vivant jusqu'à prétendre que la construction doit représenter la croissance d'un être vivant. Cela explique la haine que Wright possédait vis-à-vis des grandes villes, notamment Chicago. Cette haine le poussa à ne construire que de très rares (mais notables) bâtiments dans de grandes agglomérations. Il s'intéressera davantage à des projets de maisons individuelles, construites en harmonie avec le site.
À partir de 1897, son style se révèle, avec les « maisons de la prairie » (Prairie Houses) dont sa maison d'Oak Park est un précurseur; ce sont des pavillons d’un seul tenant ou en plusieurs parties reliées entre elles, dont Wright soigne particulièrement l'intégration au paysage par le biais de l'horizontalité. Il essaie également de tenir compte des contraintes que le climat continental de la région impose, multipliant les différences de hauteur des plafonds de manière à éclairer et à ventiler les pièces. Il introduit également un enchaînement plus fluide et plus ouvert des pièces, en opposition à la structure rigide de l'architecture victorienne, tout en respectant la fonction de chacune d'elles. Ces innovations passent par l'utilisation d'une combinaison de matériaux traditionnels (la pierre pour les façades et les sols) et novateurs pour l'époque : béton, acier qui servent de support à des claires-voies, des toits débordants, des terrasses en encorbellement ou de grandes baies. Le mobilier et l'éclairage électrique sont souvent intégrés au bâtiment. Les luminaires sont camouflés par des grilles dont les jeux d'ombre rappellent ceux du soleil à travers les arbres. Il en va de même des vitraux, typiques de l'Art déco, qui tamisent et colorent la lumière sans l'obscurcir. En 1901 il conçoit la Maison Willits à Highland Park (Chicago), l'une des premières Maisons de la Prairie.
Wright se positionne alors en rupture avec l'architecture classique européenne. Il s'intéresse à définir un style qu'il qualifie d'organique, inspiré pour une part de son maître Sullivan, et qu'il estime pouvoir devenir un fondement neuf de la culture américaine. Dans cet idéal qui ne recherche pas spécialement à imiter la nature, mais qui s'en inspire, la forme des parties de la maison doit découler de leurs fonctions, tandis que forme et fonction ne doivent faire qu'un.
En 1904, Frank Lloyd Wright dessine le Larkin Building à Buffalo qu'il organise autour d'un grand puits central éclairé par le haut et sur lequel donnent les pièces de chaque étage. L'immeuble s'ouvre donc vers l'intérieur et ménage une grande salle commune en son cœur. En utilisant la pierre et la brique, en découpant des plans horizontaux, Wright refuse la standardisation des immeubles.
La même année, il offre ses services à la congrégation religieuse unitarienne d'Oak Park, dont l'église vient d’être détruite par un incendie. Wright travaille sur le projet de 1905 à 1908. Le bâtiment, construit en béton armé, est considéré comme une de ses œuvres maîtresses et influencera notablement le milieu de l'architecture moderniste.
D'autres réalisations marquantes de Wright à cette époque sont la Robie House à Chicago et la Coonley House (en) à Riverside dans l'Illinois. La maison de Frederick Robie, avec ses toitures à larges pans inclinés en porte-à-faux et l'organisation originale de l'espace intérieur, marque une rupture avec les maisons de style victorien encore courantes à l'époque. La salle à dîner et le salon forment pratiquement une seule longue pièce en continu, démarqués seulement par le foyer central, aménagé un peu en contrebas du plancher. Trois garages préfigurent déjà l'envahissement de l'automobile ! La maison est surélevée, procurant vue, lumière et intimité aux occupants, tout en conférant une impression de grandeur et de majesté à la demeure. Une maquette de la maison fut placée à l'entrée de l’exposition rétrospective qui fut consacrée à Frank Lloyd Wright en 1941 au Musée d'art moderne de New York.
En 1909, Wright vit une période trouble et a le sentiment d'être parvenu à ses limites. Lassé de sa vie conjugale, en proie à des questionnements sur sa pratique, il part s'installer en Europe. Il abandonne au passage sa première femme et ses enfants, tout en emmenant la femme de l'un de ses clients, Mamah Borthwick Cheney, dont il était tombé amoureux. De ce fait, il provoque un scandale qui faillit ruiner sa carrière, sans compter son train de vie fastueux qui lui vaut d'être perpétuellement endetté et assailli par ses créanciers.
En Europe, Wright visite l'Italie. Il fréquente et influence les architectes d'avant-garde en Autriche, en Allemagne et aux Pays-Bas, dont Walter Gropius et Mies van der Rohe (qui ont fondé le Bauhaus). Il publie également un portfolio en 1910 grâce à Ernst Wasmuth qui fera connaître son œuvre. Ce recueil, connu sous le nom de Wasmuth portfolio et publié en deux volumes, contient plus de cent lithographies de ses projets. La même année, il expose certaines de ses œuvres à Berlin.
En 1911, il retourne aux États-Unis et s'installe dans le Wisconsin, où il fonde la communauté unitarienne de Spring Green. Il y construit une série de bâtiments à la fois communautaires, domestiques et agricoles sur un terrain offert par sa mère, et baptise l'ensemble Studio Taliesin, du nom du poète Taliesin dans la mythologie celtique. C'est là qu'il s'installe avec Mamah. Il démarre là une seconde carrière.
Mais le 15 août 1914, dans un accès de folie à la suite de son licenciement, un employé domestique du nom de Julian Carlton, met le feu au domaine et assassine sept personnes à coups de hache, dont Mamah Cheney. Wright surmonte cette douloureuse épreuve et reconstruit le domaine qui sera de nouveau détruit par un incendie le 22 avril 1925. Le domaine, encore une fois reconstruit, a été reconnu comme site patrimonial en 1960.
Entre-temps, Wright épouse Miriam Noel en 1923, mariage de courte durée en raison de la dépendance de Noel à la morphine. Wright rencontre alors Olga (Olgivanna) Lazovich Hinzenburg avec qui il s'installe à Taliesin et qu'il épousera en 1928. Toute cette période est marquée par des déboires financiers et par la raréfaction des commandes. Il ne retrouvera véritablement son élan que durant les années trente.
Quand arrive la crise de 1929, Wright n'a plus de travail à Chicago. Comme beaucoup de ses confrères, il doit faire face à une période de récession. Il part alors pour Phoenix avec ce qui lui reste de son agence, pensant ouvrir à Taliesin West, une école d'architecture pour subvenir à ses besoins. Comme tous les Américains, Wright semble très marqué par la crise de 1929, qui reste encore aujourd'hui une date charnière dans l'histoire de ce pays.
Cette crise a été engendrée par l'artificialité de la spéculation boursière, mais c'est aussi une crise plus profonde de la terre et de la main-d'œuvre agricole. La crise de 1929 amène un repli identitaire de l'Amérique, auquel Wright n'échappe pas. Cette période marque un tournant dans son œuvre. Wright en tant qu'architecte ressent le besoin de se repositionner par rapport à la société et aux valeurs américaines. Comme beaucoup d’Américains de l'époque, il participe à cet idéal de vie nomade sur les routes et de voyages à travers les différents États. Dans cette période de remise en cause des idées sur l'architecture et sur la place même de l'architecte dans la société, Wright tente de formuler un nouveau rôle pour l'architecte. Il lui incombe désormais de restructurer l'ordre social américain.
En 1928, apparaît le terme « Usonie », qui désigne pour Wright l'idéal démocratique vers lequel doit tendre l'Amérique. Ce terme définit pour lui la construction de maisons individuelles abordables en grande série.
Taliesin Fellowship s'inscrivait dans cette optique d'esprit communautaire de Wright et de son enseignement. Le domaine incluait l'atelier de Wright, une ferme, des habitations, des appartements, des salles d'archives, des jardins et aménagements extérieurs, des salons de musique et de spectacle. Wright y accueillit plusieurs architectes dont Rudolf Schindler et Richard Neutra, mais aussi Nobuko Tsuchiura et son mari Kameki Tsuchiura. Cette école a donné naissance à plus de soixante-dix projets.
Après la tragédie de Taliesin, en 1914, Wright reçoit une commande pour l'Hôtel impérial à Tokyo qui lui procure l'occasion de séjours prolongés au Japon, pays à l'architecture singulière qui l'inspire grandement. Il rapporte également de là-bas plusieurs objets d’art, estampes, rouleaux, brocards de soie, dont il fera même le commerce. La réalisation de l'hôtel contribuera à un certain retour en faveur de Wright lorsque l'établissement résista au tremblement de terre de 1923, ce qui souligna les qualités de concepteur de l'architecte.
Durant les années vingt, Wright développe un concept de construction à base de blocs de béton ouvragé qui évoque l'ornementation des temples Maya. Ce procédé s'appelait textile block system[5] ou construction en blocs tissés[6], ornés de motifs ou ajourés. Wright cherche ainsi à conférer ses lettres de noblesse à un matériau jugé ingrat. Au lieu de peindre le ciment, il préfère lui laisser sa couleur naturelle, créant ainsi une continuité visuelle entre l'intérieur et l'extérieur.
Il expérimente ce nouveau concept, en 1916, avec la commande d'Aline Barnsdall, une riche héritière installée en Californie. Wright rompt cependant avec un principe qui lui est cher – un édifice doit émaner du site sur lequel il est construit –, car le terrain n'est pas encore acheté. De fait, Wright doit remanier ses plans pour les adapter à la colline de vingt hectares près d'Hollywood choisie par sa cliente. La maison, construite un peu comme un temple, rappelle en effet l'architecture maya. Cependant, le résultat ne convainc ni l'architecte ni la cliente. Pour Wright, il s'agissait d'un projet de transition qui dégénéra en poursuites et qu'Aline Barnsdall n'habita que peu d’années avant de faire don de la maison Hollyhock à la ville en 1927.
En revanche, plusieurs maisons sont construites en Californie sur ce modèle, avec davantage de succès : la maison Millard (en) à (Pasadena), la maison Storer (en) à West Hollywood, la maison Samuel Freeman (en) à Hollywood (où la dialectique entre le dedans et le dehors, chère à Wright, est particulièrement réussie) et la maison Ennis à Los Angeles.
Wright développe son style, s'inspirant de formes géométriques, comme le cercle, le carré, le triangle, le rectangle ou l'hexagone. Il varie l'utilisation des matériaux, préférablement locaux, les méthodes de construction, les couleurs et les textures. Il affirme son souci de concevoir ses maisons en fonction du site où elles sont construites. Il joue avec les éléments naturels du paysage, l'eau, les rochers, les arbres, la végétation, les reliefs, allant souvent jusqu'à incorporer l'aménagement paysager dans les plans qu'il remet à ses clients. 
L'exemple le plus notoire de cette approche est celui de la Maison sur la cascade (Fallingwater house) qu'il conçoit en 1935 à l'âge de soixante-huit ans. Construite directement sur le rocher où Edgar Kaufmann, propriétaire, aimait pique-niquer, la maison offre une superposition de balcons en porte-à-faux surplombant la rivière attenante. Les angles sont arrondis pour mieux refléter les formes naturelles environnantes. Un escalier mène à une petite plateforme directement au-dessus de la rivière. Cet authentique chef-d'œuvre attire l'attention sur Wright, lui vaut à nouveau l'intérêt des milieux de l'architecture et relance sa carrière d'architecte. Ses années les plus fructueuses s'annoncent pour lui.
Nombreux sont ceux qui ont considéré la Fallingwater house comme l'un des exemples les plus représentatifs de l'architecture organique où l'homme et la nature sont étroitement liés. Dans cette œuvre architecturale, Frank Lloyd Wright a été précurseur en utilisant le liège comme revêtement décoratif et technique pour le sol et le mur. Dans les pièces d'eau de la chambre du propriétaire, comme dans celles des invités, le liège parcourt le sol et les murs et donne à l'ensemble une impression à la fois feutrée et terreuse. Il se fournissait auprès de la société Robinson, au Portugal, désormais disparue. Le savoir-faire a été repris par la société française Cork design, dont l’une des gammes porte le nom en son honneur.
En 1934, il commence une série de maisons dites « usoniennes » (« Usonian Homes »). Ce sont, comme la Malcolm Willey house, de petites maisons économiques se caractérisant pour la plupart par un motif en L, une dalle de béton sans fondation, intégrant un système de chauffage radiant (une innovation de Wright), par des toits plats sans grenier et par de nouveaux procédés de construction des murs. Au lieu d'un garage, un simple abri pour les voitures, afin de réduire les coûts. Les maisons usoniennes étaient destinées à la classe moyenne et conçues de manière pratique avec de petites cuisines, de petites chambres, un salon ouvert autour du foyer et du mobilier intégré. Jusqu'en 1937, il construit un peu moins d'une soixantaine de ces maisons, adoptant au fur et à mesure des techniques diversifiées.
La Seconde Guerre mondiale entraîne un ralentissement des activités de Wright, qui reprennent rapidement à la fin des hostilités. À plus de quatre-vingts ans, l'architecte se retrouve à la tête d'une importante agence avec plus de cinquante assistants afin de répondre à la demande. Wright maintient la discipline avec fermeté et se réserve les projets de prestige, tout en demeurant attaché à la conception de maisons. Il accueille chaque client avec déférence, bien que la conduite du projet conduise quelquefois à des désaccords et à des brouilles, Wright aimant imposer ses idées.
Wright sera également appelé à travailler sur des projets de plus grande envergure, comme la Price Tower, gratte-ciel de dix-neuf étages construit à Bartlesville (Oklahoma) en 1956 ou encore sur un projet d'opéra pour la capitale de l'Irak, Bagdad, en 1957[8]. La réalisation la plus connue de Frank Lloyd Wright demeure toutefois le musée Guggenheim à New York et inauguré en 1959. Sa conception unique s'articule autour d'un immense puits de lumière sur toute la hauteur de l'immeuble et d'une rampe en colimaçon. Le visiteur accède au dernier étage en ascenseur et emprunte ensuite la rampe en pente douce continue pour visiter les expositions jusqu'au rez-de-chaussée.
Le Kalita Humphreys Theater à Dallas (Texas) est le dernier projet de Wright, avant sa mort en 1959, à Phoenix en Arizona, des suites d'une complication chirurgicale. Il était âgé de 91 ans.
Wright a aussi rédigé plusieurs ouvrages sur l'architecture. Son œuvre a profondément marqué le développement de l'architecture contemporaine, aux États-Unis et en Europe. Il a influencé un grand nombre d'architectes, dont plusieurs ont étudié avec lui, et par la même occasion divers courants artistiques, dont l'expressionnisme.
Il fut l'un des premiers architectes à intégrer l'aménagement intérieur dans ses réalisations : mobilier, tapis, fenêtres, vitraux, portes, tables, chaises, dispositifs d’éclairage et éléments décoratifs. Tous ces éléments sur mesure faisaient partie intégrante de la conception qu'il présentait à son client.
À cela s'ajoute son exploration autant des formes (ellipses, rectangles imbriqués, cercles, octogones, hémicycles pour mieux laisser entrer la lumière du soleil, etc.) que des matériaux (acier, brique, bois, verre et ciment, dont un ciment rouge indien qu'il affectionnait pour les revêtements de sol). Wright marie ces éléments avec imagination, toujours dans le souci de proposer des espaces lumineux et chaleureux, en influençant le style de vie de ses occupants de la manière la plus heureuse qui soit. Wright s'accordait ainsi à l'air du temps, qui voyait la disparition progressive des domestiques et le besoin d'organiser plus efficacement la vie familiale, avec des aires ouvertes pour faciliter le soin aux enfants et l'accueil des invités.
Le plus grand mérite de Wright se résume sans doute dans le fait que les espaces de vie qu'il concevait s'adressaient à tous les types de clients, qu'ils soient fortunés ou non, et qu'ils modifiaient leur relation à l'habitat. 
Au total, Wright a dessiné près de 800 projets, dont environ la moitié a été réalisée. Plusieurs archives de Frank Lloyd Wright sont conservées à l'Art Institute of Chicago. Une fondation à son nom existe également ; elle est installée à Taliesin West.
Il fait partie des personnalités dont John Dos Passos a écrit une courte biographie, au sein de sa trilogie U.S.A..
Autres réalisations importantes :
Projets importants :
Vous lisez un « article de qualité » labellisé en 2017.
Pour les articles homonymes, voir Jean-Baptiste, Poquelin et Molière (homonymie).
Œuvres principalesmodifier Jean-Baptiste Poquelin, dit Molière, baptisé le 15 janvier 1622 à l'église Saint-Eustache de Paris et mort le soir du 17 février 1673 à son domicile de la rue de Richelieu, est le plus célèbre des comédiens et dramaturges de langue française.
Issu d'une famille de marchands parisiens, il s'associe à 21 ans avec une dizaine de camarades, dont trois membres de la famille Béjart, pour former la troupe de l'Illustre Théâtre, laquelle, en dépit de débuts prometteurs et malgré la collaboration de dramaturges de renom, ne parvient pas à s'imposer durablement à Paris. Engagés à Pâques 1646 dans une prestigieuse « troupe de campagne » entretenue par le duc d'Épernon, gouverneur de Guyenne, puis par plusieurs protecteurs successifs, Molière et ses amis Béjart parcourent pendant douze ans les provinces méridionales du royaume. Au cours de cette période, Molière compose quelques farces ou petites comédies en prose et ses deux premières comédies en cinq actes et en vers. De retour à Paris en 1658, il devient vite, à la tête de sa troupe, le comédien et auteur favori du jeune Louis XIV et de sa cour, pour lesquels il conçoit de nombreux spectacles, en collaboration avec les meilleurs architectes scéniques, chorégraphes et musiciens du temps. Il meurt brutalement, à l’âge de 51 ans.
Grand créateur de formes dramatiques, interprète du rôle principal de la plupart de ses pièces, Molière a exploité les diverses ressources du comique — verbal, gestuel et visuel, de situation — et pratiqué tous les genres de comédie, de la farce à la comédie de caractère. Il a créé des personnages individualisés, à la psychologie complexe, qui sont rapidement devenus des archétypes. Observateur lucide et pénétrant, il peint les mœurs et les comportements de ses contemporains, n'épargnant guère que les ecclésiastiques et les hauts dignitaires de la monarchie, pour le plus grand plaisir de son public, tant à la cour qu'à la ville. Loin de se limiter à des divertissements anodins, ses grandes comédies remettent en cause des principes d'organisation sociale bien établis, suscitant de retentissantes polémiques et l'hostilité durable des milieux dévots.
L'œuvre de Molière, une trentaine de comédies en vers ou en prose, accompagnées ou non d'entrées de ballet et de musique, constitue un des piliers de l'enseignement littéraire en France. Elle continue de remporter un vif succès en France et dans le monde entier, et reste l'une des références de la littérature universelle[3],[4].
Sa vie mouvementée et sa forte personnalité ont inspiré dramaturges et cinéastes. Signe de la place emblématique qu’il occupe dans la culture française et francophone, le français est couramment désigné par la périphrase .
Fils de Jean Poquelin (1595-1669) et de Marie Cressé (1601-1632), Jean-Baptiste Poquelin[n 2] est né dans les premiers jours de 1622, ce qui fait de lui, à quelques années près, le contemporain de Cyrano de Bergerac, de Furetière, de Tallemant des Réaux, de Colbert, de D'Artagnan, de Ninon de Lenclos, de La Fontaine, du Grand Condé et de Pascal. Le 15 janvier, il est tenu sur les fonts baptismaux[n 3] de l'église Saint-Eustache par son grand-père Jean Poquelin († 1626) et Denise Lecacheux, son arrière-grand-mère maternelle[n 4].
Les Poquelin de Paris, nombreux à l'époque, sont originaires de Beauvais et du Beauvaisis[5],[6]. Les parents du futur Molière habitent, dans le quartier très populeux des Halles, la maison dite du « Pavillon des singes[n 5] », à l'angle oriental de la rue des Vieilles-Étuves (actuelle rue Sauval) et de la rue Saint-Honoré[n 6], où son père, Jean, marchand tapissier, a installé son fonds de commerce deux ans plus tôt, avant d’épouser Marie Cressé[7]. Les fenêtres donnent sur la placette dite carrefour de la Croix-du-Trahoir, qui depuis le haut Moyen Âge est l'un des principaux lieux patibulaires de la capitale[n 7].
Les deux grands-pères de Jean-Baptiste tiennent eux aussi commerce de meubles et de tapisseries, à quelques pas de là, dans la rue de la Lingerie. Poquelin et Cressé sont des bourgeois cossus, comme en témoignent les inventaires après décès[n 8]. Du côté maternel, un de ses oncles, Michel Mazuel (de), collabore à la musique des ballets de cour et est nommé en 1654 compositeur de la musique des Vingt-Quatre Violons du Roi. Il jouera d'ailleurs les comédies ballets de son neveu[8].
En 1631, Jean Poquelin père rachète à son frère cadet, Nicolas[n 9], un office de , dont cinq ans plus tard il obtiendra la survivance pour son fils aîné. La même année, il perd sa femme, sans doute épuisée par six grossesses survenues entre janvier 1622 et mai 1628[9], et se remarie avec Catherine Fleurette, qui meurt à son tour en 1636, après lui avoir donné trois autres enfants[10].
Sur les études du futur Molière, il n’existe aucun document fiable. Les témoignages sont tardifs et contradictoires. Selon les auteurs de la préface des Œuvres de Monsieur de Molière (1682)[n 11], le jeune Poquelin aurait fait ses humanités et sa philosophie au prestigieux collège jésuite de Clermont (l'actuel lycée Louis-le-Grand), où il aurait eu . Dans sa Vie de M. de Molière publiée en 1705, Grimarest lui donne pour condisciples deux personnages qui seront plus tard ses amis avérés, le philosophe, médecin et voyageur François Bernier et le poète libertin Chapelle[n 13]. Ce dernier avait pour précepteur occasionnel Pierre Gassendi, redécouvreur d'Épicure et du matérialisme antique, lequel, écrit Grimarest, , l'aurait admis à ses leçons avec Chapelle, Bernier et Cyrano de Bergerac[n 14].
Toutefois, la présence même de Jean-Baptiste Poquelin au collège de Clermont est sujette à caution. Ainsi François Rey fait-il remarquer que . Certains, notant que , en viennent à douter même que Molière ait fait des études régulières, sans toutefois exclure la possibilité qu'il ait été l'élève de Gassendi entre 1641 et 1643[13].
À sa sortie du collège, s'il faut en croire un contemporain[n 15], le jeune homme serait devenu avocat. Les avis sur ce point sont partagés, mais, quoi qu'il en soit, Molière ne s’est jamais paré du titre d'avocat et son nom ne figure ni dans les registres de l'université d'Orléans où il était possible d'étudier mais aussi d'acheter sa licence de droit, ni dans ceux du barreau de Paris[14]. Toujours est-il que « de nombreux passages de ses comédies supposent de sa part une connaissance précise des règlements et des procédures de justice[15] ».
Au tournant de l'année 1643, Jean-Baptiste Poquelin, d'ores et déjà émancipé d'âge[n 16] et qui a renoncé à la survivance de la charge de son père, reçoit de celui-ci un important acompte sur l’héritage maternel. Il a quitté la maison de la rue Saint-Honoré et demeure à présent rue de Thorigny, dans le quartier du Marais, non loin des Béjart[16].
Le 30 juin, par-devant notaire, il s’associe avec neuf camarades, dont les trois aînés de la fratrie Béjart (Joseph, Madeleine et Geneviève), pour constituer une troupe de comédiens sous le nom de l'Illustre Théâtre[17]. Ce sera la troisième troupe permanente à Paris, avec celle des « grands comédiens » de l’hôtel de Bourgogne et celle des « petits comédiens » du Marais[18].
Tout, à commencer par les termes mêmes du contrat d'association, suggère que le jeune Poquelin s'est engagé dans le théâtre pour y tenir les rôles de héros tragiques aux côtés de Madeleine Béjart, de quatre ans son aînée[19].
À la mi-septembre, les nouveaux comédiens louent le jeu de paume dit des Métayers[21] sur la rive gauche de la Seine, au faubourg Saint-Germain. En attendant la fin des travaux d'aménagement de la salle, ils se rendent à Rouen, afin de s'y produire pendant la foire Saint-Romain, qui se tient du 23 octobre au 12 novembre. Rouen est la ville où réside alors Pierre Corneille, mais aucun document ne permet d'affirmer, comme le font les épigones de Pierre Louÿs, que Molière a mis à profit ce séjour pour nouer des relations avec l'auteur du Cid et du Menteur.
La salle des Métayers ouvre ses portes le 1er janvier 1644. Pendant les huit premiers mois de représentations, le succès de la nouvelle troupe est d'autant plus grand que, le jeu de paume du Marais ayant brûlé le 15 janvier, ses locataires ont dû partir jouer en province pendant sa reconstruction[22].
En octobre 1644, le théâtre du Marais, refait à neuf et doté d'une salle équipée à présent de , accueille de nouveau le public, et il semble que la salle des Métayers commence alors à se vider. Cela pourrait expliquer la décision, prise en décembre, de déménager sur la rive droite au jeu de paume de la Croix-Noire[23] (actuel 32, quai des Célestins), plus près des autres théâtres. Molière est seul à signer le désistement du bail, ce qui pourrait indiquer qu'il est devenu le chef de la troupe[24]. Cependant, ce déménagement vient accroître les dettes de la troupe — les investissements initiaux de location et d'aménagement du local, puis d'aménagement d'un nouveau local, ont été coûteux et les engagements financiers pèsent lourd par rapport aux recettes — et, dès le 1er avril 1645, les créanciers entament des poursuites[16].
Au début du mois d'août, Molière est emprisonné pour dettes au Châtelet[25], mais peut se tirer d’affaire grâce à l'aide de son père. À l’automne, il quitte Paris[n 17].
C'est au cours du premier semestre de 1644 que Jean-Baptiste Poquelin prend pour la première fois ce qui deviendra son nom de scène puis d'auteur. Le 28 juin, il signe  (sans accent)[n 18] un document notarié dans lequel il est désigné sous le nom de . Selon Grimarest, .
Certains auteurs voient dans ce choix un hommage au musicien et danseur Louis de Mollier (vers 1615-1688), auteur en 1640 d'un recueil de Chansons pour danser. Selon Paul Lacroix, par exemple, on peut avancer  ; Elizabeth Maxfield-Miller considère, quant à elle, comme « très plausible » l'hypothèse que .
D'autres font remarquer que le patronyme Molière avait été illustré, plus tôt dans le siècle, par l'écrivain François de Molière d'Essertines, proche des milieux libertins et auteur d'un roman-fleuve dans le goût de L'Astrée intitulé La Polyxene de Moliere[n 19], dont une quatrième réédition vient de paraître en cette année 1644 où Jean-Baptiste Poquelin adopte son nom de scène[n 20].
Cependant l’hypothèse la plus probable, selon l’historien Georges Forestier[30], est à chercher du côté des usages des comédiens professionnels. Il rappelle que la grande majorité des comédiens du XVIIe siècle choisissent des noms de scène se référant à des fiefs imaginaires, tous champêtres : Pierre le Messier, sieur de Bellerose, Guillaume Desgilberts, sieur de Montdory, Josias de Soulas, sieur de Floridor, Zacharie Jacob, sieur de Montfleury[31]. Pour en rester à la seule lettre B, on n’en finirait pas de dénombrer les Beaubourg, Beauchamps, Beauchâteau, Beauchesne, Beaufort, Beaulieu, Beaumont, Beaupré, Beauval, Bellefleur, Bellefond, Belleroche, Bellerose, Bellonde, Boisvert, sans oublier Edme Villequin, sieur de Brie, dont l’épouse dite Mlle de Brie fut l’une des vedettes de la troupe de Molière. Or, ajoute Forestier, dans les mois qui suivirent la constitution de L’Illustre Théâtre, tous les garçons prirent des noms de théâtre champêtres. Joseph Béjart devint le « sieur de La Borderie », Germain Clérin le « sieur de Villabé », Georges Pinel le « sieur de La Couture », Nicolas Bonenfant, le « sieur de Croisac », et Nicolas Mary s’était depuis longtemps fait connaître comme le « sieur Des Fontaines ». Et Molière signa « De Molière ». Or, des dizaines de lieux-dits ou de villages français se nomment Meulière ou Molière, et désignent des sites où se trouvaient des carrières de pierres à meule ; en Picardie, les « mollières » sont des terres marécageuses et incultes[32]. Il est donc très probable que Molière ait choisi à son tour un fief campagnard imaginaire, ce qui expliquerait qu'il ait commencé par signer  et ait été régulièrement désigné comme .
À l'automne 1645, Molière et ses compagnons du "Théâtre Illustre" tentent une tournée dans l'ouest de la France, mais elle ne semble pas avoir été fructueuse et on les retrouve empêtrés dans les procès en décembre. Heureusement, Molière et ses amis Béjart (Joseph, Madeleine et Geneviève, bientôt rejoints par leur mère qui amène le petit Louis, âgé de 16 ans) sont engagés durant le relâche de Pâques 1646 par la plus réputée des "troupes de campagne", la troupe du duc d'Épernon, gouverneur de Guyenne, et dirigée par Charles Dufrenne. En avril 1646, il quitte Paris avec cette troupe[34]. Il passe les douze années suivantes à parcourir les provinces du royaume, principalement la Guyenne, le Languedoc, la vallée du Rhône, le Dauphiné, la Bourgogne, avec des séjours réguliers à Lyon, parfois longs de plusieurs mois. Même si une chronologie complète n'a pas pu être établie, on a repéré la présence de la troupe à Agen, Toulouse, Albi, Carcassonne, Poitiers, Grenoble, Pézenas, Montpellier, Vienne, Dijon, Bordeaux, Narbonne, Béziers et Avignon (voir carte ci-contre)[35].
À cette époque, des troupes itinérantes — on en compte une petite quinzaine[36] — sillonnent les routes de France, menant le plus souvent une vie précaire, dont Scarron a brossé un tableau haut en couleur dans son Roman comique en 1651[37]. En dépit de la célèbre déclaration formulée le 16 avril 1641 par Louis XIII à l'initiative de Richelieu, déclaration qui levait l'infamie pesant sur les comédiens[38], l’Église continue, dans de nombreuses villes, petites ou grandes, de s'opposer aux représentations théâtrales. Quelques troupes cependant jouissent d’un statut privilégié, qu'elles doivent à la protection d'un grand seigneur amateur de fêtes et de spectacles. C’est le cas de celle que dirige alors le comédien Charles Dufresne et qui est entretenue depuis vingt ans par les puissants ducs d’Épernon, gouverneurs de Guyenne[39].
C'est cette troupe qui, au cours de l'année 1646, recueille les Béjart et Molière, lequel sera progressivement amené à en prendre la direction. Dès 1647, elle est appelée à jouer pour le comte d’Aubijoux, lieutenant-général du roi pour le Haut-Languedoc, , qui lui assure une , l'invitant à se produire à Pézenas, Béziers, Montpellier.
Durant l'été 1653, le prince de Conti, qui, après avoir été l'un des principaux chefs de la Fronde, capitule à Bordeaux et se rallie au pouvoir royal, quitte Bordeaux pour venir s'installer avec sa cour dans son château de la Grange des Prés à Pézenas. Il est à présent le troisième personnage du royaume. En septembre, la troupe de Dufresne-Molière est invitée à y donner la comédie devant le prince et sa maîtresse[n 22]. Ce sera le début d'une étroite relation intellectuelle entre le prince et le comédien, dont Joseph de Voisin, confesseur de Conti, témoignera quinze ans plus tard :
« Monseigneur le prince de Conti avait eu en sa jeunesse tant de passion pour la comédie qu’il entretint longtemps à sa suite une troupe de comédiens, afin de goûter avec plus de douceur le plaisir de ce divertissement ; et ne se contentant pas de voir les représentations du théâtre, il conférait souvent avec le chef de leur troupe, qui est le plus habile comédien de France, de ce que leur art a de plus excellent et de plus charmant. Et lisant souvent avec lui les plus beaux endroits et les plus délicats des comédies tant anciennes que modernes, il prenait plaisir à les lui faire exprimer naïvement, de sorte qu’il y avait peu de personnes qui pussent mieux juger d’une pièce de théâtre que ce prince[41]. »
Molière et ses camarades pourront dès lors se prévaloir, dans tous les lieux où ils joueront, de la protection et des largesses de « Son Altesse Sérénissime le prince de Conti ». Le musicien et poète d’Assoucy, qui passe plusieurs mois avec eux en 1655, décrit une troupe accueillante où l’on fait bonne chère et qui jouit d’une large prospérité[42].
En 1653 ou 1655[n 23], alors qu'elle séjourne à Lyon, la troupe crée L'Étourdi ou les Contretemps, première « grande comédie[n 24] » de Molière, largement imitée d'une pièce italienne. Exploitant des procédés typiques de la commedia dell'arte, Molière donne au rôle de Mascarille, qu'il interprète, une exceptionnelle importance, le faisant paraître dans 35 des 41 scènes que compte la pièce[43] ; ce qui fait écrire à l'historienne Virginia Scott que Molière avait alors  — comme le montrent les portraits en habit de César peints par Sébastien Bourdon et les frères Mignard.
Au cours de cette période, Molière compose aussi un certain nombre de farces[n 26]. Citant l'une de ces petites pièces, Le Docteur amoureux, que la troupe devait jouer en octobre 1658 devant le roi, La Grange écrira[44] : « Cette comédie et quelques autres de cette nature n'ont point été imprimées : il les avait faites sur quelques idées plaisantes, sans y avoir mis la dernière main, et il trouva à propos de les supprimer lorsqu'il se fut proposé pour but, dans toutes ses pièces, d'obliger les hommes à se corriger de leurs défauts. Comme il y avait longtemps qu'on ne parlait plus de petites comédies, l'invention en parut nouvelle, et celle qui fut représentée ce jour-là divertit autant qu'elle surprit tout le monde. »
Ces farces obtiennent un vif succès, comme en témoigne le contemporain Donneau de Visé, qui souligne ce qu'elles doivent aux Italiens : « Molière fit des farces qui réussirent un peu plus que des farces et qui furent plus estimées dans toutes les villes que celles que les autres comédiens jouaient. Ensuite il voulut faire une comédie en cinq actes et les Italiens ne lui plaisant pas seulement dans leur jeu, mais encore dans leurs comédies, il en fit une qu'il tira de plusieurs des leurs, à laquelle il donna pour titre L'Étourdi ou Les Contretemps[45]. »
Grimarest met également l'accent sur l'inspiration italienne de ces farces :  Pour sa part, Henry Carrington Lancaster note que, si Molière a écrit de courtes farces, .
Adaptées à un public qui avait pour l'improvisation « un goût vif et naturel[47] », ces farces, dont la plupart ne nous sont pas parvenues, ont recours, selon des recherches récentes, aux . Divers spécialistes ont identifié dans les pièces de cette époque des modules dramatiques facilement réutilisables d'une pièce à une autre, dans lesquels la répétition de phrases ou de sections de phrase peut se prolonger de façon élastique — procédé typique du théâtre improvisé —[49]. En ce sens, Molière peut être vu, selon Claude Bourqui, comme , voire, selon un critique anglais, comme le . En même temps, loin d'être un imitateur servile, Molière a transcendé ce répertoire par la cohérence de sa vision et l'arrimage délibéré du ressort comique à des questions pertinentes pour ses contemporains, ainsi que l'avait noté La Grange, cité plus haut.
En 1656, le prince de Conti, , retire sa protection à la troupe et lui interdit de porter plus longtemps son nom[n 28]. Au cours du mois de décembre 1656, Molière fait représenter à Béziers sa deuxième « grande comédie », Le Dépit amoureux, pour les États de Languedoc.
Dans les dernières semaines de l'automne 1657, la troupe séjourne à Avignon. Molière s'y lie d'amitié avec les frères Nicolas et Pierre Mignard, qui peignent plusieurs portraits de lui et un tableau le représentant en dieu Mars étreignant Vénus-Madeleine Béjart[53].
Au début de 1658, la troupe, qui est dès lors considérée comme la meilleure  du royaume, décide de gagner Paris pour tenter de s'y implanter[54]. Les comédiens commencent par se rendre à Rouen, d'où Molière et Madeleine Béjart peuvent faire aisément des allers et retours à la capitale, afin de trouver une salle et de s'assurer les appuis nécessaires[55].
Au début de l'automne 1658, Molière et ses camarades (Dufresne, Madeleine, Joseph, Geneviève et Louis Béjart, Edme et Catherine de Brie, Marquise Du Parc et son mari René, dit Gros-René) sont agréés par Philippe d'Orléans, dit « Monsieur », frère unique du roi, qui leur accorde sa protection. Le 24 octobre, ils se produisent au Louvre devant Louis XIV, Anne d'Autriche, Mazarin et les comédiens de l'hôtel de Bourgogne. Ils jouent successivement Nicomède de Corneille et une farce de Molière qui n'a pas été conservée, Le Docteur amoureux[58].
À la suite de cet « examen réussi », la salle de théâtre du Petit-Bourbon, vaste et bien équipée, est mise à leur disposition. Ils l'occuperont pendant deux ans, jouant en alternance avec Scaramouche et ses camarades de la troupe italienne. C'est sans doute durant cette période que Molière perfectionne son jeu en étudiant les techniques du grand acteur comique qu'était Tiberio Fiorilli[n 29].
La « Troupe de Monsieur » commence à représenter le 2 novembre. Outre de vieilles pièces, la troupe joue L'Étourdi et Le Dépit amoureux, qui sont fort bien accueillis[59]. Au cours du relâche de Pâques 1659, Dufresne prend sa retraite, laissant à Molière l'entière direction de la troupe. Entrent deux acteurs comiques, le célèbre « enfariné » Jodelet[n 30] et son frère L’Espy, ainsi que Philibert Gassot, sieur Du Croisy et Charles Varlet, sieur de La Grange. Ce dernier a laissé un registre personnel, conservé à la Comédie-Française, dans lequel il notait les pièces jouées, la recette et ce qu’il jugeait important de la vie de la troupe. Ce document permet de suivre dans le détail le répertoire joué par Molière à partir de 1659.
Le 18 novembre 1659, Molière fait représenter une nouvelle pièce, la « petite comédie » des Précieuses ridicules, dans laquelle il joue le rôle du valet Mascarille. Satire féroce du snobisme et du jargon de certains salons parisiens mis en vogue notamment par Madeleine de Scudéry[60], la pièce remporte un vif succès et crée un effet de mode. Selon le « nouvelliste » Jean Donneau de Visé, . Le sujet est copié et repris. Molière fait imprimer sa pièce à la hâte parce qu’on tente de la lui voler, ainsi qu'il s'en explique dans une préface qui ne manque pas de piquant[62]. C’est la première fois qu’il publie, il a désormais le statut d’auteur[n 31].
Plusieurs hauts personnages — ministres, financiers et autres « grands seigneurs », dont le prince de Condé, de retour d'exil — invitent la troupe à venir représenter Les Précieuses dans leurs hôtels. De retour de Saint-Jean-de-Luz, où il est allé épouser l'infante Marie-Thérèse d'Espagne, Louis XIV voit la pièce le 29 juillet 1660. Deux jours plus tard, il verra Sganarelle ou le Cocu imaginaire, « petite comédie » en vingt-trois scènes en vers, qui sera, jusqu'à la mort de Molière, la comédie la plus souvent représentée par la troupe[63]. Cette pièce suscite un tel intérêt qu'il s'en publie rapidement une édition pirate, due à Neuf-Villenaine, pseudonyme de Donneau de Visé[64]. Dans l'épître de cette édition, intitulée « À un ami », ce dernier écrit : « Ses pièces ont une si extraordinaire réussite, puisque l'on n'y voit rien de forcé, que tout y est naturel, que tout y tombe sous le sens, et qu'enfin les plus spirituels confessent que les passions produiraient en eux les mêmes effets qu'elles produisent en ceux qu'il introduit sur la scène[65]. »
La nouvelle troupe suscite dans le public parisien un véritable engouement, qu'elle doit moins aux tragédies qu'elle continue sans succès de mettre à l'affiche[n 32], qu'aux comédies de Molière, qui vont constituer peu à peu l'essentiel du répertoire[66].
Le 6 avril 1660, le frère cadet de Molière, Jean III Poquelin, meurt. La charge de tapissier et valet de chambre du roi revient de nouveau à l'aîné. Il la gardera jusqu'à sa mort. Elle impliquait qu'il se trouve chaque matin au lever du roi, un trimestre par an. Dans son acte d'inhumation, il sera dit [67]. Selon la préface de son œuvre parue en 1682, .
Le 11 octobre 1660, Antoine de Ratabon, surintendant des bâtiments du roi, donne l'ordre d'entamer les travaux de démolition du Petit-Bourbon, pour faire place à la future colonnade du Louvre. Une nouvelle salle, située dans le Palais-Royal, demeure de Philippe d'Orléans et Henriette d'Angleterre, est mise à la disposition de la Troupe de Monsieur, qui la partagera, là encore, avec les comédiens italiens[69].
La salle du Palais-Royal, entièrement rénovée, ouvre ses portes le 20 janvier 1661. Le 4 février, la troupe y crée une nouvelle pièce de Molière, la comédie héroïque Dom Garcie de Navarre, dans laquelle il tient le rôle-titre aux côtés de Madeleine Béjart. Mais elle ne donnera lieu qu'à sept représentations consécutives, et ce fiasco, qui marque la fin des espoirs de l'acteur Molière pour s'imposer dans le genre tragique — alors considéré comme  —, ramène définitivement l'auteur sur le terrain de la comédie[71]. Cette œuvre aujourd'hui délaissée n'en reste pas moins un moment charnière dans la carrière de Molière dramaturge. Jean de Beer écrit : 
Hostile à l'emphase qui prévalait alors dans l'interprétation de la tragédie, Molière était partisan d'une diction « naturelle »,  et ce souci du naturel se révèle aussi dans son style, qui cherche . Grimarest, qui enseignait lui-même la déclamation, fournira plus tard un autre élément susceptible d'expliquer l'échec que Molière rencontra dans les rôles sérieux :
« Dans les commencements qu'il monta sur le théâtre, [Molière] reconnut qu'il avait une volubilité de langue dont il n'était pas le maître et qui rendait son jeu désagréable ; et des efforts qu'il faisait pour se retenir dans la conversation, il s'en forma un hoquet qui lui demeura jusques à la fin. Mais il sauvait ce désagrément par toute la finesse avec laquelle on peut représenter. Il ne manquait aucun des accents et des gestes nécessaires pour toucher le spectateur […] Il est vrai qu'il n'était bon que pour représenter le comique. Il ne pouvait entrer dans le sérieux[n 33], et plusieurs personnes assurent qu'ayant voulu le tenter, il réussit si mal la première fois qu'il parut sur le théâtre qu'on ne le laissa pas achever. Depuis ce temps-là, dit-on, il ne s'attacha qu'au comique[75]. »
Le 24 juin 1661, la troupe crée L'École des maris, une petite comédie en trois actes. Le succès est tel que le surintendant des finances Nicolas Fouquet passe commande à Molière d'un spectacle pour la fête à laquelle il a convié le roi et sa cour pour le 17 août, dans le cadre somptueux de son château de Vaux-le-Vicomte[76].
C’est la première fois que Molière crée une pièce pour la cour. Connaissant le goût de Louis XIV pour les ballets, il crée un nouveau genre, la comédie-ballet, intégrant comédie, musique et danse : les entrées de ballet ont le même sujet que la pièce et sont placées au début et dans les entractes de la comédie[77]. Ce seront Les Fâcheux, pochade en trois actes et en vers, , s'il faut en croire son auteur. Le roi ayant observé qu’un fâcheux auquel Molière n’avait pas pensé méritait sa place dans la galerie, Molière modifie rapidement le contenu de sa pièce pour y ajouter la scène du chasseur importun (Acte II, scène 6)[78]. Pour concevoir et mettre au point le spectacle dans lequel s'insère sa comédie et qui intègre la musique et la danse, Molière a collaboré avec Jean-Baptiste Lully pour la musique, Pierre Beauchamp pour la danse et Giacomo Torelli pour la scénographie. À partir de septembre, le spectacle, donné au Palais-Royal avec  et en faisant , rencontre un public nombreux et lui aussi enthousiaste. La saison est une des meilleures de la troupe.
Cette première comédie-ballet (Molière en composera quatre ou cinq autres) soulève l'enthousiasme de La Fontaine, qui écrit à son ami Maucroix : 
Le 23 janvier 1662, Molière signe un contrat de mariage avec Armande Béjart, « âgée de vingt ans ou environ », qu'il épouse religieusement le 20 février. Dans les deux occasions, la jeune femme est dite fille de Joseph Béjart et Marie Hervé, et sœur de Madeleine Béjart, son aînée de vingt ans ou plus. Toutefois, certains contemporains voient en elle la fille de Madeleine. C'est ce qu'affirmera Nicolas Boileau en 1702, et c'est la thèse que Grimarest défendra trois ans plus tard dans sa Vie de M. de Molière, précisant même qu'Armande est une fille que Madeleine a eue avant de connaître le jeune Poquelin, de « Monsieur de Modène, gentilhomme d'Avignon[80] ». De fait, Esprit de Rémond de Modène[n 34] et la jeune Madeleine Béjart ont eu le 3 juillet 1638 une fille qui, huit jours plus tard à l'église Saint-Eustache, a reçu le prénom de Françoise[81], et ils seront, en 1665, respectivement parrain et marraine d'Esprit-Madeleine Poquelin, fille de Molière et d'Armande.
Les historiens s'accordent à voir la future « Mademoiselle Molière » (Armande Béjart) dans la jeune « Mlle Menou » qui, en 1653, jouait le rôle d'une néréide dans une représentation de l'Andromède de Corneille donnée à Lyon par Molière et ses camarades[82].
L’acte de baptême d'« Armande Grésinde Claire Élisabeth Béjart » aurait pu établir sa véritable filiation, mais il n'a pas été présenté lors de la signature du contrat de mariage, et il n'a jusqu'à présent pas été retrouvé.
L'incertitude née de la grande différence d'âge entre les deux « sœurs » Béjart sera exploitée par les ennemis de Molière, qui, à plusieurs reprises au cours de la décennie suivante, insinueront qu'Armande serait la propre fille de Molière et de son ancienne maîtresse[n 35]. Ainsi, dans une requête présentée à Louis XIV au plus fort de la « querelle de L'École des femmes » (voir ci-dessous), le comédien Montfleury, ridiculisé par Molière dans L'Impromptu de Versailles, accusera celui-ci [n 36].
Molière et Armande auront un premier fils, Louis, baptisé le 24 février 1664 avec pour parrain Louis XIV et pour marraine Henriette d'Angleterre, duchesse d'Orléans, mais cet enfant meurt à huit mois et demi. Ils auront ensuite une fille, Esprit-Madeleine, baptisée le 4 août 1665, morte en 1723 sans descendance ; une autre fille, Marie, morte peu après sa naissance à la fin de l'année 1668 et un deuxième fils, Pierre, baptisé le 1er octobre 1672 et mort le mois suivant.
Ce mariage a fait couler beaucoup d'encre. La jeune Armande, au dire de ses détracteurs, aimait se faire courtiser par une foule d'admirateurs, au grand dam d'un Molière fort jaloux et dont les rieurs se moquaient d'autant plus qu'il avait mis en scène des personnages de mari trompé :  Ce thème sera exploité dans la pièce Élomire hypocondre (1670)[83] et, plus encore, dans la biographie romancée La Fameuse Comédienne (1688), qui dresse de « la Molière » un portrait extrêmement négatif[n 37]. Grimarest, qui s'appuie sur les souvenirs de Baron et de nombreux témoignages, laisse entendre que le couple n'était pas heureux et présente Armande comme « une coquette outrée »[84]. Dans les moments difficiles, Molière se retirait dans la maison qu'il louait dans le village d'Auteuil depuis le milieu de la décennie 1660-1670. Toujours amoureux de sa femme, il l'aurait décrite sous les traits de Lucile dans Le Bourgeois gentilhomme (Acte III, scène 9)[85].
Le 26 décembre 1662, la troupe crée L'École des femmes, quatrième grande comédie de Molière, dans laquelle il bouscule les idées reçues sur le mariage et la condition des femmes. Le succès, éclatant, consacre Molière comme grand auteur. C'est de cette période, en particulier, que les historiens datent le début de ses relations avec Nicolas Boileau, qui fait paraître en septembre 1663 ses célèbres Stances à Molière dans lesquelles il défend vigoureusement la pièce : .
Cependant, quelques littérateurs en quête de notoriété — au premier rang desquels Jean Donneau de Visé et Charles Robinet, rédacteur de la Gazette dite de Renaudot, soutenus dans l'ombre par les frères Pierre et Thomas Corneille — pointent dans la pièce ce qu'ils feignent de considérer comme des indices d’immoralité, telle la fameuse scène du « le… » (Acte II, scène 5), et d’impiété, telle la prétendue parodie de sermon dans les recommandations d’Arnolphe à Agnès, et des commandements divins dans les Maximes du mariage ou les devoirs de la femme mariée, avec son exercice journalier (Acte III, scène 2)[86].
À cela s'ajoutent des comédies jouées par la troupe concurrente de l’hôtel de Bourgogne, qui mettent en cause la moralité de Molière et l’attaquent sur sa vie privée. La querelle de L’École des femmes va durer plus d’un an et nourrir les entretiens des salons parisiens[n 38].
Molière, qui semble avoir d'abord bien accueilli la publicité que lui attiraient ces critiques[87], réplique une première fois en juin 1663 au Palais-Royal par La Critique de l'École des femmes, dans laquelle un des personnages revient sur le scandale provoqué par la scène du « le… »[88]. Faisant valoir [89], il montre que l'art de la comédie est plus exigeant que celui de la tragédie :
« Uranie : La tragédie, sans doute, est quelque chose de beau quand elle est bien touchée ; mais la comédie a ses charmes, et je tiens que l’une n’est pas moins difficile à faire que l’autre.
— Dorante : Assurément, madame ; et quand, pour la difficulté, vous mettriez un plus du côté de la comédie, peut-être que vous ne vous abuseriez pas. Car enfin, je trouve qu’il est bien plus aisé de se guinder sur de grands sentiments, de braver en vers la fortune, accuser les destins, et dire des injures aux dieux, que d’entrer comme il faut dans le ridicule des hommes, et de rendre agréablement sur le théâtre les défauts de tout le monde[90]. »
En juin, Louis XIV fait dispenser ses premières « gratifications aux gens de lettres ». Molière, qui fait partie des bénéficiaires, compose et fait paraître à cette occasion un Remerciement au Roi[91] en vers libres. Sa gratification sera renouvelée tous les ans jusqu’à sa mort.
En octobre, il présente devant la cour L'Impromptu de Versailles, sorte de , dans laquelle il met en scène sa propre troupe en train de répéter et demande solennellement à ses ennemis de cesser de l'attaquer sur sa vie privée[92].
Le 29 janvier 1664, dans le salon de la reine-mère Anne d'Autriche au Louvre, Molière présente devant la famille royale une comédie-ballet, Le Mariage forcé, dans laquelle il reprend son personnage de Sganarelle et où Louis XIV danse, costumé en Égyptien[94],[95].
Du 30 avril au 14 mai 1664, la troupe de Monsieur est à Versailles pour les fêtes des Plaisirs de l'île enchantée, qui sont en quelque sorte l’inauguration des jardins de Versailles. C’est un véritable  et sa troupe contribue beaucoup aux réjouissances des trois premières journées[n 39]. Le deuxième jour, elle crée La Princesse d'Élide,  dont Molière, pressé par le temps, n'a pu versifier que le premier acte et une scène du deuxième[96].
Le soir du 12, alors qu'une partie des invités du roi a regagné Paris, la troupe crée une nouvelle comédie de Molière intitulée, semble-t-il, Le Tartuffe ou l'Hypocrite. Cette première version en trois actes est chaudement applaudie par le roi et ses invités. Le lendemain pourtant, ou le surlendemain, Louis XIV se laisse convaincre par son ancien précepteur, le tout nouvel archevêque de Paris Hardouin de Péréfixe, d'interdire les représentations publiques de la pièce — ce qui ne l'empêchera pas de la revoir quatre mois plus tard, en privé, avec une partie de la Cour, au château de Villers-Cotterêts, résidence de son frère Philippe d'Orléans.
Cette satire de la fausse dévotion, en plaçant la religion sous un jour comique sinon ridicule, scandalise les milieux dévots. La pièce de Molière prend en effet position sur une question éminemment politique, celle de la séparation de l'Église et de l'État : 
Quelques semaines après la première représentation, le curé Pierre Roullé, farouche adversaire du jansénisme, publie un opuscule intitulé Le Roy glorieux au monde, ou Louis XIV, le plus glorieux de tous les Roys du monde, dans lequel il traite Molière de . Molière se défend par un premier Placet présenté au Roi, à l'été 1664, dans lequel il cite les outrances de ce pamphlet comme contraires au jugement favorable qu'avait d'abord donné le roi et invoque pour sa défense le but moral de la comédie[n 40] :
« Le devoir de la comédie étant de corriger les hommes en les divertissant, j'ai cru que, dans l'emploi où je me trouve, je n'avais rien de mieux à faire que d'attaquer par des peintures ridicules les vices de mon siècle ; et comme l'hypocrisie sans doute en est un des plus en usage, des plus incommodes et des plus dangereux, j'avais eu, Sire, la pensée que je ne rendrais pas un petit service à tous les honnêtes gens de votre royaume, si je faisais une comédie qui décriât les hypocrites et mît en vue comme il faut toutes les grimaces étudiées de ces gens de bien à outrance, toutes les friponneries couvertes de ces faux-monnayeurs en dévotion, qui veulent attraper les hommes avec un zèle contrefait et une charité sophistique[99]. »
Louis XIV ayant confirmé l'interdiction de représenter la pièce en public, Molière entreprend de la remanier pour la rendre conforme à son argumentation. On sait, par une lettre du duc d’Enghien, qu'au début de l'automne 1665 il est en train d’ajouter un quatrième acte aux trois actes joués à Versailles l'année précédente.
À la fin de juillet 1667, Molière profite d’un passage du roi chez son frère et sa belle-sœur à Saint-Cloud pour obtenir l’autorisation de représenter une nouvelle version en cinq actes[100]. La pièce s’appelle désormais L’Imposteur et Tartuffe y est renommé Panulphe. Créé le 5 août au Palais-Royal devant une salle comble, le spectacle est immédiatement interdit sur ordre du premier président du Parlement, Guillaume de Lamoignon — chargé de la police en l’absence du roi —, interdiction redoublée le 11 août par l’archevêque de Paris, qui fait défense à ses diocésains, sous peine d’excommunication, de représenter, lire ou entendre la pièce incriminée[101]. Molière tente d'obtenir l'appui de Louis XIV en écrivant un Second placet, que La Grange et La Thorillière sont chargés d'aller présenter au roi, qui fait alors le siège de Lille. Cette démarche reste sans succès.
Pour que la pièce soit définitivement autorisée, sous le titre Le Tartuffe ou l'Imposteur, il faudra attendre encore un an et demi et la fin de la guerre contre les jansénistes, ce qui donne à Louis XIV les coudées franches en matière de politique religieuse. Cette autorisation intervient au moment exact de la conclusion définitive de la Paix clémentine, aboutissement de longues négociations entre, d’un côté, les représentants du roi et le nonce du pape Clément IX et, de l’autre, les représentants des « Messieurs » de Port-Royal et des évêques jansénistes. La coïncidence est frappante : l’accord étant conclu en septembre 1668, c’est le 1er janvier 1669 qu’une médaille commémorant la Paix de l’Église est frappée. Et c’est le 3 février, deux jours avant la première du Tartuffe, que le nonce du pape remet à Louis XIV deux  dans lesquels Clément IX se déclarait entièrement satisfait de la  et de l' des quatre évêques jansénistes[102].
Le Tartuffe définitif est ainsi créé le 5 février 1669. C’est le triomphe de Molière, sa pièce le plus longtemps jouée (72 représentations jusqu’à la fin de l’année) et son record de recettes[n 41].
Le dimanche 15 février 1665, la troupe de Monsieur crée Le Festin de Pierre ou l'Athée foudroyé, comédie de Molière qui constitue la troisième adaptation française de la légende de Don Juan. C'est un triomphe : la recette dépasse même celles de L'École des femmes, et les suivantes s'accroîtront encore durant les deux premières semaines du carême.
Donné quinze fois jusqu'au 20 mars, le spectacle n'est pas repris après le relâche de Pâques. Le texte de Molière ne sera édité qu'après sa mort et il faudra attendre cent cinquante ans pour qu'il soit rejoué sur une scène française.
Au cours du relâche de Pâques, un libraire spécialisé dans la publication de pièces de théâtre, et en particulier celles qui ont été créées à l'hôtel de Bourgogne, met en vente un libelle au titre presque anodin : Observations sur une comédie de Molière intitulée « Le Festin de Pierre », dans lequel un « sieur de Rochemont », dont on ignore aujourd'hui encore la véritable identité, s'en prend avec une extrême violence à Molière et à ses deux dernières pièces : Le Tartuffe et Le Festin de Pierre. Le succès de ce pamphlet est immédiat et presque aussi important que celui de la comédie qu'il dénonce[103].
Deux partisans de Molière prennent sa défense quelques mois plus tard : le premier n'a jamais été identifié ; le second serait Jean Donneau de Visé selon René Robert[104] et François Rey[105]. Ils seront rejoints en août par Charles Robinet, ancien détracteur de Molière et principal rédacteur de la Gazette dite de Renaudot[n 42].
Le roi fait taire les adversaires de Molière en prenant la troupe sous sa protection. Selon François Rey, l'événement aurait eu lieu le 14 juin[106], dans le cadre d'une grande fête donnée par Louis XIV à Versailles et où la troupe de Molière a été appelée à jouer Le Favori de Mlle Desjardins, qu'elle vient de créer au Palais-Royal[n 43]. Ce jour-là, écrira plus tard La Grange dans son Registre, en se trompant apparemment de date et de lieu, . Désormais, les trois troupes françaises de Paris sont directement sous l'autorité royale.
Contrairement à une idée reçue depuis le XXe siècle, on ne voit pas que Molière ait eu à souffrir des polémiques occasionnées par ses trois pièces réputées les plus audacieuses. Comédies-ballets créées à la cour et comédies unies créées à la ville ou à la cour[n 44] alternent avec un succès qui se dément rarement jusqu'à la mort de Molière en février 1673. Et les critiques qui ont cru que Le Misanthrope, créé en juin 1666, manifestait le désarroi de Molière face aux difficultés rencontrées par Le Tartuffe n'ont sans doute pas pris suffisamment en compte le témoignage, il est vrai tardif, de Nicolas Boileau, selon lequel Le Misanthrope aurait été entrepris dès le commencement de 1664, c'est-à-dire parallèlement au Tartuffe[n 45].
Certes, Molière dut patienter cinq ans avant que son Tartuffe reçoive enfin l'autorisation d'être représenté en public, et il lui fallut transformer sa pièce pour en gommer le côté trop manifeste de satire de la dévotion. L'Église et les dévots ne furent cependant pas dupes et continuèrent de juger la pièce dangereuse. Si Molière n'a jamais voulu renoncer à cette pièce, quoique interdite, c'est qu'il se savait soutenu par les personnages les plus puissants de la cour, à commencer par le roi lui-même, et qu'il était certain qu'une comédie qui ridiculisait les dévots attirerait la foule dans son théâtre[n 46].
Parallèlement, Molière put donner l'impression de s'orienter vers des sujets en apparence inoffensifs : c'est du moins ainsi que l'interprétèrent les critiques du XXe siècle[n 47]. En fait, Molière passa d'une satire à une autre, en apparence plus inoffensive et moins dangereuse : celle de la médecine et des médecins — dont plusieurs chercheurs ont montré les liens avec la satire anti-religieuse[108].
La troupe est d’une stabilité exemplaire. À Pâques 1670, elle compte encore trois acteurs du temps de l’Illustre Théâtre : Molière, Madeleine Béjart et sa sœur Geneviève. Sept en faisaient partie lors des débuts à Paris (les mêmes plus Louis Béjart et le couple De Brie). Neuf y jouent depuis le remaniement de 1659 (les mêmes, plus La Grange et Du Croisy).
Les nouveaux sont La Thorillière (1662), Armande Béjart (1663) et André Hubert (1664). Un seul départ volontaire : celui de Marquise Du Parc, qui, à Pâques 1667, passe à l'hôtel de Bourgogne, où elle créera le rôle-titre de l’Andromaque de Racine. Un seul départ à la retraite : celui de L'Espy, frère de Jodelet. En 1670, Louis Béjart demande à son tour à quitter le métier ; il a 40 ans. Les comédiens s’engagent à lui verser une pension de 1 000 livres aussi longtemps que la troupe subsiste.
En avril 1670, le jeune Michel Baron, alors âgé de 17 ans, entre dans la troupe. Molière tenait tellement à l’y avoir qu'il avait obtenu une lettre de cachet du roi pour l’enlever, malgré son contrat, à la troupe de campagne dont il faisait partie[n 48]. Ce dernier a une part et le couple Beauval, comédiens chevronnés, une part et demie. La compagnie compte désormais huit comédiens et cinq comédiennes, pour douze parts et demie.
Madeleine Béjart meurt le 17 février 1672, un an jour pour jour avant Molière. Elle est inhumée sans difficulté sous les charniers de l'église Saint-Paul. Elle a en effet reçu les derniers sacrements, après avoir signé (sous la contrainte) l'acte de renonciation solennelle à la profession de comédienne. Elle jouissait d’une large aisance. Son testament favorise grandement sa sœur (ou sa fille) Armande.
Pour les comédiens de Molière, c’est la prospérité. Pour les cinq dernières saisons (1668-1673), le bénéfice total annuel de la troupe — revenus du théâtre, gratifications pour les représentations privées données à des particuliers, gratifications du roi et pension du roi — s'élève en moyenne à 54 233 livres[n 49], contre 39 621 livres les cinq saisons précédentes, à répartir en douze parts environ[n 50].
Molière est riche. Roger Duchêne a calculé que, pour la saison 1671-1672, sa femme et lui ont reçu 8 466 livres à eux deux pour leurs parts de comédiens, plus ce que Molière a eu de la troupe comme auteur et ce que les libraires lui ont versé pour la publication de ses pièces. Il s’y ajoute les rentes des prêts qu’il a consentis et les revenus qu’Armande tire de l’héritage de Madeleine, soit au total plus de 15 000 livres, l’équivalent, ajoute Duchêne, du montant de la pension que verse Louis XIV au comte de Grignan pour exercer sa charge de lieutenant général au gouvernement de la Provence[109].
Durant les quatorze saisons de son activité parisienne, entre 1659 et 1673, la troupe a joué quatre-vingt-quinze pièces pour un total de 2 421 représentations, publiques ou privées[110].
Saison 1665-1666 : le 14 septembre 1665, la Troupe du Roi crée devant la cour réunie à Versailles L'Amour médecin, comédie-ballet en trois actes et en prose. Ce  a été, écrit Molière dans sa préface, .
Décembre 1665 : très longue interruption des représentations de la troupe. Le bruit court que Molière est malade[115].
Saison 1666-1667 : en mars, paraît la première véritable édition de ses Œuvres complètes en deux volumes et à pagination continue[116]. Elle contient neuf pièces et est imprimée et mise en vente par un cartel de huit libraires, avec des « lettres [de privilège] obtenues par surprise », ce qui amènera Molière à confier la publication de sa comédie suivante, Le Misanthrope, à un libraire, Jean Ribou, qui en 1660 avait piraté Les Précieuses ridicules et Sganarelle ou le Cocu imaginaire[117].
Le 4 juin 1666, il donne la première représentation publique du Misanthrope, sa 16e pièce, dans laquelle il joue le rôle d'Alceste. Cette « grande comédie » est une pièce . Au lieu de montrer un amoureux dont les desseins sont contrariés par un rival ou un père intransigeant, le protagoniste y est son propre adversaire. La pièce sera jouée 299 fois jusqu’à la fin du règne de Louis XIV (1715)[119].
Le 6 août, au Palais-Royal, Molière crée Le Médecin malgré lui, qu'il appelle une « petite bagatelle ». Selon son contemporain Subligny : 
Le 1er décembre 1666, la troupe part à Saint-Germain pour de grandes fêtes données par le roi, qui mobilisent toutes les troupes de Paris et dureront jusqu’au 27 février 1667[121]. Elle joue dans le Ballet des Muses et donne trois comédies (Pastorale comique, Mélicerte et Le Sicilien). Le poète de la cour Benserade écrit à cette occasion : 
Mars-décembre 1667 : maladie de Molière[n 51].
Le 10 juin, première du Sicilien à Paris. La recette est la plus faible jamais réalisée par la création d'une pièce de Molière[123]. Celle-ci est toutefois considérée comme la meilleure partition musicale de Lully, grâce à .
Le 5 août, création de L'Imposteur, réécriture du Tartuffe, interdit immédiatement. Ordonnance de Péréfixe qui menace d'excommunication toute personne qui verrait, lirait ou écouterait cette pièce[125]. Molière se retire de la scène pendant plusieurs mois.
Saison 1667-1668 : le 13 janvier 1668, Amphitryon, comédie en trois actes et en vers libres adaptée de Plaute, est créé au Palais-Royal[126]. Le roi et la cour assistent à la 3e représentation aux Tuileries.
Outre son appartement parisien, Molière loue une maison à Auteuil, où il se retire pour lire et se reposer, et où il invite ses amis, notamment Chapelle[127].
Saison 1668-1669 : c’est une saison faste. Pour célébrer la paix d’Aix-la-Chapelle (mai 1668), le roi donne à sa cour des fêtes grandioses. Plus de deux mille personnes assistent au Grand Divertissement royal, pastorale avec chants et danse. La musique est de Lully, le texte de Molière. La comédie George Dandin est enchâssée dans la pastorale[128].
L’Avare, comédie en cinq actes et en prose, est créé le 9 septembre au Palais-Royal. Après Amphitryon créé en janvier, c’est la deuxième pièce adaptée de Plaute en une année. Molière la jouera 47 fois dans son théâtre. Les recettes, assez modestes, montrent à l'évidence que le public ne s'est pas passionné pour la pièce[126], alors que celle-ci deviendra l’un de ses plus grands succès. L'Avare est parfois caractérisé, à l'instar du Misanthrope et des Femmes savantes, comme une « comédie sérieuse », Harpagon n’étant pas un personnage entièrement comique. Le triomphe du Tartuffe, enfin joué librement le 5 février 1669, va faire oublier le relatif échec de L'Avare[129].
Saison 1669-1670 : la troupe a suivi la cour à Chambord du 17 septembre au 20 octobre 1669. C’est là qu’est joué Monsieur de Pourceaugnac, nouvelle comédie-ballet, où . La pièce est plus dure pour les médecins que Le Malade imaginaire, aussi âpre que L'Amour médecin. Reprise à Paris en novembre, elle y obtient un vif succès[131].
Pour le carnaval, un spectacle est commandé à Molière : ce seront Les Amants magnifiques, comédie en cinq actes et en prose, . Le spectacle donné à Saint-Germain, en février 1670, .
Saison 1670-1671 : Louis XIV, qui vient de recevoir à Versailles l'ambassadeur ottoman Soliman Aga[133], veut donner à sa cour une comédie-ballet où des Turcs apparaissent sur scène à leur désavantage. Molière compose le texte, Lully la musique : l'ensemble donne Le Bourgeois gentilhomme. Le texte et l'intrigue n'ont ici qu'une importance secondaire, l'accent étant mis sur le côté spectaculaire d'une pièce qui se termine dans une . Donnée sept fois devant la cour en octobre 1670, puis au Palais-Royal à partir du 23 novembre, la pièce est .
En janvier 1671, dans la grande salle des Tuileries[n 52], la Troupe du Roi crée devant la cour la tragédie-ballet de Psyché. Pressé par le temps, Molière a dû demander l'aide de Pierre Corneille et Philippe Quinault pour la versification. La musique est de Lully. La jeune Esprit Madeleine Poquelin joue le rôle d'une petite Grâce accompagnant Vénus[135].
Saison 1671-1672 : Les Fourberies de Scapin, créées le 24 mai 1671, sont un échec : 18 représentations seulement, avec des recettes de plus en plus faibles. À croire que le public a partagé l'opinion que Boileau exprimera deux ans plus tard dans son Art poétique :  La pièce connaîtra le succès après la mort de Molière : 197 représentations de 1673 à 1715[136].
En décembre 1671, le roi commande pour l’arrivée de la nouvelle épouse de Monsieur un ballet, La Comtesse d'Escarbagnas, joué plusieurs fois devant la cour[137].
Le 11 mars 1672, Les Femmes savantes, septième et dernière grande comédie en cinq actes et en vers de Molière, est créée au Palais-Royal. C'est un franc succès : 1 735 livres de recette[138]. Bussy-Rabutin estime que c'est . La pièce sera affichée sans discontinuer jusqu'au 15 mai, en deçà et au-delà du relâche de Pâques[n 53]. Le roi la verra deux fois, la première à Saint-Cloud, le 11 août, la seconde le 17 septembre 1672 à Versailles ; ce sera alors la dernière fois que Molière jouera à la cour[140].
Le 1er octobre 1672, Molière et sa famille s’installent rue de Richelieu, dans une vaste maison à deux étages avec entresol[n 54].
Le 10 février 1673, la troupe donne la première représentation du Malade imaginaire,  employant huit chanteurs et nombre de danseurs et musiciens. Loin d'être secondaires, les intermèdes musicaux occupent plus d'une heure dans la pièce et la musique de Charpentier, . C'est un succès :  La quatrième sera fatale à Molière.
Avec Monsieur de Pourceaugnac (1669), Le Bourgeois gentilhomme (1670) et Le Malade imaginaire (1673), Molière est parvenu, écrit Georges Forestier, . En même temps, comme le fait remarquer Ramon Fernandez, Monsieur de Pourceaugnac présente , comme c'était déjà le cas dans Amphitryon, George Dandin et L’Avare : Molière s'est désintéressé de la leçon morale de la comédie[145].
À partir de 1664[146], et pendant huit ans, Molière et Lully, surintendant de la musique royale, collaborent avec succès, Lully composant la musique des comédies de Molière pour les grandes fêtes royales. Comme Molière, il pensait jusqu’alors l’opéra en français impossible. Le succès de Pomone, premier opéra français, le fait changer d’avis. En mars 1672, Lully obtient du roi l’exclusivité des spectacles chantés et interdit aux troupes théâtrales de faire chanter une pièce entière sans sa permission[n 55]. La troupe de Molière proteste, une bonne partie de son répertoire étant constituée de comédies-ballets. Le 29 mars 1672, le roi lui accorde la permission d’employer 6 chanteurs et 12 instrumentistes, à peu près l’effectif utilisé par son théâtre[147].
Le 8 juillet 1672, La Comtesse d'Escarbagnas est donnée au Palais-Royal avec une musique nouvelle de Marc-Antoine Charpentier, récemment revenu de ses études à Rome[146]. En septembre, un nouveau privilège accorde à Lully la propriété des pièces dont il fera la musique[148]. Molière confie aussi à Marc-Antoine Charpentier les intermèdes musicaux de pièces anciennes qu'il reprend, tel Le Mariage forcé dont le trio burlesque « La, la, la, la, bonjour » est resté célèbre[149].
Le goût du roi va à l’opéra, au détriment de ce que pratique Molière, attaché à l’importance du texte parlé et à la primauté de l’écrivain sur le musicien[n 56]. Mais le roi aime aussi la comédie. Le succès du Bourgeois gentilhomme — pièce qui annonce à beaucoup d'égards Le Malade imaginaire — et le triomphe de Psyché avec une musique de Marc-Antoine Charpentier au Palais-Royal, le 11 novembre 1672[150], lui ont aussi confirmé que la troupe peut prospérer en jouant des pièces avec ballets et parties chantées pour le seul public parisien.
Depuis le XVIIIe et surtout le XIXe siècles, amateurs de Molière et historiens se sont interrogés sur la santé de cet auteur qui a été emporté par la maladie au sortir de la quatrième représentation du Malade imaginaire, le 17 février 1673, et ils ont reconstruit l'histoire de sa santé à partir de la fin. Découvrant que Molière était resté éloigné du théâtre à deux reprises en février 1666 et en avril 1667 et qu'on avait alors craint pour sa vie – le 28 février 1666, le protestant Élie Richard écrit à son cousin Élie Bouhéreau, qui habite Dublin :  et en avril 1667 le gazetier Charles Robinet écrit :  – ils en ont déduit que des rumeurs avaient commencé à courir sur sa santé dès 1665 et qu'il aurait rechuté en 1666, premières atteintes du mal qui allait le ronger puis l'emporter huit ans plus tard. En fait, les gazetiers, qui ont continué à signaler les maladies et les fièvres qui mettaient en danger les personnages les plus importants de Paris et de la Cour et qui avaient les yeux constamment fixés sur Molière, n'ont plus jamais signalé quelque maladie, quelque défaillance, quelque accès de fièvre et ont manifesté, comme tous les contemporains, une surprise extrême à l'annonce de sa mort (voir l'article Mort de Molière).
De la même manière, les historiens ont lu au sens littéral des passages contenus dans une comédie-pamphlet intitulée Élomire hypocondre (1670) :  Mais la lecture de l'ensemble du texte fait découvrir au contraire que c'est un Molière en pleine forme qui énonce ce symptôme et que sa femme se désespère de voir qu'il se croit malade alors qu'il a toute sa santé : l'intention de l'auteur d'Élomire hypocondre était de retourner contre Molière la satire anti-médicale contenue dans la plus récente comédie-ballet de Molière (Monsieur de Pourceaugnac) et de présenter Molière comme un hypocondriaque qui se croit malade et veut consulter des médecins et des guérisseurs qui se moquent de lui.
Dans la préface de l'édition posthume des Œuvres de Monsieur de Molière, attribuée à La Grange, un comédien entré dans la troupe en 1659 et qui y est resté jusqu'à la fin, ce dernier écrit : « Lorsqu'il commença les représentations de cette agréable comédie [Malade imaginaire], il était malade en effet d'une fluxion sur la poitrine qui l'incommodait beaucoup, et à laquelle il était sujet depuis quelques années. Il s’était joué lui-même sur cette incommodité dans la cinquième scène du second Acte de L’Avare, lorsqu'Harpagon dit à Frosine : « Je n’ai pas de grandes Incommodités Dieu merci, il n’y a que ma fluxion qui me prend de temps en temps ; » À quoi Frosine répond : « Votre fluxion ne vous sied point mal, et vous avez grâce à tousser. » Cependant, c'est cette toux qui a abrégé sa vie de plus de vingt ans[154]. »
Après la mort de Molière, aucune des très nombreuses épitaphes qui circuleront dans les semaines et les mois suivants ne laissera pourtant entendre que Molière était malade ; bien au contraire, beaucoup joueront avec le paradoxe que Molière, à jouer le malade et à feindre le mort en scène, a été rattrapé par la maladie et par la mort qui s'est ainsi vengée.
C'est à partir d'un roman biographique diffamatoire entièrement tourné contre Armande Béjart (La Fameuse Comédienne, anonyme, 1687) qu'est apparu le thème d'un Molière hanté par les infidélités de sa femme et progressivement rongé par la jalousie au point de s'en rendre de plus en plus malade. La même idée sera reprise par son premier biographe, Grimarest (La Vie de Monsieur de Molière, 1705), ouvrage qui pourrait ensuite avoir influencé divers recueils de souvenirs sur le grand comédien. Ainsi lit-on, sous la plume de Jacques de Losme de Montchesnay (1666-1740), confident de Boileau, l'anecdote selon laquelle cet ami de Molière lui aurait conseillé de quitter le théâtre, du moins comme acteur : « Deux mois avant la mort de Molière, M. Despréaux alla le voir et le trouva fort incommodé de sa toux et faisant des efforts de poitrine qui semblaient le menacer d'une fin prochaine. Molière, assez froid naturellement, fit plus d'amitié que jamais à M. Despréaux. Cela l'engagea à lui dire : Mon pauvre M. Molière, vous voilà dans un pitoyable état. La contention continuelle de votre esprit, l'agitation continuelle de vos poumons sur votre théâtre, tout enfin devrait vous déterminer à renoncer à la représentation. À quoi le comédien aurait répondu : . »
 La maladie devait toutefois progresser et se transformer en bronchite chronique pour finalement dégénérer en pneumonie ou en pleurésie[156].
C'est à partir de ces divers témoignages — considérés comme de simples  par plusieurs spécialistes —, que l'histoire de la création de sa dernière comédie a été reconstituée. Il est en effet frappant qu'en 1673, Molière crée au Palais-Royal une comédie mêlée de musique (de Marc-Antoine Charpentier) et de danses, Le Malade imaginaire, sa trentième pièce, dans laquelle il joue le personnage d'Argan, qui doit feindre d'être mort et dont une des répliques est précisément :  Beaucoup de critiques ont dès lors estimé que le choix d'un tel sujet ne saurait être attribué à une pure coïncidence. Patrick Dandrey y voit . D'autres critiques ont reconstitué tout le parcours de Molière à partir de cette dernière pièce, tel Gérard Defaux, selon qui Molière était certainement conscient qu'il allait livrer sa dernière pièce :« À considérer [cette pièce] dans une perspective aussi globale que possible, celle de l'œuvre entière, de sa cohérence interne, de son déroulement parfaitement maîtrisé, de son dynamisme et de sa croissance pour ainsi dire organiques, l'impression s'impose très vite que Molière a composé sa dernière comédie en sachant qu'elle serait la dernière, qu'il allait bientôt mourir et que ses jours étaient comptés. Non seulement parce que la maladie, imaginaire ou non, en fournit le sujet, et que, même en apparence surmontée, l'angoisse de la mort y est bien évidemment partout présente. Mais aussi, et surtout, parce que cette comédie constitue une véritable somme de sa pensée et de son art, en quelque sorte son testament comique[159]. »
Le 17 février 1673, un an jour pour jour après la mort de Madeleine Béjart, la Troupe du Roy donne la quatrième représentation du Malade imaginaire. Molière, qui y tient le rôle d'Argan, se sent plus fatigué qu'à l'ordinaire par sa « fluxion de poitrine », mais il refuse d'annuler la représentation. Selon le témoignage de La Grange (ci-contre)[160], la mort serait survenue sur les dix heures du soir au 40, rue de Richelieu, ce que confirme la requête qu'Armande Béjart, veuve du défunt, a fait parvenir à l’archevêque de Paris, et dans laquelle elle fournit divers détails omis par Grimarest, notamment les allées et venues qui ont duré plus d’une heure et demie pour trouver un prêtre[161]. Cette requête est le témoignage le plus fiable avec celui de La Grange.
L'idée selon laquelle il fut pris d'un malaise sur scène et qu'il était  n'apparaît que dans des récits romancés postérieurs qui s'accordent seulement sur le fait qu'il mourut quelques heures plus tard[n 57].
Se fondant sur les souvenirs très peu fiables (si l'on en croit ses contemporains) de l'acteur Michel Baron, Grimarest a donné un récit circonstancié de cette fin, entièrement centré sur le seul Baron, qui sera repris sous des formes plus ou moins épurées[162] par les historiens des XVIIIe et XIXe siècles, alors même qu'il est par avance contredit par le texte de la requête présentée par Armande Béjart à l'archevêque de Paris au lendemain de la mort de Molière :
« Les comédiens tinrent les lustres allumés, et la toile levée, précisément à quatre heures. Molière représenta avec beaucoup de difficulté, et la moitié des spectateurs s'aperçurent qu'en prononçant juro dans la cérémonie du Malade imaginaire, il lui prit une convulsion. Ayant remarqué lui-même que l'on s'en était aperçu, il se fit un effort et cacha par un ris forcé ce qui venait de lui arriver. Quand la pièce fut finie, il prit sa robe de chambre et fut dans la loge de Baron, et il lui demanda ce que l’on disait de sa pièce. […]. Baron après lui avoir touché les mains qu'il trouva glacées les lui mit dans son manchon pour les réchauffer. Il envoya chercher ses porteurs pour le porter promptement chez lui. […] Quand il fut dans sa chambre, Baron voulut lui faire prendre du bouillon, dont la Molière avait toujours provision pour elle, car on ne pouvait avoir plus de soin de sa personne qu'elle en avait. « Eh ! non, dit-il, les bouillons de ma femme sont de vraie eau forte pour moi ; vous savez tous les ingrédients qu'elle y fait mettre : donnez-moi plutôt un petit morceau de fromage de Parmesan. » La Forest lui en apporta ; il en mangea avec un peu de pain, et il se fit mettre au lit. Il n'y eut pas été un moment qu'il envoya demander à sa femme un oreiller rempli d'une drogue qu'elle lui avait promis pour dormir. « Tout ce qui n'entre point dans le corps, dit-il, je l'éprouve volontiers ; mais les remèdes qu'il faut prendre me font peur ; il ne faut rien pour me faire perdre ce qui me reste de vie. » Un instant après, il lui prit une toux extrêmement forte, et après avoir craché il demanda de la lumière. « Voici dit-il du changement ! » Baron, ayant vu le sang qu’il venait de rendre s'écria avec frayeur. « Ne vous épouvantez point, lui dit Molière, vous m'en avez vu rendre bien davantage. Cependant, ajouta-t-il, allez dire à ma femme qu'elle monte. » Il resta, assisté de deux sœurs religieuses, de celles qui viennent ordinairement à Paris quêter durant le carême, et auxquelles il donnait l'hospitalité. Elles lui donnèrent à ce dernier moment de sa vie tout le secours édifiant que l'on pouvait attendre de leur charité […] Enfin il rendit l'esprit entre les bras de ces deux bonnes sœurs. Le sang qui sortait par sa bouche en abondance l'étouffa. Ainsi, quand sa femme et Baron remontèrent, ils le trouvèrent mort[163]. »
Molière n’ayant pas signé de renonciation à sa profession de comédien, il ne peut recevoir une sépulture religieuse, car le rituel du diocèse de Paris subordonne l’administration des sacrements à cette renonciation faite par écrit ou devant un prêtre[165]. L’Église est embarrassée. Le curé de Saint-Eustache ne peut, sans faire scandale, l’enterrer en faisant comme s’il n’avait pas été comédien. Et, de l’autre côté, refuser une sépulture chrétienne à un homme aussi connu risquait de choquer le public. La solution était de s’adresser à l’archevêque de Paris, ce que fait Armande le 18 février dans sa requête, où elle affirme que des trois prêtres de la paroisse de Saint-Eustache auxquels elle avait fait appel pour porter l'extrême-onction à Molière, deux avaient refusé de venir et le troisième était arrivé trop tard[161]. Pour plus de sûreté, elle va se jeter aux pieds du roi, qui la  tout en faisant écrire à l'archevêque . Ce dernier, après enquête,  recueillies, permet au curé de Saint-Eustache d’enterrer Molière, à condition que cela soit . Molière est donc enterré de nuit le 21 février dans le cimetière de la chapelle Saint-Joseph[166].
Le récit de la cérémonie est fait par un témoin anonyme sur un pli adressé à un prêtre de l'église Saint-Joseph :
« Mardi 21 février 1673, sur les neuf heures du soir, l'on a fait le convoi de Jean-Baptiste Poquelin Molière, tapissier, valet de chambre, illustre comédien, sans autre pompe sinon de trois ecclésiastiques ; quatre prêtres ont porté le corps dans une bière de bois couverte du poêle des tapissiers ; six enfants bleus[n 58] portant six cierges dans six chandeliers d'argent ; plusieurs laquais portant des flambeaux de cire blanche allumés. Le corps pris rue de Richelieu devant l'hôtel de Crussol, a été porté au cimetière de Saint-Joseph et enterré au pied de la croix. Il y avait grande foule de peuple et l'on a fait distribution de mille à douze cents livres aux pauvres qui s'y sont trouvés, à chacun cinq sols. Ledit sieur Molière était décédé le vendredi au soir 17 février 1673. Monsieur l'Archevêque avait ordonné qu'il fût ainsi enterré sans aucune pompe, et même défendu aux curés et religieux de ce diocèse de faire aucun service pour lui. Néanmoins l'on a ordonné quantité de messes pour le défunt[168]. »
Le 9 mars suivant, La Gazette d'Amsterdam consacrera un article à la mort et à l'enterrement de Molière[169]. Du 13 au 21 mars suivant, on procède à un inventaire de ses biens[170].
La fin brutale d'un comédien aussi célèbre et controversé donna lieu à une centaine d’épitaphes et de poèmes. La plupart exprimaient de l'hostilité à l'égard de Molière, d'autres célébraient ses louanges, comme l’épitaphe composée par La Fontaine : Sous ce tombeau gisent Plaute et Térence,
Et cependant le seul Molière y gît :
Leurs trois talents ne formaient qu’un esprit,
Dont le bel art réjouissait la France.
Ils sont partis, et j’ai peu d'espérance
De les revoir, malgré tous nos efforts ;
Pour un long temps, selon toute apparence,
Térence et Plaute et Molière sont morts.
Le 6 juillet 1792, désireuses d’honorer les cendres des grands hommes, les autorités révolutionnaires firent exhumer les restes présumés de Molière, et ceux de La Fontaine qui reposait dans le même lieu. L’enthousiasme étant retombé, les dépouilles restèrent de nombreuses années dans les locaux du cimetière, puis furent transférées en l'an VII au musée des monuments français. Quand ce musée fut supprimé, en 1816, on transporta les cercueils au cimetière de l’Est, l'actuel Père-Lachaise, où ils reçurent une place définitive le 2 mai 1817.
Une semaine après la mort de Molière, les représentations reprennent : Le Misanthrope d'abord, avec Baron dans le rôle d'Alceste, puis Le Malade imaginaire, avec La Thorillière dans celui d'Argan. Au cours de la clôture de Pâques, Baron, La Thorillière et le couple Beauval quittent la troupe pour rejoindre l'hôtel de Bourgogne ; un mois plus tard, le roi reprend aux camarades de Molière la salle qu'il avait accordée en 1660 à la  et la donne à Lully, afin d'y représenter ses spectacles d'opéra.
En 1680, un décret royal fait obligation à la Troupe du Roy à l'hôtel de Guénégaud de fusionner avec la Troupe Royale de l'hôtel de Bourgogne : c'est la naissance de la Comédie-Française. La nouvelle compagnie, assez nombreuse pour se partager entre Paris et les lieux de résidence de la cour, joue désormais tous les jours de la semaine, et non plus seulement les .
En 1682, La Grange[n 59], à qui Armande Béjart avait remis tous les papiers de son défunt mari[171], publie les Œuvres de Monsieur de Molière en huit tomes, dont les deux derniers, intitulés Œuvres posthumes, donnent à lire pour la première fois des pièces que Molière n'avait jamais fait paraître. Selon certains[172], La Grange n'aurait pas hésité à modifier les dialogues de plusieurs comédies ; ce faisant, il inaugurait une pratique éditoriale qui s'est prolongée jusqu'aujourd'hui[n 60]. Le premier volume s'ouvre sur une préface non signée mais assurément composée par La Grange[n 61] et qui constitue la première notice biographique consacrée à Molière.
En 1705, Jean-Léonor Le Gallois de Grimarest publie, sous le titre de La Vie de M. de Molière, la première véritable biographie du , dont une grande partie des éléments lui a été fournie par le comédien Michel Baron et qui, maintes fois rééditée en dépit des critiques dont elle a été l'objet dès sa parution, reste un document incontournable.
En 1723, la postérité de Molière s'éteint avec la mort de sa fille, Esprit-Madeleine Poquelin.
La vie de Molière reste encore mal connue. Nous ne possédons de lui ni lettres, ni brouillons, ni mémoires. Les maisons dans lesquelles il a vécu ont disparu. Les seuls restes tangibles de son existence sont un ensemble d'actes notariés signés de sa main et le fauteuil dans lequel il a eu un malaise lors de sa dernière représentation (reproduit plus haut).
On attribue à mademoiselle Poisson[n 62] un portrait de Molière assez précis, que Jean-Louis Ignace de La Serre a reproduit en 1734 :
« Il n’était ni trop gras ni trop maigre ; il avait la taille plus grande que petite, le port noble, la jambe belle ; il marchait gravement, avait l’air très sérieux, le nez gros, la bouche grande, les lèvres épaisses, le teint brun, les sourcils noirs et forts, et les divers mouvements qu’il leur donnait lui rendaient la physionomie extrêmement comique. À l’égard de son caractère, il était doux, complaisant, généreux. Il aimait fort à haranguer ; et quand il lisait ses pièces aux comédiens, il voulait qu’ils y amenassent leurs enfants, pour tirer des conjectures de leurs mouvements naturels[175]. »
Ce jugement est corroboré par de nombreux témoignages, tous, il est vrai, postérieurs à la mort de Molière. Dès 1674, Samuel Chappuzeau fait un vibrant « Éloge de Molière » :
« Outre les grandes qualités nécessaires au poète et à l'acteur, il possédait toutes celles qui font l'honnête homme ; il était généreux et bon ami, civil et honorable en toutes ses actions, modeste à recevoir les éloges qu'on lui donnait, savant sans vouloir le paraître, et d'une conversation si douce et si aisée que les premiers de la Cour et de la Ville étaient ravis de l'entretenir[176]. »
La même année, dans la comédie L'Ombre de Molière, Brécourt dresse de son ancien camarade de scène un portrait tout aussi flatteur, le décrivant comme .
Dans la préface de 1682, La Grange, qui a été son camarade durant plus de treize ans, le décrit comme :
« un homme civil et honnête, ne se prévalant point de son mérite et de son crédit, s'accommodant à l'humeur de ceux avec qui il était obligé de vivre, ayant l'âme belle, libérale : en un mot, possédant et exerçant toutes les qualités d'un parfaitement honnête homme. […] Quoi qu’il fût très agréable en conversation lorsque les gens lui plaisaient, il ne parlait guère en compagnie, à moins qu’il ne se trouvât avec des personnes pour qui il eût une estime particulière : cela faisait dire à ceux qui ne le connaissaient pas qu’il était rêveur et mélancolique ; mais s’il parlait peu, il parlait juste et d’ailleurs il observait les manières et les mœurs de tout le monde[178]. »
Ce côté rêveur est également mentionné par Grimarest :  De même, Nicolas Boileau 
En 1663, dans sa comédie Zélinde ou la véritable critique de l'École des femmes, Donneau de Visé présente Élomire [Molière] appuyé sur un comptoir et silencieux, « dans la posture d'un homme qui rêve. Il avait les yeux collés sur trois ou quatre personnes de qualité qui marchandaient des dentelles ; il paraissait attentif à leurs discours, et il semblait, par le mouvement de ses yeux, qu'il regardait jusques au fond de leurs âmes pour y voir ce qu'elles ne disaient pas[181]. »
Et Donneau précise plus loin que Molière semblait cacher sous son manteau des tablettes sur lesquelles il notait les propos entendus ou dessinait les grimaces des gens qu'il observait.
Plusieurs anecdotes attestent qu'il était aussi d'un tempérament impatient et . Grimarest relève également qu', qu' et que .
En ce qui a trait à ses qualités d'acteur,   Selon Donneau de Visé,  À propos du jeu de Molière dans Sganarelle, il écrit dans ses commentaires publiés sous le pseudonyme de Neuf-Villenaine : , ajoutant que Molière comme acteur avait .
En revanche, Molière était médiocre dans le genre sérieux et se faisait régulièrement siffler dans des rôles tragiques[187]. Comme le note Charles Perrault en 1697 : 
Ses qualités d'acteur ainsi que  l'avaient tout naturellement porté à la tête de ses camarades et il a laissé le souvenir d'un . Perfectionniste, il préparait la représentation d'une nouvelle pièce par des répétitions précises et minutieuses, qui pouvaient parfois durer plus de deux mois[190]. Donneau de Visé en donne ce témoignage :« Il a pris le soin de faire si bien jouer ses compagnons que tous les acteurs qui jouent dans la pièce sont des originaux que les plus habiles maîtres de ce bel art pourront difficilement imiter […] chaque acteur sait combien il doit faire de pas et toutes ses œillades sont comptées[190]. »
 Lors de la représentation du Tartuffe en février 1669, Charles Robinet note : 
Comme le souligne René Bray,  Ses camarades continuèrent à lui faire confiance car il s'était acquis leur loyauté et, même au cours des pires moments qu'elle eut à traverser, . Il lui fallut aussi arbitrer à plusieurs reprises les rivalités de préséance entre les trois comédiennes vedettes de la troupe : Madeleine Béjart, la plus ancienne, la Du Parc renommée pour sa beauté et la De Brie dont le talent était remarquable[194].
Excellent improvisateur, il a été jusqu'à l'automne 1664 l'orateur de la troupe, chargé de présenter la pièce avant la représentation pour obtenir l'attention du public tout en vantant l'intérêt ou le mérite des acteurs. Cette tâche, qui exigeait , fut ensuite confiée à La Grange[195].
Molière ne semble pas avoir eu de véritables liens d'amitié avec les comédiens de sa troupe, à l'exception de Baron, et un historien a pu souligner comme « l'un des paradoxes du personnage, modèle de tous les comédiens français, [le fait] que sa vie professionnelle ait été entièrement vouée au théâtre, alors que ses amitiés, ses attachements, ses goûts, ses intérêts intellectuels […] le portaient vers les salons et les compagnies savantes, vers des poètes, des traducteurs, des philosophes, des médecins, des physiciens, des voyageurs »[196]. On relève ainsi, parmi ses fréquentations plus ou moins proches, le poète libertin Chapelle, le philosophe François de La Mothe Le Vayer, précepteur de Monsieur, son fils l'abbé La Mothe Le Vayer, aumônier de Madame et passionné de comédie[n 63], et sa nièce Honorée de Bussy[197]. Il fréquentait aussi le médecin et voyageur François Bernier, vulgarisateur de l'œuvre de Gassendi[n 64], le mathématicien et physicien Jacques Rohault[198], le secrétaire d'État Louis-Henri de Loménie de Brienne, les peintres Nicolas et Pierre Mignard[199], les frères Pierre[n 65], Gilles et Nicolas Boileau, l'avocat Bonaventure de Fourcroy[200], le nouvelliste et dramaturge Jean Donneau de Visé, longtemps son détracteur, ainsi que son médecin Armand-Jean de Mauvillain[201]. Il a également compté parmi ses amis Jean-Baptiste Lully, avec qui il collabora jusqu'en 1672, et un certain M. de Saint-Gilles, intendant de Brienne, qui avait été l'ami de Cyrano de Bergerac et d'Henry Le Bret, dont on sait peu de choses, mais qu'au dire de Boileau, Molière aurait peint dans Le Misanthrope sous le nom de Timante.
Il recevait parfois ses amis dans la maison qu'il louait à Auteuil depuis 1667. Une soirée est restée célèbre sous le nom de « souper d'Auteuil », auquel participaient entre autres Chapelle, Baron, Lully, Alexis de Sainte-Maure, marquis de Jonzac, premier écuyer de Monsieur, et François du Prat, chevalier de Nantouillet[n 66].
La critique est divisée sur la façon de juger Molière. On l’a décrit comme « un bourgeois qui possède un sens aigu du travail et de la responsabilité envers sa troupe », un  ou un écrivain engagé[202]. À la suite de Henry Becque et Émile Faguet, certains moliéristes, comme Paul Bénichou et René Bray, affirment la primauté chez Molière de l’homme de théâtre et du praticien de la scène sur le penseur et l'homme de lettres, alors que d'autres voient en lui un philosophe et un [203]. Ses comédies, qui reproduisent la nature en peignant la réalité sociale, contiennent cependant suffisamment d'ambiguïté dans l'exagération même des portraits qu'elles peignent pour se prêter à des lectures contradictoires : 
Selon Cyril Chervet, il importe de dépasser les ambiguïtés de la tradition biographique pour se centrer sur les pièces et les textes qui les accompagnent, qui révèlent  Sans être « libertin » au sens moderne du mot ni porteur d'un système philosophique précis, Molière apparaît comme un homme qui  et dont les pièces présentent des échos de disputes philosophiques qui servent .
À partir de 1658, année où sa troupe est agréée par le frère du roi, . Loin de s’en tenir à représenter des divertissements anodins, il s’attaque alors à des sujets qui touchent au vif certaines institutions ou pratiques établies. Dès 1659, il propose dans Les Précieuses ridicules une critique du parler précieux dont l’effet est dévastateur sur les tenants de cette mode. Le grammairien Gilles Ménage se souvient de la première représentation de la pièce : [n 67].
Avec L'École des maris (1661) et plus encore L'École des femmes (1662), Molière se moque des tyrans domestiques et plaide en faveur de l’éducation des femmes[n 68]. Il affiche aussi une audace et une maîtrise dans le maniement des sous-entendus qui commencent à inquiéter les milieux dévots.
Allant encore plus loin dans la critique sociale, il dénonce dans Le Tartuffe les escroqueries qui se commettent sous le couvert de la dévotion et revendique le droit pour la comédie de travailler à réformer les mœurs, contestant ainsi la compétence exclusive que l'Église prétendait avoir en ce domaine. Ainsi que l'ont noté des critiques : 
Grâce à la protection du roi, Molière jouissait assurément d'une position sociale relativement enviable ; cela n'empêche, selon Roger Duchêne, que trois de ses comédies : L'École des femmes, Le Festin de Pierre et Le Tartuffe, . En faisant de la comédie un lieu de débats de société[211], il serait ainsi devenu pour les milieux dévots l'adversaire à combattre. À bien des égards, 
L'œuvre de l'écrivain Molière est indissociable de son métier d'acteur :  Au total, il a tenu 
À l'exception de quelques préfaces et poèmes de circonstance[n 69], cette œuvre est entièrement dramatique et se compose d'une trentaine de comédies, accompagnées ou non d'entrées de ballet et de musique. Dans ses débuts, au cours de ses treize années de carrière provinciale (voir plus haut), il a composé des farces, dont deux seulement, La Jalousie du Barbouillé et Le Médecin volant, ont été conservées.
Selon Claude Bourqui,  Accusé d'avoir fait plusieurs emprunts au Pédant joué de Cyrano de Bergerac, Molière aurait répondu : 
Au cours de ses pérégrinations en province, Molière pratique la farce dans le style italien de la commedia dell'arte dont il assimile les procédés et les structures dramatiques[218], tout en retenant aussi des personnages types, tels Scapin ou Covielle, ou créant de nouveaux noms à consonance italienne, tels Mascarille[219] ou Sganarelle[220]. Dans ce théâtre, le rapport au texte est très fluide et laisse aux acteurs une marge d'improvisation[221]. L'une de ses premières farces, Le Médecin volant, serait peut-être adaptée du Medico volante d'un anonyme italien[222]. L'Étourdi est assurément imité de L'Inavertito de Niccolò Barbieri (Turin, 1628), dans lequel apparaît Scappino (Scapin)[223]. Le Dépit amoureux est inspiré d'une pièce de Nicolo Secchi, L'Interesse (1581)[224]. De même, Le Festin de Pierre reprend la légende de Don Juan, que Tirso de Molina avait portée au théâtre en 1630 avec El Burlador de Sevilla y convidado de piedra[225], mais il est douteux que Molière ait lu cette première version, et les historiens s'accordent à dire qu'il a emprunté principalement au Festin de Pierre de Dorimond (1659) et à celui que les Italiens jouaient dans le début des années 1660.
Le Médecin malgré lui  Molière adapte aussi des pièces du théâtre antique. Son Amphitryon reprend, à quelques scènes près, celui de Plaute, tandis que L'Avare est une adaptation de l'Aulularia (La Marmite). Psyché est tirée d'un passage des Métamorphoses d'Apulée[226].
Parfois, les adaptations sont masquées. Aux Adelphes de Térence, il emprunte quelques éléments de L'École des maris et à son Phormion la structure des Fourberies de Scapin[227]. Dom Garcie de Navarre ou le Prince jaloux est adapté d'une pièce de Cicognini, tandis que La Princesse d'Élide est adaptée d'une pièce d'Agustin Moreto[228].
Molière emprunte parfois des éléments à diverses sources,  : L'École des maris combine une comédie espagnole d'Antonio Hurtado de Mendoza avec une farce italienne ; L'École des femmes contamine une nouvelle de Scarron avec une farce italienne[229] ; Le Tartuffe emprunte surtout à Flaminio Scala, Vital d'Audiguier et Antoine Le Métel d'Ouville ainsi que, de façon accessoire, à une nouvelle de Scarron, Les Hypocrites, qu'il contamine avec des scenari italiens[230] et, selon certains, il emprunterait aussi à la pièce de Pierre l'Arétin, Lo ipocrito[n 70].
Dans Les Précieuses ridicules, Molière exploite à titre accessoire un ouvrage de Charles Sorel sur Les Lois de la galanterie[231], tout en reprenant la trame de L'Héritier ridicule (1649) de Paul Scarron[232].
Il construit aussi des intrigues en combinant des idées de personnages qu'il a trouvés dans le Décaméron de Boccace, les nouvelles de Straparole ou les fabliaux du Moyen Âge. Le personnage du Misanthrope pourrait lui avoir été suggéré par une pièce de Ménandre, dont il connaissait des fragments, comme l'indique une déclaration qu'il aurait faite à un ami après le succès des Précieuses : [233].
Molière avait des lectures étendues : l'inventaire de sa bibliothèque mentionne quelque 180 volumes d'histoire et de littérature, dont 40 volumes de comédies françaises, italiennes et espagnoles[234].
Molière a pratiqué la plupart des genres dramatiques de comique : farce, comédie d'intrigue, comédie de mœurs, comédie de caractère, comédie-ballet. Comme le note Forestier :  Tous ses procédés se ressemblent par l'effet qu'ils provoquent : . Paul Léautaud, quant à lui, voit chez Molière « un comique douloureux, comme est le vrai comique[242] ».
Il intègre dans sa création, tout comme dans son jeu d'acteur, des personnages fort différents : .
Il ne se répète jamais et n’enferme pas ses personnages dans des stéréotypes : ses médecins se comportent tantôt comme des avocats, tantôt comme des prêtres. « Chacune de ses pièces est une entité organique (an organic whole) avec son type de comique et son propre rythme »[244].
Chez Molière, le comique, loin d'être gratuit, vise à attirer l'attention sur des défauts courants ou à stigmatiser des réalités sociales :  En un sens, c'est la fonction classique du rire, dont Bergson a dit qu'il est  Ainsi que l'observe Patrick Dandrey, .
Il ne s'attaque pas à des pratiques réputées malhonnêtes, mais aux comportements non réfléchis et aux multiples illusions par lesquelles les humains s'aveuglent sur eux-mêmes[248]. Et sa critique n'épargne pratiquement personne : 
Molière a pour ambition de fournir à ses contemporains un miroir de leurs ridicules, comme il l'affirme dans La Critique de l'École des femmes :  Le public était à même de se reconnaître dans les ridicules mis en scène. Or, faire rire aux dépens de grands personnages ne va pas sans risques, comme Il s'en explique notamment dans L'Impromptu de Versailles : « MOLIÈRE : Et pensez-vous que ce soit une petite affaire que d’exposer quelque chose de comique devant une assemblée comme celle-ci, que d’entreprendre de faire rire des personnes qui nous impriment le respect et ne rient que quand ils veulent ? Est-il auteur qui ne doive trembler lorsqu’il en vient à cette épreuve[251] ? »
En élevant l'art de la comédie au niveau jusque-là tenu par la tragédie, Molière était conscient d'innover et s'est expliqué sur ses choix théoriques dans La Critique de l'École des femmes (1663). Comme le souligne Robert Garapon : 
Baragouin « Sganarelle se levant avec étonnement : Vous n’entendez point le latin !
— Géronte : Non.
— Sganarelle en faisant diverses plaisantes postures : Cabricias arci thuram, catalamus, singulariter, nominativo hæc Musa, « la Muse », bonus, bona, bonum, Deus sanctus, estne oratio latinas ? Etiam, « oui », Quare, « pourquoi ? » Quia substantivo et adjectivum concordat in generi, numerum, et casus.
[…] 
Molière a recours à toutes les formes du comique verbal : équivoque, répétition, aparté, quiproquo, dialogue de sourds, éloge paradoxal ou parodie[255]. Il réussit à harmoniser des styles différents chez un même personnage en jouant sur l'exagération, la répétition et la symétrie[256].
Il ne répugne pas au calembour, pourvu que celui-ci s'accorde à son personnage :
« Bélise à la bonne : Veux-tu toute ta vie offenser la grammaire ?
— Martine : Qui parle d’offenser grand-mère ni grand-père[257]. »
L'éloge paradoxal apparaît notamment dans Le Festin de Pierre, où le valet Sganarelle fait l'éloge du tabac[258], tandis que le héros, Dom Juan, fait l'éloge de l'infidélité amoureuse[259] et de l'hypocrisie[260]. Ce même procédé peut prendre la forme de l'antiphrase, comme dans le passage où Sbrigani félicite Nérine de ses exploits, en réalité des méfaits qu'il présente comme des actions louables (voir encadré ci-contre).
Le comique inhérent à un éloge paradoxal peut n'être pas saisi par celui qui en est l'objet. Ainsi, dans Monsieur de Pourceaugnac, l'Apothicaire fait un éloge outré d’un médecin sans que celui-ci y trouve à redire : « J’aimerais mieux mourir de ses remèdes que de guérir de ceux d’un autre […] ; et quand on meurt sous sa conduite, vos héritiers n’ont rien à vous reprocher […] Au reste, il n’est pas de ces médecins qui marchandent les maladies : c’est un homme expéditif, qui aime à dépêcher ses malades ; et quand on a à mourir, cela se fait avec lui le plus vite du monde[253]. »
L'amphigouri est une autre figure propre à susciter le rire. En présence d’un homme bien portant, ce même médecin pose son diagnostic au moyen d’un long discours émaillé de jargon professionnel : « Premièrement, pour remédier à cette pléthore obturante, et à cette cacochymie luxuriante par tout le corps, je suis d’avis qu’il soit phlébotomisé libéralement, c’est-à-dire que les saignées soient fréquentes et plantureuses : en premier lieu de la basilique, puis de la céphalique ; et même, si le mal est opiniâtre, de lui ouvrir la veine du front, et que l’ouverture soit large, afin que le gros sang puisse sortir ; et en même temps, de le purger, désopiler, et évacuer par purgatifs propres et convenables, c’est-à-dire par cholagogues, mélanogogues, et caetera[261]. »
Dans Le Médecin malgré lui, Sganarelle feint d'être médecin et recourt à du pseudo-latin ainsi qu'à des termes techniques médicaux auxquels il mêle des absurdités (encadré). Il termine par une conclusion devenue proverbiale : 
Molière recourt volontiers à des déformations du français par un parler étranger ou régional, tels les propos en pseudo-turc dans Le Bourgeois gentilhomme[262]. Dans Monsieur de Pourceaugnac, un personnage camoufle son origine en affectant d'être un marchand flamand :  Plus loin, Lucette feint d'être une Gasconne qu'aurait épousée jadis M. de Pourceaugnac : « Que te boli, infame ! Tu fas semblan de nou me pas counouysse, et nou rougisses pas, impudent que tu sios, tu ne rougisses pas de me beyre ? Nou sabi pas, Moussur, saquos bous dont m’an dit que bouillo espousa la fillo ; may yeu bous declari que yeu soun sa fenno, et que y a set ans, Moussur, qu’en passan à Pezenas el auguet l’adresse dambé sas mignardisos, commo sap tapla fayre, de me gaigna lou cor, et m’oubligel pra quel mouyen à ly douna la ma per l’espousa[264]. »
Dans la scène suivante, Nérine feint d'être une autre épouse de M. de Pourceaugnac et ses déclarations, en picard, font écho de façon parodique aux affirmations de la pseudo-Gasconne dans une  :
« Lucette : Tout Pezenas a bist notre mariatge.
— Nérine : Tout Chin-Quentin a assisté à no noce[266]. »
Molière recourt évidemment à l'exagération, qui est un ressort comique d'autant plus efficace qu'elle s'accorde avec le personnage dont elle caricature le caractère. Ainsi, dans L'Avare, Harpagon, ayant perdu la cassette contenant sa fortune, fait venir la police. Il s’ensuit cet échange :
« Le Commissaire : Qui soupçonnez-vous de ce vol ?
— Harpagon : Tout le monde, et je veux que vous arrêtiez prisonniers la ville et les faubourgs[267]. »
Comme l'a noté Ramon Fernandez, ces divers procédés sont renforcés par le découpage des scènes et le rythme des dialogues, qui ressemblent à des mouvements de danse[268] : 
Molière exploite toutes les ressources du comique visuel hérité de la farce et de la commedia dell'arte : poursuites, coups de bâton, gesticulations, grimaces. Les contemporains ont laissé de nombreux témoignages de son extraordinaire plasticité corporelle :  Même ses adversaires reconnaissaient son talent de comédien. Selon Donneau de Visé, alias Villenaine : 
Ainsi que nous en prévient Molière dans la préface des Précieuses ridicules, la seule lecture du texte de ses pièces ne saurait donc rendre justice aux multiples éléments déclencheurs du comique que la mise en scène fait apparaître : .
Il choisit pour ses personnages des costumes très colorés[n 74], auxquels il ajoute parfois des accoutrements extravagants. Par exemple, Mascarille, le « petit marquis » des Précieuses ridicules, joué par Molière lui-même, est ainsi décrit par une spectatrice de l'époque : « Sa perruque était si grande qu’elle balayait la place à chaque fois qu’il faisait la révérence et le chapeau si petit qu'il était aisé de juger que le marquis le portait bien plus souvent dans la main que sur la tête […] ses souliers étaient si couverts de rubans qu'il ne m'est pas possible de vous dire s'ils étaient de roussi, de vache d'Angleterre ou de maroquin ; du moins sais-je bien qu'ils avaient un demi-pied de haut, et que j'étais fort en peine de savoir comment des talons si hauts et si délicats pouvaient porter le corps du marquis, ses rubans, ses canons [partie de la culotte] et la poudre[273]. »
Dans L'École des femmes, Arnolphe, impuissant à se faire aimer d'Agnès, en vient à la supplier en des termes ridiculement dramatiques : « Quelle preuve veux-tu que je t’en donne, ingrate ?
« Me veux-tu voir pleurer ? Veux-tu que je me batte ?
« Veux-tu que je m’arrache un côté de cheveux ?
« Veux-tu que je me tue ? Oui, dis si tu le veux :
« Je suis tout prêt, cruelle, à te prouver ma flamme[n 75]. »
Cette déclaration était accompagnée de , comme nous l'apprend un personnage de La Critique de l'École des femmes[n 76].
Molière fait volontiers apparaître ses personnages sous des déguisements pour tromper ou mystifier. Dans Le Malade imaginaire, la jeune servante Toinette se déguise en vieux médecin pour émettre un diagnostic qui se termine par la fameuse réplique du « poumon » (Acte III, scènes 8 et 10). Dans Dom Juan, c'est Sganarelle qui se déguise en médecin et prescrit des émétiques aux effets funestes (Acte III, scène 1). Dans Amphitryon, Mercure prend la figure du valet d'Amphitryon, Sosie, pour servir les desseins amoureux de Jupiter. Le garçon amoureux se déguise en intendant dans L'Avare, en fils du Grand Turc dans Le Bourgeois gentilhomme, en maître de musique dans Le Malade imaginaire : cela lui permet dans chacun de ces cas de [274].
Jusqu'en 1661, Molière a recours à un masque quand il interprète le personnage de Mascarille. Par la suite, il jouera Sganarelle sans masque, mais se noircit les sourcils et la moustache au charbon ou à l'encre[275]. Il continue cependant à utiliser des masques faits sur mesure pour certains personnages, tout particulièrement les médecins dans plusieurs pièces, dont L'Amour médecin et Le Malade imaginaire[276].
Il imagine des situations proprement burlesques, comme dans Monsieur de Pourceaugnac où un intrigant persuade un provincial un peu épais de se travestir en femme pour échapper à ses poursuivants (Acte III, scènes 1-4). La bouffonnerie des déguisements relève donc aussi du comique de situation.
Le comique de situation abonde dans les pièces de Molière : un personnage tient à voix haute des propos qu’il dément en aparté ; le mari sort de sa maison sans voir l'amant qui y entre ; un personnage que l’on sait attaché à la ruine d’un autre personnage accable ce dernier de compliments outrés dont il ne croit pas un seul mot. Ou encore, comme le signale Bergson : 
Un déclencheur courant est un renversement radical de situation ou de perspective. Ainsi, dans L'Avare, le courtier Simon présente à Harpagon un emprunteur potentiel qui n'est autre que Cléante, son fils. Le père découvre que son fils est dépensier, en même temps que le fils découvre que son père est un usurier : « Maître Simon montrant Cléante à Harpagon : Monsieur est la personne qui veut vous emprunter les quinze mille livres dont je vous ai parlé.
— Harpagon : Comment, pendard ! c’est toi qui t’abandonnes à ces coupables extrémités !
— Cléante : Comment ! mon père, c’est vous qui vous portez à ces honteuses actions[278] ! »
La mise en scène de deux personnages contrastés produit aussi un effet comique, comme le note Bergson, parce que la dissemblance attire l'attention sur leur physique plutôt que sur le contenu de leurs propos[279] : « Quand Molière nous présente les deux docteurs ridicules de L'Amour médecin, Bahis et Macroton, il fait parler l’un d’eux très lentement, scandant son discours syllabe par syllabe, tandis que l’autre bredouille. Même contraste entre les deux avocats de M. de Pourceaugnac. D’ordinaire, c’est dans le rythme de la parole que réside la singularité physique destinée à compléter le ridicule professionnel[280]. »
Dans une étude sur le génie de Molière parue en 1736, l'écrivain et comédien Luigi Riccoboni oppose la comédie de caractère à la comédie d'intrigue : alors que, dans cette dernière, l'action et ses rebondissements sont essentiels, la première se consacre d'abord à peindre des caractères d'où découlera une action. À cet égard, [281].
Une source importante du comique moliéresque réside en effet dans la conception des personnages principaux, souvent affligés d'une manie poussée jusqu'à l'invraisemblance[282]. C'est cette manie qui suscite le comique, selon la thèse du philosophe Henri Bergson qui, dans son étude sur le rire, s'appuie sur les pièces de Molière pour montrer que le rire est suscité par le spectacle . Le rire est  par lequel l'individu et la société tendent à se préserver : « Toute raideur du caractère, de l’esprit et même du corps, sera donc suspecte à la société, parce qu’elle est le signe possible d’une activité qui s’endort et aussi d’une activité qui s’isole, qui tend à s’écarter du centre commun autour duquel la société gravite, d’une excentricité enfin[284]. »
Comme le note Jacques Scherer, 
Cette manie pousse les personnages à un tel niveau d'aveuglement qu'ils deviennent leurs pires ennemis. Ainsi, dans L'Avare, Harpagon, rendu fou par le vol de sa cassette, s'écrie : 
Les contemporains ont si bien reconnu en Molière , qu'ils l'ont surnommé « le peintre » et ont trouvé en lui un nouveau Térence, auteur latin considéré alors comme [287].
L'ensemble de ses pièces compte quelque 150 personnages[288]. Poussant les portraits des principaux jusqu'à la caricature, Molière a réussi à en faire des types : Tartuffe reste le modèle de la dévotion feinte, Harpagon est l'avarice personnifiée, Argan est le malade imaginaire par excellence, Monsieur Jourdain est le type du bourgeois sot et vaniteux qui croit pouvoir s'acheter une apparence de noblesse.
Les noms propres sont souvent révélateurs : Trissotin est le modèle du pédant « triplement sot » dans Les Femmes savantes, le médecin Diafoirus dans Le Malade imaginaire évoque quelque lavement « foireux »[289], le bourgeois Gorgibus est le père des précieuses ridicules, Arnolphe dans L'École des femmes s'appelle « de La Souche », patronyme fort approprié à un homme hanté par le cocuage et anxieux d'assurer la transmission de son titre[290], etc.
Molière exploite les effets comiques produits par la répétition, tant sur le plan verbal des dialogues[291] que dans les grandes structures de l’action[292].
Un exemple célèbre de répétition verbale est la scène du Malade imaginaire où la servante Toinette déguisée en médecin émet le même diagnostic (« le poumon ») à chacun des symptômes énumérés par Argan[293]. De même, la réplique récurrente « Sans dot ! » que fait Harpagon aux arguments opposés à son projet de marier sa fille au vieil Anselme[294].
Parfois, les répétitions s'enchaînent en séries, montrant chez Molière , comme lorsque l'apothicaire offre un clystère à M. de Pourceaugnac : « L’Apothicaire : C’est un petit clystère, un petit clystère, benin, benin ; il est benin, benin, là, prenez, prenez, prenez, Monsieur : c’est pour déterger, pour déterger, déterger[296]… »
Cette même séquence réapparaît quelques scènes plus loin, lorsque M. de Pourceaugnac fait le récit de ses mésaventures à celui-là même qui les a machinées, mêlant le comique de répétition au comique de situation :
« Monsieur de Pourceaugnac : […] Apothicaire. Lavement. Prenez, Monsieur, prenez, prenez. Il est benin, benin, benin. C’est pour déterger, pour déterger, déterger[296]. »
On trouve aussi nombre de cas de répétition structurelle. À cet égard, la répétition d’un quiproquo est doublement comique. Dans George Dandin ou le Mari confondu, Lubin, qui est au service de l’amant, se trompe par trois fois sur 
La répétition est comique parce qu'elle suggère, comme l'a noté Bergson, l'idée d'un automatisme non maîtrisé : « Quand Dorine raconte à Orgon la maladie de sa femme, et que celui-ci l’interrompt sans cesse pour s’enquérir de la santé de Tartuffe, la question qui revient toujours : « Et Tartuffe ? » nous donne la sensation très nette d’un ressort qui part. C’est ce ressort que Dorine s’amuse à repousser en reprenant chaque fois le récit de la maladie d’Elmire. Et lorsque Scapin vient annoncer au vieux Géronte que son fils a été emmené prisonnier sur la fameuse galère, qu’il faut le racheter bien vite, il joue avec l’avarice de Géronte absolument comme Dorine avec l’aveuglement d’Orgon. L’avarice, à peine comprimée, repart automatiquement, et c’est cet automatisme que Molière a voulu marquer par la répétition machinale d’une phrase où s’exprime le regret de l’argent qu’il va falloir donner : « Que diable allait-il faire dans cette galère ? » Même observation pour la scène où Valère représente à Harpagon qu’il aurait tort de marier sa fille à un homme qu’elle n’aime pas. « Sans dot ! » interrompt toujours l’avarice d’Harpagon. Et nous entrevoyons, derrière ce mot qui revient automatiquement, un mécanisme à répétition monté par l’idée fixe[298]. »
Molière prenait grand soin non seulement des costumes, mais aussi des décors, même pour des représentations en plein air, comme celle qu'il donna de George Dandin ou le Mari confondu à Versailles en 1668 : 
De nouvelles possibilités scéniques s’étaient mises en place dès le début du XVIIe siècle dans les grandes salles parisiennes : l’Hôtel de Bourgogne, le théâtre du Marais, la salle du Petit-Bourbon et tout particulièrement le théâtre du Palais-Royal, dont Philippe Cornuaille a reconstitué l'architecture scénique à un moment clé de son histoire grâce à l’observation de plans et de documents jusqu’ici peu ou pas exploités[300].
Pour la plupart des comédies de Molière créées à la ville[n 44], il n'existe que peu de commentaires ou de documents contemporains touchant à la scénographie, hormis un manuscrit exceptionnel concernant Le Festin de Pierre[301] et quelques mémoires présentés par des fournisseurs, notamment pour la création du Malade imaginaire[302] avec la musique composée par Marc-Antoine Charpentier. La plupart du temps, c’est la fonction même du décor induite par l’action qui aide à visualiser celui-ci. Il est en effet possible de cadastrer des périmètres où le décor prend toute son importance, tant il est lié à l’action. Les exemples de L'École des maris et de L'École des femmes sont frappants avec le déplacement parfois progressif, parfois brutal, d’un endroit à un autre, d’un décor de maison vers un autre décor. Le regroupement des comédies par thème — carrefours de rues, intérieurs, changements de lieux — aide à mieux discerner une évolution dans tel ou tel type de scénographie et souligne l’importance que pouvait accorder Molière à la fonction dramaturgique d’un décor[300],[n 77].
Les copieuses relations faites par La Fontaine[306], Félibien[307] ou la Gazette donnent un luxe de détails sur les décorations de la plupart des comédies-mêlées de Molière. En plus d’une fonction inscrite dans l’action, les décors et la scénographie prennent alors une forte valeur ornementale et spectaculaire. Quelques gravures publiées à grands frais à l’occasion de divertissements royaux restent les uniques représentations visuelles crédibles de certaines mises en scène, comme ces illustrations de George Dandin ou de La Princesse d’Élide[300].
De son vivant, les détracteurs de Molière lui reprochaient de recourir à la farce, considérée comme un genre bas et vulgaire[308] — attaques amplifiées par le parti religieux qui se sentait visé par certaines de ses pièces —, mais il avait pour lui l'élite intellectuelle de l'époque[309]. Dès 1663, l'influent critique Jean Chapelain louait Molière pour la qualité de son invention et les morales de ses pièces, tout en le mettant en garde contre un excès de bouffonnerie[n 78]. La même année, Donneau de Visé écrit dans ses Nouvelles Nouvelles : 
Quant à Boileau, il assistait à ses pièces[n 79] et y riait de bon cœur[n 80], même s'il dénonce dans L'Art poétique les disparités de ton et ce qu'il juge être des faiblesses dans l'œuvre de Molière[n 81]. Selon un de ses interlocuteurs, il . À Louis XIV qui lui demandait , Boileau aurait répondu que c'était Molière[311].
On a cru pendant longtemps que La Fontaine évoquait dans Les Amours de Psyché (1669) la petite société littéraire où il retrouvait Molière, Racine et Boileau vers 1660[312]. On sait aujourd'hui qu'il n'en est rien[313]. Tout comme Molière, il assigne à l'œuvre la nécessité de plaire[n 82]. Dès 1661, il lui consacre un petit poème dans une lettre à son ami François de Maucroix[n 83] et écrit son épitaphe en 1673 (voir plus haut).
La même année, Brécourt publie une comédie intitulée L'Ombre de Molière, dans laquelle le dramaturge est confronté dans l'au-delà à une poignée de ses personnages désireux de se venger de lui pour les avoir tournés en ridicule[314]. Molière a donc bien donné lieu à un phénomène de , comme le note un critique moderne, et 
Vingt ans après la mort de Molière, Bossuet fustige « ce rigoureux censeur des grands canons, ce grave réformateur des mines et des expressions de nos précieuses », qui « étale cependant au plus grand jour les avantages d'une infâme tolérance dans les maris », qui « sollicite les femmes à de honteuses vengeances contre leurs jaloux », et « a fait voir à notre siècle le fruit qu'on peut espérer de la morale du théâtre, qui n'attaque que le ridicule du monde, en lui laissant cependant toute sa corruption ». Et l'« aigle de Meaux » de conclure en citant l'évangile de Luc : 
Dans son Dictionnaire historique et critique (1697), Pierre Bayle emprunte une bonne partie de son article sur Molière à l'édition des Œuvres complètes (1682), ajoutant : 
Au siècle suivant, Molière est  et est considéré comme un . Dès 1705, Grimarest, son premier biographe, estime que . Voltaire écrit une Vie de Molière (1739), Chamfort produit un Éloge de Molière et Diderot souligne son génie créateur[319].
Diverses pièces de Molière servent alors de canevas à des dramaturges satirisant les lieux de sociabilité des Lumières : Palissot (Le Cercle, 1755 ; Les Philosophes, 1760), Rochon de Chabannes (La Manie des arts, 1763), Poinsinet (Le Cercle, 1764), Jean-Jacques Rutlidge (Le Bureau d’esprit, 1776), Dorat (Les Prôneurs ou le Tartuffe littéraire, 1777), Jean-Louis Laya (L’Ami des lois, 1793).
En revanche, Jean-Jacques Rousseau reprend contre Molière les griefs que lui ont faits les dévots rigoristes du siècle précédent :
« Prenons [le théâtre comique] dans sa perfection, c’est-à-dire, à sa naissance. On convient, et on le sentira chaque jour davantage, que Molière est le plus parfait auteur comique dont les ouvrages nous soient connus ; mais qui peut disconvenir aussi que le théâtre de ce même Molière, des talents duquel je suis plus l’admirateur que personne, ne soit une école de vices et de mauvaises mœurs, plus dangereuse que les livres mêmes où l’on fait profession de les enseigner ? Son plus grand soin est de tourner la bonté et la simplicité en ridicule, et de mettre la ruse et le mensonge du parti pour lequel on prend intérêt ; ses honnêtes gens ne sont que des gens qui parlent, ses vicieux sont des gens qui agissent et que les plus brillants succès favorisent le plus souvent ; enfin l’honneur des applaudissements, rarement pour le plus estimable, est presque toujours pour le plus adroit[320]. »
Cette critique morale de Rousseau vaudra à Molière d'être quelque peu exclu du répertoire pendant les années de la Terreur (1793-1794)[321].
Le XIXe siècle redécouvre son théâtre, qui est célébré par Hugo, Gautier, Stendhal, Balzac et le critique Sainte-Beuve[322]. Dans Une soirée perdue (1840), Musset expose ses impressions après une représentation du Misanthrope, signalant tristement que  et admirant cette .
Molière est de loin l'auteur le plus souvent mis en scène à la Comédie-Française depuis sa fondation il y a plus de trois siècles : en 2008, cette institution totalisait 33 400 représentations de ses pièces contre 9 408 pour Racine et 7 418 pour Corneille. Ses comédies les plus souvent jouées sont Le Tartuffe, L'Avare et Le Malade imaginaire[323].
Très vite, la renommée de Molière dépasse les frontières et des traductions de ses pièces commencent à apparaître, la première étant celle de L'Amour médecin en néerlandais, en 1666[324], bientôt suivie par une adaptation de L'Étourdi en anglais, par Dryden (1667)[325]. Avant la fin du XVIIe siècle, son œuvre est traduite en italien par Nicolo Castelli[326]. Elle est traduite en anglais, d'abord partiellement par John Ozell (1714)[327], puis intégralement par Baker et Miller (1739)[328]. La personne de Molière inspire une étude à Luigi Riccoboni (Observations sur la Comédie et le génie de Molière, 1736) et la pièce Il Moliere à Carlo Goldoni (1751). Son œuvre est introduite au Japon à partir de 1868 par Koyo Osaki, où elle est immédiatement [329]. Elle est maintenant disponible, au moins partiellement, dans plus d'une cinquantaine de langues[330]. Selon Simone Bertière, [331]. Son sens aigu de l'observation le rend universel : [208].
Au terme d'une étude sur son œuvre — qu'il décrit comme une comédie de l'intelligence —, un critique américain concluait : [n 84].
À l'approche des célébrations entourant le quatrième centenaire de sa naissance, un dossier du journal Le Monde le présente comme étant  : Molière est l'incarnation du théâtre dans les pays de langue arabe ; introduit en Chine vers 1930, il y est le dramaturge français le plus connu, notamment grâce à L'Avare et Le Tartuffe ; au Sénégal, il a été récemment traduit en wolof ; toujours joué aux États-Unis, au Brésil et en Russie, son étoile a un peu pâli en Europe, où [332].
La critique moderne est divisée sur l'interprétation à faire de cette œuvre, car  Au contraire de René Bray, qui écrit, non sans provocation, que [334], Gérard Defaux estime que  et que son théâtre [335].
Les pièces se prêtent en effet à des lectures et interprétations parfois très divergentes comme le montrent les mises en scène : « Jouvet, qui n'a cessé de souligner le caractère totalement malléable de chaque pièce de Molière, faisait au milieu de notre siècle ce constat : « successivement romantique, symbolique ou réaliste, une pièce de Molière peut s'adapter encore au freudisme, au surréalisme, à l'existentialisme » ; les stars de la mise en scène de la période 1960-80, les Planchon, Chéreau, Bourseiller, Roussillon, ont incontestablement retenu la leçon[336]. »
Pour sa part, Bernard Sobel donne du Dom Juan une lecture sociologique, montrant [337]. De même, dans sa mise en scène du Tartuffe en 1990, il  et présente cette pièce comme [337]. Dès lors, le comique est évacué au profit du message politique.
À l'étranger, Molière a parfois aussi été replacé dans un contexte actuel. Ainsi, Bill Dexter , en utilisant une traduction-adaptation écrite à l'occasion du tricentenaire de la pièce par Tony Harrison[338]. Le Tartuffe a souvent aussi été transposé dans un cadre moderne, que ce soit dans la version anglaise de Ranjit Bolt en 2002[339] ou dans l'adaptation québécoise de Denis Marleau en 2016[340].
La paternité des œuvres de Molière est quelquefois l’objet de contestations depuis qu’en 1919 le poète et romancier Pierre Louÿs annonça, dans la revue littéraire Comœdia[341], avoir mis au jour une supercherie littéraire. Selon lui, Molière n'aurait pas écrit lui-même ses pièces et aurait eu Pierre Corneille pour , ou, plus précisément, Molière aurait été le prête-nom de Corneille.
Cette remise en question, quasiment oubliée après l'éclat de Pierre Louÿs, s'est renouvelée et un peu intensifiée depuis les années 2000, notamment avec la publication dans une revue scientifique anglo-saxonne de deux articles[342], dont le plus récent est le résumé d'une thèse de doctorat russe. Par des méthodes statistiques différentes, ces deux articles constatent la proximité entre le vocabulaire et la syntaxe des deux auteurs et en déduisent que la théorie de Pierre Louÿs est valide. L'un repose sur le calcul de la  du point de vue lexical ; l'autre repose sur l'analyse de données syntaxiques. Dans les deux cas, l'enquête n'a toutefois pas été élargie aux autres auteurs de comédies du XVIIe siècle pour vérifier si la proximité entre le vocabulaire et la syntaxe de Corneille et de Molière ne se retrouverait pas aussi chez leurs confrères. Or, justement, une étude plus récente a montré que, si l'on élargit le corpus à d'autres auteurs, la proximité observée entre certaines comédies de Corneille et de Molière n'a rien d'exceptionnel[343].
Un site internet, ouvert en 2011 sous la direction de Georges Forestier, déploie un ensemble d’arguments historiques, philologiques, stylistiques et lexicologiques, ainsi que des témoignages d’époque et des travaux récents qui réfutent la thèse de Louÿs[344] et recense les prétendues  qui ont pu donner du crédit à cette thèse[345].
Fin 2019, une nouvelle étude statistique, à l'aide de six méthodes différentes mais concordantes, attribue sans ambiguïté 37 pièces de Molière, Corneille et trois de leurs contemporains à leurs auteurs putatifs : Molière est bien un auteur différent des quatre autres et notamment de Pierre Corneille[346],[347].
Sa vie et des épisodes qui y sont rattachés sont mis en scène dans des pièces de théâtre, tels L'Impromptu du Palais-Royal de Jean Cocteau (1962) ou La Petite Molière de Jean Anouilh et Roland Laudenbach[348].
Mikhaïl Boulgakov consacre à l'affaire Tartuffe un drame intitulé La Cabale des dévots, dont il reprend ensuite des éléments dans Le Roman de monsieur de Molière (1933), qui tente de combler les vides que l'absence de documents a laissés dans la vie de l'écrivain.
Dans Baptiste ou la dernière saison (1990), Alain Absire expose les intrigues qui hantent la dernière saison du dramaturge[349].
Molière apparaît aussi comme personnage secondaire dans des films historiques, tels que Si Versailles m'était conté… (1953) et Si Paris nous était conté (1955) de Sacha Guitry, Marquise (1997) de Véra Belmont, Le Roi danse (2000) de Gérard Corbiau et Jean de La Fontaine, le défi (2007) de Daniel Vigne[351]. Au total, [351].
Plusieurs pièces de Molière ont donné lieu à des adaptations au cinéma, à la télévision, à l'opéra ou en bande dessinée. Les détails et références se trouvent dans les pages consacrées à chacune des pièces[355]. Les pièces les plus souvent adaptées sont : 
Sur les autres projets Wikimedia :Un article bibliographique spécifique serait utile (mars 2023). Compte tenu du nombre d'ouvrages ou d'études relatives au sujet de l'article, il serait utile de créer un article bibliographique spécifique. On garderait alors dans l'article principal les ouvrages biographiques ou de référence principaux ainsi que ceux utilisés pour écrire l'article.
« Mozart » redirige ici. Pour les autres significations, voir Mozart (homonymie).
Œuvres principalesListe des œuvres de Mozart
 Signature de Mozart
modifier Wolfgang Amadeus Mozart ou Johannes Chrysostomus Wolfgangus Theophilus Mozart[1], né le 27 janvier 1756 à Salzbourg (Principauté archiépiscopale de Salzbourg) et mort le 5 décembre 1791 à Vienne, est un compositeur autrichien de la période classique[2]. Il est considéré comme l'un des plus grands compositeurs de l'histoire de la musique européenne. Avec Haydn et Beethoven, il a porté à son apogée l'école classique viennoise.
Enfant prodige, il est produit en public dès l'âge de sept ans à travers l'Europe où il subjugue les assistances. Mort à trente-cinq ans, il laisse une œuvre considérable (893 œuvres sont répertoriées dans le catalogue Köchel). Selon le témoignage de ses contemporains, il était, au piano comme au violon, un virtuose.
Wolgang Amadeus Mozart a écrit dans tous les genres musicaux de son époque et a excellé dans chacun d'eux. On reconnaît généralement qu'il a porté à un point de perfection le concerto, la symphonie et la sonate, qui devinrent après lui les principales formes de la musique classique : il fut l'un des plus grands maîtres de l'opéra. Son succès ne s'est jamais démenti et son nom est passé dans le langage courant comme synonyme de génie et de virtuosité[3].
Mozart naît le 27 janvier 1756 à 8 heures du soir au numéro 9 de la Getreidegasse à Salzbourg. Il est le fils de Léopold Mozart, musicien, compositeur et pédagogue originaire d'Augsbourg, ville libre d'Empire, qui occupe alors la fonction de vice-maître de chapelle à la cour du prince-archevêque de Salzbourg, et d'Anna Maria Pertl, sa femme[4], fille d'un fonctionnaire de la cour de Salzbourg.
Comme Trèves, Cologne ou Mayence, Salzbourg est une principauté ecclésiastique du Saint-Empire, sous l'autorité d'un prince-archevêque, et rattachée au Cercle de Bavière. Elle est alors une petite ville (10 000 habitants), sur un des itinéraires joignant l'Empire et l'Italie, et tout entière centrée avec ses familles nobles, ses bourgeois, ses petits fonctionnaires et ses artisans sur la cour du prince-archevêque. Souabe par son père, salzbourgeois par sa mère et sa naissance, Mozart ne se dira jamais autrichien ou bavarois, mais toujours allemand.
Wolfgang est le cadet de sept enfants. Trois enfants sont morts en bas âge avant la naissance de sa sœur aînée Maria Anna (surnommée , née en 1751), et deux autres sont encore morts de maladie entre la naissance de Nannerl et la sienne[5].
Wolfgang est baptisé le lendemain de sa naissance dans une chapelle de la cathédrale Saint-Rupert de Salzbourg. Son acte de baptême porte les prénoms de Joannes Chrysost[omus][n 1] Wolfgangus[n 2] Theophilus. Theophilus, signifiant , a des équivalents, allemand (Gottlieb, prénom que son père lui attribue un mois après sa naissance), italien et latin (Amedeo prénom adopté lors de son voyage en Italie en décembre 1769)[6]. Wolfgang se fera appeler généralement « Wolfgang Amadè Mozart », mais s’amuse tout au long de sa vie à déguiser et à déformer ses différents noms en de Mozartini, Gangflow (Wolfgang à l’envers), Trazom, etc.[7]. Mais les signatures de sa correspondance ne comportent jamais le prénom Amadeus, qui ne sera employé qu'après sa mort.
Mozart est un petit garçon émotif et tendre, joignant la plus attentive docilité à une spontanéité primesautière, avide de tout apprendre (les mathématiques) et racontant des histoires avec une imagination débordante. Il s'épanouit au sein d'un foyer uni et aimant. Il joue avec sa sœur Nannerl, de peu son aînée et bonne musicienne, et reçoit l'enseignement du remarquable pédagogue qu'est son père. Dès l'âge de trois ans, il révèle des dons prodigieux pour la musique : il a l'oreille absolue et certainement une mémoire eidétique[8]. Ses facultés déconcertent son entourage et incitent son père à lui apprendre le clavecin dès sa cinquième année. Le jeune Mozart apprend par la suite le violon, l'orgue et la composition. Il sait déchiffrer une partition a prima vista et jouer en mesure avant même de savoir lire, écrire ou compter. À l'âge de six ans (1762), il compose déjà ses premières œuvres (menuets KV. 2, 4 et 5, allegro KV. 3 inscrits dans le Nannerl Notenbuch, « cahier de musique pour Nannerl »)[9].
Mozart ne reçoit pas d'autre éducation que celle donnée par son père. C'est cependant moins au génie en herbe qu'au virtuose que sa famille prend garde. Léopold a envie de faire connaître cet élève hors de pair et son maître, le prince-archevêque, autorisera des tournées qui feront honneur à sa cour.
Entre 1762 et 1766, le jeune Mozart entreprend le Grand Tour lors d'un long périple musical avec son père, employé par le prince-archevêque Sigismond de Schrattenbach, ainsi qu'avec sa sœur aînée Maria Anna qu'il appelle Nannerl. Ils vont d'abord à Munich, puis à Vienne, avant de s'engager, le 9 juin 1763, dans une longue tournée en Europe, qui les emmène de nouveau à Munich, puis à Augsbourg, Mannheim, Francfort, Bruxelles où il loge une nuit au château de Hasselbrouck, Paris, Versailles, Londres, La Haye, Amsterdam, Dijon, Lyon, Genève[12] et Lausanne.
Le jeune musicien émerveille les cours et les souverains, les dilettantes et les curieux. Il est cajolé et récompensé, parfois en espèces mais plus souvent en bagues ou en montres difficilement monnayables. On admire la simplicité naturelle dont il fait preuve avec les princes. Pour mieux éprouver sa virtuosité, on lui fait accomplir des prouesses comme jouer sur un clavier recouvert d'un drap. À Londres, le naturaliste Daines Barrington tente de montrer que Wolfgang n'est qu'une sorte de singe savant exhibé par son père devant la noblesse européenne et qu'il s'agit d'une supercherie, mais les épreuves auxquelles il soumet l'enfant révèlent qu'il est bien un prodige[13]. Le jeune Mozart démontre ses qualités exceptionnelles de virtuose non seulement au clavecin, et plus tard au pianoforte, mais aussi au violon et à l'orgue.
Jamais un apprentissage aussi riche et divers n'a été offert à un jeune musicien. Il rencontre deux musiciens qui vont le marquer définitivement : Johann Schobert à Paris, et Johann Christian Bach, fils cadet de Jean-Sébastien Bach, à Londres. Ce dernier lui fait découvrir le pianoforte, inventé au début du siècle, et l'opéra italien ; il lui apprend également à construire une symphonie. C'est déjà la moisson des premières œuvres : seize sonates pour violon et clavier, onze symphonies et en 1767, à l'âge de onze ans, un premier opéra, Apollo et Hyacinthus (K.38), une comédie latine destinée à être interprétée par les élèves du lycée dépendant de l'université de Salzbourg. 
De retour à Salzbourg, Mozart se rend régulièrement à Vienne, et, durant l'été 1768, compose deux autres opéras : Bastien et Bastienne et La finta semplice ; il n'a alors que douze ans. L'année suivante, le prince-archevêque Schrattenbach le nomme Konzertmeister (l'équivalent de premier violon). Onze plus tard, il n'aura toujours pas monté en grade.
Son père obtient un congé sans solde ce qui lui permet de faire découvrir l'Italie à son fils. De 1770 à 1773, il effectue trois voyages successifs en Italie : Vérone, Florence, Rome, Naples, Bologne, Venise et surtout Milan. Alors qu'il visite Rome, il entend le Miserere de Gregorio Allegri le mercredi de la Semaine Sainte, le 11 avril 1770. Après une seule audition, il aurait parfaitement retranscrit l'œuvre, morceau célèbre mais complexe, d'une durée d'un quart d'heure et alors non publié. Une autre version de l'anecdote mentionne une seconde écoute le Vendredi Saint, Mozart regardant cette fois sa transcription et y apportant quelques modifications[14]. Le pape Clément XIV le nomme chevalier dans l'ordre de l'Éperon d’or.
À Bologne, le père Martini, érudit illustre, l'initie au vieux style sévère et le fait recevoir à l'Académie philharmonique qui n'admet en principe que des membres âgés de plus de vingt ans. Mozart a alors quatorze ans et c'est la dernière haute distinction qu'il recevra de sa vie.
En Italie, Mozart étudie l'opéra, genre musical dans lequel il excellera, mais découvre surtout la bouffonnerie et le travestissement des masques, la concision dense et la netteté du trait, le brio d'une vivacité jamais alourdie. La musique italienne l'instruit moins qu'elle ne le révèle à lui-même en libérant son tempérament des docilités de l'enfance. Il italianise en Amadeo le dernier de ses prénoms, Gottlieb.
Les œuvres de cette période correspondent bien à cette découverte personnelle : symphonies, musiques de chambre, un premier opera seria, Mitridate (1770), une réussite formelle de virtuosité vocale, un oratorio, La Betulia liberata (1771, composé à Salzbourg entre deux voyages), un spectacle de cérémonie, Ascanio in Alba, un autre opera seria plus personnel, Lucio Silla (1772) qui ne reçoit qu'un demi-succès.
Le jeune Mozart qui a parcouru l'Europe n'a plus d'autre horizon que Salzbourg. Cette perspective est rendue d'autant plus étouffante par l'avènement, le 22 juin 1772, du nouveau prince-archevêque Hieronymus von Colloredo-Mansfeld. Prince éclairé et progressiste par certains côtés, le prince-archevêque Colloredo, à la différence de son prédécesseur Schrattenbach, est entiché de la seule musique italienne et bien décidé à mettre au pas les Mozart père et fils qu'il trouve arrogants et trop souvent absents. Son nouvel employeur lui impose la forme des pièces qu'il doit composer pour les cérémonies religieuses. À dix-sept ans, Mozart a du mal à accepter ces contraintes, et ses relations avec le prince-archevêque vont en se dégradant au cours des trois années qui suivent.
Mozart réagit à cette situation par une surabondance créatrice : son premier vrai concerto pour piano, son premier quintette à cordes, trois symphonies dont la première (K.183) des deux symphonies qu'il écrira en sol mineur, une partition pour le drame de Thamos. Cette poussée créatrice marque le début de la première maturité mozartienne. Une accentuation et une mobilité nouvelle dans l'expression des sentiments se fait jour, parfois jusqu'au tragique le plus brutal. Avec un dramatisme aigu et un art personnel pour combiner rythmes et mélodies, l'art du jeune Mozart ne ressemble déjà plus à aucun autre.
C'est à cette époque qu'il fait la connaissance, à Vienne, de son illustre aîné Joseph Haydn, avec qui il entretiendra tout au long de sa vie une correspondance et une amitié teintée d'admiration, réciproque. Mozart lui donnera le surnom affectueux de « papa Haydn », resté aujourd'hui encore vivace. Joseph Haydn à Léopold Mozart qui le rapporte : « Je vous le dis devant Dieu, en honnête homme, votre fils est le plus grand compositeur que je connaisse, en personne ou de nom, il a du goût, et en outre la plus grande science de la composition. »
Wolfgang Amadeus Mozart à propos de Joseph Haydn : « Lui seul a le secret de me faire rire et de me toucher au plus profond de mon âme. »
En 1776, Mozart qui a alors vingt ans, décide de quitter Salzbourg. Mais le prince-archevêque refuse de laisser partir son père et lui impose de démissionner de son poste de maître de concert. Après une année de préparatifs, il part avec sa mère, tout d'abord à Munich où il n'obtient pas de poste, puis à Augsbourg, et enfin à Mannheim, où il se lie d'amitié avec de nombreux musiciens au premier rang desquels Christian Cannabich dont il dira dans une lettre du 9 juillet 1778 qu'il était le meilleur chef d'orchestre qu'il ait jamais connu[15]. Toutefois, ses démarches pour obtenir un poste restent là encore infructueuses. C'est à Mannheim également qu'il tombe éperdument amoureux de la cantatrice Aloysia Weber, ce qui suscite la colère de son père, qui lui demande de ne pas oublier sa carrière. Couvert de dettes, Mozart comprend qu'il doit reprendre ses recherches et part pour Paris, au mois de mars 1778.
À Paris, Mozart espère trouver de l'aide auprès de Friedrich Melchior Grimm, qui s'était occupé de sa tournée lorsqu'il avait sept ans, mais sans succès ; l'homme de lettres lui reprochant "un manque de savoir-faire pour se mettre en valeur". Grimm met fin, déçu, au séjour de son jeune protégé. Mozart ne trouve pas non plus de poste qui lui convienne, et a même du mal à se faire payer ses leçons d'un noble qui le traite avec condescendance ; comportement des nobles en général qui marquera Mozart. Lors de ce séjour, sa mère Anna Maria tombe malade et meurt le 3 juillet 1778 rue du Gros-Chenet (actuellement au 8 rue du Sentier où se trouve une plaque commémorative) à Paris. Elle est inhumée à Paris après une messe à l'église Saint-Eustache en présence de son fils qui signe le registre paroissial de cette église[n 3].
Mozart rentre alors à Salzbourg où son père réussit à convaincre le prince-archevêque de le reprendre à son service. Sur le trajet de son retour, il passe par Munich où vit la famille Weber. Mais Mozart apprend qu'Aloysia aime un autre homme. Après tous ces événements malheureux, il arrive déprimé à Salzbourg le 29 janvier 1779. Il retrouve son ancien poste de Konzertmeister auquel Colloredo ajoute la fonction d'organiste de la Cour pour 450 florins par an.
En novembre 1780, il reçoit une commande pour l'opéra de Munich où il se rend comme son contrat l'y autorise. La création, le 29 janvier 1781, de Idomeneo, re di Creta (Idoménée, roi de Crète), opera seria, est accueillie très favorablement par le public. De retour à Salzbourg, Mozart doit suivre son employeur à Vienne, où le prince-archevêque le traite publiquement, après des remarques du jeune musicien jugées impertinentes, de  et de  avant de le congédier le 9 mai 1781[16]. Mozart s'installe alors dans la capitale autrichienne, dans la pension de Madame Weber, comme compositeur indépendant.
Mozart visita trois fois la ville de Mayence jusqu'en 1790[17].
Désormais débarrassé de l'autorité de son père et de son employeur, Mozart peut enfin composer plus librement, mais doit établir sa notoriété à Vienne.
Le 24 décembre 1781, à l'invitation de l'empereur Joseph II, il participe devant la cour à une joute musicale au pianoforte contre Muzio Clementi, célèbre virtuose du clavier tout juste arrivé à Vienne. Mozart a la préférence de l'empereur, Clementi celle de la grande-duchesse Marie-Louise. Les deux pianistes improvisent sur des thèmes imposés, déchiffrent à vue une partition autographe de Paisiello et jouent des morceaux de leur composition. Mozart interprète des variations sur le thème de Ah vous dirais-je maman !. L'empereur déclare la joute nulle et remet cinquante ducats à chacun. Le pianiste Ludwig Berger se souviendra de Clementi lui disant en 1806 de Mozart : « Jamais jusqu'alors je n'avais entendu quelqu'un jouer avec autant d'esprit et de grâce. J'ai été particulièrement impressionné par un adagio et par plusieurs de ses variations extempore, dont l'empereur avait choisi le thème, et que nous devions concevoir alternativement. »[18]
En 1782, Joseph II commande un opéra à Mozart. Ce sera Die Entführung aus dem Serail (L'Enlèvement au sérail), en langue allemande, qui incitera Gluck, compositeur et directeur des concerts publics à Vienne, à féliciter Mozart et sera l'opéra de Mozart le plus joué à Vienne. Joseph II est enchanté, voilà l'opéra allemand dont il rêve.
Mozart a fait la connaissance de la troisième fille de madame Weber, Constance, et décide de l'épouser sans attendre le consentement écrit de son père qui en sera furieux. Le mariage est célébré à Vienne le 4 août 1782 à la cathédrale Saint-Étienne. Peu après, le baron van Swieten, directeur de la bibliothèque impériale, lui fait découvrir deux compositeurs qui sont alors tombés dans l'oubli : Bach et Haendel. Mozart, homme de théâtre tout comme Haendel, admire les effets musicaux créés par ce dernier pour accentuer le caractère dramatique de ses œuvres. Il est en outre fasciné par l'art du contrepoint de Bach, qui influence directement sa Grande messe en ut mineur KV. 427, et nombre de ses œuvres par la suite. La même année, il commence une série de six quatuors dédiés à son ami Joseph Haydn, qui se terminera en 1785.
Pétri des idées des Lumières, Mozart entre le 14 décembre 1784 en franc-maçonnerie dans la loge Zur Wohltätigkeit (la Bienfaisance), et accède au grade de maître, le 13 janvier 1785[19]. Très épris des idéaux de la maçonnerie qui diffusent cette philosophie des Lumières, il écrit par la suite une douzaine d'œuvres pour ses frères maçons, dont Die Maurerfreude (La Joie des maçons, K. 471) en février 1785, la Maurerische Trauermusik (Musique funèbre maçonnique, K. 477) en novembre 1785, et surtout, en 1791, La Flûte enchantée (dit « opéra maçonnique ») KV. 620, qui serait une transcription de l'initiation à la franc-maçonnerie avec ses épreuves, son maître de cérémonie, la répétition de thèmes avec trois notes et une musique évoquant l'idéal maçonnique.
En 1786, Mozart fait la connaissance du librettiste Lorenzo da Ponte, « poète impérial » à Vienne avec un rang directorial comparable à celui de Salieri directeur musical du Théâtre d'opéra impérial et kappelmeister. Da Ponte, alors bien en cour, contrairement à Mozart, convainc l'empereur d'autoriser la création d'un opéra basé sur Le Mariage de Figaro, de Beaumarchais, alors qu'il avait fait auparavant interdire la pièce, jugée subversive. Mozart met en musique le livret de Lorenzo da Ponte, et la première de Le nozze di Figaro (Les Noces de Figaro) a lieu le 1er mai 1786 à Vienne. Son succès n'empêche pas son retrait rapide de l'affiche, l’œuvre mécontentant la noblesse viennoise. Mozart part alors à Prague, où Le nozze connaît un grand succès. En hommage à cette ville, il compose la Symphonie no 38 en ré majeur.
Il reçoit alors du directeur du théâtre de Prague, ville qui lui a fait fête, la commande d'un opéra pour la saison suivante. Mozart fait à nouveau appel à Lorenzo da Ponte librettiste à succès, pour créer le livret de Don Giovanni. Il s'inspire d'un opéra buffa italien de Gazzaniga produit à Venise sur un livret de Bertati quelques mois auparavant[20]. Le 28 mai 1787, son père, Léopold, meurt. Il avait rompu avec lui. Ce décès bouleverse Mozart, et va influencer la composition de son opéra alors en chantier. Don Giovanni est créé au théâtre des États de Prague le 28 octobre 1787 avec un grand succès, mais qui ne se confirmera cependant pas à Vienne. Mozart note Don Giovanni comme un opéra buffa, sans doute en raison du genre d'opéra, dans son catalogue[21], mais cet opéra sera publié et produit comme dramma giocoso, mêlant le comique et le tragique.
Le 7 décembre 1787, Joseph II, satisfait de Mozart, le nomme musicien de la chambre impériale et royale avec un traitement confortable de 800 florins par an. Il le charge de la musique de danse. Mozart tentera en vain d'obtenir le poste de Konzertmeister impérial, la fonction occupée par Gluck. À ce traitement, Mozart ajoute ses cours privés donnés à la noblesse ou à la bourgeoisie de Vienne, le fruit des concerts par souscription qu'il organise et qu'il dirige et des gratifications pour chacun de ses opéras. Des opéras qui ne connaissent pas un grand succès selon Robbins Landon, la Cour et le public préférant l'opéra napolitain de Paisiello et Martin y Soler notamment, bien qu'il s'inspire de ce style dans la trilogie, mais à sa manière. C'est cette manière qui à cette époque ravit les amateurs. Même Goethe qui admire Mozart, lui préfère Cimarosa. Après la mort de son protecteur Joseph II, Léopold II lui succède. Ce dernier ne semble pas apprécier Mozart qui perd sa situation, puis les faveurs de la noblesse, sans doute à cause du procès pour dettes intenté par le prince Lichnowsky à l'issue d'un voyage effectué en commun.
Durant les dernières années de sa vie, Mozart est souvent malade et chroniquement endetté, ceci malgré de nombreux succès très bien rétribués, car il mène grand train de vie. Il compose beaucoup : sonates, concertos, symphonies, opéras (dont Così fan tutte, sa dernière collaboration avec Lorenzo da Ponte). L'année 1790, qui voit le décès de l'empereur Joseph II (son successeur Léopold II n'est pas favorable aux francs-maçons) et le départ de Joseph Haydn pour Londres, est peu productive[23].
En 1791, Emanuel Schikaneder, franc-maçon comme lui, mais d'une autre loge, directeur d'un petit théâtre populaire de la banlieue de Vienne, le Freihaustheater auf der Wieden, sollicite sa participation à un opéra populaire en allemand. Il en écrit le livret, et Mozart écrit la musique de son avant-dernier opéra, Die Zauberflöte (La Flûte enchantée). Sa création le 30 septembre dans le théâtre privé de Schikaneder est un triomphe. Ce dernier a prévu de mettre en scène plusieurs opéras populaires de langue allemande inspirés de Lulu, ou La Flûte enchantée et les Garçons judicieux, tirés du recueil de contes intitulé Dschinnistan, de Wieland et Johann August Liebeskind (1786-1789). Le livret de La Flûte enchantée (Die Zauberflöte) représente un opéra féérique, mi-chanté, mi-parlé. D'après des recherches récentes[24], les airs de l'opéra émaneraient de compositeurs divers collaborant avec Schikaneder et pas seulement de Mozart, mais toute la musique aurait été attribuée à ce dernier. Il s'agirait donc d'une production collective[25] qui se serait poursuivie dans un autre opéra féérique Der Stein der Wiese. La Flûte enchantée passe pour avoir créé un 
En juillet, un inconnu lui aurait commandé un Requiem (KV. 626), qui devait rester anonyme. On sait aujourd'hui qu'il était commandité par le comte Franz von Walsegg, et on suppose que celui-ci souhaitait soit faire deviner à ses amis le nom de l'auteur, soit s'en attribuer la paternité. On a retrouvé le contrat entre le comte et Mozart selon le Dictionnaire Dermoncourt. Celui-ci, affaibli par la maladie et les privations, doit, en outre, faire face à une surcharge de travail, car il a reçu (début août) la commande d'un opéra (La Clemenza di Tito, KV. 621) pour le couronnement du roi de Bohême Léopold II, qu'il doit composer[21] en trois semaines. L'opéra est mal accueilli, l'impératrice qualifie l’œuvre  et de  ; quant à la cour, elle lui est hostile dès le départ (elle avait ) et n'aimait que l'opéra italien[27].
Mozart meurt le 5 décembre 1791, cinq minutes avant une heure du matin[28], à l'âge de trente-cinq ans, sans avoir pu achever ce Requiem (qui sera terminé à la demande de Constance par trois de ses élèves, Franz Xavier Süssmayer, Joseph Eybler, Freystadler et probablement l'abbé Stadler d'après Robbins Landon[29]). Les raisons de sa mort restent inconnues. Il était alors fiévreux, le corps gonflé et alité.
L'état de santé de Mozart au cours de sa vie et au moment de sa mort ont fait l'objet de nombreuses publications[30] et près de cent quarante causes possibles ont ainsi été citées par Lucien Karhausen, chercheur et psychiatre germanique[31] : grippe, hémorragie cérébrale, trichinose, obésité, syndrome maniaco-dépressif, fièvre rhumatismale aiguë par streptocoque[32], empoisonnement au mercure par Salieri jaloux (hypothèse peu vraisemblable[33]), par les francs-maçons furieux de voir leurs rites révélés dans La Flûte enchantée (hypothèse peu crédible car la Franc-maçonnerie éditait une gazette librement distribuée et n'était pas secrète à Vienne), ou par prise de la [34], hypothèse également peu vraisemblable et très peu évoquée qui met en cause Van Swieten père, médecin et ami de l'empereur François Ier d'Autriche. Pour Robbins Landon, les deux hypothèses vraisemblables sont que Mozart est mort [35].
La légende, reprise dans le film Amadeus de Miloš Forman (film inspiré du célèbre ouvrage sur Mozart de Hildesheimer) qui veut que Mozart ait composé ce Requiem en prémonition de sa mort prochaine relève plus de l'imagerie romantique que de la réalité. Mozart reçoit un enterrement de troisième classe, usuel pour la bourgeoisie moyenne à cette époque. Sa femme Constance laisse Gottfried van Swieten, ami et mécène du compositeur, organiser les funérailles : le service funèbre se déroule, sans messe ni musique[36], dans la chapelle du Crucifix, une chapelle latérale de la cathédrale Saint-Étienne de Vienne[37]. Le 6 décembre 1791 le corbillard conduit la dépouille à la tombée de la nuit au cimetière Saint Marx, dans la banlieue de Vienne, dans un des seize caveaux d'un , conformément aux règles d'inhumation viennoises, dit un seul auteur. Il fut en fait enterré, de l'avis général des ouvrages de référence, dans une fosse commune ordinaire; une fosse pouvant contenir seize corps avec des couches de terre par rangées de quatre selon le Dictionnaire Mozart[36], au tarif le moins cher, 8 florins et 36 kreutzers, comme la majorité des classes moyennes[2]. Être enterré dans une fosse commune anonyme[n 6] n'avait rien d'inhabituel. L'Empereur avait imposé une loi en ce sens, pour éviter que les Viennois ne se rendent aux cimetières pour rendre hommage à leurs morts et ramènent en ville des maladies. Mozart n'eut pas de croix, ce qui a choqué à l'époque les admirateurs du compositeur. Une légende non fondée veut que Joseph Rothmayer, un des fossoyeurs, note l'emplacement du corps en entourant le linceul d’un fil de fer et, lors du remembrement du cimetière en 1801, récupère le crâne supposé de Mozart pour le confier à un anatomiste viennois, qui en fera don au Mozarteum de Salzbourg et sera l'objet d'études anatomo-pathologiques[38]. Des analyses ADN récentes n'ont pas pu authentifier le crâne comme étant celui de Mozart. Si ni la famille ni les amis — sauf Salieri, Süssmayer, Deiber et van Swieten franc-maçon comme Mozart, cinq personnes en tout — n'accompagnent le cercueil à son inhumation, cela pourrait être en raison d'un décret impérial qui interdisait aux convois funèbres l'accès aux faubourgs en raison d'épidémies, dont le choléra[39]. Il est établi que devant l'inaction de la veuve de Mozart, plusieurs personnes ont ensuite cherché à retrouver ses restes dans le cimetière, en vain, les fosses communes étant régulièrement remaniées pour accueillir de nouveaux corps.
Il faut ajouter pour mieux comprendre la situation de la fin de vie de Wolfgang Amadeus Mozart, que, probablement joueur, très seul en raison de son caractère difficile,  selon sa sœur Nannerl, condamné par la Cour de Basse-Autriche à Vienne le 12 novembre 1791 et saisi pour une dette de 1 435 florins 32 kreuzers, à la demande du prince Karl von Lichnowski[40], pourtant son ami, il est décédé ruiné, quoique disposant d'un traitement confortable de musicien impérial de 800 florins par an, depuis 1787 grâce à la bienveillance de Joseph II. Le souverain appréciait Mozart, mais toutefois préférait, comme le public, les Italiens. Mozart ne connut pas le grand succès de son temps pour cette raison, à la différence de Gluck qui bénéficia en 1787, quelques années plus tôt, d'obsèques solennelles et d'un enterrement dans une belle tombe avec une pierre distinctive à son nom, le tout Vienne musical étant présent. À Salzbourg, Léopold Mozart, père de Wolfgang, et Michael Haydn, frère du grand Haydn et ami de Mozart, ont été l'objet d'obsèques plus relevées avec tombe individuelle et cortège officiel. On peut donc s'interroger sur la personnalité de Mozart et son probable rejet par l'aristocratie comme le fait le musicologue Robbins Landon au XXe siècle. D'autres musiciens que lui ont eu droit en effet à un traitement différent pour leurs obsèques, y compris avant 1791. Robbins Landon a recherché les raisons pour lesquelles Mozart n'avait pas été accepté par la société de Vienne, voire rejeté ; peut-être dit-il parce qu'il affichait ostensiblement son appartenance aux loges, alors qu'après 1789, le point de vue de l'aristocratie change à cet égard. Il y a aussi la condamnation à une peine de prison et saisie pour dettes, à la demande d'un prince actif à Vienne, découverte assez récemment par Robbins Landon et la manière dont Mozart traite la noblesse, qu'il déteste (Correspondance), dans plusieurs de ses opéras.  écrit Robbins Landon[41].
Un service commémoratif a lieu à Prague le 14 décembre, cette fois devant des milliers de personnes. Emanuel Schikaneder en organise un préalablement le 10 décembre 1791 à Vienne, au cours duquel le début du Requiem (Introït et Kyrie) pourrait avoir été chanté, la partie composée par Mozart lui-même[42].
En son honneur est érigé un cénotaphe conçu en 1859 par le sculpteur Hanns Gasser : une statue en bronze représente la muse de la musique assise sur un socle de granit. Elle porte dans sa main droite une partition du Requiem et dans sa main gauche, reposant sur une pile d'œuvres de Mozart, une couronne de laurier. Le monument est vandalisé à plusieurs reprises (1868, 1879) et à la suite de la fermeture du cimetière Sankt Marx en 1874, il est transféré en 1891 (l'année du centenaire de la mort du compositeur) dans le cimetière central de Vienne pour faire partie du « carré » des sépultures de grands musiciens comme Beethoven ou Strauss. Le groupement actuellement à Sankt Marx, constitué d'un « génie rêveur » appuyé à une colonne tronquée, est rajouté à la fin du XIXe siècle par Alexander Kugler, gardien de cimetière et admirateur du compositeur. Il entreprend de manière non officielle de refaire connaître ce lieu abandonné, à partir de sculptures récupérées sur des tombes voisines à l'abandon. Endommagé à la fin de la Seconde Guerre mondiale, le monument funéraire de Sankt Marx est restauré en 1950 par le sculpteur Florian Josephu-Drouot[43].
Mozart épousa Constance Weber (1763-1842) le 4 août 1782. Ils eurent six enfants en près de neuf ans :
Seuls deux des enfants, Karl Thomas et Franz Xaver Wolfgang, survécurent, passé la petite enfance.
De nombreux portraits présumés de Mozart semblent avoir été effectués de seconde main, probablement sans voir le musicien ou bien rétrospectivement après la mort du compositeur. Ils montrent des physionomies différentes et sont douteux quant à leur ressemblance. Le portrait peint vers 1782 par Joseph Lange, beau-frère de Mozart et peintre amateur, était considéré par sa femme Constance comme étant « de loin la meilleure image de lui »[44]. Mais le portrait de qualité le plus ressemblant est un portrait en miniature réalisé par Dora Stock le 16 ou le 17 avril 1789 lors d'un séjour de Mozart à Dresde. D'une grande finesse, il est dessiné selon la technique de la carta tinta sur un carton préparé de couleur ivoire. Il montre le compositeur en buste de profil à gauche. Le format ovale réduit (76 x 60 mm) et le portrait de profil sont courants dans les portraits en miniature de la fin du XVIIIe siècle. Ce portrait de 1789 est considéré comme le dernier portrait connu de Mozart réalisé d'après le modèle vivant.
Dans son livre de mémoires Reminiscences, le ténor Michael Kelly décrit Mozart comme un homme de petite taille (1,52 m, sa croissance ayant été probablement freinée lors de sa tournée européenne exténuante qui le privait de sommeil et d'hormone de croissance sécrétée la nuit[45]), pâle et maigre, la chevelure blonde, le visage grêlé par la petite vérole. Vêtu de manière élégante, il se révèle un grand séducteur[46].
Dans son livre Les confessions de Constanze Mozart, la romancière Isabelle Duquesnoy décrit Mozart comme blond (il a cessé de porter des perruques dès son arrivée à Vienne), aux yeux bleus, le regard doux, myope, gaucher et affublé d'une malformation congénitale à l'oreille. Sa femme sera accusée d'adultère, notamment avec un élève de Mozart (Süssmayer), qui l'avait accompagnée en cure à Baden durant une grossesse difficile ; les rumeurs cesseront lorsque l'enfant (Franz Xaver Wolfgang Mozart) naîtra, porteur de cette même malformation.
Mozart est, avec Haydn et Beethoven, l’un des principaux représentants du style classique . Cela ne suffit certes pas à le définir. Dans une époque dominée par le style galant, Mozart réalise la synthèse des complexités contrapuntiques propres au baroque tardif et des formes novatrices influencées notamment par les fils Bach ou par Haydn. Si Mozart est considéré comme le meilleur représentant du style classique, son style va toutefois bien au-delà : il est l’un des plus personnels et des plus immédiatement reconnaissables à l’oreille.
Né dans une famille de musiciens, tôt habitué à voyager et à rencontrer des instrumentistes et compositeurs d’horizons et nationalités différents, Mozart devient dès l’enfance un imitateur de génie et s’approprie tout ce qu’il entend. Il suit cette méthode tout au long de sa vie, notamment quand il s’agit de se familiariser avec le contrepoint, ce  (ou ) si difficile à assimiler à l’époque où on lui oppose le style galant dans lequel Mozart baigne depuis l’enfance. Mozart commence par transcrire plusieurs fugues de Bach pour trio à cordes, sur une commande de Van Swieten (KV. 404a), puis se consacre réellement à composer des fugues, non sans difficultés : celle entamée pour le final de la Sonate pour violon KV. 402 reste inachevée ; tandis que celles du Prélude et fugue KV. 394 composé en 1782 ou de la Suite dans le style de Haendel KV 399 sont d’une extrême complexité, qui traduit les difficultés rencontrées par Mozart dans l’étude du contrepoint. Pourtant, celui-ci nourrit la Messe en ut mineur KV. 427 entamée à la même époque. Dans les mois suivants, on retrouve des fugues pour vents (Sérénade KV. 388), pour piano (Fugue en do mineur pour 2 pianos KV. 426, par la suite transcrite pour orchestre dans l’Adagio et fugue KV. 546), et plus tard pour orgue (KV. 594 et KV 608). Puis, dans les années suivantes, Mozart abandonne la simple imitation, mais des œuvres bénéficient de ce travail : le final du Quatuor en sol majeur (KV. 387) ou le final de la Symphonie « Jupiter » (KV. 551), deux mouvements où la superposition des lignes atteint une maîtrise inégalée.
Il est impossible de définir Mozart par un genre précis. Opéra, symphonie, concerto, musique de chambre, musique sacrée… Mozart est un touche-à-tout qui s’approprie chaque genre, chaque forme, chaque instrument pour mieux le réinventer. Si les traits principaux du style classique sont bien présents dans ses œuvres (clarté de la structure et de ses articulations, équilibre de la formation, harmonie simple), si son don inné pour la mélodie est une évidence, Mozart en joue pour mieux faire ressortir tel motif, telle dissonance, surprendre par des audaces peu prisées de ses contemporains : quelques œuvres, à l’époque confidentielles, en portent la marque (comme la Fantaisie en ut mineur KV. 475 ou le Quatuor  KV. 465, dont l’introduction justifie le nom).
Mozart n’était pas pour autant un révolutionnaire. Il est l’auteur d’une abondante production de divertimenti, menuets et airs très conformes aux conventions de l’époque, sans jamais se laisser enfermer dans un registre. Lorsqu’il compose ses opéras, c’est chaque fois avec une alternance entre opéra buffa (Les Noces de Figaro, Così fan tutte) inspirés de l'opéra napolitain qui connait alors un grand succès et opéra seria (Idomeneo). Et son avant-dernier opéra rompt avec chacun de ces deux styles puisqu’il s’agit d’un singspiel, une opérette allemande chargée de symbolisme et, à vrai dire, inclassable : la Flûte enchantée qui prend place dans une série d'opéras populaires créés par Schikaneder, directeur de troupe.
Cultivé, curieux, sans cesse à l’écoute des inventions musicales ou artistiques de son époque, Mozart a su jusqu’au bout faire évoluer son style au gré des découvertes. On sent facilement l’influence débutante du Sturm und Drang allemand dans les dernières années mozartiennes (et pas seulement dans Don Giovanni ou dans le Requiem inachevé, qui reprend des thèmes du remarquable requiem de Michael Haydn, son ami). Le propre du génie mozartien est là : avoir su s’inspirer de ses contemporains sans jamais suivre d’autre modèle que le sien propre. La conscience de son génie lui donne une impertinence acérée qui fait partie de sa tournure d'esprit foncièrement anti-conformiste[47].
La force et la grâce, la puissance et l’émotion, le pathétique, l’humour, l’élégance la plus exquise sont réunis dans son œuvre pour faire de Mozart le compositeur le plus accompli de sa génération avec Haydn.
On peut dire de sa musique qu'elle a poussé la forme classique, la musique du XVIIIe siècle, à son paroxysme, avant l'avènement du romantisme, son génie est d'avoir mis toute la tendresse, toute la musicalité dans cette forme dite « classique ».
Mozart a eu une grande influence sur l’histoire de la musique, et ce directement auprès de ses contemporains. Il clôt une période plus qu'il en ouvre une autre. Même son aîné, Haydn, ami et admirateur de Mozart, en subit l’influence dans ses dernières symphonies et messes, et dans ses deux oratorios.
Les successeurs de Mozart n’y échappent pas : Beethoven, qui l'a rencontré en 1787, et Schubert, davantage encore, qui grandit à Vienne quelques années après sa mort, à l’époque même où le génie de Mozart est enfin unanimement reconnu. D’autres compositeurs, moins à l’avant-garde du romantisme, restent plus proches de l’esprit mozartien classique, notamment son élève Johann Nepomuk Hummel ou Louis Spohr. Les opéras de Gioachino Rossini doivent à Mozart en tant qu'auteur d'opéras buffe d'inspiration napolitaine, et ce n’est pas un hasard si l'italien choisit de mettre en musique Le Barbier de Séville de Beaumarchais, premier volet des mésaventures de Figaro ayant déjà fait l'objet d'un opéra de Paisiello (célèbre en son temps) que Mozart a connu à Vienne. Enfin, Mendelssohn, Chopin, Brahms, et même Busoni assument l’héritage de Mozart dans une grande partie de leurs œuvres, souvent à la même hauteur que celui de Bach alors peu joué.
Le disque et les œuvres de fiction contribuent à faire de lui le plus populaire des compositeurs classiques.
Quelques décennies après la mort de Mozart, plusieurs tentatives ont été faites afin d’inventorier ses compositions. Toutefois, ce n'est qu'en 1862 que le musicologue autrichien Ludwig von Köchel achèvera un catalogue chronologique de 626 œuvres, qui fait, aujourd’hui encore, figure de référence. Sa sixième édition recense désormais 893 œuvres.
Autres concertos :
Autres œuvres concertantes :
Liste de sonates pour piano :
Liste de autres pièces pour piano :
Liste des pièces pour orgue :
Liste des pièces pour cordes :
Liste des pièces pour vents :
Liste des pièces pour ensemble mixte :
Bien que certaines des premières pièces de Mozart aient été écrites pour clavecin, il s'est également familiarisé dans ses premières années avec des pianos fabriqués par le constructeur de Ratisbonne Franz Jakob Späth[48]. Plus tard, lors d'une visite à Augsbourg, Mozart a été impressionné par les pianos Stein et en a fait part à son père dans une lettre[48]. Le 22 octobre 1777, Mozart a créé son concerto pour trois pianos (K.242) sur des instruments fournis par Stein[49]. L'organiste de la cathédrale d'Augsbourg Demmler jouait la première partie, Mozart la deuxième et Stein la troisième[50]. En 1783, alors qu'il vivait à Vienne, il a acheté un instrument de Walter[51]. Leopold Mozart a confirmé l'attachement que Mozart avait pour son pianoforte Walter : "Il est impossible de décrire l'agitation. Le pianoforte de votre frère a été déplacé au moins douze fois de sa maison au théâtre ou chez quelqu'un d'autre"[52].
Mozart ne peut pas être considéré comme écrivain. Toutefois, son abondante correspondance, qui a fait l’objet d’éditions partielles puis complètes, n’est pas seulement une source importante pour la compréhension du compositeur et de son époque, mais également une œuvre d’une qualité littéraire certaine bien qu'elle soit avant tout utilitaire et dépourvue de commentaires sur la musique et la vie musicale et culturelle de son temps.
L’Autriche considère Mozart comme l’un de ses enfants les plus illustres[n 8] bien qu'historiquement il soit né dans la principauté ecclésiastique de Salzbourg incluse alors dans le cercle de Bavière et non dans le cercle d'Autriche. Salzbourg ne devient en effet autrichienne qu'en 1805 lors des guerres napoléoniennes.
Dans sa correspondance, Mozart se dit allemand, dans un sens général et non un sens administratif, l'ensemble politique du Saint-Empire romain germanique ne donnant aucune postérité étatique[53]. Mozart est donc avant tout un Salzbourgeois de langue allemande, et par extension un sujet du Saint-Empire romain germanique, auquel participe également l'Autriche et qu'elle domine en partie (depuis le XIIIe siècle, la couronne impériale est dans la famille régnante autrichienne des Habsbourg).
Quatre œuvres de Mozart ont été interprétées lors du traditionnel concert du nouvel an à Vienne :
Pour les articles homonymes, voir Marco Polo (homonymie).
modifier - modifier le code - modifier WikidataMarco Polo[n 1] (né le 15 septembre 1254, à Venise, et mort aux alentours du 9 janvier 1324, à Venise également)[1] est un marchand vénitien (actuelle Italie), célèbre pour son voyage en Chine, qu'il raconte dans un livre intitulé Devisement du monde ou Livre des merveilles ou encore Livre de Marco Polo.  
À l'âge de 17 ans, Marco Polo part avec son père, Niccolò, et son oncle Matteo pour l’Asie, où il se met, avec eux, au service de Kubilai Khan, l'empereur mongol. Après avoir exercé diverses missions officielles durant une vingtaine d'années, il entreprend son voyage de retour à l'occasion d'une mission diplomatique.
Après un périple de 24 ans, il est de retour en Italie en 1295. L'année suivante, il participe à une guerre navale entre Venise et Gênes, au cours de laquelle il est fait prisonnier par les Génois. Durant son emprisonnement, il dicte à un compagnon de cellule, Rustichello de Pise, une description des États de Kubilaï et de l'Orient. Ce manuscrit ayant connu de nombreuses versions et traductions, il est pratiquement impossible d'en reconstruire l'état original. Il semble toutefois qu'il ait été d'abord rédigé en langue franco-vénitienne.
Marco Polo n’était pas le premier Européen à se rendre à la cour de l'empereur mongol, mais il est le premier à décrire des réalités chinoises, tel le papier monnaie. Il décrit aussi les lamaseries du Tibet et mentionne l'existence du Japon (Cipango), jusqu'alors inconnu. Son récit a influencé Christophe Colomb et d'autres voyageurs. L'atlas catalan et la carte de Fra Mauro sont établis en partie sur la foi de son récit.
Marié, père de trois filles, il meurt en 1324 et est enterré dans l’église de San Lorenzo à Venise.
Marco Polo est né le 15 septembre 1254 dans la République de Venise, très probablement à Venise[n 2]. Il n'est pas élevé par son père Niccolò Polo, négociant vénitien spécialisé dans le grand commerce oriental et très souvent absent, mais par son grand-père Andréa Polo, lui aussi grand commerçant selon le modèle typique du capitalisme familial. Son père et son oncle Niccolò et Matteo Polo partent en effet en 1260 pour le quartier vénitien de Constantinople où ils possèdent plusieurs comptoirs. Lorsque la capitale de l'empire latin de Constantinople est reprise en 1261 par les forces de l'empire de Nicée de Michel VIII Paléologue qui chassent les Latins de la ville, Niccolò et Matteo Polo cherchent d'autres débouchés commerciaux en Asie centrale en s'installant dans le petit comptoir de Soldaïa, sur les bords de la mer Noire, qui vient de s'ouvrir aux marchands occidentaux avec la quatrième croisade[2].
Marco Polo a quinze ans lorsque son père et son oncle reviennent en 1269 d'un long voyage en Asie centrale où ils ont rencontré en Chine le premier empereur mongol de la dynastie Yuan, Kubilai Khan, petit-fils de Gengis Khan, qui leur propose le monopole de toutes les transactions commerciales entre la Chine et la Chrétienté et demande en échange l'envoi d'une centaine de savants et artistes pouvant illustrer l'Empire des chrétiens. Ils sont porteurs d'un message de sympathie et de cette demande pour le pape, qui voit dans ces tribus (appelées alors tartares en Occident) depuis 1250 un possible allié dans la lutte contre l'Islam. Pendant deux années, les deux frères, Niccolò et Matteo, vont attendre l'élection d'un nouveau souverain pontife, Grégoire X, le conclave s'éternisant depuis la mort de Clément IV en 1268[3].
En 1271, à titre de commerçants mais aussi d'ambassadeurs, ils quittent à nouveau Venise pour retourner en Chine avec le jeune Marco. Ils sont accompagnés de deux dominicains menant une mission diplomatique au nom du pape, Nicolas de Vincenza et Guillaume de Tripoli, mais ceux-ci abandonneront l'expédition à Lajazzo par peur des rumeurs de guerre[4]. À partir du comptoir vénitien de l'Ayas, ils empruntent la plus septentrionale des routes de la soie. Après trois ans de voyage, Marco Polo est reçu avec ses parents à la très fastueuse cour mongole, peut-être à Cambaluc. D'abord, semble-t-il, envoyé en légation avec son oncle dans la ville frontière de Ganzhou, à l'extrémité ouest de la Grande Muraille, où il fait ses classes (apprenant probablement le ouïghour), il devient ensuite un enquêteur-messager du palais impérial suzerain de la Chine, de l'empire Perse et de la Horde d'or. À ce titre il accomplira diverses missions pour le grand khan, tant en Chine que dans l'océan Indien (voir fonctions de M. Polo) : Corée, Birmanie, Sumatra, Cambodge, Viêt Nam (par contre il ne mentionne l'île de Cypango, le Japon, que par ouï-dire)[5].
Vers la fin du règne de Kubilai Khan, Marco Polo et ses parents obtiennent le droit de retourner dans leur pays contre un dernier service officiel : en 1291 ils embarquent à destination de la Perse, où ils accompagnent la princesse Kokejin, promise par Kubilai Khan à l'ilkhan Arghoun d'Iran[n 3]. Beaucoup d'incertitudes subsistent sur le trajet exact qu'il a suivi. En 1292, bloqué par la mousson d'hiver, il fait escale durant cinq mois à Perlak dans le nord de l'île de Sumatra (dans l'actuelle Indonésie). Il arrive à Ormuz au printemps 1293 et séjourne en Perse durant plusieurs mois[6]. À Trébizonde, plus ou moins sous l'influence des Génois, il est dépouillé d'une partie de sa fortune[7].
Rentrés à Venise en 1295, Marco et ses parents sont méconnaissables après un quart de siècle d'absence. La légende veut que, pour frapper l'imagination, ils aient offert à leurs parents et amis un grand banquet à l'issue duquel Marco se serait saisi des misérables vêtements tartares dont il était habillé et en aurait défait les coutures pour en extraire des pierres précieuses en quantité[8].
En 1296, la guerre ayant éclaté entre Venise et Gênes, Marco Polo fait armer une galère pourvue d'une pierrière[n 4] afin de participer au combat. Il est fait prisonnier, probablement lors d'une escarmouche, en 1296, au large de la Turquie, entre Adana et le golfe d'Alexandrette[n 5]. Au cours de ses trois années de prison, devant l'intérêt que suscitent ses souvenirs d'Orient, il décide de les faire mettre par écrit par son compagnon de captivité, Rustichello de Pise. À cette fin, selon Ramusio, il aurait demandé à son père de lui faire parvenir les carnets de notes qu'il avait rapportés de son voyage[9]. Rustichello date son récit de 1298[7].
En 1299, avec la signature de la paix entre Gênes et Venise, Marco est libéré. Il épouse alors Donata Badoer, dont il aura trois filles. Sans doute fut-il, comme patricien, membre du Grand Conseil de Venise, mais on ignore quel rôle il joua dans la création en 1310 du Conseil des Dix (institution secrète peu ordinaire qui ressemble au Tchoû-mi-Yuan, le conseil de sécurité de Kubilai). M. Polo vit alors à Venise dans la Casa Polo (quartier de Cannaregio, maison familiale détruite par un incendie en 1598[n 6]) où il vit désormais comme un commerçant prospère mais prudent, bien loin de l'image du grand explorateur[10].
Tombé malade, il dicte son testament le 8 janvier 1324. Le texte, qui en a été conservé, précise notamment qu'il lègue 5 lires à chacun des couvents installés sur le Rialto et 4 lires à chacune des guildes dont il est membre. Il libère aussi Pierre, son « serviteur tartare », et veut qu'il lui soit payé 100 lires[n 7]. Il est enterré comme son père à l'église San Lorenzo mais sa tombe a disparu à la suite de différentes restaurations de l'édifice[11],[12]. Son testament permet d'estimer la fortune qu'il laisse, soit 10 000 ducats, ce qui ne le situe pas parmi les plus riches marchands de Venise[7].
La Casa Polo où a vécu Marco Polo à Venise.
Armoiries des Polo[n 8].
La Plaque sur le théâtre Malibran.
L'église San Lorenzo.
Partis de Venise avant la naissance de Marco, Niccolò et Matteo Polo achètent vers 1255 des pierres précieuses à Constantinople (alors sous administration vénitienne) et en Crimée (où résidait leur frère), puis vont les vendre à la cour du khan de Russie, sur la Volga, où ils restent un an. Ils poussent jusqu'à Boukhara (alors capitale perse d'Asie centrale) où ils restent trois ans. Un enquêteur-messager de Kubilai ou de l'ilkhan d'Iran les invite à se présenter au grand khan, en qualité d'Européens.
Compte tenu du contexte des croisades, l'historien Pierre Racine doute que le voyage des Polo ait été de simple nature commerciale : « L’on doit alors s’interroger sur le but véritable des Polo lors de leur première expédition. Outre les intérêts commerciaux, n’y avait-il pas chez eux le désir d’approcher les Mongols à des fins politiques ? N’avait-il pas une sorte de mission à remplir ? Ils seraient en quelque sorte venus relayer des religieux qui, tels Jean de Plan Carpin, Guillaume de Rubrouck et André de Longjumeau, avaient été chargés de se renseigner sur ce peuple encore mal connu en Occident ? Le texte de Marco demeure fort silencieux à ce sujet. Ce qui transparaît cependant mérite d’être retenu. Les Occidentaux voulaient approcher les Mongols et nouer un accord avec eux, tout en constatant qu’il y avait chez ce peuple conquérant une culture à découvrir[13]. »
Ont-ils atteint Pékin quand ils rencontrent Kubilai en 1265 ou 1266 ? Il n'est pas nécessaire de le supposer, les affaires de l'ouest se traitaient souvent à sa résidence d'été en Mongolie, Shangdu aussi appelée Xanadu. Ils ne restent pas longtemps car ils sont chargés de plusieurs missions :
Le parcours exact est difficile à établir pour plusieurs raisons. D'abord, l'objectif du récit n'est pas de donner un journal de voyage mais une description (« devisement ») des choses vues susceptibles d'intéresser le lecteur par leur étrangeté. Dans un texte rédigé plus de vingt ans après les événements, les imprécisions sont parfaitement compréhensibles. Enfin, nombre de villes traversées peuvent avoir disparu ou ont vu leur nom modifié, parfois plusieurs fois, comme c'est souvent le cas en Chine : Quinsai s'appelle aujourd'hui Hangzhou; Campision est devenu Kan-tcheou puis Zhangye; Sacion s'est appelée Shachou puis Dunhuang; Carcan est devenue Shache; Ciarciam est aujourd'hui Qiemo; Quengianfu s'est appelée King-tchao avant de devenir Xi'an[14].
Yamashita (2004) donne l'itinéraire suivant :
Le Livre de Marco Polo pourrait s'intituler le Livre de Kūbilaï Khān car il décrit, non l'histoire de Marco, mais l'empire du plus puissant empereur de l'Histoire du monde. Quand le livre évoque la Russie, l'Asie centrale, l'Iran, l'Afghanistan, c'est parce que Kūbilaï était le suzerain de ces terres. Quand il parle du Japon (qu'il dénomme Cypango), du Viêt Nam, de la Birmanie, c'est parce que Kūbilaï Khān y envoyait des armées. Quand il présente le Sri Lanka, l'Inde du sud et jusqu'à Madagascar, c'est que Kūbilaï Khān y dépêchait des émissaires pour obtenir leur soumission. Quand il décrit les côtes de l'océan Indien, de l'Inde, de l'Arabie et de l'Afrique, c'est que les marchandises de la Chine y parvenaient.
Kūbilaï Khān est le sujet, le centre et l'unité du livre. Tout ce que M. Polo relate n'a de sens que par lui. Aussi est-il naturel que certains manuscrits aient donné pour titre à cet ouvrage Le livre du Grand Khan[16]. Ce livre est aussi un condensé des histoires que Marco lui racontait, car il avait su le séduire par ses talents d'observateur et de narrateur[n 9]. Certains historiens ont voulu y voir une encyclopédie, une géographie, d'autres une chronique du grand khaân, un miroir des princes, un livre de marchand[17],[18], mais il correspond plus exactement à un reportage[19].
Envoyé de l'empereur, ses déplacements étaient des missions, avec insignes du palais central et souvent escorte militaire. Au service de Kubilai, M. Polo ne dépendait pas du gouvernement ni de l'administration chinoise, mais directement du palais de l'empereur, le suzerain mongol, le khagan. Il n'était pas fonctionnaire mais homme de l'empereur. Les déplacements effectifs de Marco Polo entre 1271 et 1295 semblent les suivants :
Outre qu'il est allé dans le Sichuan (ch. 115), aux confins de la Birmanie (ch. 120) et dans les vallées du Yunnan (ch. 117), Marco a aussi voyagé dans les régions méridionales : 
Selon Pierre Racine (2011), il semble que, dans le ch. 145 sur le siège de Saianfu[n 10], Marco cherche à tromper le lecteur et 
« entend donner le beau rôle à la famille en lui attribuant l’invention des trébuchets [ou pierrières], bien connus pourtant avant l’arrivée des Polo en Chine
[22] »
En fait, le texte en franco-vénitien semble impliquer moins l'invention de trébuchets, que la fabrication d'un modèle plus efficace :
« A donc distrent les .II. freres et lor filz meser Marc. "Grand Sire, nous avons avec nos en nostre mesnie homes qe firont tielz mangan qe giteront si grant pieres qe celes de la cité ne poront sofrir mes se renderont maintenant »
— Le devisement du monde, CXLV, ed. Mario Eusebi, p. 163Selon les Annales chinoises, le siège de cette ville par les armées mongoles a duré six ans, de 1268 à 1273, et s'est terminé avant l'arrivée des Polo en Chine (1275). Igor de Rachewiltz soutient que la phrase « et lor filz meser Marc » n'est pas présente dans tous les manuscrits et peut donc être un enjolivement successif[23]. Il est attesté que, 
« après trois ans de siège infructueux, le général mongol a demandé un renfort technique et des machines de guerre. Celles-ci auraient été réalisées par des ingénieurs musulmans venus de Perse, Ismaïl et Ala al-Din, qui rejoignirent le théâtre des opérations vers la fin de l'année 1272
[24]. »
Selon les Annales Yuan : « En réponse au khaân, l'ilkhan Abaqa envoya Alaowating et Isemayin avec leur famille jusqu'à Pékin, où une première pierrière fut montée devant les Cinq Portes et essayée ». En 1273, quand Xiangfan tombe aux mains des Mongols après un siège de cinq ans, c'est grâce à des pierrières : , notamment sur le fleuve Yangtze où la flotte Song fut anéantie. L'année suivante, l'empire Song se rend enfin aux Mongols.
Selon certaines interprétations, les parents de Marco — qui sont rentrés à Venise en 1269 — auraient proposé les trébuchets à Kubilai, fait réserver des madriers, et été les messagers dépêchés à l'ilkan Abaqa, lequel fit réquisitionner les ingénieurs[n 11].
Voici ce que disent les annales officielles de la dynastie Yuan :
Il n'y a pas une preuve irréfutable que les deux idéogrammes chinois[27] qui se réfèrent phonétiquement à « Po-lo » correspondent vraiment à Marco Polo. En effet, des références à Po-lo existent bien avant l'arrivée de Polo en Chine. Cela dit, les inscriptions ci-dessus correspondent exactement au livre :
S'il amasse avec ses parents un trésor en pierres précieuses, il ne dit pas que ce fut par le commerce ; leurs émoluments et les cadeaux de Kubilai durent suffire à leur constituer une fortune. S'ils étaient désignés comme « marchands », les patriciens vénitiens étaient souvent aussi officiers d'active, diplomates, conseillers d'État.
Comme le note l'historien Pierre Racine, Marco est . Dans ce qui est essentiellement un carnet de voyage, Marco accorde une attention particulière aux . Il décrit aussi .
Il porte un intérêt particulier aux pierres précieuses : .
En marchand avisé, il est aussi intéressé par les épices, mentionnant . Il s'intéresse aussi aux divers types de tissus, qu'il désigne par les termes techniques locaux — cendal, bougueran, moselin, nach, nasich — et signale au passage les endroits où l'on fabrique les soieries épaisses lamées d’or[31].
On peut se demander avec Pierre Racine quel est son véritable visage :  Selon Borlandi, ce serait d'abord un marchand qui écrit pour un public de marchands : 
Polyglotte, Marco Polo parlait vraisemblablement le mongol, le chinois, le persan, le ouïgour et l'arabe[n 14]. Il maîtrisait aussi quatre systèmes d'écriture[n 15].
À travers son récit, il fait preuve d'une grande sensibilité à la diversité des sociétés et . Loin d'opposer sa culture à celles qu'il découvre, il .
Adoptant le ton neutre des encyclopédies, au lieu de donner des renseignements sur son voyage proprement dit, il accumule les observations factuelles sur les pays visités : géographie, distances, faune, alimentation, habillement, curiosités, grandes dimensions des jonques de mer chinoises, présence de pirates dans la mer de Java, etc.[35]. Il marque volontiers son émerveillement devant la richesse de l'empereur, l'intense activité des ports, l'usage exclusif du papier monnaie, l'empereur ayant seul le droit d'accumuler or et argent[36].
En ethnologue, il s'intéresse aux pratiques sociales et religieuses d'Extrême-Orient : bouddhisme lamaïste, taoïsme (ch. 74), islam, religions dérivées du christianisme (nestoriens, jacobites, culte de saint Thomas) ainsi que les peuplades animistes qui adorent des idoles. Mais il s'arrête aux aspects extérieurs et  
Il porte rarement un jugement sauf dans des cas extrêmes. Ainsi, il est horrifié par la coutume d'une tribu de Sumatra où les malades que le sorciers jugent inguérissables sont étouffés, mis à cuire et mangés en famille, sans en rien laisser —  (ch. 165) —, ceci afin que l'âme du défunt ne se charge pas de vers morts[n 16].
En escale à Ceylan (« Selyam »), il mentionne le Pic d'Adam, lieu de pèlerinage pour les musulmans, qui y vénèrent les reliques d'Adam, ainsi que pour les bouddhistes, qui en font le lieu de naissance du Bouddha et y vénèrent ses cheveux, ses dents et son bol à aumônes (Ch. 168). Se basant sur la tradition chrétienne, Marco écarte l'hypothèse que ce serait le lieu de naissance d'Adam et ne retient que le récit du Bouddha. Il se pose ainsi en .
Ce livre illustre également le monde de légendes que constituait l'Extrême-Orient chez les chrétiens : il croyait que Gog et Magog étaient les Mongols cruels ; l'arbre sec marque la limite entre l'Orient et l'Occident ; la « Barrière d'Alexandre » que constitue le Caucase est une frontière dangereuse à franchir ; il imagine le Royaume du prêtre Jean en Inde, etc.[7].
 est un leitmotiv de son livre. « Incroyable mais vrai » est sa recette. Cependant il est douteux qu'il ait été accueilli avec scepticisme à son retour par les patriciens de Venise : la République avait les moyens de savoir qu'il n'affabulait pas. De même les Génois qui lui firent rédiger son mémoire (dont ils avaient besoin pour leurs expéditions), et le frère du roi de France qui dépêcha pour en obtenir copie.
Marco Polo émaille son reportage de faits divers, de mythes, de légendes, mais ses récits de miracles sont peu nombreux, souvent symboliques, et séparés des autres narrations. Il démystifie plutôt les légendes (Arbre sec, Gog et Magog, prêtre Jean, salamandre). Les bourdes sont rares : hommes à queue de Sumatra, jambes de boas dans le Yunnan (mais l'histoire naturelle référence des boas ayant des traces de pattes), enfin l'obscurité en plein jour dont il témoigne en Iran[n 17]. En effet, dans le premier chapitre Rustichello explique que son livre fera toujours la distinction entre ce que Marco a vu de ses propres yeux et ce qu'il a entendu avec ses oreilles, afin de permettre au lecteur de distinguer le vrai du vraisemblable (chap. 1).
« Et por ce met{r}eron les chouses veue por veue et l’entendue por entandue, por ce que notre livre soit droit et vertables sanç nulle mensonge; et chascuns que cest livre liroie, ou hoiront, le doient croire, por ce que toutes sunt chouses vertables[39] »
L'histoire racontée par Ramusio[40], selon laquelle Marco Polo et ses parents se seraient présentés en habits de mendiants, avec une doublure pleine de rubis et joyaux qu'ils montrèrent au cours d'un dîner pour se faire reconnaître, est un enjolivement tardif (1559).
Paru en 1298, le livre de reportage qui a rendu Marco Polo célèbre est l'un des premiers ouvrages importants en langue vulgaire[n 18]. Le Devisement du monde[42], que l'on trouve aussi sous d'autres dénominations comme Il Milione ou Le livre des merveilles, est un des rares ouvrages manuscrits, avec La Légende dorée de Jacques de Voragine et Le Roman de la Rose (Guillaume de Lorris et Jean de Meung), à connaître un succès considérable avant même sa première impression à Nuremberg en 1477. Ce succès est en partie dû à sa rédaction initiale en français, langue de communication en vigueur à l'époque, que maîtrisait Rustichello de Pise, l'écrivain qui a transcrit les mémoires de Marco Polo alors qu'il était son compagnon de détention durant les guerres opposant Venise à Gènes en 1298.
En dépit du succès rencontré, l'ouvrage était surtout lu comme un récit fantaisiste et ce n'est que cinquante ans après la mort de Marco que son livre commencera à avoir quelque influence sur la cartographie[43]. L'atlas catalan de 1375 intègre les informations données par Marco Polo pour dessiner la carte de l'Asie centrale et de l'extrême Orient, ainsi que, partiellement, pour l'Inde : même si les noms sont déformés, Cathay est bien situé à la place de la Chine[44].
De même, la mappemonde de Fra Mauro détaille la Via mongolica, voie de Mongolie des épices et de la soie[n 19]. Cet ouvrage servira de référence pour les explorateurs ultérieurs. Au XIVe, il inspire Andalò da Savignone, auteur de quatre voyages (1330, 1334, 1336 et 1339), Galeotto Adorno (it) et Gabriele Basso[45]. Au siècle suivant, il inspire Vasco de Gama et Christophe Colomb. Ce dernier, lors de son troisième voyage, avait emporté le Devisement et l'avait scrupuleusement annoté (son exemplaire en latin compte 366 notes de sa main)[46].
Marco Polo n'a pas laissé de carte de ses voyages. Toutefois, au milieu du XXe siècle, Marcian Rossi, Américain d'ascendance italienne, a présenté une douzaine de parchemins contenant des cartes et de courts textes en prose censés avoir été réalisés par les trois filles de Marco Polo : Moreta, Fantina et Bellela. Le professeur Benjamin Olshin a décrit ces documents dans un ouvrage intitulé The Mysteries of the Marco Polo Maps (2014). Après analyse, toutefois, il est clair que ces documents sont tous largement postérieurs à Marco Polo, datant sans doute du XVIIIe siècle, comme le prouvent à la fois la datation au carbone 14, l'étude paléographique des textes en italien et les anachronismes flagrants en matière géographique et codicologique. Dans le compte rendu de cet ouvrage, Suzanne Conklin Akbari démonte l'argumentation d'Olshin comme étant entachée d'illogismes récurrents et visant à créer un pseudo-mystère en misant sur l'attrait que continue à exercer le nom de l'explorateur sur l'imaginaire contemporain[47].
Dès sa publication, le récit de Marco Polo suscite énormément d'intérêt et il est souvent recopié. Beaucoup le voient toutefois comme un récit inventé. Ce récit, qui témoigne de l’âge des premières explorations géographiques, décrit de façon émerveillée les richesses des traditions et coutumes asiatiques. Un passage célèbre consacré à la description enchanteresse de la résidence d’été du grand khan à Ciandu (maintenant Shangdu) en est un bon exemple. Ses récits au sujet de la richesse du Cathay (la Chine) sont d'abord accueillis avec scepticisme par les Vénitiens. Pourtant, plus d'un siècle plus tard, en 1430, un voyageur raconte que la ville de Venise avait installé un exemplaire de ce livre attaché par une chaîne dans un lieu public pour que chacun puisse le lire[48].
Son contemporain, le philosophe et médecin Pietro d'Abano décrit Marco Polo comme . Il signale des curiosités dont le voyageur lui a fait part, notamment  et raconte qu'il a rapporté de son voyage .
Même s'il a révélé l'existence du Japon (Cipangu), servi de base à des cartographes et inspiré l'expédition de Christophe Colomb[49], l'ouvrage continuera longtemps à être controversé, notamment en raison d'omissions marquantes (rien sur la Grande muraille ni sur le bandage des pieds des femmes) ou d'exagérations. Il connaît un regain d'intérêt au XIXe siècle, grâce aux récits de voyageurs britanniques, comme en témoigne le jugement de Baudelaire pour qui « les récits de Marco Polo, dont on s'est à tort moqué, comme de quelques autres voyageurs anciens, ont été vérifiés par les savants et méritent notre créance[51] ».
À la fin du XIXe siècle, Henry Yule, grand connaisseur de l'Asie et ancien haut fonctionnaire en Inde, a retracé le parcours suivi par Marco Polo et a produit une édition abondamment commentée du Devisement du monde[52], ne laissant aucun doute sur l'authenticité de ce voyage. En 1997, le voyageur Michael Yamashita a entrepris à son tour de reprendre la route de Marco Polo au cours d'un voyage qui a donné lieu à un reportage du National Geographic en mai 2001, suivi d'un livre en 2002. Au terme de cette expédition qui a duré quatre ans, il conclut : .
Pour l'historien Jacques Heers, toutefois, cet ouvrage n'est pas un récit de voyage, mais un traité encyclopédique fait de souvenirs de . La question de la véracité est encore soulevée en 1995 par Frances Wood avec son livre Did Marco Polo go to China?, qui suggère que Marco Polo n'a pas été en Chine. Ce qui a été aussitôt réfutée par plusieurs sinologues, dont Rachewiltz[55]. Philippe Ménard reconnaît que les chiffres donnés par le voyageur 
En 2012, évoquant la controverse sur la véracité du récit, l'historien Pierre Racine, tout en reconnaissant en Marco Polo certains traits de crédulité propres à un esprit médiéval, voit en lui  dont le récit  Pour cet historien, 
Cette même année, le sinologue Hans Ulrich Vogel, de l'université de Tübingen, établit qu'on ne peut trouver dans aucune autre source de l'époque — occidentale, arabe ou persane — des renseignements aussi précis que ceux que donne Marco Polo, par exemple sur le format et la dimension du papier, l'utilisation des sceaux, les dénominations du papier monnaie (fabriqué à partir d'écorce de mûrier) ou l'utilisation des coquillages au Yunnan[58]. Pour Mark Elvin, professeur à Oxford, les recherches de Vogel établissent que . En conclusion, note Philippe Ménard, professeur à la Sorbonne, il apparait, à l'examen du Devisement, que Marco Polo , au point que l'on peut supposer qu'il a été 
Le Livre des Merveilles eut un succès immédiat et tout l'Occident, qui venait de perdre des positions en Orient avec l'échec de la dernière Croisade, fut fasciné par ce récit. Henri le Navigateur, Vasco de Gama et Christophe Colomb lurent le livre au moment des Grandes Découvertes. La curiosité scientifique, caractéristique de l'Occident, montre une grande vivacité à ce moment[61].
En hommage à leur plus célèbre concitoyen, les Vénitiens ont baptisé de son nom leur aéroport international (Aéroport de Venise - Marco Polo), et les billets italiens de 1 000 lires ont longtemps porté son effigie. Le mouton de Marco Polo (en), appelé aussi mouflon de Marco Polo, est une sous-espèce d’Ovis aries. Le personnage de Marco Polo est le héros de nombreux livres et films.
Relations entre l'Empire romain et la Chine
Sur les autres projets Wikimedia :Autres Européens ayant rencontré le Khan au XIIIe siècle, mais sans aller jusqu'en Chine :
Famille :
Pour les articles homonymes, voir Faraday.
modifier Michael Faraday (Newington, 22 septembre 1791 - Hampton Court, 25 août 1867) est un physicien et chimiste britannique, connu pour ses travaux fondamentaux dans le domaine de l'électromagnétisme, l'électrochimie, l'induction électromagnétique, le diamagnétisme et l'électrolyse. Il donne son nom à de multiples lois et phénomènes dans ces domaines, notamment la loi de Faraday (ou Lenz-Faraday) en induction électromagnétique, les lois de Faraday en électrochimie, l'effet Faraday, ou encore à des dispositifs expérimentaux comme la cage de Faraday et la cavité de Faraday. Le farad, unité de capacité électrique, est également nommée en son honneur.
Michael Faraday naît le 22 septembre 1791, à Newington Butts, une bourgade du Surrey (Angleterre), aujourd'hui intégrée dans le grand Londres. Sa famille, pauvre, appartient à une secte, les sandemaniens, issus de l'Église d'Écosse. Son père, James Faraday, a été le forgeron du village de Outhgill dans le Westmorland, d’où il a émigré vers 1790. Le jeune Michael, issu d’une fratrie de quatre enfants, ne reçoit qu’une éducation primaire[1], principalement du fait que ses capacités n'étaient pas conformes au système scolaire, très rigide, de cette époque (il ne parvenait pas à répéter simplement des choses comme on lui demandait, il n'en comprenait pas l'intérêt).
Dès l'âge de 14 ans, il est apprenti auprès de George Riebau, un libraire-relieur et fait preuve de grands talents manuels et de curiosité : . Parmi ceux-ci, mentionnons le livre d'Isaac Watts, L’amélioration de l’esprit, dont il tirera les « six principes de Faraday » et les livres de vulgarisation scientifiques de Jane Marcet, dont Conversations sur la chimie.
En 1812, un des clients de la librairie lui offre des places pour assister à des conférences de chimie du chimiste Sir Humphry Davy, membre de la Royal Institution et de la Royal Society. Faraday est très vite impressionné et fasciné par les travaux que mène Davy auquel il écrit, joignant à sa lettre un livre de 300 pages basé sur les notes prises lors des conférences. À la suite d'un accident de laboratoire, Davy est blessé à l’œil gauche et fait appel au jeune Faraday, fin 1812, pour lui servir de secrétaire.
Le 22 février 1813[2], à la suite d'une violente dispute avec un collègue, William Payne, assistant de laboratoire de la Royal Institution, est licencié. Faraday est embauché pour le remplacer à compter du 1er mars 1813.
Le 2 juin 1821, Michael Faraday se marie avec Sarah Barnard (1800-1879), rencontrée à l'église glasite, mariage resté sans enfant.
Il est élu à la Royal Society en 1824, et nommé directeur du laboratoire de cette institution en 1825. En juin 1832, l'université d'Oxford le nomme docteur honoris causa en droit civil. S'il accepte ce titre honoraire et universitaire, Faraday rejettera son anoblissement au titre de chevalier et refusera par deux fois l'honneur de devenir président de la Royal Society. En 1833, il est le premier titulaire de la chaire fullerienne de chimie (Fullerian professorship) à la Royal Institution, sans obligation d'enseigner.
Ses problèmes de mémoire de plus en plus nombreux ne l'empêchent pas de continuer son travail, de tout noter et de faire de remarquables découvertes.
En 1848, sur proposition du prince-consort, Albert de Saxe-Cobourg-Gotha, Michael Faraday se voit attribuer une maison dans Hampton Court, libre de toute servitude. Cette maison, connue comme étant celle du maître-maçon, est plus tard appelée Faraday House, et se trouve au numéro 37, dans Hampton Court Road. En 1858, Faraday prend sa retraite et l'habite définitivement[3]. C'est là qu'il meurt, le 25 août 1867. Fondamentalement modeste, il avait refusé d'être enterré dans l'abbaye de Westminster (où une plaque, non loin de la tombe d'Isaac Newton, célèbre néanmoins sa mémoire) et sa tombe se trouve au cimetière de Highgate à Londres.
Ses plus grands travaux concernent l'électricité. En 1821, après la découverte du phénomène de l'électromagnétisme par le chimiste danois Ørsted, Faraday inverse l'expérience du danois en construisant deux appareils pour produire ce qu'il appelle une rotation électromagnétique : lorsqu'un câble électrique, trempant dans un bain de mercure au milieu duquel est placé un aimant statique, est traversé par un courant électrique, le câble se met alors à tourner autour de l'aimant. Par ce mouvement circulaire continu d'une force magnétique autour d'un fil, Faraday fait la démonstration du moteur électrique[4].
Dix ans plus tard, en 1831, il commence une longue série d'expériences. Le 29 août 1831 il découvre l'induction électromagnétique[5]. Ces expériences forment la base de la technologie électromagnétique moderne. Dans son travail sur le courant continu, Faraday démontre que la charge se situe seulement à l'extérieur d'un conducteur chargé, et que celle-ci n'a aucun effet sur ce qui peut être situé à l'intérieur : c'est l'effet de « blindage », utilisé dans la cage de Faraday.
Il a été l'un des principaux fondateurs de l'électrochimie en tant que discipline scientifique. En 1833, il introduit les termes d'anode, de cathode, d'anion, de cation et d'ion (sans pour autant connaître la notion de courant électrique, découverte plus tard par André-Marie Ampère).
Il a donné son nom au farad, l'unité SI de capacité électrique, ainsi qu'à une charge électrique, la constante de Faraday. Son portrait figure sur certains billets anglais de 20 livres.
Il a également donné son nom à l’instabilité de Faraday, mise en évidence en 1831, déclenchée lorsqu'un bain liquide est vibré verticalement avec une amplitude suffisamment importante. Lorsque cette instabilité est déclenchée, la surface du liquide se réorganise et des ondes de surface sous-harmoniques apparaissent...
Faraday réalise ses premières expériences en chimie alors qu'il est assistant de Humphry Davy. En étudiant le chlore il découvre deux nouveaux chlorures de carbone. Il conduit des expériences sur l'effusion des gaz, un phénomène identifié par John Dalton et dont l'importance sera mise en lumière par Thomas Graham et Joseph Loschmidt. Il réussit la liquéfaction de quelques gaz naturels, dont le chlore. Il analyse différents alliages d'acier et obtient des nouveaux types de verres à usage optique. L'un d'entre eux deviendra important pour la science puisque c'est grâce à lui que Faraday identifie la rotation du plan de polarisation de la lumière quand le verre est placé dans un champ magnétique. Il s'attache aussi à la vulgarisation des méthodes d'analyse en chimie.
On lui doit encore d'avoir mis au point un modèle rudimentaire de brûleur à gaz qui deviendra le bec Bunsen, par la suite universellement utilisé dans les laboratoires[6],[7].
Faraday découvre, entre autres substances chimiques, le benzène[8] et invente le système du nombre d'oxydation. En 1820, Faraday réussit la première synthèse des composés de carbone et de chlore, C2Cl6 et C2Cl4, résultats qu'il publie l'année suivante[9],[10],[11]. Faraday définit la composition du clathrate de chlore qui avait été découvert par Humphry Davy en 1810[12],[13].
Faraday est le premier à mentionner l'existence de ce qui sera connu sous le vocable de nanoparticules métalliques. En 1847, il observe que les propriétés optiques du colloïde d'or diffèrent de celles du métal pur, observation que l'on pourrait considérer comme la naissance des nanosciences[14].
Faraday s’intéressait à la spiritualité et prit quelque temps la direction de la secte des sandemaniens[15], ce qui fait dire à Paul Valéry qu'il [16].
Le mentor et sponsor de Faraday était John « Mad Jack » Fuller (en), qui créa le « Fullerian Professorship » de chimie à la Royal Institution. Faraday fut le premier et le plus fameux des détenteurs de ce poste pour lequel il fut nommé à vie. La Royal Society lui décerne la médaille Copley en 1832 et 1838, et la médaille Rumford en 1846. Il est également lauréat de la Royal Medal en 1835 et 1846. Faraday fut également membre de l’Académie des sciences en France : élu correspondant pour la section de chimie le 22 septembre 1823, puis associé étranger le 23 décembre 1844[17].
Pour les articles homonymes, voir Nikola Tesla (homonymie).
modifier - modifier le code - modifier WikidataNikola Tesla (en serbe cyrillique : Никола Тесла), né le 10 juillet 1856 à Smiljan dans l'Empire d'Autriche (actuelle Croatie) et mort le 7 janvier 1943 à New York, est un inventeur et ingénieur américain d'origine serbe. Il est connu pour son rôle prépondérant dans le développement et l'adoption du courant alternatif pour le transport et la distribution de l'électricité.
Tesla a d'abord travaillé dans la téléphonie et l'ingénierie électrique avant d'émigrer aux États-Unis en 1884 pour travailler avec Thomas Edison puis avec George Westinghouse, qui enregistra un grand nombre de ses brevets. Considéré comme l’un des plus grands scientifiques dans l’histoire de la technologie, pour avoir déposé quelque 300 brevets couvrant au total 125 inventions[1] (qui seront pour beaucoup attribuées à tort à Edison)[2] et avoir décrit de nouvelles méthodes pour réaliser la « conversion de l’énergie », Tesla est reconnu comme l’un des ingénieurs les plus créatifs de la fin du XIXe et du début du XXe siècle. Quant à lui, il préférait plutôt se définir comme un découvreur.
Ses travaux les plus connus et les plus largement diffusés portent sur l’énergie électrique. Il a mis au point les premiers alternateurs permettant la naissance des réseaux électriques de distribution en courant alternatif, dont il est l’un des pionniers. Tesla s’est beaucoup intéressé aux technologies modernes, se focalisant sur l’électricité qui était le noyau de ses inventions. Il est connu pour avoir su mettre en pratique la découverte du caractère ondulatoire de l’électromagnétisme (théorisé par James Clerk Maxwell en 1864), en utilisant les fréquences propres des composants des circuits électriques, afin de maximiser leur rendement.
De son vivant, Tesla était renommé pour ses inventions, ainsi que pour son sens de la mise en scène, faisant de lui un archétype du « savant fou ». Grand humaniste, qui se fixait comme objectif d'apporter gratuitement l'électricité dans les foyers et de la véhiculer sans fil[3],[4], il demeura toutefois relativement méconnu pendant plusieurs décennies après sa mort, mais son œuvre entraîne un regain d'intérêt dans la culture populaire depuis les années 1990. En 1960, le nom tesla (T) est donné à l’unité internationale d’induction magnétique. En 2003 est créé un constructeur automobile novateur de voitures électriques, dont le nom, Tesla Inc., est choisi en référence à Nikola Tesla.
Son lieu de sépulture se trouve à Belgrade, en Serbie, dans le musée Nikola-Tesla.
Nikola Tesla naît dans une famille serbe orthodoxe de Lika, en Krajina croate, venue de l'ouest de la Serbie, près du Monténégro[5]. Fier de ses origines, Tesla a toujours revendiqué à la fois ses ascendances serbes et son héritage croate, s'identifiant comme un Serbe de Croatie[6],[7],[8],[9],[10]. Cependant, né au sein de l'Empire d'Autriche, Tesla s'est déclaré de nationalité autrichienne lors de sa demande de naturalisation américaine en 1891[11].
Les Tesla seraient issus de la famille Draganić, dont une branche aurait adopté le surnom « Tesla » signifiant herminette en serbe, donné en raison d'une caractéristique physique particulière de ses membres[5]. Une autre légende les lie à la famille noble d'Herzégovine de Pavle Orlović, un chevalier serbe semi-mythologique[12].
Toutefois, parce qu'il est né dans la partie croate des confins militaires (une zone tampon contrôlée par les Habsbourg le long de la frontière ottomane), certains Croates revendiquent pour Tesla la nationalité croate[13],[14]. Ainsi, depuis sa mort en 1943, de nombreuses controverses ont éclaté quant à sa nationalité, des nationalistes serbes et croates se livrant à de nombreux débats pour s'attribuer son origine[15],[16],[17]. Toutefois en 2006, pour les 150 ans de sa naissance, une cérémonie est organisée dans son village natal. Y assistent à la fois le président croate Stjepan Mesić qui présente Tesla comme  et le président de la Serbie Boris Tadić pour qui Tesla fait partie du  aux pays de la région. Lors de cette réunion, la majorité des discours insistent sur le passé commun à la Serbie et la Croatie, dont Tesla est un symbole[6].
Nikola Tesla naît dans la nuit du 9 au 10 juillet 1856[a], à Smiljan, dans les confins militaires de l’Empire d’Autriche[6]. Il naît lors d'une nuit d'orage très violente. Sa grand-mère interprète cela en disant que l'enfant serait l', alors que sa mère au contraire déclare qu'il serait l'[18]. Son père, Milutin Tesla, est le prêtre orthodoxe serbe de Smiljan[19]. Sa mère, Duka Mandić, est la fille d’un prêtre orthodoxe serbe originaire de Lika et Banija et antérieurement du Kosovo. Elle a un don pour la fabrication d’outils artisanaux et, bien qu’analphabète, est capable de mémoriser des textes de poésie épique serbe et des passages de la Bible[20]. Nikola est le quatrième de cinq enfants. Il a trois sœurs, Milka, Angelina et Marica, et un frère aîné, Dane, qui décède après un accident de cheval alors que Nikola a sept ans[21].
La mort de Dane a un impact négatif sur la relation qu'a Nikola avec ses parents, plus particulièrement avec son père. En effet, Dane est vu comme  et est le fils préféré de Milutin. Sa mort est ainsi très difficile à accepter, et Nikola se sent rejeté par ses parents : [c 1]. Dane étant censé suivre son père en devenant à son tour prêtre, c'est sur Nikola que Milutin repose ses espoirs. Il donne alors à Nikola des exercices, tels que du calcul mental, répéter de longues phrases ou essayer de deviner ses pensées, tous dans un but d'améliorer son esprit critique. Sa relation compliquée avec son père provoque des obsessions étranges chez Nikola : il ne supporte pas la vue de boucles d’oreilles et de perles sur les femmes, il refuse de toucher les cheveux d'autres personnes, est dérangé par l'odeur du camphre, est obligé de compter ses pas et se force à faire un nombre d'actes divisible par trois sans quoi il recommence toute action[22].
Dès son enfance, Tesla montre de grandes aptitudes intellectuelles, bénéficiant d’une mémoire eidétique hors du commun et d’un génie inventif qu'il attribue plus tard aux gènes et à l'influence de sa mère[20]. Tesla est cependant troublé par des visions et a de la peine à contrôler ses émotions. Il se passionne pour le tir à l'arc et utilise son imagination pour . Il essaie alors d'imaginer un appareil qui lui permettrait de voler et conçoit un objet volant utilisant les propriétés du vide et de la pression pour faire tourner un cylindre. Ses premières expérimentations s'avèrent concluantes, mais la vitesse de rotation du cylindre n'est finalement pas suffisante pour faire décoller l'engin[23]. Un autre projet de machine volante, censée [c 2], consiste à mouvoir quatre hélices en y attachant des hannetons. Ce projet est cependant abandonné après qu'un autre enfant ait attrapé et mangé un des insectes. Dégoûté par l'événement, Tesla promet de ne plus jamais toucher d'insectes de sa vie[24].
Peu après la mort de son frère, Tesla commence à lire dans la bibliothèque de son père, ce que ce dernier n'approuve pas. Milutin cache les bougies pour empêcher Nikola de se , mais Tesla finit par créer ses propres bougies et continue à lire[25]. À 12 ans, il découvre le livre Abafi de Miklós Jósika, qui raconte l'histoire d'un [c 3]. Ce livre est une révélation pour Tesla et il lui permet d'enfin prendre contrôle de ses émotions : [c 4],[26].
En 1861, Tesla fréquente l'école primaire de Smiljan où il étudie l'allemand, l'arithmétique et la religion. En 1862, la famille Tesla s'installe dans la localité voisine de Gospić, où le père de Tesla travaille comme curé de paroisse. Tesla y termine l'école primaire, puis le collège[27]. En 1870, Tesla s'installe à Karlovac pour suivre les cours du lycée au Gymnase Karlovac, où les cours sont dispensés en allemand, comme c'était l'usage dans les écoles situées à l'intérieur de la frontière militaire austro-hongroise[28],[29]. Tesla est capable de faire du calcul intégral de tête, ce qui fait croire à ses professeurs qu'il triche[30]. Il termine un cursus de quatre ans en trois ans et obtient son diplôme en 1873[31].
Tesla commence à s'intéresser à l'électricité très jeune. Alors qu'il joue avec le chat familial Macak, il découvre l'électricité statique, . Il déclare plus tard : . Curieux, il interroge son père, qui lui répond qu'il s'agit d’électricité, . Ces premiers questionnements deviennent une véritable obsession pour Tesla : [32].
Au Gymnase Karlovac, Tesla est impressionné par les démonstrations de son professeur de physique[b]. Elles le motivent plus que jamais à étudier l'électricité, cette . Tesla lit alors tout ce qu'il trouve sur le phénomène, se prend de passion pour le radiomètre de Crookes et expérimente avec des batteries, des bobines et des générateurs électriques[33].
Alors qu'il étudie à Karlovac, Tesla vit avec la sœur de son père dans une zone très marécageuse. Les moustiques y sont ainsi nombreux, et il contracte le paludisme, qu'il traite en prenant énormément de quinine. La maladie ne le quitte cependant pas avant plusieurs années[34],[35].
Une fois son diplôme au Gymnase Karlovac obtenue, Tesla retourne à Smiljan. Peu après son arrivée, il contracte le choléra, reste alité pendant neuf mois et frôle la mort à plusieurs reprises. Le père de Nikola, dans un moment de désespoir, et bien qu'il veuille que son fils entre dans la prêtrise, promet de l'envoyer dans la meilleure école d'ingénieurs s'il se remet de la maladie[36],[27],[37]. Alors qu'il est malade, il lit les œuvres de Mark Twain, ce qu'il considère comme l'ayant aidé à se remettre miraculeusement de sa maladie[38].
En 1874, Tesla échappe au service militaire obligatoire dans l'armée austro-hongroise à Smiljan en s'enfuyant au sud-est de Lika à Tomingaj, près de Gračac[31]. Là, il explore les montagnes en tenue de chasseur, lit de nombreux livres et conçoit des inventions qu'il juge lui-même [39],[40]. Selon Tesla, ce contact avec la nature l'a rendu plus fort, tant physiquement que mentalement[27].
En 1875, Tesla s'inscrit à l'université technique de Graz grâce à une bourse de la Frontière militaire[41],[42]. Il s’enrôle dans les départements de physique et mathématiques dans l'idée de devenir plus tard professeur, probablement pour satisfaire son père qui aurait eu du mal à imaginer Nikola en tant qu'ingénieur[43]. Durant sa première année, Tesla ne manque jamais un cours, obtient les meilleures notes possibles, réussit neuf examens (presque deux fois plus que le nombre requis) et fonde un club culturel serbe[41],[42]. Le doyen de la faculté technique adresse alors une lettre à son père dans laquelle il déclare : [42]. Quand Tesla rentre à Smiljan à la fin de l'année scolaire, il s'attend à impressionner ses parents avec ses résultats, mais les retrouve peu enthousiastes et inquiets pour sa santé. En effet, durant l'année scolaire, Tesla a un rythme de vie effréné ; il se réveille à 3 heures du matin et lit généralement jusqu'à 23 heures, ne se laissant aucune pause pour des loisirs, même les dimanches et les jours de fête. Un de ses professeurs, craignant pour la santé de Tesla, envoie ainsi plusieurs lettres tout au long de l'année scolaire au père de Nikola le priant de retirer son fils de l'école[c],[44].
En 1876 ou 1877, Tesla entre en désaccord avec son professeur de physique Jakob Pöschl lors de la démonstration d'une machine de Gramme. Pöschl raccorde la machine à une batterie pour l'utiliser comme moteur en courant continu, mais les balais, mal ajustés, créent des étincelles. Tesla observe alors la machine et conclut qu'elle pourrait fonctionner de la même manière sans balais. Pöschl, qui pense que ce sont les balais qui convertissent l'énergie électrique en énergie mécanique, contredit Tesla en précisant que, selon lui, cela serait équivalent à tenter de créer un mouvement perpétuel. Convaincu qu'il a raison, Tesla abandonne ses plans de devenir professeur et intègre la faculté d'ingénierie[45],[d].
Bien que sa position d'étudiant en ingénierie lui aurait permis de construire un modèle fonctionnel de moteur sans balais, Tesla choisit de simplement explorer l'idée dans son imagination, en deux étapes : [47],[46]. Tesla pense donc que la solution se trouve en courant alternatif, une innovation puisque, à cette époque, on utilise du courant continu pour presque toutes les applications de l'électricité. Tesla a également l'idée de coupler un moteur à un générateur, et non pas une batterie, ce qui est également une innovation probablement inspirée par la présentation d'Hippolyte Fontaine à Vienne, que Pöschl a expliqué à Tesla, durant laquelle Fontaine raccorde un moteur à une dynamo. Malgré un concept de base bien établi, Tesla n'arrive pas à réaliser son idée physiquement[48].
Après une altercation avec un camarade de classe allemand, durant laquelle celui-ci se moque de Tesla pour son assiduité au travail scolaire, Tesla commence à sortir avec d'autres étudiants jusqu'à tard le soir. Il y apprend les dominos, les échecs, devient un très bon joueur de billard et développe une addiction aux jeux de cartes et d'argent[27],[50]. Lors du premier semestre de sa troisième année d'études, Tesla ne va plus en cours et il n'est enregistré dans aucune classe au printemps 1878. Tesla perd ainsi sa bourse militaire et tente en vain d'en obtenir une nouvelle auprès d'un journal pro-serbe de Novi Sad[51].
Selon certaines sources, Tesla aurait obtenu son diplôme de premier cycle de l’université de Graz[52],[53]. Toutefois, selon l’université, il n’aurait pas poursuivi ses études au-delà du premier semestre de sa troisième année et n'aurait ainsi obtenu aucun diplôme[54],[55],[56].
En décembre 1878, Tesla quitte Graz, n'a plus de contact avec sa famille et déménage à Maribor où on l’emploie comme assistant ingénieur. Il passe alors ses soirées dans une taverne, « Le paysan heureux », à jouer aux cartes. En janvier 1879, son ancien colocataire Kosta Kulišić séjourne à Maribor et rencontre par hasard Tesla à la taverne. Kulišić contacte alors la famille de Tesla, leur indiquant qu'il se trouvait à Maribor. Deux mois plus tard, Milutin Tesla se rend à Maribor pour convaincre Nikola de reprendre ses études à l’université Charles de Prague. Tesla refuse alors de rentrer chez ses parents et tient tête à son père, qui tombe malade. Tesla est cependant renvoyé à Gospić quelques semaines plus tard après avoir été arrêté pour vagabondage. Milutin, frappé de voir son fils ramené par la police, meurt le 17 avril 1879[57].
Après la mort de son père, Tesla reste à Gospić, continue de participer à des jeux d'argent et enseigne dans son ancienne école[27],[58]. Avec l'aide de sa mère, il parvient à se contrôler et accepte finalement de reprendre ses études à Prague, ses oncles maternels lui fournissant les fonds nécessaires[58]. Là, il est influencé par Ernst Mach et suit un cours de Carl Stumpf intitulé  dans lequel Tesla apprend le concept de tabula rasa. Tesla continue de travailler sur son idée de moteur à courant alternatif à Prague, bien qu'il soit uniquement inscrit à des cours de mathématiques, physique expérimentale et philosophie : . Il y fait des expérimentations, telles que , sans vraiment parvenir à des résultats concluants. Malgré cela, ces essais s'avèrent importants pour Tesla car ils lui permettent de mieux comprendre le fonctionnement d'un moteur et il sent qu'il [58].
En janvier 1881, alors que ses oncles ont arrêté de lui envoyer de l'argent, Tesla quitte Prague pour Budapest[59]. Tesla choisit Budapest, car il a récemment appris que Tivadar Puskás, un collaborateur de Thomas Edison, est sur le point d'y construire des centraux téléphoniques. Les travaux doivent être supervisés par Ferenc Puskás, le frère de Tivadar, qui a servi dans l'armée dans la même unité que l'oncle de Tesla, Pavle Mandić. Tesla demande ainsi à son oncle de le recommander pour aider à construire le réseau téléphonique de Budapest mais, n'arrivant pas à financer leur projet immédiatement, les frères Puskás trouvent un travail à Tesla comme dessinateur pour l'Office central du télégraphe du gouvernement hongrois[60]. Très vite repéré par l'inspecteur en chef, il est finalement transféré à un poste où il fait des calculs et des estimations et aide à la conception d'une nouvelle installation téléphonique. Tesla réalise alors sa première réelle invention, un ancêtre du haut-parleur qu'il n'a jamais breveté ni exposé publiquement[61].
Bien qu'il soit d'abord heureux d'être en contact direct avec des appareils électriques, Tesla se lasse vite d'un travail qu'il juge trop limité[60]. Tesla finit donc par démissionner et se tourne vers ses inventions. Durant l'année 1881, Tesla est affecté par une étrange maladie que les médecins n'arrivent pas à diagnostiquer. Il souffre notamment d'une sensibilité aiguë de tous les sens — John Joseph O'Neill décrit son ressenti entre autres de la façon suivante :  — et reste cloué au lit plusieurs mois, les médecins lui donnant peu de chance de s'en sortir[62]. Il est possible que cette maladie soit une dépression nerveuse due au peu d'intérêt que les gens portent à ses nouvelles inventions. Tesla doit sa rémission à Antal Szigeti, un homme qu'il rencontre à Budapest et avec qui il se lie d'amitié : [60].
Pour aider Tesla à se remettre de sa maladie, Szigeti l'invite à venir marcher avec lui au Városliget. Tesla accepte et passe la plupart de ses soirées avec Szigeti à discuter de ses idées de moteur à courant alternatif. Un jour, alors que Tesla récite un poème de Johann Wolfgang von Goethe, il a un  pendant lequel une idée lui vient . Il commence alors à dessiner les plans d'une nouvelle invention dans le sable tout en l'expliquant à Szigeti[65]. Les images seraient apparues tellement clairement à Tesla qu'il en serait venu à demander à son ami s'il [66]. C'est les images  et  du poème de Goethe qui inspirent Tesla à utiliser un champ magnétique tournant pour la conception de son moteur[67],[e].
L'épisode du parc à Budapest n'a pas seulement aidé Tesla à avancer dans ses recherches sur le premier moteur à courant alternatif, il lui a également confirmé qu'il était capable de devenir inventeur. Tesla prend alors pleine conscience de son pouvoir créatif et il se voit devenir . En 1882, il profite de son travail aux Compagnies Ganz pour en apprendre plus sur le courant alternatif — Károly Zipernowsky, Ottó Bláthy et Miksa Déri, qui travaillent également chez Ganz, créent plus tard le premier réseau électrique en alternatif. C'est en travaillant chez Ganz que Tesla, en faisant des expériences avec un transformateur électrique défectueux en forme d'anneau, confirme sa théorie selon laquelle il est possible d'utiliser un courant alternatif pour créer un champ magnétique tournant[68].
En 1882, Tesla est enfin engagé par Ferenc Puskás pour aider à développer le réseau téléphonique de Budapest. C'est alors que Tesla invente de nouveaux répéteurs et amplificateurs pour téléphone. Une fois le travail terminé, Tivadar Puskás, qui est à Paris pour aider Thomas Edison à introduire sa nouvelle technologie d'éclairage public à incandescence en France, invite Tesla et Szigeti à le rejoindre pour travailler pour la Edison General Electric Company[69].
À Paris, Tesla se distingue auprès du directeur de la Continental Edison, Charles Batchelor (en), probablement grâce à ses études supérieures en physique et mathématiques qui le rendent autant bon en théorie qu'en pratique. Il tente alors à plusieurs occasions d'expliquer ses idées de moteur à courant alternatif à ses collègues, en vain. Ceux-ci ne sont pas intéressés par ses idées, car la Edison Company est occupée à commercialiser des solutions d'éclairages électriques et ne voit pour l'instant pas l'intérêt de développer des systèmes motorisés. Une autre raison du désintérêt des employés d'Edison vient peut-être aussi du fait que les idées de Tesla semblent être trop gourmandes en cuivre. En effet, une des politiques imposées par Thomas Edison consiste à utiliser le moins de cuivre possible dans ses systèmes, et Tesla prévoit d'alimenter son moteur avec six fils de cuivre, de façon à créer trois courants alternatifs déphasés[70],[f].
Tesla est ensuite employé par l'entreprise d'Edison en France et en Allemagne dans plusieurs stations d'éclairage. Il impressionne le directeur d'une des filières d'Edison, Louis Rau et est envoyé à Strasbourg, qui fait alors partie de l'Empire allemand. Là-bas, il est chargé de réparer les dommages causés par un court-circuit à la gare de Strasbourg-Ville, en construction dans le cadre de la Neustadt. La gare de Strasbourg comporte alors un système d'éclairage de 1 200 lampes alimentées par quatre générateurs en plus d'une installation de Siemens & Halske de cinq générateurs à courant continu pour lampes à arc. Durant les travaux, Tesla trouve un générateur à courant alternatif de Siemens et commence à construire un prototype de moteur à courant alternatif dans son temps libre[72]. Après plusieurs essais, il réussit à faire tourner son moteur pour la première fois : [73].
À Strasbourg et à Paris, Tesla tente à plusieurs reprises de trouver des investisseurs, mais personne ne semble vraiment s'intéresser à ses projets. Au printemps 1884, Charles Batchelor est rappelé aux États-Unis par Edison pour gérer Edison Machine Works (en) à New York. Batchelor demande alors à Tesla de le suivre pour travailler sur son moteur[74].
Tesla émigre en juin 1884 et commence à travailler presque immédiatement chez Machine Works dans le Lower East Side de Manhattan, aux côtés d'une vingtaine d' qui peinent à mettre en place l'usine électrique de la ville[76],[77]. Comme à Paris, Tesla travaille au dépannage des installations et à l'amélioration de générateurs. L'atelier où il travaille est surpeuplé : plusieurs centaines de machinistes, d'ouvriers et de cadres y sont employés. L'historien W. Bernard Carlson note que Tesla n'a peut-être rencontré Thomas Edison qu'à quelques reprises[77]. Une de ces rencontres est décrite dans l'autobiographie de Tesla : après avoir passé la nuit à réparer les dynamos endommagées du paquebot SS Oregon, il rencontre Batchelor et Edison, qui l'appellent . Après que Tesla leur a dit qu'il était resté debout toute la nuit pour réparer l'Oregon, Edison fait remarquer à Batchelor que [75]. L'un des projets confiés à Tesla consiste à mettre au point un système d'éclairage public basé sur des lampes à arc[78],[79]. L'éclairage à arc est le type d'éclairage public le plus populaire, mais il nécessite des tensions élevées et est incompatible avec le système à incandescence basse tension d'Edison, et la société perd des contrats dans des villes qui veulent un éclairage public. Les dessins de Tesla ne sont jamais mis en production, peut-être en raison d'améliorations techniques apportées à l'éclairage public à incandescence ou d'un contrat d'installation qu'Edison a conclu avec une société d'éclairage à arc[80].
Après six mois de travail chez Machine Works, en 1885, Tesla démissionne[77]. L'événement qui a précipité son départ n'est pas certain. Il s'agit peut-être d'une prime qu'il n'a pas reçue, soit pour avoir conçu de nouveaux générateurs, soit pour le système d'éclairage à arc qui avait été mis de côté[78]. Tesla a déjà eu des démêlés avec la société Edison à propos de primes non versées qu'il pensait mériter. Dans son autobiographie, Tesla déclare que le directeur d'Edison Machine Works lui a promis une prime de 50 000 dollars pour concevoir , [81],[82]. Dans des versions ultérieures de cette histoire, Thomas Edison aurait offert lui-même une prime puis serait revenu sur sa parole, en disant . Tesla aurait alors donné sa démission, et Edison aurait tenté de le faire rester en lui offrant une augmentation[83]. Le journal de Tesla ne contient qu'un seul commentaire sur sa démission, une note qu'il a griffonnée sur les deux pages couvrant la période du 7 décembre 1884 au 4 janvier 1885, en disant  ()[84].
Peu après avoir quitté la société Edison, Tesla s'attelle à la tâche de breveter un système d'éclairage à arc, peut-être le même qu'il a développé chez Edison[85],[77]. En mars 1885, il rencontre l'avocat Lemuel W. Serrell, qui travaille entre autres pour Edison, afin d'obtenir de l'aide pour soumettre les brevets[85]. Serrell présente Tesla à deux hommes d'affaires, Robert Lane et Benjamin Vail, qui acceptent de financer une société de fabrication et d'utilité d'éclairage à l'arc à son nom, la Tesla Electric Light & Manufacturing (en)[86]. Tesla travaille pendant le reste de l'année sur un générateur à courant continu amélioré et la construction et l'installation du système à Rahway, dans le New Jersey, et obtient ses premiers brevets aux États-Unis[87]. Le nouveau système de Tesla est remarqué par Electrical Review, un journal technique de New York, qui publie un article sur l'installation de Rahway en première page[88].
Les investisseurs montrent cependant peu d'intérêt pour les idées de Tesla concernant de nouveaux types de moteurs à courant alternatif et d'équipements de transmission électrique. Après la mise en service de l'entreprise en 1886, ils décident que la manufacture de lampes à arc est un marché trop compétitif. Ils optent dès lors pour la simple exploitation d'une entreprise d'électricité, qu'ils créent, abandonnant la société de Tesla. Tesla perd même le contrôle de ses brevets, puisqu'il les a précédemment cédés à Tesla Electric & Manufacturing en échange d'actions[89]. Sans le sou, l'inventeur ne trouve plus de travail comme ingénieur et multiplie les petits boulots, tels que réparateur de circuits électriques ou comme ouvrier dans une société qui creuse des fossés pour 2 dollars par jour (57 dollars en 2023[90]). Tesla lui-même considère cette période comme très difficile, écrivant : [c 6],[89],[91].
Fin de 1886, Tesla rencontre Alfred S. Brown, un surintendant de la Western Union, et Charles Fletcher Peck, un avocat de New York. Les deux hommes ont de l'expérience dans la création d'entreprises et la promotion des inventions et savent comment en tirer un profit[92]. Sur la base des nouvelles idées de Tesla, notamment une idée de moteur thermomagnétique, ils acceptent de soutenir financièrement l'inventeur et de s'occuper de ses brevets[93]. Ensemble, ils forment la Tesla Electric Company en avril 1887 et s'accordent pour que Tesla reçoive un tiers des bénéfices, tandis que Brown et Peck se partagent un tiers ; le dernier tiers est réinvesti dans de futures inventions. Ils louent un laboratoire pour Tesla à Manhattan, où il travaille à l'amélioration et au développement de nouveaux types de moteurs électriques, de générateurs et d'autres dispositifs, avec Sziget comme assistant[94].
En 1887, Tesla met au point un moteur à induction qui fonctionne sur courant alternatif. En plus de faire breveter le moteur, Peck et Brown organisent des tests indépendants pour vérifier qu'il est fonctionnel et en font la promotion au moyen de communiqués de presse envoyés à des publications techniques pour des articles à paraître en même temps que la délivrance du brevet. Le physicien William Arnold Anthony, qui a testé le moteur, et le rédacteur en chef du magazine Electrical World, Thomas Commerford Martin, organisent une démonstration du moteur de Tesla le 16 mai 1888 à l’American Institute of Electrical Engineers[95]. Les ingénieurs de la Westinghouse Electric signalent alors à George Westinghouse que Tesla a un moteur à courant alternatif viable, ce dont Westinghouse a besoin pour un système à courant alternatif qu'il a déjà commencé à commercialiser. Bien que Westinghouse ait cherché à obtenir un brevet sur un moteur similaire développé en 1885 par le physicien italien Galileo Ferraris, il se rabat sur le modèle de Tesla, estimant que celui-ci va probablement dominer le marché[96],[97].
En juillet 1888, Brown et Peck négocient un accord de licence avec George Westinghouse pour la conception des moteurs et transformateurs de Tesla pour 60 000 dollars en espèces et en actions et une redevance de 2,50 dollars par cheval-vapeur produit par chaque moteur (respectivement 1 707 333 dollars et 71 dollars en 2023[90]). Westinghouse engage également Tesla pendant un an pour la somme de 2 000 dollars par mois (56 911 dollars en 2023[90]) pour être consultant dans les laboratoires de Pittsburgh de la Westinghouse Electric[99].
Au cours de cette année, Tesla travaille à Pittsburgh, aidant à créer un système de courant alternatif pour alimenter les tramways de la ville. Il trouve cette période frustrante en raison de conflits avec les autres ingénieurs de Westinghouse sur la meilleure façon de mettre en place le courant alternatif. Entre eux, ils se mettent d'accord sur un système à courant alternatif de 60 cycles que Tesla a proposé (pour correspondre à la fréquence de fonctionnement du moteur de Tesla), mais ils s’aperçoivent vite que cela ne fonctionnera pas pour les tramways, puisque le moteur à induction de Tesla ne peut fonctionner qu'à une vitesse constante. Ils finissent par utiliser un moteur de traction à courant continu à la place[100],[101].
La démonstration par Tesla de son moteur à induction et l'obtention ultérieure par Westinghouse d'une licence sur le brevet, tous deux en 1888, ont lieu alors que les compagnies d'électricité se livrent une intense concurrence[102],[103]. Les trois grandes entreprises, Westinghouse, Edison et Thomson-Houston, essayent de se développer dans un secteur à fort capital et se sous-estiment mutuellement. Edison Electric va jusqu'à faire de la propagande et essaye de faire valoir que son système à courant continu est meilleur et plus sûr que le système à courant alternatif de Westinghouse. La concurrence signifie que Westinghouse n'a finalement pas les moyens financiers et techniques pour développer immédiatement le moteur de Tesla[102].
Deux ans après la signature du contrat avec Tesla, Westinghouse Electric se retrouve en difficulté. Le quasi-effondrement de la Barings Bank, à Londres, déclenche la panique de 1890, poussant les investisseurs à demander le remboursement de leurs prêts à Westinghouse Electric en retour[104]. Le manque soudain de liquidités oblige l'entreprise à renflouer ses dettes. De nouveaux prêteurs exigent alors que Westinghouse réduise ce qui semble être des dépenses excessives dans l'acquisition d'autres sociétés, la recherche et les brevets, y compris la redevance par moteur prévue dans le contrat Tesla[105],[106]. À ce moment-là, le moteur à induction Tesla peine à être mis en place et est bloqué en phase de développement[102],[104]. Westinghouse paye alors une redevance garantie de 15 000 dollars par an bien que les modèles fonctionnels du moteur de Tesla soient rares et que les systèmes d'alimentation polyphasés nécessaires pour le faire fonctionner soient encore plus rares[107],[104].
Au début de l'année 1891, George Westinghouse explique de manière très claire ses difficultés financières à Tesla, en disant que s'il ne répond pas aux exigences de ses prêteurs, il ne contrôlera plus Westinghouse Electric et Tesla devra traiter avec les banquiers lui-même pour tenter de percevoir ses futures redevances[108]. Le fait de voir Westinghouse continuer à défendre le moteur convainc Tesla, et il accepte de libérer la société des redevances prévues dans son contrat[108],[109]. Six ans plus tard, Westinghouse achète le brevet de Tesla pour une somme de 216 000 dollars (6 638 112 dollars en 2023[90]) dans le cadre d'un accord de partage de brevet signé avec General Electric (une société issue de la fusion, en 1892, entre Edison et Thomson-Houston)[110],[111],[112].
L'argent que Tesla gagne grâce à ses brevets le rend financièrement indépendant et lui donne le temps et les fonds nécessaires pour poursuivre ses propres intérêts[113]. En 1889, Tesla quitte le magasin de Liberty Street que Peck et Brown louent pour lui. Il travaille alors pendant les douze années suivantes dans plusieurs ateliers et laboratoires à Manhattan : un laboratoire au 175 Grand Street (1889-1892), le quatrième étage du 33-35 South Fifth Avenue (1892-1895), et les sixième et septième étages des 46 & 48 East Houston Street (1895-1902)[114],[115],[116].
Alors que Tesla quitte Westinghouse, Szigeti, son ami et associé depuis plus de neuf ans, lui annonce qu'il aimerait partir pour développer ses propres idées. Son départ est particulièrement difficile à accepter pour Tesla et le blesse profondément[105].
Au cours de l'été 1889, Tesla se rend à l'Exposition universelle de Paris et prend connaissance des expériences de Heinrich Hertz qui prouvent l'existence de rayonnements électromagnétiques[117]. Tesla trouve cette nouvelle découverte  et décide de l'explorer plus en profondeur. En répétant, puis en développant ces expériences, Tesla invente la bobine Tesla, utilisée pour produire de l'électricité à haute tension, à faible courant et à haute fréquence en courant alternatif[118].
Les travaux de Tesla se concentrent alors sur la haute fréquence, et plus particulièrement sur la conversion d'électricité en lumière, alors que Guglielmo Marconi, un autre inventeur, a développé les théories de Hertz pour des utilisations en télécommunications. Afin de promouvoir ses découvertes, Tesla utilise ce qu'il a appris avec Peck et Brown : il publie plusieurs articles dans des journaux spécialisés, pose des brevets sur ses inventions et donne une série de conférences dans des universités et écoles d'ingénieurs. Lors de démonstrations publiques pour un système d'éclairage, il allume des tubes de Geissler et des ampoules à incandescence sans utiliser de fils[119]. En 1893, Tesla déclare aux spectateurs de plusieurs conférences qu'il est sûr qu'un système comme le sien pourrait éventuellement conduire  en le conduisant à travers la terre[120],[121]. Tesla entre également en conflit avec Elihu Thomson, qui travaille sur la haute fréquence et n'observe pas toujours les mêmes résultats que lui ; Tesla et Thomson se répondent par une série d'articles dans des journaux spécialisés entre mars et avril 1891[122].
Le 30 juillet 1891, à l'âge de 35 ans, Tesla est naturalisé citoyen des États-Unis[11]. En 1892, et jusqu'en 1894, Tesla devient vice-président de l’American Institute of Electrical Engineers, le précurseur de l’Institute of Electrical and Electronics Engineers[123].
Début 1893, les ingénieurs de Westinghouse Charles F. Scott puis Benjamin G. Lamme font des progrès sur une version fonctionnelle du moteur à induction de Tesla. Lamme trouve un moyen de rendre l'alimentation polyphasée compatible avec les anciens systèmes monophasés à courant alternatif et à courant continu en développant une commutatrice[124]. Westinghouse Electric a dès lors un moyen de fournir de l'électricité à tous les clients potentiels et commence à donner à son système polyphasé à courant alternatif le nom « Tesla Polyphase System ». Ils pensent alors que les brevets de Tesla leur donnent la priorité sur les autres systèmes polyphasés à courant alternatif et envoient des brochures à leurs clients les avertissant que, s'ils venaient à acheter un système alternatif ailleurs, ils pouvaient être poursuivis en justice[125].
Westinghouse Electric demande à Tesla de participer à l'exposition universelle de 1893 à Chicago, où la société dispose d'un grand espace consacré aux expositions sur l'électricité. Bien que l'entreprise soit passée très proche de la faillite, Westinghouse Electric remporte également l'appel d'offres pour éclairer l'exposition avec du courant alternatif ; Westinghouse obtient le contrat en proposant des prix bien plus bas que ses compétiteurs, ce qui force ses ingénieurs à utiliser des alternateurs plus gros et une tension électrique plus élevée[126]. C'est néanmoins un moment clé dans l'histoire du courant alternatif, car la société démontre au public américain la sécurité, la fiabilité et l'efficacité d'un système alternatif polyphasé qui peut alimenter les autres stands de la foire en courant alternatif et en courant continu[127],[128],[129].
Un espace d'exposition spécial est mis en place pour présenter différents modèles de moteur à induction de Tesla. Le champ magnétique rotatif qui les actionne est expliqué par une série de démonstrations, dont un œuf de Colomb qui utilise la bobine biphasée d'un moteur à induction pour faire tourner un œuf en cuivre et le faire tenir debout[131]. Tesla visite notamment la foire pour assister au Congrès international sur l'électricité et faire une série de démonstrations au stand Westinghouse[132]. Une pièce spécialement obscurcie est aménagée où Tesla montre son système d'éclairage sans fil, en utilisant une démonstration qu'il avait déjà faite à travers l'Amérique et l'Europe ; il s'agit notamment d'utiliser un courant alternatif à haute tension et haute fréquence pour allumer des lampes à décharge sans fil[133].
Lors de sa présentation au Congrès international sur l'électricité, dans le hall agricole de l'exposition universelle, Tesla présente un générateur électrique alternatif à vapeur qu'il a breveté cette année-là, ce qu'il pense être une meilleure façon de générer du courant alternatif[134]. La vapeur est forcée dans un oscillateur et s'échappe par une série d'orifices, poussant un piston fixé à une armature de haut en bas. L'armature magnétique vibre alors de haut en bas à grande vitesse, produisant un champ magnétique alternant. Celui-ci induit un courant électrique alternatif dans les bobines de fil situées à proximité. Cela permet de supprimer les pièces compliquées d'un moteur/générateur à vapeur, mais n'est finalement jamais considéré comme une solution technique viable pour produire de l'électricité[135].
En 1893, Edward Dean Adams, qui dirige la Niagara Falls Hydraulic Power and Manufacturing Company, demande l'avis de Tesla pour un système de transmission de l'électricité produite aux chutes du Niagara. Pendant plusieurs années, Adams a reçu une série de propositions et a ouvert plusieurs concours afin de déterminer la meilleure façon d'utiliser l'énergie produite sur le site. Parmi les systèmes proposés par plusieurs entreprises américaines et européennes figurent alors des systèmes de courant alternatif biphasé et triphasé, de courant continu à haute tension et d'air comprimé. Adams demande à Tesla des informations sur tous les systèmes en compétition. L'inventeur recommande alors à Adams un système biphasé, jugé plus fiable par Tesla, et lui explique que Westinghouse propose un système compatible avec l'éclairage à incandescence en utilisant un courant alternatif biphasé. L'entreprise d'Adams attribue alors un contrat à Westinghouse Electric pour la construction d'un système de production de courant alternatif biphasé aux chutes du Niagara, sur la base des conseils de Tesla et de la démonstration de Westinghouse à l'exposition universelle, qui prouve qu'ils sont capables de construire un système complet de courant alternatif. Un autre contrat est cependant passé avec General Electric pour la construction du système de distribution[136].
En 1895, Edward Dean Adams, impressionné par ce qu'il a vu lors de sa visite du laboratoire de Tesla, accepte de participer à la fondation de la Nikola Tesla Company, créée pour financer, développer et commercialiser une variété de brevets antérieurs et nouveaux. Alfred Brown s'engage avec Tesla, apportant avec lui les brevets développés sous Peck et Brown. William Birch Rankine et Charles F. Coaney font également partie du conseil d'administration de la société. Tesla trouve peu d'investisseurs ; le milieu des années 1890 est une période difficile sur le plan financier, et les brevets d'éclairage sans fil et d'oscillateurs qu'il a mis sur le marché n'ont jamais abouti[137].
Le 13 mars 1895, aux aurores, le bâtiment de la Cinquième Avenue qui abrite le laboratoire de Tesla prend feu. L'incendie commence dans le sous-sol du bâtiment et est si intense que le laboratoire de Tesla, situé au quatrième étage, s'effondre au deuxième étage. L'incendie fait non seulement reculer les projets en cours de Tesla, mais il détruit également une collection de notes et de matériel de recherche, de modèles et de pièces de démonstration, dont beaucoup ont été présentées à l'Exposition universelle de Colombie de 1893. Les pertes s'élèvent à près de 100 000 dollars (3 073 200 dollars en 2023[90])[138].
La destruction de son laboratoire touche énormément Tesla. Il passe alors plusieurs jours dans son lit et, selon le New York Herald, il apparaît [c 8]. Tesla revient quelques jours plus tard sur les lieux de l'incendie avec des ouvriers afin de sauver ce qui peut l'être. Pour surmonter cette épreuve, Tesla s'essaye à l'électrothérapie ; il s'administre plusieurs chocs par jour, probablement au moyen d'une de ses bobines, de façon à [c 9],[139].
Une fois sa dépression atténuée, Tesla déménage au 46 & 48 East Houston Street et reconstruit son laboratoire aux 6e et 7e étages. Une grande partie de son matériel ayant été détruit dans le feu, Tesla commence à travailler sur de nouveaux sujets : les rayons X et la radiocommande[140].
Tesla étudie déjà les rayons X depuis 1894 ; il parle alors du phénomène comme d'une « énergie radiante invisible », constatée lors de précédentes expériences[141]. Il mène des expériences sur des tubes de Crookes et est peut-être le premier homme à capturer, par accident, une image de rayons X : en décembre 1895, lorsqu'il essaie de photographier Mark Twain éclairé par un tube de Geissler, la seule chose apparente sur l'image est la vis de blocage métallique de l'objectif de l'appareil photo[142]. Quelques mois plus tard, après avoir appris la découverte des rayons X par Wilhelm Röntgen, Tesla réalise qu'il a manqué une découverte scientifique majeure : [c 10],[143].
Tesla procède dès lors à ses propres expériences sur les rayons X et met au point un tube à vide à haute énergie qui émet un rayonnement continu de freinage et est alimenté par bobine Tesla. Dans le cadre de ses recherches, Tesla conçoit plusieurs dispositifs expérimentaux pour produire des rayons X. Il soutient alors que, grâce à ses inventions, [c 11],[144].
Tesla remarque lui-même les dangers de travailler avec les appareils produisant des rayons X. Dans ses nombreuses notes sur les premières investigations de ce phénomène, il attribue les dommages cutanés à diverses causes. Très tôt dans ses recherches, il croit que les dommages à la peau sont causés par l'ozone généré au contact de la peau. Tesla réalise finalement que les rayons sont dangereux après qu'un de ses assistants, qui est exposé pendant cinq minutes à trente centimètres du tube, présente de graves brûlures à son abdomen. Tesla abandonne ses recherches sur les rayons X au cours de 1896, probablement lorsqu'il se rend compte qu'il ne sera pas capable d'être compétitif sur le marché face à General Electrics ou même face aux petits fabricants d'instruments scientifiques[145]. Il est également possible que la seule raison pour laquelle Tesla a conduit ses recherches sur les rayons X était de faire avancer son travail sur la transmission d'énergie sans fil. Ainsi, lorsque Tesla se serait rendu compte que les rayons X ne l'aideraient pas dans cette tâche, il serait passé à autre chose[146].
Alors qu'il est enfant, des cauchemars récurrents amènent Tesla à la réalisation que l'être humain n'est qu'une [c 12]. Il se passionne dès lors pour l'automation et, en 1897, après avoir abandonné son travail sur les rayons X, son nouveau projet consiste à développer des automates radio-contrôlés — il appelle cette discipline « télautomatique »[g],[147].
La course à l'armement naval entre la Grande-Bretagne et ses rivaux pousse Tesla à s'essayer à un prototype de torpille télécommandée en forme de bateau[147]. La commande du bateau se fait au moyen d'un transmetteur, qui émet un signal à une certaine fréquence, celui-ci étant reçu par un cohéreur sur la machine. La rotation du levier de commande active quant à elle un des quatre contacts, qui à leur tour désactivent le signal pour un moment donné. Lorsque le signal est coupé, dans le bateau, un disque avec des contacts tourne, activant une manœuvre prédéfinie selon la position du disque[148]. Plus tard, pour s'assurer que des fréquences parasites n'activent pas le bateau, Tesla rajoute un second signal avec une fréquence différente en parallèle à la première[149].
Tesla fait breveter son invention et donne des démonstrations privées dans son laboratoire ; John Pierpont Morgan, William Kissam Vanderbilt, John Hays Hammond, et Charles A. Cheever (en) auraient assisté à ces présentations. Le bateau radiocommandé de Tesla arrive sur le marché quelques mois après le début de la guerre hispano-américaine, et Tesla en profite pour annoncer que son invention va mettre fin à la guerre. Plusieurs pays demandent alors les droits à l'invention de Tesla ; en un an, treize pays possèdent le bateau de Tesla[150]. L'appareil fait sensation, certains considérant qu'il est mû par l'esprit de Tesla ou piloté par un singe savant caché à l'intérieur du bateau, et est bien reçu par le magazine Electrical Review, mais il est très critiqué par d'autres. Il est vu par Cyrus Fogg Brackett, professeur à l'université de Princeton, comme peu pratique à déployer en temps de guerre, et Electrical Engineer, un journal spécialisé tenu par Thomas Commerford Martin, un ami de Tesla, trouve l'invention  et l'accuse d'avoir copié la torpille de William Clarke[151],[152],[153].
De 1890 à 1906, Tesla consacre une grande partie de son temps et de sa fortune à une série de projets visant à développer la transmission d'énergie électrique sans fil. Il s'agit d'une extension de son idée d'utiliser des bobines pour transmettre l'énergie, qu'il a déjà démontrée avec l'éclairage sans fil. À l'époque où Tesla formule ses idées, il n'existe aucun moyen de transmettre sans fil des signaux de communication sur de longues distances, et encore moins de grandes quantités d'énergie. Tesla étudie les ondes radio et arrive à la conclusion qu'une partie de l'étude existante de Hertz à leur sujet est incorrecte[h],[154],[155]. De plus, Tesla note que, même si les théories sur les ondes radio sont véridiques, elles ne l'aident pas dans ses objectifs, puisque cette forme de « lumière invisible » diminue lorsque la distance augmente, comme tout autre rayonnement, et voyage en ligne droite dans l'espace, devenant « irrémédiablement perdue »[156].
Afin d'étudier plus avant la nature conductrice de l'air à basse pression, Tesla installe une station expérimentale à haute altitude à Colorado Springs en 1899[157],[158],[159],[160]. Il peut y faire fonctionner en toute sécurité des bobines beaucoup plus grandes que celles confinées dans son laboratoire de New York, et un associé passe un accord avec la compagnie d'électricité du comté d'El Paso pour lui fournir gratuitement du courant alternatif. Pour financer ses expériences, il persuade John Jacob Astor IV d'investir 100 000 dollars (3 073 200 dollars en 2023[90]) pour devenir actionnaire majoritaire de la Nikola Tesla Company. Astor pense qu'il investit surtout dans le nouveau système d'éclairage sans fil. Au lieu de cela, Tesla utilise cet argent pour financer ses expériences à Colorado Springs[27],[161]. Après son arrivée, il déclare aux médias qu'il prévoit de mener des expériences de télégraphie sans fil, en transmettant des signaux du pic Pikes à Paris[162].
Là, il mène des expériences avec une grande bobine fonctionnant dans la gamme des mégavolts, qui produit des éclairs artificiels (et du tonnerre) de millions de volts et des décharges jusqu'à 41 mètres de long[163] ; lors d'une expérience, il brûle par inadvertance le générateur d'El Paso, provoquant une panne de courant[164]. Ses observations du bruit électronique de la foudre l'ont amené à conclure, à tort, qu'il pouvait utiliser la totalité du globe terrestre pour conduire de l'électricité[165],[166].
Pendant son séjour, en 1899, Tesla observe des signaux inhabituels provenant de son récepteur, qu'il suppose être des communications provenant d'une autre planète. Il profite du début du nouveau siècle en janvier 1901[j] pour faire part de sa découverte. Bien que Tesla soit très prudent dans sa déclaration, les signaux sont très vite interprétés par les journalistes comme venant de Mars, et l'histoire fait le tour de la presse à sensation[168]. Une hypothèse de Marc Seifer, un biographe de Nikola Tesla, avance qu'il aurait détecté une expérience de Guglielmo Marconi, mais celui-ci se trouve alors en Italie, et la puissance de son système n'était probablement pas assez puissante pour envoyer des signaux jusqu'à Colorado Springs[k]. En 1996, les frères Corum ont recréé les conditions de test de Tesla et sont parvenus à l'hypothèse qu'il aurait alors capté un signal venant de Io, une des lunes de Jupiter, qui émet un signal de 10 kHz lorsqu'elle passe à travers un tore de particules de plasma chargées[170].
Tesla passe également un accord avec l'éditeur du Century Magazine pour produire un article sur ses découvertes. Le magazine envoie un photographe au Colorado pour photographier le travail de Tesla. L'article, intitulé The Problem of Increasing Human Energy, paraît dans le numéro de juin 1900 du magazine. Il y explique la supériorité du système sans fil qu'il a imaginé, mais l'article est davantage un traité philosophique qu'une description scientifique compréhensible de son travail, illustré par ce qui devient plus tard des images emblématiques de Tesla et de ses expériences à Colorado Springs[171].
En mars 1901, après être retourné à New York — au Waldorf-Astoria, où il vit — pour tenter de trouver des investisseurs pour ce qu'il pense être un système de transmission sans fil viable, il obtient 150 000 dollars (4 609 800 dollars en 2023[90]) de John Pierpont Morgan en échange de 51 % des brevets sans fil générés et commence à planifier l'installation de la tour Wardenclyffe à Shoreham, dans l'État de New York, à 161 kilomètres à l'est de la ville, sur la côte nord de Long Island[172],[173].
En juillet 1901, Tesla a élargi ses plans pour construire un émetteur plus puissant afin de devancer le système radio de Marconi, que Tesla pense être une copie du sien[168]. Il approche Morgan pour lui demander plus d'argent pour construire un plus grand système, mais Morgan refuse[174]. En décembre 1901, Marconi réussit à transmettre la lettre S depuis l'Angleterre jusqu'à Terre-Neuve, battant Tesla dans la course pour être le premier à réaliser une transmission transatlantique. Un mois après le succès de Marconi, Tesla tente de convaincre Morgan de soutenir un projet encore plus vaste visant à transmettre des messages et de l'énergie en contrôlant [168]. Au cours des cinq années suivantes, Tesla écrit plus de 50 lettres à Morgan, demandant et exigeant des fonds supplémentaires pour achever la construction de Wardenclyffe. Tesla poursuit le projet pendant neuf mois supplémentaires en 1902. La tour est érigée à une hauteur de 57 mètres. En juin 1902, Tesla déplace les activités de son laboratoire de Houston Street à Wardenclyffe[172].
De par son succès, Marconi a commencé à attirer de plus en plus d'investisseurs, tandis qu'une partie de la presse commence à parler du projet de Tesla comme d'une imposture. Tesla manque cruellement d'argent, ce qui ne lui permet pas, en 1903, de recouvrir le sommet de la tour de cuivre, empêchant celle-ci de fonctionner comme prévu. Morgan, suivi par d'autres investisseurs, se retire. Le projet est donc un échec, et Tesla finit par hypothéquer la propriété de Wardenclyffe pour couvrir ses dettes au Waldorf-Astoria[175].
Après la fermeture de Wardenclyffe, Tesla continue d'écrire à Morgan ; après la mort du , Tesla écrit au fils de Morgan, Jack, pour essayer d'obtenir des fonds supplémentaires pour le projet. En 1906, Tesla ouvre des bureaux au 165 Broadway à Manhattan, essayant de lever des fonds supplémentaires en développant et en commercialisant ses brevets. Il tient ensuite des bureaux à la Metropolitan Life Tower de 1910 à 1914. Il loue pendant quelques mois le Woolworth Building, mais déménage car il ne peut pas payer le loyer au 8 West 40th Street de 1915 à 1925. Après avoir déménagé au 8 West 40th Street, il fait faillite : la plupart de ses brevets ont expiré et il rencontre des difficultés avec les nouvelles inventions qu'il essaye de développer[176].
Le jour de son 50e anniversaire, en 1906, Tesla fait la démonstration d'une turbine sans aube de 200 chevaux (150 kilowatts) à 16 000 tr/min. En 1910-1911, à la Waterside Power Station de New York, plusieurs de ses moteurs à turbine sans aube sont testés entre 100 et 5 000 ch[177]. Tesla travaille avec plusieurs entreprises, notamment de 1919 à 1922 à Milwaukee pour Allis-Chalmers[178]. Un manque d'intérêt de la part de l'industrie empêche les problèmes techniques d'être surmontés et l'invention de Tesla n'est finalement jamais transformée en un dispositif pratique[179]. Tesla accorde une licence pour l'idée à une société d'instruments de précision et elle est finalement utilisée sous la forme de compteurs de vitesse de voitures de luxe et d'autres instruments[180].
Lorsque la Première Guerre mondiale éclate, les Britanniques coupent le câble télégraphique transatlantique reliant les États-Unis à l'Allemagne afin de contrôler le flux d'informations entre les deux pays. Ils tentent également de couper les communications sans fil allemandes à destination et en provenance des États-Unis en faisant en sorte que la société américaine Marconi poursuive la société de radio allemande Telefunken pour violation de brevet[181]. Telefunken fait appel aux physiciens Jonathan Zenneck et Karl Ferdinand Braun pour sa défense et engage Tesla comme témoin pendant deux ans pour 1 000 dollars par mois. L'affaire piétine puis devient sans objet lorsque les États-Unis entrent en guerre contre l'Allemagne en 1917[181].
En 1915, Tesla tente de poursuivre la société Marconi pour violation de ses brevets d'accord sans fil. Le premier brevet radio de Marconi est accordé aux États-Unis en 1897, mais sa demande de brevet de 1900 couvrant les améliorations de la transmission radio est rejetée plusieurs fois, avant d'être finalement approuvée en 1904, au motif qu'elle viole d'autres brevets existants, y compris deux brevets de Tesla de 1897 sur le réglage de l'énergie sans fil[155],[182],[183]. L'affaire Tesla de 1915 n'aboutit pas[184], mais dans une affaire connexe, où la Marconi Company tente de poursuivre le gouvernement américain pour violation de brevets de la Première Guerre mondiale, une décision de la Cour suprême des États-Unis de 1943 rétablit les brevets antérieurs d'Oliver Lodge, de John Stone Stone et de Tesla[185]. La cour déclare que sa décision n'a aucune incidence sur la revendication de Marconi en tant que premier à réaliser une transmission radio, simplement que puisque la revendication de Marconi à certaines améliorations brevetées est douteuse, la société ne peut pas revendiquer la violation de ces mêmes brevets[155],[186].
Le rival de Nikola Tesla remporte le prix Nobel de physique en 1909 et la tour de Wardenclyffe est détruite en 1917. Nikola Tesla vit désormais reclus dans une chambre de l'hôtel New Yorker, refusant toute charité, mais recevant de la Westinghouse Electric & Manufacturing Company un salaire mensuel de 125 $ pour continuer ses différentes recherches[188].
En 1928, Tesla dépose son dernier brevet, un biplan à décollage et atterrissage verticaux.
À l'automne 1937, voulant éviter un taxi, Tesla est victime d'une mauvaise chute alors qu'il fait son trajet régulier vers la cathédrale et Central Park où il a l'habitude de nourrir les pigeons et de les recueillir dans son hôtel. Refusant de consulter un médecin ou d'être amené à l'hôpital, il est raccompagné dans sa chambre d'hôtel où il ne se rétablira jamais complètement.
Perclus de TOC[189], insomniaque chronique, il meurt le 7 janvier 1943 dans sa chambre d'hôtel à New York, seul, sans un sou et couvert de dettes, laissant derrière lui plus de 300 brevets et la réputation de savant génial, visionnaire et à moitié fou. Il reçoit le 17 janvier des funérailles nationales dans la cathédrale Saint-Jean le Théologien de New York, regroupant 2 000 personnes. Son corps est ensuite transporté au Ferncliff Cemetery où il est par la suite incinéré[190].
Après le décès de Nikola Tesla, sa famille engage avec l’administration américaine une longue procédure judiciaire pour acquérir ses documents de travail et ses effets personnels. En 1952, son neveu Sava Kosanović obtient que sa succession entière (manuscrits originaux, milliers de lettres, de photographies et la plupart de ses inventions) soit expédiée à Belgrade. Après un long procès, ce même neveu réussit, en 1957, à récupérer l’urne funéraire de son oncle. L’urne et les documents sont aujourd’hui au musée Nikola-Tesla à Belgrade en Serbie[191].
Tesla est l'auteur d’environ 300 brevets dont beaucoup sont attribués à tort à Thomas Edison[2] traitant de nouvelles méthodes pour aborder la conversion de l’énergie.
Les théories de Tesla sur la possibilité de la transmission sans fil remontent à des conférences et des démonstrations qu'il a réalisées en 1893 à Saint-Louis dans le Missouri, au Franklin Institute en Pennsylvanie, et à la National Electric Light Association (en). Il met au point notamment la bobine Tesla vers 1891, puis entre 1895 et 1898 un transmetteur à amplification (en)[197].
En août 1917, Tesla propose les fréquences et l’énergie nécessaires pour un système de repérage à distance des obstacles dans le périodique The Electrical Experimenter[198]. Il propose l’utilisation d’une onde entretenue pour repérer les objets, cette onde formant une onde stationnaire avec la réflexion par la cible lorsque la fréquence est ajustée convenablement (Radar à ondes entretenues). La fréquence utilisée permet alors de déterminer la distance de l’objet ou sa variation dans le temps permet de déduire la vitesse radiale de déplacement. Il propose comme alternative l’utilisation d’impulsions pour obtenir le même résultat. Tesla envisageait l’affichage des échos résultants sur un écran fluorescent, une idée reprise par le radar.
Nikola Tesla mesure 1,88 m et pèse 64 kg entre 1888 et 1926, son poids variant très peu pendant cette période[201]. Son apparence est décrite par le rédacteur en chef du journal Arthur Brisbane comme [202]. Alors qu'il vit à New York, il est élégant et stylé, méticuleux dans sa toilette, ses vêtements et dans ses activités quotidiennes, une apparence qu'il entretient afin de favoriser ses relations d'affaires. Il s'est autoproclamé [203]. Il est également décrit comme ayant les yeux clairs, de  et des pouces [202].
Au cours de sa vie, Tesla lit de nombreux ouvrages, mémorisant des livres complets, et possède soi-disant une mémoire photographique[204]. Il est polyglotte, parlant huit langues : serbo-croate, tchèque, anglais, français, allemand, hongrois, italien et latin[205]. Tesla raconte dans son autobiographie qu'il a vécu des moments d'inspiration détaillés. Il lui arrive à plusieurs occasions de voir des éclairs de lumière aveuglants, souvent accompagnés de visions. Souvent, les visions sont liées à un mot ou à une idée qu'il aurait pu rencontrer ; à d'autres moments, elles apportent la solution à un problème particulier qu'il rencontre. En entendant le nom d'un objet, il peut l'imaginer dans ses moindres détails. Tesla peut visualiser une invention dans son esprit avec une extrême précision, en incluant toutes les dimensions, avant de passer à l'étape de la construction, une technique parfois appelée . Il ne fait généralement pas de dessins à la main, mais travaille de mémoire. Dès son enfance, Tesla a de fréquents flashbacks sur des événements qui se sont produits auparavant dans sa vie[204].
Tesla est asocial et a tendance à s'isoler dans son travail[206],[207],[208]. Cependant, lorsqu'il s’engage finalement dans des activités sociales, de nombreuses personnes parlent de Tesla de manière très positive et admirative. Robert Underwood Johnson l'a décrit comme ayant atteint [209]. Sa secrétaire, Dorothy Skerrit, a écrit : [203]. L'ami de Tesla, Julian Hawthorne, a écrit : [210].
Tesla ne s'est jamais marié, expliquant sa chasteté comme très utile pour ses capacités scientifiques[204]. Margaret Cheney, auteure de Nikola Tesla : L'homme qui a éclairé le monde, explique également que ses multiples phobies font de Tesla [c 14]. Tesla aurait cependant eu un nombre important de femmes intéressées à lui et, alors qu'il vit à New York, il aurait loué deux chambres d'hôtels en même temps à deux endroits différents de la ville pour [212].
Tesla est un bon ami de Francis Marion Crawford[213], Robert Underwood Johnson[214], Stanford White[215], Fritz Lowenstein, George Scherff et Kenneth Swezey[216],[217],[218]. Au milieu de sa vie, Tesla devient un ami proche de Mark Twain ; ils passent beaucoup de temps ensemble dans son laboratoire et ailleurs[219]. Twain a notamment décrit l'invention du moteur à induction de Tesla comme [220]. Lors d'une fête organisée par l'actrice Sarah Bernhardt en 1896, Tesla rencontre le moine hindou indien Vivekananda et tous deux discutent de la façon dont les idées de l'inventeur sur l'énergie semblent correspondre à la cosmologie védantique[221]. À la fin des années 1920, Tesla se lie d'amitié avec George Sylvester Viereck, poète, écrivain, mystique et plus tard, propagandiste nazi. Tesla assiste occasionnellement à des dîners organisés par Viereck et sa femme[222],[223].
Tesla peut parfois être dur et exprimer ouvertement son dégoût pour les personnes en surpoids. Il est prompt à critiquer les vêtements ; à plusieurs reprises, Tesla ordonne à une employée de rentrer chez elle et de changer de robe[224]. À la mort de Thomas Edison, en 1931, Tesla fournit la seule opinion négative d'une vaste couverture de la vie d'Edison pour The New York Times :
« Il n'avait pas de passe-temps, ne s'occupait d'aucune sorte d'amusement et vivait au mépris des règles d'hygiène les plus élémentaires... Sa méthode était extrêmement inefficace, car il fallait couvrir un terrain immense pour obtenir quoi que ce soit, à moins que le hasard n'intervienne et, au début, j'ai presque été un témoin désolé de ses actes, sachant qu'un peu de théorie et de calcul lui aurait permis d'économiser 90 % du travail. Mais il avait un véritable mépris pour l'apprentissage des livres et les connaissances mathématiques, se fiant entièrement à son instinct d'inventeur et son sens pratique américain[225]. »
Tesla a affirmé ne jamais dormir plus de deux heures par nuit, mais il a admis  de temps en temps [226],[227]. Durant sa deuxième année d'études à Graz, Tesla a développé une compétence passionnée pour le billard, les échecs et le jeu de cartes, passant parfois plus de 48 heures d'affilée à une table de jeu[228]. À une occasion, dans son laboratoire, Tesla a travaillé pendant 84 heures sans repos[229]. Kenneth Swezey, un journaliste avec qui Tesla s'était lié d'amitié, a confirmé que Tesla dormait rarement. Swezey se souvient d'une fois où Tesla l'a appelé à 3 heures du matin : [227].
Tesla travaillait tous les jours de 9 h à 18 h ou plus tard, avec un dîner à 20 h 10 exactement, au restaurant Delmonico's, ou plus tard dans sa vie à l'hôtel Waldorf-Astoria. Tesla commandait alors par téléphone son dîner au maître d'hôtel, qui était le seul à pouvoir le servir. [230]. Tesla est devenu végétarien dans les dernières années de sa vie, ne vivant que de lait, de pain, de miel et de jus de légumes[231],[232].
Tesla n'est pas d'accord avec la théorie selon laquelle les atomes sont composés de particules subatomiques plus petites, affirmant qu'il n'existe pas d'électrons créant une charge électrique. Il pense que si les électrons existent, ils sont un quatrième état de la matière ou  qui ne peut exister que dans un vide expérimental et qu'ils n'ont rien à voir avec l'électricité[233],[234]. Tesla pense que les atomes sont immuables — ils ne peuvent pas changer d'état ou être divisés de quelque façon que ce soit. Il croit au concept du XIXe siècle d'un éther omniprésent qui transmet l'énergie électrique[235].
Tesla est généralement en désaccord avec les théories sur la conversion de la matière en énergie[236]. Il est également critique de la théorie de la relativité d'Albert Einstein, disant :
« Je soutiens que l'espace ne peut être courbé, pour la simple raison qu'il ne peut avoir aucune propriété. On pourrait tout aussi bien dire que Dieu a des propriétés. Il n'a pas de propriétés, mais seulement des attributs et ceux-ci sont de notre propre fabrication. On ne peut parler de propriétés que lorsqu'il s'agit de la matière qui remplit l'espace. Dire qu'en présence de grands corps, l'espace devient courbe équivaut à dire que quelque chose peut agir sur rien. Pour ma part, je refuse de souscrire à une telle opinion[237]. »
Tesla prétend avoir développé son propre principe physique concernant la matière et l'énergie, sur lequel il commence à travailler en 1892 et, en 1937, à l'âge de 81 ans, il affirme dans une lettre avoir achevé une  qui [238],[239]. Il affirme que la théorie est  et qu'il espère la donner bientôt au monde. Ses écrits n'ont jamais permis d'élucider davantage sa théorie[240].
Tesla est largement considéré par ses biographes comme étant un humaniste dans sa vision philosophique en plus de ses dons de scientifique technologique[241],[242],[243]. Cela ne l'empêche pas, comme beaucoup de son époque, de devenir un partisan d'une version de l'eugénisme, basée sur une sélection artificielle imposée[244]. Bien que son argumentation ne dépende pas du concept de  ou de la supériorité inhérente d'une personne par rapport à une autre, il plaide en faveur de l'eugénisme dans une interview accordée en 1937 et déclare : 
Jeune, Tesla ne s'estime pas digne d'une femme, considérant les femmes comme supérieures sur tous les points. Son opinion commence à osciller plus tard, lorsqu'il se met à penser que les femmes essayent de surpasser les hommes et de se rendre plus dominantes. Cette  est accueillie avec beaucoup d'indignation par Tesla, qui a le sentiment que les femmes perdent leur féminité en essayant d'être au pouvoir. Dans une interview accordée au Galveston Daily News le 10 août 1924, il déclare : [246].
Au 21e siècle, l'interview du Galveston Daily News rend Tesla populaire auprès du mouvement MGTOW, une communauté masculiniste anti-mariage qui considère l'inventeur comme l'un des pionniers de leur mouvement. Selon Massimo Teodorani, auteur de la biographie Tesla, L'éclair du génie, le rapprochement entre Tesla et ce mouvement relève de l'instrumentalisation politique. Toujours selon Teodorani, l'interview démontre plus la personnalité étrange de Tesla qu'une mentalité sexiste[247]. Dans une autre interview datée de 1926, Tesla déclare d'ailleurs : [c 15],[247],[248].
Tesla est élevé en tant que chrétien orthodoxe mais ne se considère pas comme un . Il déclare s'opposer au fanatisme religieux et que . Il a également déclaré :  et [245].
Accessoirement, il s’intéresse aussi à la mythologie hindoue, ainsi qu'au sanskrit[249].
La plupart des membres de la famille de Nikola Tesla ont été tués par les Oustachis lors du génocide contre les Serbes dans l'État indépendant de Croatie[250],[251] ; William Terbo, petit-fils de Angelina Tesla — la sœur aînée de Nikola — et décédé en 2018 dans le New Jersey, a revendiqué être le dernier membre connu de la famille de Nikola Tesla[252],[253]. Des descendants de cousins germains de Nikola Tesla portent toujours le nom de famille « Tesla »[254].
L'héritage de Tesla perdure cependant dans de nombreux livres, films, séries télévisées, pièces de théâtre, bandes dessinées, jeux vidéo et également dans le monde de la musique. L'impact des technologies inventées ou imaginées par Tesla est un thème récurrent dans divers types de science-fiction[255],[256].
Plusieurs lieux, objets et entreprises ont été nommés en hommage à Tesla, comme la bobine Tesla, l'oscillateur de Tesla, la tour de Tesla, la turbine de Tesla, l'unité de l'induction électromagnétique, la société Tesla, Inc. et l'aéroport Nikola-Tesla de Belgrade[257],[258],[259],[260],[261],[262]. Un cratère sur la Lune et l'astéroïde (2244) Tesla portent également le nom de l'inventeur[263]. Depuis 2023, son portrait orne les pièces de 10, 20 et 50 centimes d'euro croates[264].
Le scientifique a également été célébré par des dates commémoratives nationales et régionales dans le monde entier, comme en Serbie, au Canada, à Niagara Falls dans l'État de New York et à Palo Alto, aux États-Unis, à Niagara Falls en Ontario et à Hamilton, au Canada, et à Bakou, en Azerbaïdjan[265],[266],[267],[268],[269],[270],[271],[272].
Pour les articles homonymes, voir Sartre.
modifier - modifier le code - modifier WikidataJean-Paul Sartre [ ʒɑ̃pol saχtχ][n 1] est un philosophe et écrivain français, né le 21 juin 1905 dans le 16e arrondissement de Paris et mort le 15 avril 1980 dans le 14e arrondissement. 
Représentant du courant existentialiste, il a marqué la vie intellectuelle et politique de la France de 1945 à la fin des années 1970.
Écrivain prolifique, fondateur et directeur de la revue Les Temps modernes (1945), il est connu aussi bien pour son œuvre philosophique et littéraire qu'en raison de ses engagements politiques[n 2], d'abord en liaison avec le Parti communiste, puis avec des courants gauchistes, au sens léniniste[2] du terme, plus particulièrement maoïstes, dans les années 1970.
Intransigeant et fidèle à ses idées, il a toujours rejeté tant les honneurs que toute forme de censure ; il a notamment refusé le prix Nobel de littérature en 1964. Exception notable, il a cependant accepté le titre de docteur honoris causa de l'Université de Jérusalem en 1976. Il refusa de diriger une série d'émissions télévisées qu'on lui proposait, parce qu'on y mettait comme condition la réalisation d'une maquette préalable, et expliqua : « Je n'ai plus l'âge de passer des examens. » Il contribua à la création du journal Libération, allant jusqu'à le vendre lui-même dans les rues pour donner plus de publicité à son lancement.
Il a partagé sa vie avec Simone de Beauvoir, philosophe de l'existentialisme et féministe, avec laquelle il a formé un couple célèbre du XXe siècle. Leurs philosophies, bien que très proches, ne sauraient être confondues. De 1949 jusqu'à sa mort, il a simultanément vécu une liaison avec Michelle Vian, la première épouse de Boris Vian, qui tape notamment ses textes à la machine en vue de leur parution dans la revue Les Temps modernes[3],[4].
D'autres intellectuels ont joué pour lui un rôle important à différentes étapes de sa vie : Paul Nizan et Raymond Aron, ses condisciples à l'École normale supérieure ; Maurice Merleau-Ponty et Albert Camus dans les années d'après-guerre, puis Benny Lévy (alias Pierre Victor) à la fin de sa vie.
Selon de nombreux commentateurs et pour Sartre lui-même, sa vie est séparée en deux par la Seconde Guerre mondiale. On distingue alors deux grandes périodes dans l'œuvre sartrienne : une approche philosophique théorique axée sur l'ontologie de L'Être et le Néant (1943) ; puis une période plus pratique, où l'auteur cherche à appliquer sa méthode exposée dans la Critique de la raison dialectique (1960)[5]. Cette seconde période de son œuvre a fortement influencé les sociologues qualitativistes comme Erving Goffman.
Jean-Paul Sartre laisse derrière lui une œuvre considérable, sous forme de romans, d'essais, de pièces de théâtre, d'écrits philosophiques ou de biographies. Sa philosophie a marqué l'après-guerre, et il est, avec Albert Camus, un symbole de l'intellectuel engagé. Comme ce dernier l'avait été en 1957, il sera distingué en 1964 du prix Nobel de littérature, qu'il déclinera.
De son supposé engagement dans la résistance en 1941 (engagement mis en doute en raison de son attitude trouble durant l'Occupation) jusqu'à sa mort en 1980, Sartre n'a cessé de défrayer la chronique. Il s'investit en effet sur de nombreux sujets, embrassant avec ferveur les causes qui lui ont semblé justes. Parfois assimilé à un Voltaire[n 3] du XXe siècle, Sartre demeure un militant jusqu'au bout de sa vie.
Jean-Paul Charles Aymard Sartre naît le 21 juin 1905, au no 13 rue Mignard[7], au domicile de ses grands-parents maternels[8] dans le 16e arrondissement de Paris. Fils unique, il est issu d’une famille bourgeoise[9] : sa mère Anne-Marie Schweitzer[10],[11] appartient à une famille d’intellectuels et de professeurs alsaciens, les Schweitzer (elle est la cousine d'Albert Schweitzer[12]), son oncle maternel Georges Schweitzer, frère de sa mère donc, est polytechnicien (X 1895), ingénieur du Génie maritime[13],[n 4], son père Jean-Baptiste, fils d'un docteur en médecine de Thiviers[15], est également polytechnicien de la promotion 1895, sorti en 1897 officier de marine[16],[17],[n 5]. Le couple s'est uni le 3 mai 1904 dans le 16e arrondissement de Paris[19] et le petit Sartre, né quelque treize mois plus tard, n'a jamais connu son père[n 6], qui meurt de la fièvre jaune le 17 septembre 1906, quinze mois après sa naissance[20].
De 1907 à 1917 le petit « Poulou », comme on l’appelle, va en effet vivre avec sa mère chez ses grands-parents maternels. Il y passe dix années heureuses, adoré, choyé, félicité tous les jours, ce qui contribue sans doute à construire chez lui un certain narcissisme. Comme il est orphelin à l'âge de quinze mois, c’est son grand-père, Charles Schweitzer[21], professeur agrégé d'allemand à la retraite, auteur du Deutsches Lesebuch, une méthode expérimentale reconnue sous la IIIe République[22], qui fit son instruction avant son admission à l’école publique à dix ans. Dans la grande bibliothèque de la maison Schweitzer il découvre très tôt la littérature, et préfère lire plutôt que de fréquenter les autres enfants (enfance évoquée dans son autobiographie Les Mots[23]).
Cette période se termine le 26 avril 1917 lorsque sa mère se remarie dans le 5e arrondissement de Paris[24] avec Joseph Mancy, polytechnicien (X 1895), ingénieur du Génie maritime[25],[n 7], de la même promotion que son frère Georges et son défunt mari. Sartre, alors âgé de 12 ans, ne finira jamais de haïr son beau-père. Le couple déménage alors à La Rochelle[26], où Sartre restera jusqu'à l'âge de 15 ans, trois années qui seront pour lui des années de calvaire : il passe en effet du climat familial heureux à la réalité des lycéens qui lui paraissent violents et cruels.
Vers l’été 1920, malade, Jean-Paul Sartre est ramené d’urgence à Paris. Soucieuse de son éducation qui pourrait être « pervertie » par les mauvais garçons du lycée Eugène-Fromentin[29] de La Rochelle, sa mère décide que son fils restera à Paris.
À treize ans Sartre est brièvement inscrit au lycée Montaigne[30]. Il revient à seize ans au lycée Henri-IV où il avait été élève en sixième et cinquième. Il y retrouve Paul Nizan, lui aussi apprenti écrivain, avec qui il nouera une amitié si fusionnelle qu'ils étaient surnommés « Nitre et Sarzan »[31]. Épaulé par cette amitié, Sartre commence à se construire une personnalité. Pour l’ensemble de la « classe d’élite » — « option » latin et grec — dans laquelle il étudie, Sartre devient le SO, c'est-à-dire le « satyre officiel » : il excelle en effet dans la facétie, la blague.
Sartre, toujours accompagné de Paul Nizan, prépare le concours d'entrée à l'École normale supérieure au lycée Louis-le-Grand. Il y fait ses premières armes littéraires en écrivant notamment deux petits contes, deux sinistres histoires de professeurs de province, dans lesquelles éclatent son ironie et son dégoût pour les vies conventionnelles. Dans le même temps Sartre reprend son rôle d’amuseur public avec Nizan, jouant blagues et petites scènes entre les cours. En 1924, deux ans après leur entrée à Louis-le-Grand, Sartre et Nizan sont tous deux reçus au concours de l'École normale supérieure de Paris (ENS)[32],[33].
Sartre se fait tout de suite remarquer dans ce que Nizan appelle « l’école prétendue normale et dite supérieure ». Sartre reste en effet le redoutable instigateur de toutes les plaisanteries, de tous les chahuts, allant jusqu’à provoquer un scandale en jouant avec ses amis un sketch antimilitariste dans la revue de l’ENS de 1927, après lequel Gustave Lanson, directeur de l'école, démissionnera. La même année il signe avec ses condisciples, et à la suite d'Alain, Lucien Descaves, Louis Guilloux, Henry Poulaille, Jules Romains, Séverine…, la protestation (parue le 15 août dans la revue Europe[34]) contre la loi sur l’organisation générale de la nation pour le temps de guerre qui abroge toute indépendance intellectuelle et toute liberté d'opinion.
Sartre a ainsi déjà un goût pour la provocation et le combat contre l’autorité. Il acquiert aussi une grande notoriété parmi ses professeurs et se fait ovationner dans chacune de ses arrivées au réfectoire. Si Sartre est volontiers un boute-en-train, c’est aussi un grand travailleur, dévorant plus de 300 livres par an, écrivant chansons, poèmes, nouvelles, romans à tours de bras. Sartre se lie d'amitié avec d'aucuns qui deviendront par la suite célèbres, comme Pierre-Henri Simon, Raymond Aron[35],[36], Maurice Merleau-Ponty ou encore Henri Guillemin.
Pourtant, au cours de ces quatre années à l'École normale supérieure, Sartre ne paraît pas s’intéresser à la politique. Spontanément anarchisant, il ne va à aucune manifestation, ne s’enflamme pour aucune cause. À la surprise de ses admirateurs, qui s'interrogent sur une possible erreur du jury, Sartre échoue en 1928 au concours d'agrégation de philosophie, alors que Raymond Aron est classé premier[37],[38] ; il dira lui-même avoir fait preuve de trop d’originalité.
Préparant d'arrache-pied le concours pour la seconde fois, il rencontre dans son groupe de travail Simone de Beauvoir, présentée par un ami commun, René Maheu[n 8], qui la surnommait « castor », par référence à l'anglais beaver (qui signifie « castor ») : d'une part, cet animal symbolise le travail et l’énergie, ou l'esprit constructeur ; d'autre part, la sonorité du mot beaver est proche de celle du nom « Beauvoir ». Ce surnom sera adopté par Sartre et elle deviendra sa compagne jusqu'à la fin de sa vie. Elle sera son  en opposition aux  qu’ils seront amenés à connaître tous deux[40]. En 1929, à la seconde tentative[41], Sartre est reçu premier au concours d'agrégation. Simone de Beauvoir obtenant la deuxième place[42].
Sur les conseils de Raymond Aron, Sartre accomplit à partir de novembre 1929 son service militaire obligatoire d'un an dans la section Météorologie de l'Armée de l'air, avec Aron pour sergent instructeur[43],[44],[n 9]. Ce même Raymond Aron lui conseille en 1930 de lire la Théorie de l’intuition dans la phénoménologie de Husserl, un ouvrage d’Emmanuel Levinas[n 10]. Sartre se procure l’ouvrage. La découverte de Husserl est un choc : [46]. Sartre se dit : [47].
Au retour du service militaire, âgé alors de 26 ans, Sartre convoite un poste de lecteur au Japon, pays qui l’a toujours intéressé[48]. Rêve brisé, le poste lui est refusé et il est envoyé au lycée du Havre, aujourd'hui lycée François-Ier, à compter de mars 1931[43]. C’est une épreuve qui commence pour Sartre qui craint tellement les vies rangées et a tellement critiqué dans ses écrits la vie ennuyeuse de professeur de province.
Sartre entre alors de plain-pied dans la vie réelle, le travail et la vie quotidienne. S’il choque quelque peu les parents et les professeurs par ses manières, comme arriver en classe sans cravate, il séduit ses élèves, pour lesquels il est un excellent professeur, chaleureux et respectueux, et souvent un ami. De là naît sa complicité avec l’adolescence, un contact qu’il aimera toujours avoir tout au long de sa vie[réf. nécessaire].
Il découvre Voyage au bout de la nuit de Louis-Ferdinand Céline en 1932[n 11], une œuvre qui le marquera durablement. À l'été 1933, il prend à l’Institut français de Berlin la succession de Raymond Aron, lequel, échange de bons procédés, lui succède au lycée du Havre de l'automne 1933 à l'été 1934[43]. Sartre complète alors son initiation à la phénoménologie de Husserl et découvre l'ouvrage de Martin Heidegger, Sein und Zeit (Être et Temps). Là encore, c’est un choc[50].
En octobre 1934 il reprend son poste au Havre[7]. Il publie en 1936 L'Imagination et La Transcendance de l'Ego. Il remet le manuscrit de Melancholia à Gallimard qui le refuse malgré la bonne appréciation de Paulhan[7]. La gloire qu'il pensait obtenir depuis l'enfance, ces années au Havre la remettent en cause. En octobre 1936 il obtient sa mutation à l'école normale d'instituteurs de Laon, dans l'Aisne[7] où il enseigne un an en classe de terminale[51].
L'année suivante Sartre est muté en octobre 1937 au lycée Pasteur de Neuilly, où il fait la connaissance de Robert Merle[52]. Commence alors pour lui une brève phase de notoriété avec la publication en juillet 1937, dans la Nouvelle Revue française, d'une nouvelle, Le Mur, reprise en 1939 dans le recueil Le Mur. Gide tient ce recueil pour un « chef-d'œuvre »[7]. Il reprend ensuite son manuscrit de Melancholia et accepte en avril 1938 le titre définitif La Nausée que lui propose Gaston Gallimard. Le livre, qui manquera de peu le prix Goncourt[7], est un roman philosophique (« phénoménologique ») et quelque peu autobiographique, marqué par l'influence de Céline, racontant les tourments existentiels d'Antoine Roquentin, célibataire de 35 ans et historien à ses heures.
Cette notoriété naissante est brusquement éclipsée par le déclenchement de la Seconde Guerre mondiale : Sartre est mobilisé le 2 septembre 1939[53],[54].
Avant la guerre, Sartre n’a pas de conscience politique. Pacifiste, mais sans militer pour la paix, l’antimilitariste Sartre assume pourtant la guerre sans hésiter. L’expérience de la guerre et de la vie en communauté va le transformer du tout au tout.
Il est affecté pendant la « drôle de guerre » à la 70e division, au camp d'aviation militaire d'Essey-lès-Nancy, comme soldat chargé des sondages météorologistes[44]. Puis sa myopie le fait transférer, une semaine plus tard, à Marmoutier (Bas-Rhin), puis Ittenheim, Brumath, Morsbronn et Bouxwiller[53],[54].
Sa fonction lui laisse beaucoup de temps libre, qu'il utilise pour écrire énormément (en moyenne douze heures par jour pendant neuf mois, soit 2 000 pages, dont une petite partie sera publiée sous le titre de Carnets de la drôle de guerre). Il écrit d’abord pour éviter le contact avec ses compagnons de route, car il supporte en effet assez mal les relations sérieuses et hiérarchiques de l’armée[n 12].
La drôle de guerre prend fin en mai 1940, et le faux conflit devient bien réel. Le 21 juin Sartre est fait prisonnier à Padoux, dans les Vosges, et est transféré dans un camp de détention (Stalag XII D) de 25 000 détenus en Allemagne, près de Trêves. Son expérience de prisonnier le marque profondément : elle lui enseigne la solidarité avec les hommes. Loin de se sentir brimé, il participe à la vie communautaire : il raconte histoires et blagues à ses copains de chambrée, participe à des matchs de boxe, écrit et met en scène une pièce pour la veillée de Noël 1940, Bariona, ou le Fils du tonnerre[56]. Elle se déroule en Judée au temps de l'occupation romaine, et l'on y voit un fonctionnaire romain tenir des propos sévères à l'égard des Juifs. Le caporal Jean Pierre, compagnon de captivité de Sartre, juge que la pièce est d'inspiration antisémite. Annie Cohen-Solal, biographe de Sartre, se demande si ces propos doivent être mis sur le compte de l'inconscience ou de la maladresse de Sartre[57].
Cette vie dans le camp de prisonniers est importante, car elle est le tournant de sa vie : dorénavant, il n’est plus l’individualiste des années 1930, mais se fixe un devoir dans la communauté.
En mars 1941 Sartre est libéré grâce à un faux certificat médical alléguant une [58]. D'après les auteurs Gilles et Jean-Robert Ragache, il doit sa libération à l'intervention de Drieu la Rochelle : [59].
De retour à Paris, il aurait fondé avec certains de ses amis, dont Simone de Beauvoir, un mouvement de Résistance, « Socialisme et liberté »[60]. Annie Cohen-Solal mentionne plusieurs réunions avec notamment, Maurice Merleau-Ponty, Raymond Marrot, Simone de Beauvoir, François Cuzin, Simone Debout-Oleszkiewicz, Yvonne Picard, Jean Pouillon, Jacques-Laurent Bost. Elle évoque la rédaction et la diffusion de tracts et cite les témoignages de Beauvoir, Georges Chazelas, Dominique et Jean-Toussaint Desanti[61]. Herbert R. Lottman rapporte également la brève existence de ce groupe[62]. Il faut noter cependant qu'aucune recherche n'a pu mettre en évidence une quelconque existence de ce mouvement (Le Catalogue des périodiques clandestins diffusés en France de 1939 à 1945, publié par la Bibliothèque nationale en 1954, n'en fait aucune mention) ou d'activité de résistance de Sartre durant cette période, ce que confirme le journaliste résistant Henri Noguères à l'historien Gilbert Joseph[63] :
« Je maintiens qu'en une vingtaine d'années consacrées à la recherche et à des travaux sur l'histoire de la Résistance en France, je n'ai jamais rencontré Sartre ou Beauvoir[64],[65]. »
Il sera d'ailleurs profondément critiqué par Jankélévitch qui lui reprochera de s'être occupé davantage de l'avancement de sa carrière que de dénoncer ou contrarier l'occupant[66]. En été 1941 il aurait traversé la province à vélo pour tenter en vain d’étendre le mouvement hors de la capitale et de rallier d’autres intellectuels comme Gide ou Malraux[60]. Après l’arrestation de deux camarades, le groupe « Socialisme et liberté » se serait auto-dissous vers la fin 1941[67]. Sartre a toujours reconnu l'échec de cette tentative, précisant après la guerre : « Nous n'étions pas des résistants qui écrivaient, mais des écrivains qui résistaient »[réf. souhaitée].
En octobre 1941 Jean-Paul Sartre est affecté au lycée Condorcet sur le poste de professeur de khâgne en remplacement de Ferdinand Alquié. Ce poste était initialement (et jusqu'en 1940) occupé par le professeur Henri Dreyfus-Le Foyer, évincé en raison de sa judéité. Sartre a au préalable certifié sur l'honneur qu'il n'était ni franc-maçon ni juif, comme l'exigent les autorités françaises[68]. Effet d'aubaine indubitable, ce fait révélé en octobre 1997 par Jean Daniel dans un éditorial du Nouvel Observateur lui sera reproché, mais pas à Ferdinand Alquié. Ingrid Galster (de) se pose la question de la qualité de l'engagement de Sartre et remarque  Il publie à cette époque plusieurs articles dans la revue Comœdia[70], collaborationniste, qui fut dirigée du 21 juin 1941 au 5 août 1944 par René Delange et contrôlée par la Propaganda-Staffel[71],[72].
Sartre fait jouer en 1943 une pièce qu’il a composée, Les Mouches[69], reprenant le mythe d’Électre et qui sera présentée comme un appel symbolique à résister à l'oppresseur. Cette interprétation sera contestée, notamment par Gilbert Joseph pour qui l'intention résistante y est absente ou invisible[60]. C'est lors de la première qu'il fait la connaissance de Camus. En cette période d'occupation la pièce n'a pas le retentissement escompté : salles vides, représentations interrompues plus tôt que prévu. Pour Jean Amadou, cette représentation est plus ambiguë :
« En 1943, dans l'année la plus noire de l'Occupation, il fit jouer à Paris Les Mouches. C'est-à-dire qu'il fit très exactement ce que fit Sacha Guitry, donner ses pièces en représentation devant un parterre d'officiers allemands[71], à cette différence qu'à la Libération, Guitry fut arrêté alors que Sartre fit partie du Comité d'épuration, qui décidait quel écrivain avait encore le droit de publier et quel autre devait être banni. André Malraux qui, lui, avait risqué sa vie dans la Résistance, ne se crut pas autorisé pour autant à faire partie de ce tribunal autoproclamé[73]. »
À compter de 1943 et sur l'invitation de Claude Morgan, Sartre assiste aux réunions du Comité national des écrivains (CNE) et publie à quatre reprises dans les Lettres françaises[74]. La même année il publie L'Être et le Néant, ouvrage influencé par les idées du philosophe allemand Heidegger, dans lequel il fait le point sur son système de pensée et en approfondit les bases théoriques. Du 17 janvier au 10 avril 1944 il livre douze émissions pour Radio-Vichy[75]. Il écrit ensuite une pièce de théâtre, Les Autres, qui deviendra Huis clos[76], jouée en mai 1944 et qui, elle, rencontre un franc succès, notamment auprès des officiers allemands invités à la première représentation[77],[78]. Lottman observe que « l'occupation allemande coïncida pour eux deux (Sartre et Beauvoir) avec leur accession à la célébrité ».
Peu après la libération de Paris, Sartre est recruté par Camus pour le réseau résistant Combat. Il écrit du 28 août 1944 au 4 septembre, dans les premiers numéros du journal du même nom, une série de sept articles sur la libération de Paris intitulée Un promeneur dans Paris insurgé[79],[80],[81],[82],[83],[84],[85], signés Jean-Paul Sartre[86] mais écrits en réalité par Simone de Beauvoir[87],[88]. Dès septembre 1944 il lance dans les Lettres françaises son fameux : « Jamais nous n’avons été aussi libres que sous l’occupation allemande… puisqu'une police toute puissante cherchait à nous contraindre au silence, chaque parole devenait précise comme une déclaration de principe ; puisque nous étions traqués, chacun de nos gestes avait le poids d'un engagement[89],[90]. »
 Fin 1944 le département d'État américain invite une douzaine de reporters français à découvrir les États-Unis. Sartre en fait partie et devient durant quelques mois l'envoyé spécial du Figaro[91], journal de droite gaulliste, libérale et conservatrice alors qu'il se revendique socialiste ; il est accueilli comme un héros de la Résistance[n 13]. C'est ainsi que commence sa renommée mondiale[92]. La guerre a donc doublement coupé sa vie en deux : auparavant et jusqu'à L'Être et le Néant, philosophe de la conscience individuelle, peu concerné par les affaires du monde, Sartre se transforme en intellectuel engagé politiquement. Professeur parisien connu dans le monde intellectuel, il devient après la guerre une sommité internationale.
Après avoir séjourné pendant l'occupation à l'hôtel La Louisiane[93] au 60, rue de Seine, Jean-Paul Sartre s'installe en 1945 au 42, rue Bonaparte et y vit jusqu'en 1962. Après la Libération, Sartre connaît un succès et une notoriété importants ; il va, pendant plus d'une dizaine d’années, régner sur les lettres françaises. La diffusion de ses idées existentialistes se fera notamment au travers de la revue qu’il a fondée en 1945, Les Temps modernes.
Il entretient avec la journaliste Dolorès Vanetti, rencontrée aux États-Unis en 1945[94], une liaison de plusieurs années, révélée par les Mémoires de Simone de Beauvoir qui la réduit à la lettre « M. » C'est d'ailleurs À Dolorès, que le philosophe dédie, en octobre 1945, sa Présentation du premier numéro des Temps modernes[95], seule et unique allusion à sa passion de cinq ans pour cette journaliste française de New York qui, à un moment historique crucial, lui a ouvert la porte d'un autre continent et donné toutes les clés pour comprendre les États-Unis au cours de deux longs voyages[91].
Sartre y partage sa plume avec, entre autres, Simone de Beauvoir, Merleau-Ponty et Raymond Aron. Dans le long éditorial du premier numéro il pose le principe d'une responsabilité de l'intellectuel dans son temps et d'une littérature engagée. Pour lui, l'écrivain est dans le coup [96]. Cette position sartrienne dominera tous les débats intellectuels des années 1960 aux années 1980.
Lorsqu'en octobre 1945 Sartre fait une conférence dans une petite salle, c'est un événement : une foule nombreuse tente d'entrer, les gens se bousculent, des coups se donnent, des femmes s'évanouissent ou tombent en syncope[97],[98]. Sartre y présente un condensé de sa philosophie[99], qui sera retranscrit dans L'existentialisme est un humanisme. Sa publication en 1946 par l'éditeur Nagel est faite à l'insu de Sartre qui juge la transcription ex abrupto, nécessairement simplificatrice, peu compatible avec l'écriture et le travail du sens que celle-ci implique[100].
Sartre veut à l'époque se rapprocher des marxistes, qui rejettent une philosophie de la liberté radicale, susceptible d'affaiblir les certitudes indispensables au militant : dans le texte de la conférence Sartre expose le leitmotiv de l'existentialisme, l'homme ne peut pas refuser sa liberté, la liberté tend au futur, tout acte de liberté est projet, la réalisation d'un projet individuel modifie la réalisation d'autres projets individuels, chaque individu est responsable vis-à-vis de son projet individuel et du projet des autres, la liberté est le fondement de toutes les valeurs humaines, l'engagement dans les choix des sociétés rend l'homme un homme à part entière[101]. Elsa Triolet, femme de lettres communiste, va jusqu'à déclarer : [102] et son ancien élève Jean Kanapa, intellectuel et dirigeant du parti communiste français, publier en 1947 un texte intitulé L'existentialisme n'est pas un humanisme[103].
C'est sous l'angle de la liberté et du libre-arbitre qu'il édite à cette époque Descartes, un choix de textes du philosophe[104].
Cette même année 1946 Sartre se brouille avec Raymond Aron à l'occasion d'une émission radiophonique de l'équipe des Temps modernes sur et contre de Gaulle. Aron ne fait plus partie de l'équipe depuis juin 1946, mais lors de la confrontation il est néanmoins appelé comme arbitre par Sartre et ses amis venus pour ferrailler avec des gaullistes. Sartre, en verve, compare de Gaulle à Hitler, arguant que les deux ont la moustache en commun, ce qui provoque la fureur des gaullistes présents. Aron, pris entre deux feux, demeure silencieux et Sartre en conclut qu'il est contre lui sur le plan politique. Simone de Beauvoir affirme quant à elle de façon encore plus catégorique et tranchante qu'Aron « se solidarisa » avec les ennemis de Sartre[105]
Les élites intellectuelles veulent maintenant « être » existentialistes, « vivre » existentialistes. Saint-Germain-des-Prés, lieu où habite Sartre, devient le quartier de l'existentialisme, en même temps qu'un haut lieu de vie culturelle et nocturne : on y fait la fête dans des caves enfumées, en écoutant du jazz, ou encore en allant au café-théâtre. C'est dans ces caves que Boris Vian se lie d'amitié avec le couple Sartre-Beauvoir – le Jean Sol Partre et la Duchesse de Bovouard de L'Écume des jours – et toute la bande des sartriens. En décembre 1946 le couple Vian, qui donne une « tartine-partie », assiste, éberlué, à la rupture entre Merleau-Ponty et Camus, ainsi qu'à la première brouille entre Sartre et Camus[106]. L'épouse de Vian tape à la machine les textes de Sartre en vue de leur parution dans la revue Les Temps modernes[3],[4] ; elle vivra de 1949 jusqu'à sa mort une liaison avec lui.
Sartre met sa plume au service des minorités délaissées, en particulier les Juifs français et les Noirs. En effet, il publie en 1945 plusieurs articles consultables dans la nouvelle édition de Situations II sur la condition des Noirs aux États-Unis, le racisme et les discriminations dont ils sont victimes. En 1946, il publie ses Réflexions sur la question juive dont il a écrit en 1945 la première partie, le Portrait de l'antisémite, dans le no 3 des Temps Modernes[95]. Il s'attaque alors à l'antisémitisme en France[107] à une période où les Juifs qui rentrent des camps sont rapidement délaissés[108]. [110],[111]. En 1948, en introduction de l'Anthologie de la nouvelle poésie nègre et malgache[112] de Léopold Sédar Senghor, il écrit Orphée Noir, repris dans Situations III, critique du colonialisme et du racisme à l'aune de la philosophie qu'il avait développée en 1943 dans L'Être et le Néant. "La libération des Noirs doit en passer par un moment de négativité qui consiste à assumer une distinction entre Noirs et Blancs, à revendiquer une différence pour mieux exiger l’égalité."[110]. C'est la "négritude", qu'il défend dans sa Préface l'anthologie éditée par Léopold Sedar Senghor.
Les écrits de Sartre inquiètent le FBI qui le surveille dès 1945-1946 et jusqu'aux années 1970, allant jusqu'à lui voler des carnets de brouillons[113].
Pendant ce temps, Sartre va affirmer son engagement politique en éclairant sa position, au travers de ses articles dans Les Temps modernes : Sartre épouse, comme beaucoup d'intellectuels de son époque, la cause de la révolution marxiste, mais sans pour autant donner ses faveurs au Parti communiste, aux ordres d'une URSS qui ne peut satisfaire l'exigence de liberté. Simone de Beauvoir, Sartre et ses amis continuent donc à chercher une troisième voie, celle du double refus du capitalisme et du stalinisme. Il soutient Richard Wright, un écrivain noir américain ancien membre du Parti communiste américain exilé en France dès 1947.
Dans sa revue Les Temps modernes, il prend position contre la guerre d'Indochine, s'attaque au gaullisme et critique l'impérialisme américain, sachant qu'il se rendit aux États-Unis en 1945 comme journaliste du Figaro pour populariser ses théories. Une fois de retour en France, il ira jusqu'à affirmer, dans cette même revue, que « tout anti-communiste est un chien »[36].
C'est alors que Sartre décide de traduire sa pensée en expression politique : en participant à la fondation d'un nouveau parti politique, le Rassemblement démocratique révolutionnaire (RDR). Mais malgré le succès de quelques manifestations, le RDR n’atteindra jamais un effectif suffisant pour devenir un véritable parti. Sartre donne sa démission en octobre 1949. Aron, qui a pour sa part rejoint le RPF gaulliste, juge le nom du RDR oxymorique, estimant que la révolution souhaitée par Sartre ne peut pas être démocratique[36].
La guerre de Corée, puis la répression musclée d'une manifestation antimilitariste du Parti communiste français (PCF) pousse Sartre à choisir son camp : Sartre voit alors dans le communisme une solution aux problèmes du prolétariat.
Ce qui lui fait dire : [114].
Sartre devient un compagnon de route du Parti communiste entre les années 1952[115] et 1956[116]. Dès lors, il participe à sa mouvance : il prend la présidence de l'Association France-URSS. En décembre 1952, il soutient les communistes au Conseil mondial de la paix[117].
En 1954, de retour d'URSS, il déclare lors d'un entretien pour Libération : . Seule la première partie de la phrase est généralement citée[118],[119].
Cet engagement auprès du PCF était en partie motivé par la répression policière et juridique dont celui-ci était la cible[120].
Ce ralliement idéel de Sartre au communisme sépare de même Sartre et Camus, très proches auparavant[121]. Pour Camus, l'idéologie marxiste ne doit pas prévaloir sur les crimes staliniens, alors que pour Sartre on ne doit pas utiliser ces faits comme prétexte à l'abandon de l’engagement révolutionnaire.
Cette fidélité au PCF va tenir jusqu'en automne 1956, date à laquelle les chars soviétiques écrasent l'insurrection de Budapest[120]. Après avoir signé une pétition d'intellectuels de gauche et de communistes contestataires, il donne le 9 novembre une longue interview au journal l'Express (journal mendésiste), pour se démarquer de manière radicale du parti.
L'existentialisme semble en perte de vitesse, dans les années 1960, l'influence de Sartre sur les lettres françaises et l'idéologie intellectuelle diminue peu à peu, notamment face aux structuralistes comme l'ethnologue Lévi-Strauss, le philosophe Foucault ou le psychanalyste Lacan. Le structuralisme s'oppose à l'existentialisme : il n'y a en effet dans le structuralisme que peu de place pour la liberté humaine, chaque homme est imbriqué dans des structures qui le dépassent. En fait Sartre, défenseur de la primauté de la conscience sur l'inconscient et de la liberté sur la nécessité des structures sociales, ne prend pas la peine de discuter de ce nouveau courant qu'est le structuralisme : il préfère se dédier à l'analyse du XIXe siècle, de la création littéraire, et surtout à l'étude d'un auteur qui l'avait toujours fasciné, Flaubert. De plus dans les années 1960 sa santé se détériore rapidement. Sartre est prématurément usé par sa constante suractivité littéraire et politique, mais aussi par le tabac et l'alcool qu'il consomme en grandes quantités.
Le 22 octobre 1964, l’académie du Nobel décerne à Jean-Paul Sartre le prix Nobel de littérature, mais le philosophe, se confiant le jour même au journaliste François de Closets alors à l'AFP, lui déclare : [122]. Deux jours plus tard, le 24 octobre 1964, il s'en explique plus longuement dans une lettre ouverte adressée à l'académie suédoise et dont le texte sera publié respectivement par les quotidiens français Le Monde et Le Figaro[123]. Ce fait inédit aura un très grand retentissement dans le monde[124]. Car, selon Sartre, .
Il avait de même refusé la Légion d'honneur, en 1945, ou encore une chaire au Collège de France[n 14]. Ces honneurs auraient, selon lui, aliéné sa liberté, en faisant de l'écrivain une espèce d'institution. Cette action restera célèbre car elle illustre bien l’état d’esprit de l'intellectuel qui se veut indépendant du pouvoir politique.
En 1964, il adopte Arlette Elkaïm.
Si Sartre a pris ses distances avec le parti communiste (même si, à la suite d'un de ses voyages en Union soviétique en juillet 1954, il donne cinq longs entretiens dans le quotidien Libération à Jean Bedel qui résume la teneur du premier d'entre eux par ce titre : La liberté de critique est totale en URSS)[126], il continue de s'engager pour de nombreuses causes. Il est une des cibles du Congrès pour la liberté de la culture, une association culturelle anticommuniste fondée en 1950.
En 1950 éclate l'Affaire Henri Martin, un marin et militant du Parti communiste français arrêté pour avoir distribué des tracts contre la guerre d'Indochine dans une enceinte militaire, l'arsenal de Toulon. L'accusation porte également sur un acte de sabotage en faveur du Viêt Minh, accusation dont il est lavé par le tribunal de Toulon, pourtant exclusivement composé d'officiers. Jean-Paul Sartre s'engage en publiant notamment un ouvrage, L'affaire Henri Martin, qui résume les arguments de la défense. Preuve de la grande portée de cette affaire, d'autres intellectuels de gauche renommés participent au même ouvrage : Michel Leiris, Hervé Bazin, Prévert, Vercors… Jusqu'à la fin de la guerre, Sartre restera très vigilant, coordonnant notamment un numéro spécial des Temps modernes (Viet Nam, octobre 1953)[127].
Dès 1955, Sartre et la revue Les Temps modernes prennent parti contre l'idée d'une Algérie française et soutiennent le désir d'indépendance du peuple algérien. Sartre s'élève contre la torture[128], revendique la liberté pour les peuples de décider de leur sort, analyse la violence comme une gangrène, produit du colonialisme[129]. Dans sa célèbre préface des Damnés de la Terre, œuvre de Frantz Fanon qui étudie les rapports entre violence et oppression, il va jusqu'à écrire : [130]. Cette citation sera par la suite abondamment reprise et commentée[131],[132]. En 1960, lors du procès des réseaux de soutien au FLN, il se déclare « porteur de valise »[n 15] du FLN[133]. En septembre 1960, il signe le Manifeste des 121, titré « Déclaration sur le droit à l’insoumission dans la guerre d’Algérie ». Il participe à la manifestation du 1er novembre 1961 consécutive aux massacre du 17 octobre 1961, au cours duquel des dizaines de manifestants algériens sont tués à Paris par la police, et à celle du 13 février 1962, en protestation contre la répression meurtrière (neuf morts) du métro Charonne[128].
Ces prises de position ne sont pas sans danger, son appartement sera plastiqué deux fois par l'OAS, et Les Temps modernes saisis cinq fois[128].
Il soutient également la cause du Néo-Destour en Tunisie et de l'Istiqlal au Maroc, tous deux indépendantistes. Selon Sartre, [128].
Sartre soutient activement la révolution cubaine dès 1960, comme un grand nombre d'intellectuels tiers-mondistes. En juin 1960, il écrit dans France-Soir 16 articles intitulés Ouragan sur le sucre[134]. Mais il rompt avec le gouvernement cubain en 1971 à cause de l’« affaire Padilla », lorsque le poète cubain Heberto Padilla est emprisonné pour avoir critiqué le régime castriste[134]. Il dira de Fidel Castro : . Face à la répression des homosexuels, notamment avec la mise en place des Unités militaires d'aide à la production, Sartre déclare que [n 16].
Sartre, qui a déjà publié en 1960 le tome I de la Critique de la raison dialectique et prépare le tome II, paru inachevé et posthume, participe activement aux événements de mai 1968. Déjà en 1967, il était revenu sur le devant de la scène en présidant avec Bertrand Russell le tribunal Russell, un tribunal autoproclamé, une assemblée internationale d'intellectuels, de militants et de témoins chargés de juger les guerres et de les condamner, en particulier la guerre des Américains au Vietnam.
S'il n'a pas été l'inspirateur des événements de mai 1968, il se fera l'écho de la révolte dans la rue, sur les estrades, dans les journaux, et aux portes des usines en grève. , déclare-t-il dès le 10 mai 1968, dans une tribune publiée par Le Monde[135],[136].
Pour mieux comprendre la révolte estudiantine, il demande à rencontrer des nanterriens. Une assemblée générale vote pour envoyer deux représentants, Alain Geismar et Herta Alvarez, lycéenne de 18 ans, fille et petite-fille d'anarchistes espagnols et future documentariste[n 17]. Reçus chez Simone de Beauvoir et repartis le 12 mai vers 2 heures du matin, ils garderont en mémoire l'humilité de Sartre, vérifiant qu'il comprend bien[137].
Dans Le Nouvel Observateur du 20 mai, il interviewe Daniel Cohn-Bendit[138], où il insiste plusieurs fois pour que Cohn-Bendit s’exprime sur le « programme » et les « objectifs » à long terme des étudiants, même si ce dernier refuse catégoriquement qu’il y en ait, car  serait [138] et car [138]. Peu après, il écrit que [139].
À maintenant 63 ans, il se rend à la Sorbonne investie par les étudiants, afin de discuter avec eux. Il dénonce ensuite les «élections pièges à cons» de de Gaulle.
Sur le plan international, en septembre 1968, il condamne fermement l'intervention soviétique contre le printemps de Prague en Tchécoslovaquie.
Dans les 2 891 documents déclassifiés sur autorisation du président américain Donald Trump, le 21 octobre 2017, en rapport avec l'assassinat de JFK, la CIA affirme que dans les années 1960, Jean-Paul Sartre, Simone de Beauvoir et plus étonnamment Catherine Deneuve auraient financé un « réseau d'activistes » qui « aidait les déserteurs »[n 18] de la guerre du Vietnam, dont Larry Cox (en) (né en 1945)[n 19], activiste qui a refusé à trois reprises d'intégrer l'armée américaine et de partir au Vietnam[n 20],[140],[141].
Sartre soutiendra ensuite le  face aux tribunaux et à la police, plus que le mouvement mao en général. Depuis deux ans, il n'a plus rencontré les gauchistes. La première rencontre de Sartre avec les maoistes est un déjeuner à la Coupole à la mi-avril 1970. Alain Geismar connaît Sartre depuis la nuit 11 ou 12 mai 1968 où il s'est longuement entretenu avec lui dans le duplex de Simone de Beauvoir. Deux ans après, il lui présente Benny Lévy[137], qui tire les ficelles du journal maoïste La Cause du Peuple étant saisi systématiquement sous la pression des autorités pompidoliennes, il accepte le 1er mai 1970 d'en devenir le directeur afin de le protéger, puis exige un rectificatif par rapport à son communiqué de soutien, pour que la formule  soit remplacée par [142].
Il fait de même avec deux autres journaux maoïstes, Tout!, lancé en septembre 1970. Le 21 octobre 1970, à l'extérieur de l'usine Renault-Billancourt, il plaide pour un rapprochement entre intellectuels et ouvriers, expliquant que c'est à ces derniers, qui subissent aussi la violence de savoir ou non si Alain Geismar, ex-leader de Mai 68, jugé pour violences ailleurs, a raison ou tort[143]. Sartre s'oppose plusieurs fois aux maoïstes, sur l'enlèvement d'un député[137], puis à celui d'un cadre de Renault après la mort de Pierre Overney[137].
En décembre 1970, il est le procureur général du Tribunal populaire de Lens, un Tribunal d'opinion organisé devant 500 personnes réunies dans une grande salle de la Mairie de Lens, dans l'esprit du Tribunal Russel de 1967 et sous l'égide du Secours rouge (France). Ce Tribunal populaire de Lens doit faire la lumière sur les responsabilités de l'État et des ingénieurs dans un accident minier survenu en février 1970 à Fouquières-lès-Lens. Dans la foulée, il s'investit dans les deux premiers numéros du journal J'accuse lancé le 15 janvier 1971 (dont le titre est inspiré de Zola) mais s'éloigne ensuite comme beaucoup de personnalités et partis qui avaient soutenu la création en juin 1970 du Secours rouge (France). Dès mars-avril 1971, sa partenaire dans ces projets, Simone de Beauvoir s'investit avec Gisèle Halimi dans la rédaction du Manifeste des 343 femmes avouant avoir avorté puis la création en juin de Choisir la cause des femmes.
Au printemps 1973, Sartre lance avec Serge July, Philippe Gavi, Bernard Lallement et Jean-Claude Vernier, un quotidien populaire, Libération ; Jean-Paul Sartre et Jean-Claude Vernier en sont les premiers directeurs de publication, et le restent jusqu’à leur démission le 24 mai 1974 pour désaccord avec Serge July, qui leur succède. Pendant toute cette période il se lie avec divers autres mouvements gauchistes et féministes, prêtant volontiers son nom afin de les aider.
En 1974, après la mort de famine d'Holger Meins en prison, Sartre, par la demande d'Ulrike Meinhof et par l'intermédiaire de Catherine Deneuve, visite Andreas Baader dans la prison de Stuttgart-Stammheim. Ceci à raison des conditions sous lesquelles les membres du Fraction armée rouge sont détenus.
Sartre va s'occuper, alors qu'il arrive à la fin de sa vie, du conflit israélo-palestinien. Tout en reconnaissant la légitimité de l'État d'Israël, il dénonce les conditions de vie déplorables des Palestiniens qui expliqueraient selon lui le recours au terrorisme.
En 1976, il accepte le seul titre honorifique de sa carrière, celui de docteur honoris causa de l'université de Jérusalem, qui lui est remis à l'ambassade d'Israël à Paris par le philosophe Emmanuel Levinas. Il accepte ce titre pour des raisons « politiques » afin de créer une [144].
À 65 ans, le 18 mai 1971, Sartre est victime d'une attaque cérébrale[n 21] qui le laisse très affaibli. Le 5 mars 1973 une seconde attaque lui laisse la vie sauve, mais lui enlève presque totalement la vue. Sartre entre dans ses années d'ombre. Déjà diminué, il est alors contraint de décider « librement » que son œuvre est achevée, et qu'il ne finira donc jamais le tome IV de son Flaubert. Cela ne l'empêchera néanmoins pas de continuer à penser et à produire : il engage comme secrétaire un jeune normalien Benny Lévy, connu sous le nom de Pierre Victor lorsque ce dernier dirigeait le groupe maoïste de La Gauche prolétarienne, qui est chargé de lui faire la lecture, et avec qui il débat parfois violemment. Un an plus tard sort l'ouvrage On a raison de se révolter, livre d'entretiens avec Benny Lévy et Philippe Gavi, où Sartre évoque, entre autres, les problèmes liés à l'engagement contestataire.
En 1975, Marcel Jullian lui propose de réaliser une série documentaire pour l'ORTF, à la condition de lui fournir, d'abord, un épisode pilote. Ce dernier refuse, assimilant cela à un examen et, donc, à un fait de censure[145].
Sartre poursuit ses engagements jusqu'à la fin de sa vie : quelques interventions politiques, telles que la visite à Andreas Baader, et un voyage de soutien à la révolution des œillets au Portugal, font renaître dans les milieux de l'extrême gauche européenne des élans de sympathie pour le vieil homme.
Sartre signe aussi différents appels pour la libération de dissidents soviétiques, et, lors de la rencontre entre Brejnev et Valéry Giscard d'Estaing à Paris en 1977, il organise au même moment une rencontre avec des dissidents soviétiques. Ce soir-là, pour Sartre entouré de Michel Foucault, Gilles Deleuze, André Glucksmann, Simone Signoret et bien sûr Simone de Beauvoir, 105 radios et télévisions sont venues du monde entier, plus qu'à l'Élysée pour Brejnev[réf. nécessaire]. La même année, il signe avec Louis Aragon, Simone de Beauvoir, Jack Lang, Bernard Kouchner la lettre ouverte parue dans Le Monde à la veille du procès de Bernard Dejager, Jean-Claude Gallien et Jean Burckardt accusés d’avoir eu des relations sexuelles avec des filles et des garçons de 13 et 14 ans[146].
Jean-Paul Sartre a condamné l'intervention américaine au Vietnam, au Laos et au Cambodge dans les années 1960 et 1970 et accordé, comme la majeure partie de la gauche mondiale, son soutien aux mouvements communistes indochinois, y compris aux Khmers Rouges, jusqu'à leur victoire en 1975.
En 1979, un dernier événement médiatique pour Sartre émeut le grand public : accompagné de son meilleur ennemi Raymond Aron, et du jeune philosophe André Glucksmann, un Sartre très affaibli se rend à l'Élysée pour demander à Valéry Giscard d'Estaing d'accueillir des réfugiés d'Indochine, les « boat people », qui se noient par centaines en tentant de quitter le Viêt Nam. Indépendamment des différences d'opinions politiques auxquelles il attache désormais moins d'importance, Sartre affirme, au crépuscule de sa vie, l'exigence de sauver des vies partout où elles sont menacées. Il invoque ainsi désormais les , qu'il avait autrefois condamnés, critiquant leur caractère [36]. Sartre a également adhéré, avec Simone de Beauvoir, au comité de soutien à l'ayatollah Khomeyni, opposant principal au régime impérial du Shah, lorsque Khomeyni vivait en exil à Neauphle-le-Château[147].
Entre 1978 et 1980, Benny Lévy fait découvrir à Sartre l’œuvre d'Emmanuel Levinas. Des entretiens enregistrés de Sartre avec Benny Lévy sur Levinas et sur le judaïsme, résulte le dialogue L’Espoir maintenant, publié dans Le Nouvel Observateur, sur trois numéros, le 10, le 17 et le 24 mars 1980.
L’Espoir maintenant provoque un scandale. Benny Lévy est accusé par l'entourage de Sartre d'avoir abusé de son état de faiblesse pour lui imposer sa pensée. Olivier Todd parle d'un [148], tant semble différente la parole de Sartre dans ces entretiens. Simone de Beauvoir reproche à Benny Lévy d’avoir contraint Sartre à des déclarations démentes[149]. Jean Guitton tient de telles déclarations pour un reniement de l'athéisme de Sartre et y voit l'influence de son nouveau et dernier secrétaire. John Gerrasi (en), l’un des biographes de Sartre, dénonce la  de Benny Lévy, , , devenu [150]. Un an après la mort de son compagnon, Simone de Beauvoir écrit encore que « l'entretien avec Benni Lévi (sic) n'était pas du vrai Sartre »[151]. L'avocate Gisèle Halimi, qui a été une amie très proche du philosophe depuis 1957, est revenue, en 2005, sur ces propos en affirmant : [152].
Toutefois, Jean Daniel, le directeur du Nouvel Observateur, témoigne que Sartre est parfaitement conscient de ce qu'il fait en publiant L’Espoir maintenant. Il a fallu que Sartre appelle Jean Daniel pour que ce dernier décide de le publier. Daniel lui a demandé : , a répondu Sartre. Et, en effet, , assure Daniel. Et Sartre d'insister : [154]. C'est le dernier scandale que Sartre a provoqué.
Pour Vincent de Coorebyter, spécialiste de Sartre, , certainement plus  qu' mais il n'y a ni  par Benny Lévy car l'instrumentalisation est réciproque[155], ni  qui relèverait du mythe ni non plus de « rupture » ou de « révolution » dans sa philosophie. Le penseur belge considère même que dans ces entretiens, [156].
Alors qu'il mène ces entretiens avec Sartre, de 1975 à 1980, Benny Lévy prend des notes dans des cahiers qui seront publiés chez Verdier en 2007 sous le titre Pouvoir et Liberté.
Atteint d'urémie, Jean-Paul Sartre meurt le 15 avril 1980 à près de 75 ans à l’hôpital Broussais de Paris[157], à la suite d'un œdème pulmonaire.
Dans le monde entier, l'annonce de sa mort provoque une émotion considérable. Pour son enterrement, le 19 avril 1980, cinquante mille personnes descendent dans les rues de Paris, accompagnant son cortège pour lui rendre un ultime hommage ; une foule énorme, sans service d'ordre, pour celui qui aura su captiver trois générations de Français. Parmi eux, ses anciens élèves des années du Havre ou de Paris, les camarades de la Libération et les communistes des années 1950, les anciens militants de la paix en Algérie, enfin de jeunes maos.
Il est inhumé au cimetière du Montparnasse à Paris (14e), dans la 20e division — juste à droite de l’entrée principale boulevard Edgar-Quinet. Simone de Beauvoir, décédée le 14 avril 1986, est inhumée à ses côtés. Sur la tombe, une plaque porte cette simple inscription : « Jean-Paul Sartre, 1905-1980 ».
Sartre est considéré comme le père de l'existentialisme français et sa conférence de 1945, L'existentialisme est un humanisme, est considéré comme le manifeste de ce mouvement philosophique. Toutefois, la philosophie de Sartre, en 20 ans, a évolué entre existentialisme et marxisme. Ses œuvres philosophiques majeures sont L'être et le Néant (1943) et la Critique de la raison dialectique (1960).
Dans L'Être et le Néant, Sartre s'interroge sur les modalités de l'être. Il en distingue trois : l'être en-soi, l'être pour-soi et l'être pour autrui.
L'homme se distingue de l'objet en ce qu'il a conscience d'être (qu'il a conscience de sa propre existence). Cette conscience crée une distance entre l'homme qui est et l'homme qui prend conscience d'être. Or toute conscience est conscience de quelque chose (idée d'intentionnalité reprise de Brentano). L'homme est donc fondamentalement ouvert sur le monde, « incomplet », « tourné vers », existant (projeté hors de soi) : il y a en lui un néant, un « trou dans l'être » susceptible de recevoir les objets du monde.
« Le pour soi est ce qu'il n'est pas et n'est pas ce qu'il est »
— Sartre, L'Être et le Néant« Il n'y a pour une conscience qu'une façon d'exister, c'est d'avoir conscience qu'elle existe »
— Sartre« En fait, nous sommes une liberté qui choisit, mais nous ne choisissons pas d'être libres : nous sommes condamnés à la liberté. »
— Sartre« Les objets sont ce qu'ils sont, l'homme n'est pas ce qu'il est, il est ce qu'il n'est pas. »
— SartreDans la conférence intitulée L'existentialisme est un humanisme, du 29 octobre 1945, Sartre développe l'idée que l'homme n'ayant pas de nature définie a priori, il est libre de se définir lui-même par son projet. [158].
Sartre rattache la liberté de l'homme au fait que Dieu n'existe pas, reprenant en un sens positif la phrase de Dostoïevski, . Il prend cette formule au sérieux : . L'homme n'est pas de toute éternité, dans l'esprit d'un Dieu créateur, comme l'idée d'un objet technique (tel un coupe-papier) dans l'esprit de l'artisan. Par conséquent, aucune norme transcendante n'indique à l'homme ce qu'il doit faire. L'homme est libre, , et n'est rien d'autre que ce qu'il se fait.
Sartre explique que cette liberté implique une responsabilité : en se choisissant lui-même, l'homme établit un modèle de ce qui vaut pour l'homme en général. [159]. En faisant de chacun « un législateur qui choisit pour l'humanité entière », Sartre retrouve aussitôt l'universel, dont il semblait s'écarter en confrontant l'individu à la liberté absolue de son choix, sur fond d'« angoisse » et de « délaissement », deux concepts inspirés de la lecture de Kierkegaard et de Heidegger. On ne peut échapper ni à la liberté du choix de son existence et de ses actions, ni à leur caractère exemplaire pour tout homme : l'invocation de motifs pour ne pas exercer sa liberté est assimilée à de la « mauvaise foi ».
Certaines formules de L'existentialisme est un humanisme sont restées célèbres, comme , ou bien , qui fait écho à son provocateur , publié en septembre 1944 dans les Lettres françaises[160].
Selon Sartre, l'homme est ainsi libre de choisir son essence. Pour lui, contrairement à Hegel, il n'y a pas d'essence déterminée, l'essence est librement choisie par l'existant. L'homme est absolument libre, il n'est rien d'autre que ce qu'il fait de sa vie, il est un projet. Sartre nomme ce dépassement d'une situation présente par un projet à venir, la transcendance.
L'existentialisme de Sartre s'oppose ainsi au déterminisme qui stipule que l'homme est le jouet de circonstances dont il n'est pas maître. Sartre estime que l'homme choisit parmi les événements de sa vie, les circonstances qu'il décidera déterminantes. Autrement dit, il a le pouvoir de 'néantiser', c'est-à-dire de combattre les déterminismes qui s'opposent à lui.
Au milieu de sa vie intellectuelle, il réussit à concilier une part de mécanicisme marxiste avec sa doctrine de l'existentialisme, qui refuse le déterminisme fondé dans les conditions socio-économiques. Il développe ainsi une philosophie de l'histoire et une ontologie qu'il appelle méthode progressive-régressive. Cette pensée de l'influence de la société sur l'homme s'inscrit dans son concept d'extéro-conditionnement, qui décrit l'action de transmission d'informations d'un groupe sur un autre dans le but de les conditionner socialement. Il ne s'agit donc pas d'un pouvoir de contrainte mais de l'utilisation par un groupe déterminé d'outils d'influence.
Au nom de la liberté de la conscience, Sartre refuse le concept freudien d'inconscient remplacé par la notion de « mauvaise foi » de la conscience. L'homme ne serait pas le jouet de son inconscient mais choisirait librement de se laisser nouer par tel ou tel traumatisme. Ainsi, l'inconscient ne saurait amoindrir l'absolue liberté de l'Homme.
Selon Sartre, l'homme est condamné à être libre. L'engagement n'est pas une manière de se rendre indispensable mais responsable. Ne pas s'engager est encore une forme d'engagement.
L'existentialisme de Sartre est athée, c'est-à-dire que, pour lui, Dieu n'existe pas (ou en tout cas ), donc l'homme est seule source de valeur et de moralité ; il est condamné à inventer sa propre morale et libre de la définir. Le critère de la morale ne se trouve pas au niveau des « maximes » (Kant) mais des « actes ». La « mauvaise foi », sur un plan pratique, consiste à dire : « c'est l'intention qui compte ».
Selon Sartre, la seule aliénation à cette liberté de l'homme est la volonté d'autrui. Ainsi fait-il dire à Garcin dans Huis clos : .
Jean-Paul Sartre présente le marxisme comme [161]. Après avoir observé et analysé l'existence et la liberté de l'homme en tant qu'individu, Sartre s'est interrogé sur l'existence d'une conscience collective et son rapport avec la liberté individuelle. Dans sa Critique de la raison dialectique (1960), Sartre affirme que la liberté de l'homme est aliénée par les sociétés féodales ou capitalistes. Il analyse comment, dans les sociétés aliénées, les libertés individuelles peuvent conduire à un effet opposé à l'intention générale et à l'aliénation de la liberté collective. Il suggère alors d'inverser le processus : le groupe doit pouvoir décider de regrouper les libertés individuelles pour permettre le développement de l'intention générale. Sartre pense que cette sorte d'aliénation de la liberté individuelle doit être librement choisie et s'oppose ainsi à toute forme de totalitarisme.
La question du respect d’autrui traverse toute l’œuvre de Sartre, mais avec une acuité particulière quand il revient sur la question juive. Dans L’Espoir maintenant, Sartre met toujours en jeu , pour Yvan Salzmann[162]. , écrit Sartre dans L’Espoir maintenant[163].
La publication de ce texte fit scandale parce que ses détracteurs ont cru que Sartre se convertissait au judaïsme. En réalité, ce qui l’intéresse dans le judaïsme, c’est toujours la question du respect d’autrui et son lien avec la question de l’éthique et celle de l’histoire. , remarque Bernard-Henri Lévy, [164].
Mais il ne s’agit nullement d’une conversion religieuse, pour Bernard-Henri Lévy. Au contraire, Sartre va jusqu’au bout de la logique athée, en contestant la vision hégélienne de l’histoire dans ce texte[165]. Sartre retient l’espoir, mais l’espoir va bien au-delà de la religion, pour Sartre[n 22].
Certains philosophes défendent l'idée que la pensée de Sartre est contradictoire. Plus spécifiquement, ils pensent que Sartre présente des arguments métaphysiques en dépit de son affirmation que ses vues philosophiques ignorent la métaphysique. Herbert Marcuse critiqua le fait que l’Être et le néant projette une anxiété et une absence de sens sur la nature de l'existence elle-même : [167].
Dans Lettre sur l'Humanisme, Heidegger critiquait l'existentialisme de Sartre[168] :
« L'Existentialisme dit que l'existence précède l'essence. Dans cette déclaration, il utilise existence et essence selon leur sens métaphysique, qui, depuis l'époque de Platon, a dit que l’essence précède l’existence. Sartre inverse cet énoncé. Mais l'inverse d'un énoncé métaphysique reste un énoncé métaphysique. Avec lui, il reste dans la métaphysique, dans l'oubli de la vérité d'Être. »
Les philosophes Richard Wollheim et Thomas Baldwin (en) ont défendu l'idée que la tentative de Sartre de montrer que la théorie de l'inconscient de Sigmund Freud est une erreur était fondée sur une mésinterprétation de Freud[169],[170]. Richard Webster (en) considère que Sartre est l'un des penseurs modernes qui a reconstruit les orthodoxies judéo-chrétiennes sous une forme séculière[171].
Brian C. Anderson a accusé Sartre d'être un apologiste de la tyrannie et de la terreur, et un partisan du stalinisme, du maoïsme, et du régime de Fidel Castro à Cuba[n 23].
Sartre, qui déclara dans sa préface des Damnés de la Terre de Frantz Fanon qu' a été critiqué par Anderson et Michael Walzer pour soutenir le meurtre de civils européens par le FLN pendant la guerre d'Algérie. Walzer suggère que Sartre, un Européen, était un hypocrite pour ne pas se porter volontaire pour aller se faire tuer[173],[174].
Clive James a condamné Sartre dans son livre de mini-biographies Cultural Amnesia (2007). James attaque la philosophie de Sartre comme étant « tout de la pose »[175].
Au sein de son œuvre, les études esthétiques de Sartre forment, à côté des écrits philosophiques et des textes littéraires, un troisième ensemble, souvent négligé, voire passé sous silence. Dans les études qu'il consacre à des écrivains — Baudelaire, Faulkner, Genet, Mallarmé et Flaubert — ou à des artistes — Alberto Giacometti, Alexander Calder et au Tintoret —, Sartre s'attache à éclairer le rapport de ces créateurs à leurs œuvres. Leurs créations démontrent, selon lui, que la liberté est une condition préalable de l'art[176].
« Je pense que Sartre est une outre pleine de vent » - George Orwell, lettre à un ami[181].
Pour les articles homonymes, voir Charles de Gaulle (homonymie) et Gaulle.
« De Gaulle » redirige ici. Pour les autres membres de la famille, voir Famille de Gaulle.
Charles de Gaulle (/ʃaʁl də ɡol/[n 2] Écouter), communément appelé le général de Gaulle ou parfois simplement le Général, né le 22 novembre 1890 à Lille (Nord) et mort le 9 novembre 1970 à Colombey-les-Deux-Églises (Haute-Marne), est un militaire, résistant, homme d'État et écrivain français.
Il est notamment chef de la France libre puis dirigeant du Comité français de libération nationale pendant la Seconde Guerre mondiale, président du Gouvernement provisoire de la République française de 1944 à 1946, président du Conseil des ministres de 1958 à 1959, instigateur de la Cinquième République, fondée en 1958, et président de la République de 1959 à 1969, étant le premier à occuper la magistrature suprême sous ce régime.
Élevé dans une culture de grandeur nationale, Charles de Gaulle choisit une carrière d'officier dans l'armée de Terre. Au cours de la Première Guerre mondiale, il est blessé et fait prisonnier. Par la suite, il sert et publie dans l'entourage de Philippe Pétain, prônant auprès de personnalités politiques l'usage des divisions de blindés dans la guerre contemporaine. En mai 1940, alors colonel, il est placé à la tête d'une division blindée et mène plusieurs contre-attaques pendant la bataille de France ; il est dans la foulée promu général de brigade à titre temporaire. Pendant l'exode qui suit, il est sous-secrétaire d'État à la Guerre et à la Défense nationale dans le gouvernement Reynaud.
Rejetant l'armistice demandé par Pétain à l'Allemagne nazie, il lance de Londres, à la BBC, l'« appel du 18 Juin », qui incite le peuple français à résister et à rejoindre les Forces françaises libres. Condamné à mort par contumace et déclaré déchu de la nationalité française par le régime de Vichy, il entend incarner la légitimité de la France et être reconnu en tant que puissance par les Alliés. Ne contrôlant que quelques colonies, mais reconnu par la Résistance, il entretient des relations froides avec Franklin Roosevelt, mais bénéficie généralement de l'appui de Winston Churchill. En 1943, il fusionne la France libre au sein du Comité français de libération nationale, dont il finit par prendre la direction. Il dirige le pays à partir de la Libération ; favorable à un pouvoir exécutif fort, il s'oppose aux projets parlementaires et démissionne en 1946. Il fonde l'année suivante le Rassemblement du peuple français (RPF), mais son refus de tout compromis avec le « régime des partis » l'écarte de toute responsabilité nationale.
Il revient au pouvoir après la crise de mai 1958, dans le cadre de la guerre d'Algérie. Investi président du Conseil, il fait approuver la Cinquième République par un référendum. Élu président de la République par un collège élargi de grands électeurs, il prône une « politique de grandeur » de la France. Il affermit les institutions, la monnaie (nouveau franc) et donne un rôle de troisième voie économique à un État planificateur et modernisateur de l'industrie. Il renonce par étapes à l'Algérie française malgré l'opposition des pieds-noirs et des militaires, qui avaient favorisé son retour. Il poursuit la décolonisation de l'Afrique subsaharienne et y maintient l'influence française. En rupture avec le fédéralisme européen et le partage de Yalta, de Gaulle défend l'« indépendance nationale » : il préconise une « Europe des nations » impliquant la réconciliation franco-allemande et qui irait « de l'Atlantique à l'Oural », réalise la force de dissuasion nucléaire française, retire la France du commandement militaire de l'OTAN, oppose un veto à l'entrée du Royaume-Uni dans la Communauté économique européenne, soutient le « Québec libre », condamne la guerre du Viêt Nam et reconnaît la Chine communiste.
Sa vision du pouvoir, à savoir un chef directement approuvé par la Nation, l'oppose aux partis communiste, socialiste et centristes pro-européens. Ces formations critiquent un style de gouvernance trop personnel, voire un « coup d'État permanent », selon la formule de François Mitterrand, contre lequel de Gaulle est réélu en 1965 au suffrage universel direct — un mode de scrutin qu’il a fait adopter par référendum en 1962 à la suite de l’attentat du Petit-Clamart le visant. Il surmonte la crise de Mai 68 après avoir semblé se retirer, convoquant des élections législatives qui envoient une écrasante majorité gaulliste à l'Assemblée nationale. Mais en 1969, il engage son mandat sur un référendum (sur la réforme du Sénat et la régionalisation) et démissionne après la victoire du « non ». Il se retire dans sa propriété de Colombey-les-Deux-Églises, où il meurt dix-huit mois plus tard.
Considéré comme l'un des dirigeants français les plus influents de l'histoire, Charles de Gaulle est aussi un écrivain de renom. Il laisse notamment des Mémoires de guerre, où il affirme s'être toujours , jugeant que . Si sa présidence ne fut pas exempte de contestations, il apparaît, plus d'un demi-siècle après sa mort, comme une figure morale toujours omniprésente dans la vie politique de la Cinquième République, la quasi-totalité de la classe politique lui rendant hommage et revendiquant à divers degrés son héritage, au-delà de la seule droite gaulliste.
Charles André Joseph Marie de Gaulle naît le 22 novembre 1890 à 4 heures du matin, au 9 rue Princesse à Lille[2]. Il est baptisé quelques heures après sa naissance en l'église Saint-André de Lille[3] : son parrain est son oncle Gustave de Corbie et sa marraine sa tante Lucie Maillot née Droulers[4]. Charles est le troisième enfant d'Henri de Gaulle (1848, Paris - 1932, Sainte-Adresse, Seine-Inférieure) — précepteur, fonctionnaire, enseignant puis fondateur d'établissement d'enseignement privé — et de son épouse, Jeanne Maillot (1860, Lille - 1940, Paimpont, Ille-et-Vilaine), qui est également sa cousine issue de germain. Il est le petit-fils de Julien-Philippe de Gaulle (1801, Paris - 1883, Paris), historien, et de Jules Maillot (1819, Lille - 1891, Lille), entrepreneur manufacturier textile dans le Nord.
Les de Gaulle sont une famille de juristes parisiens originaires de la province de Champagne, et dont le patronyme pourrait être une déformation du néerlandais de Walle. Dans ses travaux de généalogie, le grand-père de Charles de Gaulle faisait l'hypothèse d'une lointaine ascendance noble[5], bien que la famille ne figurât dans aucun nobiliaire[6] et qu'il n'existât aucune preuve à l'appui de ces prétentions[7]. L’arrière-grand-père, Jean-Baptiste de Gaulle (1759-1832), est avocat ; fils d'un procureur au parlement de Paris[n 3] né en Champagne[11], il échappe de peu à la guillotine devant le Tribunal révolutionnaire pendant la Terreur[12] et devient directeur des Postes militaires de la Grande Armée. Il meurt du choléra en 1832. Son fils, Julien-Philippe enseigne alors à Lille, où un de ses oncles a un poste à la manufacture des tabacs. Julien de Gaulle y épouse la fille d'un administrateur de la manufacture, Joséphine Maillot. Le pensionnat qu'ils créent à Valenciennes fait faillite. Ils s'installent à Paris pour écrire ; il rédige deux études (sur un peintre paysagiste et sur un biographe de Saint Louis). Sa vaste Histoire de Paris et de ses environs d'inspiration monarchiste et catholique est préfacée par Charles Nodier. Elle, prolifique, collabore à des revues littéraires et écrit plus de 70 ouvrages dont certains dénoncent la pauvreté ouvrière du Nord.
Ils ont trois fils. Les deux oncles du général sont des chercheurs érudits : l'aîné, Charles, son homonyme, paralysé par la poliomyélite, étudie les langues celtes, et le cadet, Jules, est entomologiste. Henri, père du général, naît en 1848, un 22 novembre comme son fils. Formé par le jésuite Olivaint, il se lie aux milieux monarchistes et catholiques sociaux, et entre au secrétariat de Talhouët-Roy dont il est précepteur des enfants. Admissible à Polytechnique, il s'engage et est blessé au cours de la guerre de 1870. Il s'inscrit au barreau et dans un cercle jésuite influent. Mais, pour entretenir la famille, il renonce à une carrière militaire ou politique et fait partie de l'administration du ministère de l'Intérieur jusqu'en 1884. Il a ensuite trois doctorats (lettres, sciences, et droit) et enseigne lettres, histoire et les mathématiques au collège de l'Immaculée-Conception de Paris, tenu par les jésuites. À trente-sept ans, il épouse Jeanne Maillot, une petite-cousine de sa mère.
Charles de Gaulle est ainsi doublement issu de la famille Maillot, par sa mère et sa grand-mère paternelle. Originaires de la Flandre française, ces industriels catholiques descendent d'administrateurs de la manufacture des tabacs.
Le grand-père maternel de Charles de Gaulle, Jules-Émile Maillot (mort l'année de sa naissance), est un entrepreneur qui a rapporté une nouvelle machine à tisser le tulle d'Angleterre. Il était issu de l'union de deux familles des manufactures du tabac, les Maillot et les Kolb. Louis Philippe Kolb, grand-père de Jules-Émile Maillot, luthérien du duché de Bade, était, avant 1791, sergent major au régiment de Rheinach. Marié à Maubeuge en 1790 avec une certaine Marie Nicot[13], il avait réorganisé des manufactures de tabac, en particulier à Lille. Ses deux fils y réussissent : l'un, Henri, est urbaniste ; l'autre, Charles Kolb-Bernard, industriel sucrier, devient sénateur chrétien social et légitimiste[14].
La grand-mère maternelle du futur « homme de Londres », Justine Maillot-Delannoy, reçoit jusqu'à sa mort en 1912 ses enfants et petits-enfants. Elle était la fille d'un avocat et d'une Britannique. Son grand-père maternel descendait d'un membre du clan irlandais MacCartan (en) qui, jacobite, s'était réfugié en France après la Glorieuse Révolution[15] ; sa grand-mère maternelle, quant à elle, était issue d'une famille écossaise et protestante, les Fleming.
Charles de Gaulle est marqué par les valeurs familiales : catholicisme légitimiste, goût des études et du service de l'État (droit, administration des tabacs ou de l'armée).
Ses parents forment une famille catholique qui réside à Paris au 15 de l'avenue de Breteuil. Bien que la famille de Gaulle vécût à Paris, la mère du général de Gaulle se rendit dans sa famille à Lille pour donner naissance à son fils, en accord avec la tradition familiale de la famille Maillot[16]. La famille se rend régulièrement à Lille pour voir la grand-mère Julia Delannoy-Maillot. Toute sa vie, Charles de Gaulle garde une relation particulière avec sa région d'origine[n 4].
Charles de Gaulle a trois frères et une sœur :
Henri de Gaulle vers 1890.
Jeanne Maillot vers 1890.
De gauche à droite : Xavier, Marie-Agnès, Charles, Jacques et Pierre de Gaulle vers 1899.
Très tôt, son père lui fait découvrir les œuvres de Maurice Barrès, Henri Bergson et Charles Péguy. Henri de Gaulle se dit monarchiste de regret et lit L'Action française, mais finit par douter de la culpabilité du capitaine Dreyfus ; pour autant, malgré des témoignages ultérieurs, rien n'indique qu'il se soit engagé politiquement dans le combat dreyfusard[18]. Jeanne de Gaulle est davantage passionnée de politique : dès la première page des Mémoires de guerre, Charles de Gaulle rend hommage à sa mère admirée, .
Charles de Gaulle fait une partie de ses études primaires à l'école des Frères des écoles chrétiennes de la paroisse Saint-Thomas-d'Aquin. Il a son père comme enseignant chez les jésuites au Collège de l'Immaculée-Conception de la rue de Vaugirard à Paris. Lors de la crise politico-religieuse résultant des lois de 1901 et de 1905 qui interdit aux congrégations d'enseigner, le professeur de Gaulle fonde à Paris en 1907 un cours libre secondaire, l'École Louis de Fontanes, et inscrit son fils Charles chez les jésuites français en Belgique au collège du Sacré-Cœur installé au château d'Antoing[19]. Le jeune lycéen vit ainsi sa première expérience d'exil.
Le jeune Charles a quinze ans quand, en 1905, il rédige un récit dans lequel il se décrit en « général de Gaulle » sauvant la France, témoignage d'une ambition nationale précoce[20]. Plus tard, il explique à son aide de camp Claude Guy avoir eu dès son adolescence la conviction qu'il serait un jour à la tête de l'État[21],[n 5].
Entré 119e sur 221 à l'École militaire de Saint-Cyr en 1908, après avoir suivi une année de préparation au collège Stanislas[22]. Il en sort diplômé en 1912, se classant à la 13e place[n 6], il rejoint le 33e régiment d'infanterie à Arras et se retrouve sous les ordres du colonel Pétain puis du lieutenant-colonel Stirn[23].
Au collège Stanislas en 1908 (rang debout, 3e en partant de la gauche), lors de son année de préparation à Saint-Cyr.
En uniforme de saint-cyrien, 1910.
Les quatre frères de Gaulle sont mobilisés comme officiers. Ils reviennent tous vivants et décorés. Charles, qui était lieutenant depuis le 1er octobre 1913, est nommé capitaine en janvier 1915[25]. Dès son premier combat à Dinant le 15 août 1914, il est touché à la jambe (« fracture du péroné par balles avec éclats dans l'articulation »)[26]. Il rejoint ensuite le 33e RI sur le front de Champagne pour commander la 7e compagnie. Il est à nouveau blessé le 10 mars 1915, à la main gauche, au Mesnil-lès-Hurlus en Champagne. Décidé à en découdre, il désobéit à ses supérieurs en ordonnant de tirer sur les tranchées ennemies. Cet acte lui vaut d'être relevé huit jours de ses fonctions. Officier tatillon, volontiers cassant, son intelligence et son courage face au feu le distinguent au point que le commandant du 33e RI lui offre d'être son adjoint[27].
Le 2 mars 1916, son régiment est attaqué et décimé, anéanti par l'ennemi en défendant le village de Douaumont, près de Verdun. Sa compagnie est mise à mal au cours de ce combat et les survivants sont encerclés. Tentant alors une percée, il est obligé par la violence du combat à sauter dans un trou d'obus pour se protéger, mais des Allemands le suivent et le blessent d'un coup de baïonnette à la cuisse gauche[28]. Capturé par les troupes allemandes, il est soigné à l'hôpital de Mayence puis interné à Osnabrück en Westphalie[29]. Tenu pour mort au combat, cette disparition lui vaut d'être cité à l'ordre de l'armée[30],[n 7].
Plaque sur le pont de Dinant commémorant l'endroit où il fut blessé alors qu'il traversait la Meuse en 1914.
Le capitaine de Gaulle en 1915.
Prisonnier au camp allemand de Sczuczyn (Biélorussie), le capitaine de Gaulle sert la soupe à ses camarades, vers 1916-1917.
Extrait d'une lettre de Charles de Gaulle à son commandant, où il revient sur les circonstances de sa capture.
Après une tentative d'évasion manquée à Osnabrück[35], il est transféré à Neisse en Silésie puis à Sczuszyn en Empire russe (territoire moderne de la Biélorussie) et enfin au fort d'Ingolstadt, en Bavière, un camp de représailles destiné aux officiers prisonniers remuants[36]. Il y croise le futur général Georges Catroux, l'aviateur Roland Garros, le journaliste Rémy Roure, le colonel Lucien Nachin[n 8],[16],[37],[38] et le futur maréchal soviétique Mikhaïl Toukhatchevski, dont il partage la cellule[39]. Dans une lettre adressée à sa mère, il décrit sa situation de captif comme un « lamentable exil ». Pour tromper l'ennui, de Gaulle organise pour ses compagnons de captivité des exposés magistraux sur l'état de la guerre en cours. Mais surtout, il tente de s'évader à cinq reprises, sans succès, au cours de sa détention de trente-deux mois dans une dizaine de camps différents (Osnabruck, Neisse, Sczuczyn, Ingolstadt, forteresse de Rosenberg (de), prison militaire de Passau, camps de Wülzburg (de) ou de Würzburg[n 9] et de Magdebourg)[44]. Il est libéré après l'armistice du 11 novembre 1918 et retrouve les siens le mois suivant. De ces deux ans et demi de captivité, il garde un souvenir amer, estimant être un « revenant », un soldat inutile qui n'a servi à rien[45]. Toutefois, il reçoit la croix de chevalier de la Légion d'honneur, le 23 juillet 1919, et la croix de guerre 1914-1918 avec étoile d'argent[25].
Charles de Gaulle poursuit sa carrière militaire sous la protection de Pétain, dans un premier temps.
Le 20 janvier 1919, il arrive à Saint-Maixent pour suivre les cours de remise à niveau destinés aux officiers de retour de captivité. Désireux de relancer sa carrière militaire compromise par ses mois de détention, il cherche à s'engager sur un théâtre d'opération, et postule simultanément pour un engagement dans l'armée d'Orient et auprès de l'armée de Pologne. Début avril 1919, il obtient son détachement auprès de l'Armée polonaise autonome qui commence à quitter la France pour la Pologne. Il effectue dans le pays deux séjours très rapprochés, le premier d'avril 1919 à mai 1920, et le second de juin 1920 à la fin du mois de janvier 1921[46]. Dans le cadre de la mission militaire française du général Henrys, le capitaine de Gaulle est affecté comme instructeur à l'école d'infanterie de Rembertow. Il y exerce successivement les fonctions d'instructeur, de directeur des études en novembre, et enfin de directeur du cours des officiers supérieurs à partir de décembre. Repoussant l'offre du général Henrys qui lui proposait de poursuivre sa mission auprès de lui, de Gaulle, qui ambitionne de se présenter au concours de l’École supérieure de guerre dans les meilleures conditions, retourne en France. Déçu par le poste qui lui échoit au cabinet des décorations du ministre, et alors que la guerre soviéto-polonaise fait rage, il repart en Pologne en mai 1920. D'abord témoin des épreuves traversées par la population polonaise, il prend ensuite activement part aux opérations avec le général Bernard au sein du 3e bureau du groupe d'armées Sud (puis Centre) commandé par le général polonais Rydz-Śmigły. Il y gagne une citation. Après la victoire de la Pologne, il rédige notamment un rapport général sur l'armée polonaise. Si à l'analyse de l'action de l'unique régiment de chars FT 17, il a pu écrire , de Gaulle découvre surtout en Pologne la guerre de mouvement et l'emploi des grandes unités de cavalerie comme élément de choc et moyen d'obtenir une décision à portée stratégique.
Son père (qui s'était fait rappeler à 66 ans en 1914) se retire progressivement de l'enseignement et Charles de Gaulle indique à sa famille qu'il souhaite se marier. Il a été affecté par le décès sous les bombes d'une « quasi fiancée », en 1916 en Belgique. Les familles lui présentent une jeune fille issue de la bourgeoisie du Nord. Le 6 avril 1921, Charles de Gaulle épouse Yvonne Vendroux ; le mariage religieux est célébré le lendemain à l'église Notre-Dame de Calais[47]. Le couple a trois enfants :
Mariage de Charles de Gaulle et Yvonne Vendroux (avril 1921).
Charles de Gaulle et sa fille Anne, en 1933.
À son retour, le capitaine de Gaulle est chargé de cours d'histoire à l'École de Saint-Cyr[49], avant son admission à l'École supérieure de guerre en 1922. En conflit de doctrine avec ses supérieurs dont il conteste la vision stratégique trop liée à la planification défensive et compartimentée du terrain, mais bénéficiant de la protection de Philippe Pétain, il est mal noté, mais continue de se faire une réputation prometteuse.
En 1924, à l'occasion d'une visite à l'École de guerre, Pétain s'étonne de la faiblesse des notes attribuées à de Gaulle. Ses professeurs appréciaient peu l'indépendance de celui-ci, trait de caractère qu'il partageait avec Pétain. L'intervention de Pétain a probablement conduit à une rectification à la hausse desdites notes[50].
En 1925, il est détaché à l'état-major de Philippe Pétain, vice-président du Conseil supérieur de la Guerre. Celui-ci l'impose comme conférencier à l'École de guerre et lui demande de préparer la rédaction d'un ouvrage sur l'histoire du soldat. En 1927, en présence de Pétain, il présente à l'École de guerre trois conférences remarquées, respectivement intitulées : « L'action de guerre et le chef », « Du caractère », et enfin « Du prestige ».
Promu chef de bataillon le 25 septembre 1927, il part le mois suivant pour Trèves prendre le commandement du 19e bataillon de chasseurs à pied (BCP)[51]. Il y conduit un commandement énergique et continue ses conférences comme dans son poste suivant.
En novembre 1929, il est affecté à l’État-major des Troupes du Levant à Beyrouth où il est responsable des 2e et 3e bureaux (renseignement militaire et opérations). Accompagné de sa famille, il y demeure jusqu'en janvier 1932[52]. Il effectue plusieurs missions à Alep, Damas, Homs, Palmyre. En juin 1930, il participe à une expédition de pacification sur les territoires majoritairement kurdes du nord-est de la Syrie. Dans une lettre de juillet 1930 à son père, il exprime sa fierté d’avoir atteint le Tigre au nom de la France : « C’était, je pense, la première fois dans l’histoire que des soldats français y allaient en armes[53] ».
Grâce à l'appui du maréchal Pétain, il est affecté en novembre 1931 au secrétariat général de la Défense nationale à Paris. Ce nouveau poste est capital, car c'est l'occasion de s'initier aux affaires de l'État[54], puisqu'il est chargé en particulier de travailler au projet de loi militaire. Le 25 décembre 1933, il est promu lieutenant-colonel.
C'est durant ces années que Charles de Gaulle développe ses théories militaires : il publie La Discorde chez l'ennemi (1924), Le Fil de l'épée (1932), Vers l'armée de métier (1934) et enfin La France et son armée (1938).
Ce dernier livre est préparé depuis 1925 pour Philippe Pétain ; de Gaulle s'y consacre pendant deux ans (sous le titre de Le Soldat), et Pétain lui permet même de présenter les trois conférences citées plus haut. Mais, jugeant que la partie sur la Grande Guerre n'est pas suffisante, le maréchal veut confier la suite du travail au colonel Audet. Ceci blesse de Gaulle qui prétend finir seul le travail ; l'ouvrage est mis au placard jusqu'en 1938. En 1932, de Gaulle dédicace néanmoins au maréchal Pétain son ouvrage Le Fil de l'épée : « Car rien ne montre mieux que votre gloire, quelle vertu l'action peut tirer des lumières de la pensée ». Mais en 1938, de Gaulle décide de publier sous son nom le texte du Soldat, et en avertit Pétain, qu'il cite dans la préface comme « inspirateur de l'ouvrage », dont il a retiré toutes les suggestions et observations faites par son supérieur. Pour arranger les choses, Pétain le reçut chez lui et lui proposa de rédiger une dédicace que de Gaulle estime pouvoir adapter dans un premier temps ; devant cette réécriture, Pétain intervient directement auprès de l'éditeur pour demander une correction, que de Gaulle lui accorde bien volontiers, . Mais Pétain semble considérer désormais que le colonel n'est plus qu'un ambitieux dépourvu d'éducation, et le lieutenant-colonel De Gaulle a perdu sa considération pour Pétain (depuis déjà le renvoi par Pétain de Lyautey) d'où une brouille définitive entre les deux hommes qui ne se reverront brièvement qu'en juin 1940[55],[56].
Dans son premier ouvrage, de Gaulle insiste sur la nécessité de l'unité du commandement et de la nation, donnant la primauté au politique sur le militaire. C'est selon lui à cause de ses divisions que l'Allemagne a perdu. En publiant la reprise de ses conférences sur le rôle du commandement, en 1932, dans Le Fil de l'épée il rappelle l'importance de la formation des chefs et le poids des circonstances. Si de Gaulle étudie l'importance de la défense statique au point d'écrire : « La fortification de son territoire est pour la France une nécessité permanente […] L'encouragement de l'esprit de résistance d'un peuple par l'existence de fortifications permanentes, la cristallisation, l'exaltation de ses énergies par la défense des places sont des faits que les politiques comme les militaires ont le devoir de reconnaître dans le passé et de préparer dans l'avenir », il n'en est pas moins sensible aux idées du général Jean-Baptiste Eugène Estienne sur la nécessité d'un corps de blindés[57], alliant le feu et le mouvement, capable d'initiatives et d'offensives hardies. Sur ce point il entre de plus en plus en opposition avec les doctrines officielles, en particulier celles de Pétain.
Dans son ouvrage Vers l'armée de métier, il développe cette question de fond qui nécessite la création d'une armée professionnelle aux côtés de la conscription. Il devient alors le promoteur de la création d'unités blindées autonomes non liées à l'infanterie. Cependant, cette idée rencontre peu d'échos favorables, à l'exception notable de Paul Reynaud, député de centre-droit, ou de Philippe Serre.
À ce sujet, l'armée comptait d'ardents partisans des divisions cuirassées : les généraux Weygand, Billotte, Héring, Doumenc, Delestraint et, en particulier, la plupart des généraux issus de la cavalerie, comme Prioux qui sera en 1940 vainqueur tactique d'une bataille contre un corps d'armée de panzers allemands, ou Touzet du Vigier qui commandera l'une des deux divisions blindées de 1944 sous les ordres de de Lattre[58].
À l'étranger, en revanche, l'idée du général Estienne d'employer des blindés dans une « percée motorisée » reprise par de Gaulle a déjà suscité la plus grande attention (Heinz Guderian, Liddell Hart). Vers l'armée de métier n'a en France qu'un bref succès de curiosité et ne fait que conforter le général Guderian dans ses idées, lui qui était déjà en train de créer la force mécanique allemande[n 10]. Néanmoins, les théories de Charles de Gaulle sont suivies avec intérêt par Adolf Hitler, Albert Speer rapportant que le Führer avait lu à plusieurs reprises le livre du général de Gaulle et qu'il affirmait avoir beaucoup appris grâce à lui[60].
En revanche, contrairement à son influent aîné le colonel Émile Mayer (dont il est intellectuellement proche, se considérant comme son élève[61]), de Gaulle ne perçoit pas l'importance de l'aviation à laquelle il n'attribue qu'un rôle secondaire : « Les troupes à terre recevront de l'aviation une aide précieuse quant à leur camouflage. Les fumées épandues sur le sol du haut des airs cachent en quelques minutes de vastes surfaces du sol tandis que le bruit des machines volantes couvre celui des moteurs chenillés ». Il faudra attendre l'édition de 1944 où il fera ajouter une phrase : « Mais surtout en frappant elle-même à vue directe et profondément, l'aviation devient par excellence l'arme dont les effets foudroyants se combinent le mieux avec les vertus de rupture et d'exploitation de grandes unités mécaniques ».
À Paris, de Gaulle est introduit par Lucien Nachin dans le salon non conformiste qui se tient autour du colonel Mayer, retraité très ouvert, favorable à une réforme de la stratégie : l'état-major ne doit pas se contenter d'une stratégie défensive derrière la ligne Maginot. Cependant, ni l'un ni l'autre ne sont écoutés[38]. Partant des idées du général Fuller et du critique militaire britannique Liddell Hart, Charles de Gaulle défend une guerre de mouvement menée par des soldats de métier, et appuyée par des blindés.
Charles de Gaulle fait une conférence à la Sorbonne au printemps 1934, sous l'égide du cercle Fustel de Coulanges, une vitrine de l’Action française[62]. Influencé originellement par la tradition monarchiste, Charles de Gaulle, militaire soumis au devoir de réserve, révèle dans sa correspondance privée son peu de considération pour le parlementarisme et lui préfère un régime fort, tout en se tenant publiquement à l'écart de l’anti-républicanisme d'une partie de l'armée[63]. Cette méfiance à l'égard du parlementarisme explique que Charles de Gaulle se soit senti avant la guerre proche de l'Action française, avant que la position de Maurras relative aux accords de Munich ne l'en éloigne. Ainsi, Paul Reynaud, qui rencontra en captivité en Allemagne la sœur du général de Gaulle, Marie-Agnès Cailliau, note dans ses carnets de captivité parlant de cette dernière[64] :  De même, Christian Pineau dira à André Gillois « que le général avait reconnu devant lui qu’il avait été inscrit à l’Action française et qu’il s’était rallié à la République pour ne pas aller contre le sentiment des Français »[65]. Lui-même résistant de gauche, Claude Bourdet qualifiera de Gaulle d’homme de droite, longtemps proche de l’Action française, devenu républicain par mimétisme[66]. Selon Edmond Michelet, de Gaulle subit l’influence de Maurras[67],[n 11].
Pourtant, si la pensée de Maurras a influencé de Gaulle[n 12], celui-ci est aussi un disciple de Charles Péguy[69],[70],[71].
De fait il fréquente le colonel Émile Mayer, officier israélite, dreyfusard et socialisant. Ayant avant la Première Guerre mondiale assisté à Lille à des meetings de Jaurès, il a aussi fréquenté le socialiste Club du Faubourg et les mouvements non-conformistes des années 30 (Esprit). Il adhéra également aux Amis de Temps présent, groupe de militants qui soutenait Temps présent, comme l'indique Éric Roussel, qui signale cependant que de Gaulle  Cet hebdomadaire est en effet de la mouvance catholique progressiste et proche du Sillon de Marc Sangnier[73], mouvance qui fut favorable au Front populaire et à l'intervention de la France aux côtés des républicains espagnols. L'hebdomadaire Temps présent saluera la nomination de Charles de Gaulle comme sous-secrétaire d'État à la Guerre dans son dernier numéro de juin 1940, comme le signale le Centre d'information sur le gaullisme[74], signalant au passage que de Gaulle fut aussi l'un des premiers abonnés à Sept, hebdomadaire à direction religieuse dont Temps présent était le successeur.
Le 1er janvier 1934, Charles de Gaulle publie dans la revue militaire une étude sur la mobilisation économique à l'étranger. À la recherche d'exemples pour la France, il cite parmi d'autres l'Italie mussolinienne[n 13], mais étudie aussi favorablement l'exemple de l'Amérique de Roosevelt. Le futur général de Gaulle fera l'apologie du livre La réforme de l'État publié par André Tardieu en 1934 et dira s'en être inspiré pour la constitution de la Ve République[75].
De fait, avant la guerre, de Gaulle n'est pas un idéologue, mais un homme de réflexion et d'action[n 14] et d'ambition.
À cette fin, il se rapproche d'hommes politiques de différentes tendances pour se faire connaître et faire progresser ses idées. Dans le salon de Mayer, il a fait la connaissance de l'avocat Jean Auburtin, qui affirme être son principal mentor politique. De fait, Auburtin peut lui présenter Paul Reynaud (accompagné de son conseiller d'alors, Gaston Palewski), que de Gaulle fréquente ensuite régulièrement (il lui écrira soixante fois de 1936 à 1940[n 15]), et qui portera au palais Bourbon le système du colonel. Auburtin lui présente également d'autres personnalités politiques plus à gauche, telles que Léo Lagrange (président de la commission de l'armée à la Chambre des députés) et Marcel Déat, tous deux intéressés par l'armée de métier[76] ; si le premier ne s'engage pas par loyauté envers Léon Blum, le second, qui vient de rompre avec le dirigeant de la SFIO, accepte de lui prêter son concours (après sa défaite aux élections de 1936 — et son attirance pour une voie opposée, Déat ne sera plus d'un réel soutien)[77]. De Gaulle affirme à propos de Déat en novembre 1937, après avoir reçu un exemplaire de Le Front populaire au tournant[78] :  Il enverra à Déat en 1940 un exemplaire de son mémorandum L’Avènement de la force mécanique, lui manifestant alors encore un intérêt certain[79].
À la publication de l'ouvrage, Léon Blum manifeste sa vive hostilité pour les idées de l'armée de métier du colonel de Gaulle dans trois articles publiés par le Populaire, car il craint qu'elle ne soit utilisée contre le peuple, notamment les grévistes. Et, de fait, comme le montre une lettre de 1935 envoyée à Paul Reynaud, de Gaulle n'excluait nullement une telle possibilité. Certains passages des livres publiés par le colonel de Gaulle suscitent d'ailleurs l'approbation de l'Action française[80].
En 1935, de Gaulle approuve le pacte franco-soviétique signé par Laval et Staline, évoquant l'alliance de François Ier avec les musulmans contre Charles Quint pour justifier une alliance destinée à assurer la survie du pays pour justifier un accord avec les Russes [81]. De Gaulle décide de faire abstraction des — vices — et des  en ne retenant que la théorie de l'ennemi : [82]. Comme le dit Claude Bouchinet-Serreulles, [83].
Charles de Gaulle explique dans Vers l'armée de métier quelle est la condition pour faire aboutir ses idées qui sont d'abandonner le service militaire universel au profit d'une armée motorisée composée exclusivement de professionnels : . Il affirme également :  Cet appel à la figure du grand homme était déjà présent dans Le Fil de l'épée, où, dès 1932, il exalte[84],  ; dans cet ouvrage, il affirme également :  Dans le Fil de l'épée, il brosse le portrait de l'ambitieux de haute stature qui n'est pas forcément un soldat, à tout le moins un émule de Louvois, Carnot, ou au moins de Gouvion Saint-Cyr ou Thiers[85].
Néanmoins, Blum se laisse progressivement intéresser par la thématique des chars qu'il soutiendra tardivement au moment de la guerre.
En juillet 1937, le lieutenant-colonel de Gaulle est affecté au 507e régiment de chars de combat basé au quartier Lizé à Montigny-lès-Metz. C'est la rencontre concrète avec « son » outil. Il en prend le commandement par intérim le 5 septembre suivant, puis est promu colonel le 25 décembre 1937[86]. Lors des manœuvres, il tente d'imposer, contre le règlement, sa conception de l'usage autonome des blindés, ce qui lui vaut l'hostilité de son supérieur, le général Henri Giraud.
Lorsque la guerre éclate, Charles de Gaulle est toujours colonel, et commande par intérim les chars de la 5e armée du général Bourret. Le 26 janvier 1940, il envoie à quatre-vingts personnalités civiles ou militaires, dont Léon Blum et Paul Reynaud, ainsi qu'aux généraux Maurice Gamelin et Maxime Weygand, un mémorandum fondé sur les opérations de Pologne. Intitulé L'Avènement de la force mécanique, le texte insiste sur la nécessité de constituer de grandes unités autonomes blindées plutôt que de disperser les chars au sein d'unités tactiques plus larges, comme le préconise l'état-major. Trois jours avant l'offensive allemande du 10 mai 1940, qui conduit à une percée rapide du front français, le colonel de Gaulle est averti de la décision du commandement de lui confier la 4e DCR (364 blindés[87]) dont il prend effectivement le commandement le 11 mai. De Gaulle est conseillé par Georges Boris[88],[89],[90].
Le 15 mai, il reçoit la mission de retarder l'ennemi dans la région de Laon afin de gagner des délais nécessaires à la mise en place de la 6e armée chargée de barrer la route de Paris. Mais sa division blindée n'est encore qu'en cours de constitution, ses unités n'ayant jamais opéré ensemble. Il dirige pourtant avec cette unité une contre-attaque vers Montcornet, au nord-est de Laon. C'est l'une des seules qui parviennent à repousser momentanément les troupes allemandes. Prévoyant la défaite rapide de l'armée française sous l'offensive allemande, les civils et les militaires désarmés sur les routes, il affirme que c'est durant la journée du 16 mai que « ce qu'[il] a pu faire, par la suite, c'est ce jour-là qu'[il] l'a résolu. »[n 16]. N'ayant reçu qu'une partie des unités de la 4e DCR, le colonel de Gaulle lance une première attaque avec 80 chars pour tenter de couper les lignes de communication des divisions blindées allemandes le 17 mai. Après avoir atteint ses objectifs dont la ville de Montcornet, la 4e DCR, n'étant pas appuyée, est contrainte de se replier face à l'intervention de renforts ennemis. Les autres unités de la 4e DCR l'ayant rejoint, une nouvelle attaque peut être lancée avec 150 chars qui, après avoir permis d'atteindre les premiers objectifs, est arrêtée par l'intervention de l'aviation d'assaut et de l'artillerie allemandes.
Le 21 mai, à la suite de la bataille de Montcornet, l'état-major envoie un correspondant de guerre pour interroger de Gaulle, qui lance à cette occasion, à Savigny-sur-Ardres, un premier appel radiodiffusé destiné à remonter le moral des Français en vantant les mérites des divisions blindées et qui se termine par la phrase : [92].
Le 25 mai, il est nommé général de brigade à titre temporaire[93],[94]. Cette nomination, dans une promotion de six colonels, correspond au fait que de Gaulle en tant que commandant d'une division blindée depuis le 7 mai 1940, fait déjà fonction de général, ses trois collègues commandants de division blindée (DCR) étant tous déjà généraux. Elle suscite la satisfaction de Charles Maurras dans l'Action française[95],[96],[n 17].
Trois jours plus tard, le 28 mai, il attaque à deux reprises pour détruire une poche que l'ennemi a conquise au sud de la Somme, à hauteur d'Abbeville. Malgré un déplacement préalable de 200 km qui a lourdement éprouvé le matériel de la 4e DCR, l'opération permet de résorber toute la poche en capturant 400 soldats allemands, mais pas de prendre la ville d'Abbeville. De Gaulle ne parvient que plus tard à franchir la Somme au nord d'Abbeville, une seconde attaque ne permettant pas de prendre la ville, avant de se replier avec la 4e DCR.
Ces résultats limités n'empêchent pas le général Weygand, chef des armées, de décerner le 31 mai 1940 au général de Gaulle une citation très élogieuse en tant que commandant d'une division blindée près d'Abbeville : [97]. Commentant le comportement militaire de De Gaulle sur le terrain, l'historien Henri de Wailly juge que celui-ci, loin d'avoir été particulièrement brillant, a montré dans la bataille « les mêmes faiblesses et les mêmes incompétences » que les autres dirigeants militaires[98].
Au cours de la bataille de Montcornet du 17 mai, la division de de Gaulle perd une quinzaine de soldats tués, une dizaine d'autres blessés et 25 chars contre 85 engagés (les Allemands n'ayant de leur côté perdu aucun char), mais a une centaine de tués, qui fut plutôt une défaite, mais reste une victoire symbolique[pas clair][99]. Elle doit être mise en vis-à-vis de la bataille de Hannut des 12-14 mai. Celle-ci est livrée en Belgique par un corps d'armée blindé dirigé par le général Prioux contre un corps allemand de deux panzer-divisions commandé par le général Hoepner. Elle est considérée comme une victoire sans lendemain, du fait de l'effondrement militaire sur la droite et la gauche du corps Prioux. Côté français, sont engagés sans soutien aérien 411 chars (dont 105 à 164 détruits ou perdus), 104 canons (dont 40 antichars et 12 de DCA). Côté allemand, 623 chars (dont 50 à 164 détruits et 200 endommagés) qui sont engagés, 397 canons (dont 159 antichars et 72 de DCA), ainsi qu'un soutien aérien très actif. Le corps de panzer est arrêté par le corps d'armée français qui doit reculer par la suite pour ne pas être enveloppé sur ses ailes où d'autres unités ont été vaincues[100].
Entre le 26 mai et le 2 juin, la Grande-Bretagne décide, sans concertation avec le commandement français, de replier son armée en rembarquant par Dunkerque la totalité de son corps expéditionnaire de 200 000 hommes, ainsi que 139 229 Français, laissant le reste de l'armée française seule face aux Allemands qui capturent tout leur matériel (2 472 canons, près de 85 000 véhicules, 68 000 tonnes de munitions, 147 000 tonnes de carburant, 377 000 tonnes d'approvisionnements) et font prisonnier les 35 000 soldats français restants.
Le 16 juin, le chef du gouvernement Paul Reynaud relève de ses fonctions Daladier et exerce lui-même les fonctions de ministre de la Guerre.
Le 6 juin, le général de Gaulle est convoqué d'urgence à Paris par Paul Reynaud, président du Conseil et ministre de la Guerre, pour occuper un poste ministériel dans son gouvernement, celui de sous-secrétaire d'État à la Guerre et à la Défense nationale. Charles de Gaulle sort alors de la hiérarchie militaire pour commencer une carrière politique. Il a pour mission de coordonner l'action avec le Royaume-Uni pour la poursuite du combat. Le 9 juin, il rencontre le Premier ministre du Royaume-Uni, Winston Churchill.
Charles de Gaulle quitte Paris, qui est déclarée ville ouverte et occupée par les Allemands, le 10 juin. Il rejoint alors Orléans, Briare et Tours[101].
C'est le moment des ultimes réunions du Conseil suprême interallié où Churchill, lors de la conférence de Briare à laquelle de Gaulle participe avec un rôle important, puisqu'il est presque le ministre de la Guerre[102], tente de convaincre le gouvernement français de continuer la guerre, malgré la défection totale de l'armée anglaise rembarquée à Dunkerque. Le général Weygand demande l’intervention des 25 escadrilles de chasse de la RAF qui avaient été promises par les Anglais pour pousser la France à entrer en guerre, mais Churchill refuse, car il veut les réserver pour la défense contre une attaque directe du territoire de l'Angleterre.
Le 16 juin, il est en mission à Londres et dicte au téléphone la note de Jean Monnet à Paul Reynaud, intitulée Anglo-French Unity, d'une Union franco-britannique votée le jour même par la Chambre des communes, consistant dans la fusion des armées, notamment des marines, des territoires, des colonies et du gouvernement français dans l'Empire britannique. Il fait valoir que dans le cadre « d'un gouvernement unique franco-britannique et vous, Monsieur le Président, pouvez être Président du cabinet de Guerre franco-britannique. »
De retour à Bordeaux, il apprend avec consternation, le 17 juin, la démission du président du Conseil, Paul Reynaud, son remplacement par Philippe Pétain. Le même jour, la nomination du général Weygand, alors chef d'état-major de l'Armée, comme ministre de la Défense nationale et de la Guerre sonne le glas des ambitions ministérielles de De Gaulle. Le transfert des pouvoirs de chef de gouvernement à Pétain n'ayant lieu que le lendemain, de Gaulle est encore membre du gouvernement Reynaud et se dit qu'il court peu de risques en quittant la France[103].
Le représentant de Churchill auprès du gouvernement français, le général Edward Spears, est venu à Bordeaux pour tenter de convaincre Paul Reynaud et Georges Mandel de rejoindre Londres, comme le prévoit le projet d'Union franco-britannique, mais sans succès[104],[105]. Ceux-ci avaient l'intention d'embarquer pour l'Afrique du Nord à bord du Massilia. N'ayant plus de rôle à jouer dans le nouveau gouvernement, et Paul Reynaud lui ayant fait remettre par son ex-directeur de cabinet Jean Laurent 100 000 francs prélevés sur les fonds secrets pour sa logistique à Londres, De Gaulle et son aide de camp Geoffroy Chodron de Courcel, obtiennent du général Spears, après maintes hésitations[106], d'embarquer avec lui dans le de Havilland Flamingo qui repartait à Londres le 17 juin, tout en faisant croire à un enlèvement[107].
Le gouvernement britannique avait tenté vainement de convaincre Paul Reynaud de transférer le gouvernement français au Royaume-Uni avec Georges Mandel, ancien ministre des Colonies devenu ministre de l'Intérieur, qui aurait lancé lui-même un appel à poursuivre les combats avec toutes les ressources de l'Empire français. Réfugiés à Bordeaux avec leurs familles pour fuir l'invasion allemande, ceux-ci avaient réquisitionné le paquebot Massilia, qui devait appareiller le 19 juin 1940 pour l'Afrique du Nord.
Le 18 juin 1940, de Gaulle se prépare à parler aux officiers et aux soldats français sur Radio Londres de la BBC.
Le ministre des Affaires étrangères lord Halifax n'est pas favorable à cet appel, car il veut éviter de gêner le gouvernement Pétain dans ses négociations d'un armistice le plus favorable possible aux Alliés. Tout au long de la journée du 18 juin, le Conseil des ministres britannique discute du texte de De Gaulle. Le cabinet britannique tente de s'opposer à cette intervention radiophonique, mais il semble que le soutien de Winston Churchill l'ait permise[110].
Après avoir déjeuné avec Duff Cooper, ministre de l’Information britannique, le général de Gaulle doit rendre son texte plus neutre : le cabinet de guerre britannique veut ménager Philippe Pétain, chef du gouvernement français, dont il ne connaît pas encore l'orientation[111]. Pétain n'est pas nommé dans le discours, et la première phrase du discours faisant référence à la trahison du nouveau gouvernement qui  est également supprimée[112],[113] et remplacée par :
« Le gouvernement français a demandé à l’ennemi à quelles conditions honorables un cessez-le-feu était possible. Il a déclaré que, si ces conditions étaient contraires à l’honneur, la dignité et l’indépendance de la France, la lutte devait continuer[114]. »
Cette modification longtemps occultée disparaît dans le Bulletin officiel des Forces françaises libres du 15 août 1940, dans le premier numéro du Journal officiel de la France libre le 20 janvier 1941, puis dans les Mémoires de guerre et dans l'ensemble des recueils de discours du général de Gaulle, qui continuent à faire commencer l'appel avec la phrase supprimée qui décrivait parfaitement la situation d'éviction que De Gaulle vivait à ce moment :
« Les chefs qui, depuis de nombreuses années, sont à la tête des armées françaises, ont formé un gouvernement.
Ce gouvernement, alléguant la défaite de nos armées, s'est mis en rapport avec l'ennemi pour cesser le combat[111]. »
Aucun enregistrement de l'appel radiophonique n'ayant été conservé, son texte est souvent confondu, soit avec celui de l'appel du 22 juin 1940, soit avec un appel encore différent filmé le 2 juillet 1940 pour les actualités cinématographiques[115], soit avec celui de la célèbre affiche placardée dans des rues de Londres le 5 août 1940.
En janvier 2023, le journal Le Monde en partenariat avec l'Institut de Recherche et Coordination Accoustique Musique (IRCAM), avec la participation de l'acteur François Morel, a reconstitué une version du discours radiodiffusé le 18 juin 1940, avec l'aide de l'intelligence artificielle[116].
« Le gouvernement français a demandé à l’ennemi à quelles conditions honorables un cessez-le-feu était possible. Il a déclaré que, si ces conditions étaient contraires à l’honneur, la dignité et l’indépendance de la France, la lutte devait continuer.
[…] Certes, nous avons été, nous sommes submergés par la force mécanique terrestre et aérienne de l'ennemi. Infiniment plus que leur nombre, ce sont les chars, les avions, la tactique des Allemands qui nous font reculer. Ce sont les chars, les avions, la tactique des Allemands qui ont surpris nos chefs au point de les amener là où ils en sont aujourd'hui. […]
La France n'est pas seule […] elle a un vaste empire derrière elle. Elle peut faire bloc avec l'Empire britannique qui tient la mer et continue la lutte. Elle peut comme l'Angleterre utiliser sans limite l'industrie des États-Unis. […]
Moi, Général De Gaulle, actuellement à Londres, j'invite les officiers et les soldats français qui se trouvent en territoire britannique ou qui viendraient à s'y trouver, avec leurs armes ou sans leurs armes, à se mettre en rapport avec moi. Quoi qu'il arrive, la flamme de la résistance française ne doit pas s'éteindre et ne s'éteindra pas.[114] »
En France, l'appel du 18 Juin peut être entendu à 19 h. Il appelle tous les officiers et les soldats qui se trouvent en territoire britannique ou qui pourraient s'y trouver à le rejoindre et à continuer les combats. Ce texte est à l'origine du mythe faisant du général le « père de la Résistance » alors que ce dernier ne prendra conscience de l'intérêt de la Résistance intérieure qu'à partir de 1941[117].
La BBC a communiqué le texte du Ministry of Information (MOI) à la presse, il est publié dans The Times du 19 juin 1940, page 6 col. 3, et le Daily Express, et par quelques quotidiens régionaux français, Le Petit Provençal à la une (colonnes 5 et 6) de son édition de Marseille du mercredi 19 juin 1940[118]. Très peu de personnes se souviennent d'avoir entendu ce discours, ou même de l'avoir remarqué dans la presse.[réf. nécessaire]
Les actions de De Gaulle à Londres se font sans aucun ordre de mission. Le 19 juin, le général Weygand, qui est ministre de la Guerre et son supérieur hiérarchique, lui donne l'ordre de revenir de Londres[120],[121] et le 22 juin il annule sa promotion au grade de général à titre temporaire. Le 23 juin, le président de la République Albert Lebrun prend un décret décidant de mettre le colonel de Gaulle à la retraite d'office par mesure disciplinaire[122], et de le traduire devant le Conseil de guerre, qui le condamne le 4 juillet à quatre ans de prison et à la perte de sa nationalité française[123],[n 18].
Un mois après l'attaque sur Mers el-Kébir et l'attaque du Richelieu par les Fairey Swordfish du porte-avions HMS Hermes (le 8 juillet), et tandis que De Gaulle préparait l'attaque de Dakar, il est inculpé de « trahison, atteinte à la sûreté extérieure de l'État, désertion à l'étranger en temps de guerre sur un territoire en état de guerre et de siège » et condamné à Clermont-Ferrand le 2 août 1940 à la « peine de mort, dégradation militaire et confiscation de ses biens meubles et immeubles ». Sa déchéance de la nationalité française est confirmée dans un décret du 8 décembre 1940[125],[n 19].
De Londres, de Gaulle crée puis dirige les Forces françaises libres. Il est reconnu par Winston Churchill chef des Français libres le 27 juin 1940. Mais son but est devenu beaucoup plus ambitieux que de mettre en place une légion de volontaires qui continuerait la lutte aux côtés de l'Empire britannique. Il s'agit pour de Gaulle d'ignorer le traité d'armistice qui a été signé et de poursuivre le projet établi de Paul Reynaud, de garder la France dans la guerre contre Hitler, en créant une armée et un contre-État doté de tous les attributs de souveraineté et légitimité, et qui se donne une base territoriale en ralliant les territoires français de l'Empire colonial, future plate-forme de la reconquête[126].
Dès le début de l'été 1940, à partir de presque rien et assisté de quelques volontaires, de Gaulle jette ainsi les bases d'une marine (FNFL), d'une aviation (FAFL), de forces terrestres (FFL), d'un service de renseignements (le BCRA du colonel Passy, vite actif en métropole). La croix de Lorraine proposée par l'amiral Muselier[n 20],[128],[129], devient son emblème. Les statuts juridiques de la France libre et ses rapports avec le gouvernement anglais sont fixés par le juriste René Cassin. La France libre a bientôt sa banque, son journal officiel, ses décorations — le Général fonde l'ordre de la Libération à Brazzaville dès octobre 1940, pour honorer ses « compagnons ». Des comités français libres actifs dans le monde entier se constituent et tentent de rallier à de Gaulle les Français de l'étranger, les opinions et les gouvernements[130]. Il y organise également le 27 octobre le Conseil de défense de l'Empire, à la suite de son « manifeste à Brazzaville »[131],[132].
En France, de Gaulle a été condamné deux fois par contumace[n 21]. En Grande-Bretagne, il trouve en revanche le soutien de Winston Churchill, mais aussi celui du Parlement, de la presse et de l'opinion publique, reconnaissantes au gallant French d'être resté aux côtés de leur pays au pire moment de la menace allemande. Cet appui, comme celui de l'opinion américaine, se révèle plus tard un atout très précieux lors des tensions avec Londres et Washington[136].
Convaincu de l’importance stratégique de l’empire colonial, de Gaulle annonce dès le 30 juin 1940 son intention d’instituer un Conseil de défense de l'Empire et adresse un appel à tous les fonctionnaires civils et militaires des colonies les exhortant à se rallier à son mouvement de résistance. D'abord, seuls les territoires insulaires du Pacifique, isolés dans un environnement géopolitique australo-britannique — les Nouvelles-Hébrides, puis la Polynésie et la Nouvelle-Calédonie — et l'Inde française, se rallient. Le 26 août 1940, le ralliement du Tchad, également frontalier de territoires britanniques (Soudan anglo-égyptien et Nigéria), est accompli par le gouverneur Félix Eboué, et à la suite de quelques coups de force militaires, de Gaulle se rend maître du reste de l'Afrique-Équatoriale française. C’est dans la foulée de ces ralliements rapides qu'il tente de faire basculer l'Afrique-Occidentale française du côté de la France libre. L'opération de Dakar, ratée, tourna à la confrontation entre les flottes vichyssoises et britanniques les 23-25 septembre 1940. À la suite de cet échec, c'est presque tout l'Empire — Afrique-Occidentale française, Afrique du Nord, Levant, Madagascar, Djibouti, Indochine et Antilles—qui se ferme à de Gaulle, parfois farouchement et pendant longtemps. Malgré tout, le petit domaine colonial dont il dispose lui offre une base territoriale et humaine qui lui permet d'asseoir son mouvement[137],[138].
De Gaulle se place à la tête du Comité national français à partir du 24 septembre 1941. Mais il fait surtout en sorte que la France reste présente dans le camp allié, par ses Forces françaises libres (FFL) qui combattent l'armée de Vichy sur les différents fronts. En outre, à partir de 1941-1942, il stimule et obtient le ralliement de la résistance intérieure, grâce au colonel Passy, à Pierre Brossolette et à Jean Moulin. Le 13 juillet 1942, le Comité national français propose au gouvernement britannique, qui l'accepte, de changer l'appellation officielle du mouvement France libre en France combattante, afin d'intégrer la Résistance intérieure[139].
De nombreux facteurs s'opposaient à ce rapprochement de la résistance intérieure et des forces françaises libres. Dans La France de Vichy, Robert O. Paxton remarque qu'en 1940, bien des résistants de gauche refusent de voir un chef convenable dans ce militaire qu'ils croient à tort proche de l'Action française, et qui en 1940, est entouré par des Français libres favorables à un changement de régime. Selon Jean Pierre-Bloch, Christian Pineau, Henri d'Orléans (comte de Paris) et même le gaulliste Pierre Lefranc, le ralliement à la République n'aurait d'ailleurs été que tactique. À l'inverse, beaucoup de résistants de droite lui reprochent sa dissidence explicite avec Vichy — à moins qu'ils ne préfèrent, comme Marie-Madeleine Fourcade, n'avoir de relations qu'avec les services secrets britanniques. Le rôle de la radio, qui permet à De Gaulle d'être la voix de la France et son acceptation politique d'un retour à la république permettent à Jean Moulin de le faire reconnaître comme chef par l’essentiel des réseaux, y compris communistes.
Dès 1940, de Gaulle n'a de cesse que soient protégés les intérêts de la France, dans la guerre et après le conflit. Le 7 août 1940, il obtient ainsi de Churchill la signature de l'accord de Chequers, par lequel le Royaume-Uni s'engage à sauvegarder l'intégrité de toutes les possessions françaises et à la « restauration intégrale de l'indépendance et de la grandeur de la France ». Le gouvernement britannique s'engage de plus à financer toutes les dépenses de la France libre, mais de Gaulle insiste pour que ces sommes soient des avances remboursables et pas des dons qui jetteraient une ombre, aussi ténue soit-elle, sur l'indépendance de son organisation.
Malgré les relations de confiance scellées par traités entre Churchill et de Gaulle, les deux hommes ont des relations parfois tendues, gênées par l'anglophobie que manifestait le Général dans les années 1920 et 1930. Churchill lance à de Gaulle[Quand ?] : , de Gaulle réplique immédiatement : « J'agis au nom de la France. Je combats aux côtés de l'Angleterre mais non pour le compte de l'Angleterre. Je parle au nom de la France et je suis responsable devant elle. »
Churchill abdique alors en poussant un  De Gaulle recadre alors le débat en précisant : « Je prends cela comme une plaisanterie, mais elle n'est pas du meilleur goût. S'il y a un homme dont les Anglais n'ont pas à se plaindre, c'est bien moi. »
Ils sont au bord de la rupture en 1941, au sujet de la Syrie, puis en 1942 au sujet de sa convocation à Alger après le débarquement allié en Afrique du Nord (opération Torch).
Les relations avec Franklin Delano Roosevelt sont plus problématiques. Le président américain, personnellement francophile, a été déçu par l'effondrement de la France en 1940 et refroidi à l'égard de De Gaulle par l'échec de son entreprise devant Dakar (fin septembre 1940). Les antigaullistes français sont nombreux à Washington, par exemple l'ancien secrétaire général du Quai d'Orsay Alexis Léger (Saint-John Perse) qui lui décrit ce général comme un « apprenti dictateur ». Le président est aussi très mal informé sur la situation en France par l'ambassadeur américain à Vichy (jusqu'au mois de mai 1942), l'amiral Leahy. Il n'a donc aucune confiance en de Gaulle. Un mot de De Gaulle à Churchill explique en partie l'attitude française face à l'Amérique : « Je suis trop pauvre pour me courber. » De surcroît, au contraire du Général qui mise beaucoup sur l'Empire français, le président américain est profondément hostile au système colonial. Roosevelt projetait de faire de la France un État faible, et le projet d'Allied Military Government of Occupied Territories (AMGOT) allait d'ailleurs très loin dans cette direction, en traitant la France comme un vaincu, plutôt que comme une des puissances victorieuses. La haine de Roosevelt était tellement flamboyante (il considérait de Gaulle au pire comme un futur tyran, au mieux comme un opportuniste) que même ses adjoints finirent par en prendre ombrage, y compris le secrétaire d'État Cordell Hull qui, finalement, se rangea aux côtés de la France libre et de son chef.
Jusqu'en 1943, les gouvernements en exil en Angleterre s'étaient contentés de relations de bon voisinage avec les gaullistes. C'est que tous ces gouvernements, qui étaient légaux, s'estimaient installés dans une meilleure position que les gaullistes qui étaient, de fait, des dissidents par rapport au gouvernement Pétain que les Français avaient installé dans des conditions reconnues légales, au début, par les grandes puissances. Cette situation évolua lentement. Mais, en 1943, le gouvernement belge en exil de Hubert Pierlot et Paul-Henri Spaak précipita le mouvement et fut le premier à reconnaître officiellement les « Français libres » et de Gaulle comme seuls représentants légitimes de la France. Le gouvernement anglais, en l'occurrence Anthony Eden, un proche de Churchill, avait tenté de dissuader les Belges, craignant que leur initiative serve de modèle aux autres gouvernements en exil. Les Américains eux-mêmes intervinrent, croyant pouvoir utiliser les relations commerciales belgo-américaines pour faire pression sur les Belges (notamment quant à leurs commandes d'uranium du Congo belge). Rien n'y fit. Malgré les pressions britanniques et américaines, Spaak fit savoir officiellement que la Belgique considérait dès lors le gouvernement Pétain comme dépourvu de légitimité et le Comité des Français libres, plus tard Gouvernement provisoire de la France, comme seuls habilités à représenter légalement la France[140].
Malgré son exclusion par Roosevelt du débarquement américano-britannique en Afrique du Nord (opération Torch), et surtout malgré le soutien apporté par les États-Unis à l'amiral François Darlan, puis au général Henri Giraud, de Gaulle réussit à prendre pied à Alger en mai 1943. Le Comité national français fusionne avec le Commandement en chef français civil et militaire dirigé par Giraud, pour donner naissance au Comité français de libération nationale (CFLN), dont Giraud et de Gaulle sont coprésidents. Mais en quelques mois, de Gaulle marginalise Giraud au sein du CFLN, avant de l'évincer en novembre à la faveur de la formation d'un nouveau gouvernement, et de s'affirmer comme le seul chef politique des forces françaises alliées[141]. Les Forces françaises libres fusionnent quant à elle avec l'Armée d'Afrique placée sous le commandement de Giraud : l'Armée française de la Libération, composée de 1 300 000 soldats, participe aux combats aux côtés des Alliés. Le 3 juin 1944 à Alger, le CFLN devient le Gouvernement provisoire de la République française (GPRF).
Après le débarquement de Normandie, le 6 juin 1944, le général de Gaulle pose le pied en territoire français sur la plage de Courseulles-sur-Mer, en Normandie, le 14 juin, en descendant du torpilleur La Combattante. Il se rend à Creully pour y rencontrer le général Montgomery, qui avait installé son quartier général sur la pelouse du château de Creullet[142]. Ce même jour, il prononce le premier discours de Bayeux et les Français découvrent alors son imposante silhouette (il mesure 1,93 m).
La fermeté et la rapidité avec lesquelles le général de Gaulle rétablit l'autorité d'un gouvernement national permettent d'éviter la mise en place de l'AMGOT, prévu par les Américains, qui aurait fait de la France libérée un État administré et occupé par les vainqueurs.
L'itinéraire du 20 au 25 août 1944 du général de Gaulle n'est pas tout à fait clair ; il comporte des imprécisions et même des incohérences selon les sources. Le 20 août, il est à Cherbourg. Il rencontre le général Eisenhower à Tournières. Il passe par Coutances, Avranches, Fougères pour se rendre à Rennes. Le 21 août, il se recueille à Paimpont sur la tombe de sa mère. Le 22 août, il est à Laval[143], où il prononce un discours type dans la suite du discours de Bayeux. Il passe ensuite à Meslay-du-Maine, Sablé, Le Mans, puis le 23 août à La Ferté-Bernard, Nogent-le-Rotrou, Chartres, et arrive enfin à Rambouillet à 18 h.
La 2e division blindée du général Leclerc libère Paris le 25 août et celui-ci reçoit la reddition de Von Choltitz. Ce même jour, le général de Gaulle se réinstalle au ministère de la Guerre, rue Saint-Dominique à Paris, dans le bureau qu'il occupait jusqu'au 10 juin 1940, signifiant ainsi que « Vichy » était une parenthèse et que la République n'avait jamais cessé d'exister. Il se rend ensuite à l'hôtel de ville, où il prononce un discours dans lequel il insiste sur le rôle essentiel joué par les Français pour leur propre libération. Le lendemain, 26 août, il descend triomphalement les Champs-Élysées et fleurit la tombe du Soldat inconnu. Le « peuple dans ses profondeurs » manifeste un enthousiasme indescriptible[144].
Le GPRF est transféré à Paris. Le 9 septembre 1944, un gouvernement d'unité nationale est constitué, sous la présidence du général de Gaulle. L'Assemblée constituante est ensuite élue en octobre 1945, six mois après la fin de la guerre.
De Gaulle prononçant son discours après la libération de Paris.
Avec son entourage, défilant sur les Champs-Élysées après la libération de Paris en août 1944.
De Gaulle prononçant un discours à Cherbourg, en août 1944.
Bien après d'autres pays européens, les femmes françaises obtiennent le droit de vote, exercé pour la première fois aux élections municipales de 1945. Pour la professeure d’histoire à l’université d’Angers Christine Bard : 
D'autres réformes figurant dans ce même programme sont entreprises à la Libération : nationalisations (dont la Banque de France[146]), mise en place du monopole de l'assurance maladie obligatoire qu'est la sécurité sociale (l'Alsace et la Moselle conserveront le système d'assurance maladie instauré par Bismarck). Il s’agit notamment de revendications de la SFIO, du Parti communiste français et du Mouvement républicain populaire (MRP), qui étaient les forces politiques les plus représentées dans le Conseil national de la Résistance.
Président du Gouvernement provisoire, mais en désaccord avec l'Assemblée constituante sur la conception de l'État et le rôle des partis, le général de Gaulle remet sa démission sur la question des crédits militaires au président de l'Assemblée nationale, Félix Gouin, le 20 janvier 1946. Il a rempli la mission qu'il s'était donnée le 18 juin 1940 : libérer le territoire, restaurer la République, organiser des élections libres et démocratiques, entreprendre la modernisation économique et sociale. Durant cette période, il exerça de fait une fonction équivalente à celle de chef de l'État. Le 8 avril 1946, il reçoit une lettre de Edmond Michelet, lui proposant de , et lui indiquant que Félix Gouin souhaite l'élever à la dignité de maréchal de France[147]. Charles de Gaulle refuse, disant qu'il est impossible de [148].
Le 16 juin 1946, de Gaulle expose sa vision de l'organisation politique d'un État démocratique fort à Bayeux, en Normandie, dans un discours resté célèbre ; mais il n’est pas suivi. Il inaugure alors sa fameuse « traversée du désert » jusqu'en 1958, date de son retour au pouvoir.
En 1947, il fonde un mouvement politique, le Rassemblement du peuple français (RPF), afin de transformer la scène politique française, de lutter contre le régime « exclusif » des partis, de s'opposer à l'avancée du communisme et de promouvoir une nouvelle réforme constitutionnelle privilégiant le pouvoir exécutif. Il propose également une troisième voie économique (l'association capital-travail). Le RPF reprend également les thèmes de la droite la plus traditionnelle : ultra-conservatisme colonial (il critique jusqu'à la construction de lycées d'enseignement général à Madagascar), anticommunisme virulent (exploitant les inquiétudes sur l'avancée du communisme dans l'Union française et en Indochine) et même, au moins jusqu'en 1950, la clémence à l'égard de Philippe Pétain. Toutefois, les déclarations du colonel Rémy réhabilitant le rôle de Pétain seront immédiatement désavouées par le général de Gaulle, mais pas l'initiative de Terrenoire, demandant son amnistie. Il est vrai, comme le rappelle l'historien René Rémond (dans Les Droites en France), que c'est au nom de la réconciliation nationale qu'en 1949 et 1950, le même général de Gaulle plaidait pour l'élargissement du « vieillard de quatre-vingt-quinze ans ».
Le parti rallie des résistants (dont Jacques Chaban-Delmas) mais aussi des notables comme Édouard Frédéric-Dupont ou Edmond Barrachin (qui fut, dans les années 1930, directeur du comité central du Parti social français). D'anciens pétainistes et même d'anciens collaborateurs parviennent à s'y faire admettre, notamment dans les sections d'Indochine et d'Algérie, dans le service d'ordre, dans les rangs des syndicats ouvriers proches du R.P.F. et parmi les maires élus en 1947. Certains polémistes du parti, notamment Jean Nocher, déploient une extrême agressivité verbale. Pour ces raisons, l'historien Henry Rousso (dans Le Syndrome de Vichy) discerne au RPF « des tendances pro-pétainistes, soit qu’elles aient été envoûtées par la magie du verbe maréchaliste, soit qu’elles aient été convaincues de son impact dans l’opinion ». René Rémond (Les Droites en France) préfère rapprocher le RPF de la lignée du bonapartisme et du boulangisme, tout en observant que le RPF est, dans l'histoire du gaullisme, l'épisode le moins éloigné de « ce qu'en France on a l'habitude de qualifier de fascisme ».
Après un grand succès en 1947-1948 (35 % des suffrages aux municipales de 1947, 42 % des sénateurs élus en 1948), le RPF décline de 1949 à 1951. La gestion efficace des événements sociaux de l'automne 1947 par le gouvernement de la troisième force a affaibli le mouvement gaulliste. Le recours à de Gaulle semble alors moins nécessaire pour les conservateurs, les modérés et le patronat. Dans l'opposition, le RPF est frappé d'un véritable ostracisme de la part des autres partis politiques, entretenu par le refus du général de Gaulle de se compromettre avec les autres partis. En 1951, le RPF obtient encore plus de 4 millions de voix (22,3 % des suffrages et 16,8 % des inscrits) et 117 députés.
Le RPF est irrémédiablement affaibli par la défection de vingt-sept députés : ainsi, contre les consignes du Général, Édouard Frédéric-Dupont et Edmond Barrachin votent la confiance au gouvernement d'Antoine Pinay en 1952. En juillet, quarante-cinq autres font défection. Les gaullistes se divisent alors entre les loyalistes, qui fondent l'Union des républicains d'action sociale (URAS), et les autres, qui rejoignent l'Action républicaine et sociale (ARS).
Aux élections locales de 1953, le RPF perd la moitié de ses suffrages. Il entre alors en hibernation. Les élus gaullistes participeront encore avec le PCF à l'échec de la Communauté européenne de défense (CED) en 1954, avant la mise en sommeil définitive du RPF le 13 septembre 1955.
À la suite de la défaite électorale de son parti, le général de Gaulle se retire à Colombey-les-Deux-Églises et rédige ses Mémoires de guerre. Pour certains observateurs, ce sont les cinq années qui suivent qui constituent sa « traversée du désert » proprement dite (voir ci-dessus).
L'instabilité ministérielle, l'impuissance de la IVe République face à la question algérienne, déclenchée par une insurrection le 1er novembre 1954, conduisent le régime à une crise grave. Des responsables politiques de tous bords en viennent à souhaiter le retour du Général. Cette même année 1954, les Mémoires de guerre sont publiées.
Même si de Gaulle s’est officiellement retiré de la vie politique, il continue à recevoir et écouter bon nombre de sympathisants gaullistes qui ne souhaitent que son retour au pouvoir. Ce qu’on appelle alors les événements d’Algérie semblent présager ce retour.
Il convient de rappeler que l'action de De Gaulle à Londres a été la création d’un réseau de renseignement qui donna naissance aux services secrets français et du réseau français de résistance commandé par Jean Moulin. Ce réseau de renseignement est, en quelque sorte, un réseau social ou un média communicationnel très discret mais actif. À l’instar des événements de la Seconde Guerre mondiale, ces anciens compagnons de la résistance seront ceux qui le mèneront au pouvoir ; tous continuent de vouer une admiration à l’artisan de la Libération. Le mouvement gaulliste étant bien structuré, notamment grâce au concours du Rassemblement du peuple français (RPF), plusieurs acteurs du mouvement sont placés à des postes stratégiques. Jacques Chaban-Delmas (résistant), ministre de la Défense nationale en 1957, envoie Léon Delbecque (résistant) à Alger où, vice-président du Comité de salut public (CSP), il conseille le général Salan, qui appellera publiquement de Gaulle au pouvoir[149]. Le CSP d’Alger prendra notamment le contrôle de radio Algérie, transformant ce qui était alors une radio provinciale en canal majeur de propagande gaulliste en Algérie comme en métropole. Il se trouve que le responsable de cette chaîne est le résistant Lucien Neuwirth, épaulé par Léon Delbecque, tous deux envoyés en Algérie par Chaban-Delmas. Le 17 mai, Jacques Soustelle, également résistant, est chargé de la propagande pour le CSP[150]. D’autres figures de la résistance contribueront à répandre le nom du Général partout en France et en Algérie mais le feront de leur plein gré. Le général à la retraite ne leur a rien demandé[151]. Seul le souvenir qu’il a laissé à ses compagnons de la résistance (ethos) a suffi à les motiver. C’est parce que tous se souvenaient de ce qu’il avait promis en démissionnant en 1946, c’est-à-dire qu’il reviendrait s’il était appelé au pouvoir.
Le 13 mai 1958, un comité de vigilance appelle à manifester contre le FLN à Alger. Un comité de salut public est créé, à la tête duquel se trouve le général Massu, et dont fait aussi partie le général Salan. Ce dernier, poussé par Léon Delbecque, lance le 15 mai devant la foule son appel au retour du général de Gaulle, « Vive de Gaulle ! », du haut du balcon du Gouvernement général. L'insurrection prend de l'ampleur et risque de dégénérer en guerre civile. Le 19, le Général se dit « prêt à assumer les pouvoirs de la République[152] ». Certains voient dans cette déclaration un soutien à l'armée et s'inquiètent. Il rassure et insiste sur la nécessité de l'union nationale ; s'il se présente encore comme le recours, il ne donne officiellement aucune caution ni à l'armée ni à quiconque. Néanmoins, un plan d'action militaire, baptisé « Résurrection », a déjà été mis en place en cas d'échec des négociations politiques.
De Gaulle entre officiellement en scène avec pour intention d’appliquer la réforme qu’il avait voulue lors de sa première présidence et étayée à Bayeux en 1946[151]. Il explique alors au président René Coty que son retour sera conditionnel à l’obtention des pleins pouvoirs. La conférence de presse donnée le 19 mai 1958 servira, entre autres, à rassurer le public quant à cette période spéciale qu’il exige. Sa réponse marquera les esprits : « Est-ce que j'ai jamais attenté aux libertés publiques fondamentales ? Je les ai rétablies. Et y ai-je une seconde attenté jamais ? Pourquoi voulez-vous qu'à 67 ans, je commence une carrière de dictateur ? » Cette conférence de presse donne le ton à toutes celles qui suivront durant sa présidence et deviendra le symbole de la communication présidentielle pour la Ve République[153].
Le 29 mai, le président de la République, René Coty, fait appel au « plus illustre des Français ». Charles de Gaulle accepte de former un gouvernement. Sous pression, l'Assemblée nationale l'investit le 1er juin, par 329 voix sur 553 votants. Le général de Gaulle devient ainsi le dernier président du Conseil de la IVe République. Les députés lui accordent la possibilité de gouverner par ordonnances pour une durée de six mois, et l'autorisent à mener à bien la réforme constitutionnelle du pays[154].
À la suite du soutien accordé par l’Assemblée nationale, il quitte Paris et se rend à Alger le 4 juin. Il y prononce un discours, le premier de sa tournée algérienne, où il scande « Je vous ai compris ! » Cette communication aux Français d’Algérie, sans rien offrir de concret, charme autant les partisans de l'Algérie française que ceux de l’« Algérie algérienne », qui voient en lui la possibilité d’une paix négociée. Deux jours plus tard, à Mostaganem, il affirme clairement soutenir l’Algérie française, renforçant ainsi le soutien qui lui serait accordé plus tard au moment de voter la nouvelle Constitution[155].
La nouvelle Constitution, élaborée au cours de l'été 1958, est très proche des propositions avancées à Bayeux, avec un exécutif fort. Le général de Gaulle accepte cependant que le Parlement ait plus de poids qu'il ne le souhaitait. En particulier, de Gaulle doit renoncer à l'élection du président de la République au suffrage universel (un élément central de son dispositif constitutionnel qu'il finira par imposer en 1962).
Comme le rapporte Jean-Marie Domenach dans la revue politique Esprits[156], le référendum de 1958 ne porte pas seulement sur la Constitution. La stratégie communicationnelle du Général présente ce référendum constitutionnel comme une nouvelle forme de cohésion nationale en proposant un plan d’adhésion des colonies d’outre-mer et profite de l’occasion pour plébisciter sa légitimité. En métropole, les électeurs sont appelés à voter pour une Constitution ; dans les colonies, le Général propose l'indépendance à celles qui le veulent ou l’adhésion pour celles qui veulent de la France mais, à travers tout l’empire, il est appelé à voter pour ou contre un des grands hommes de la patrie[156]. De Gaulle utilise sa personne comme un lien entre ces deux électorats divisés et garantit sa victoire au référendum car, comme le dit la devise du Panthéon, « aux grands hommes, la patrie reconnaissante ».
La Constitution est adoptée par référendum le 28 septembre 1958, avec 79,2 % de « oui ». L'Empire l'approuve également, sauf la Guinée qui devient ainsi la première colonie française de l'Afrique subsaharienne à obtenir son indépendance. Charles de Gaulle est élu président de la République le 21 décembre[157] : il prend ses fonctions le 8 janvier suivant.
Entre le moment de son entrée en fonctions comme président du Conseil et son élection à la présidence de la République, Charles de Gaulle a largement amorcé la politique qui marquera son passage au pouvoir : outre la volonté de doter la France d'une nouvelle Constitution, le Général se soucie de la politique européenne de la France (rencontre avec le chancelier Adenauer le 14 septembre[n 22]), de l'indépendance du pays face aux États-Unis (mémorandum du 17 septembre adressé au président Eisenhower), de l'assainissement des finances publiques (mesures du 27 décembre) et du sort de l'Algérie (il refuse les choix des comités de salut public et appelle à la « paix des Braves » en octobre).
À la suite des échecs de la IVe République en Indochine et en Algérie, une insurrection éclate à Alger et les putschistes civils et militaires organisent un Comité de salut public (en référence à celui de la Révolution française) le 13 mai 1958 pour maintenir l'Algérie française. Ils en appellent au retour du général de Gaulle. L'antenne d'Alger mise en place par le ministre de la Défense Jacques Chaban-Delmas dès 1957, dirigée par Lucien Neuwirth et Léon Delbecque, a influencé les partisans de l'Algérie dans la République française. Comme l'a rapporté Olivier Guichard dans Avec de Gaulle (voir bibliographie), l'antenne d'Alger faisait surtout de la transmission : le travail d'influence était supervisé par les deux plus proches collaborateurs du général de Gaulle, Guichard lui-même et, pour les militaires, Jacques Foccart.
C'est sous l'autorité de De Gaulle que les réseaux de ce que l'on appellera plus tard la Françafrique furent mis en place[158]. À la tête d'une partie de son cabinet, issue de l'éphémère Communauté française, Jacques Foccart maintient des liens étroits, non seulement de coopération, mais souvent de contrôle, avec les nouveaux pouvoirs des États africains ayant accédé à l'indépendance, notamment au moyen d'accords de coopération militaire et financiers, mais aussi par l'action des services secrets. L'expression de « pré carré » est alors courante, et ces liens politiques et économiques assurent un soutien diplomatique dans la stratégie d'entre deux blocs de De Gaulle.
En novembre 1958, les gaullistes remportent les élections législatives et obtiennent une confortable majorité. Le 21 décembre suivant, de Gaulle est élu président de la République et de la Communauté africaine et malgache avec 78,51 % des voix, au suffrage indirect, par un collège de plus de 80 000 grands électeurs[159].
Charles de Gaulle prend ses fonctions de président de la République le 8 janvier 1959, succédant ainsi à René Coty. Il gère le conflit algérien, met en place une nouvelle politique économique et engage d’importantes mesures pour revitaliser le pays, avec en particulier une dévaluation de 29 % et l'introduction du nouveau franc (valant 100 anciens francs), qui fait revenir les centimes, disparus en 1945.
Sur la scène internationale, refusant la domination des États-Unis comme de l'URSS, il défend une France indépendante, disposant de la force de frappe nucléaire. Il met en place également les débuts du programme spatial français. En tant que membre fondateur de la Communauté économique européenne (CEE), il pose son veto à l'entrée du Royaume-Uni.
Le 6 février 1968, dans le stade olympique de Grenoble, il devient le second président français à ouvrir une cérémonie olympique, à l'occasion des Xe jeux olympiques d'hiver[160].
En 2020, Charles de Gaulle est le seul président de la Ve République à avoir visité tous les départements français, outre-mer comprise. Il se rend en train ou en avion dans les régions, avant de visiter leurs départements en voiture durant une semaine en moyenne[161],[162].
En ce qui concerne la guerre d'Algérie, de Gaulle suscita d’abord de grands espoirs parmi les Français d’Algérie, auxquels il déclara à Alger le 4 juin 1958 : « Je vous ai compris ». Ce jour-là, il se garda de rien leur promettre de précis, lors de ce discours, et ne reprit ni leur mot d'ordre d'« intégration » ni leur slogan « Algérie française ». Il proclame que « à partir d'aujourd'hui, la France considère que, dans toute l'Algérie, il n'y a qu'une seule catégorie d'habitants : il n'y a que des Français à part entière ». Ce n'est qu'à Mostaganem, le 6 juin, qu'il prononça les mots : « Vive l'Algérie française[163] », exception davantage révélatrice d'un désaccord que d'une adhésion, pour René Rémond[164].
Mais il adopta aussi quelques mesures libérales en direction des indépendantistes algériens : « paix des Braves » proposée au FLN en octobre 1958, grâces accordées à plusieurs rebelles, dont Yacef Saâdi, condamné à mort comme ancien dirigeant du FLN pendant la bataille d'Alger, interdiction officielle formelle des actes de torture. C'est également sous de Gaulle que les femmes musulmanes d'Algérie obtinrent le droit de vote, que l'on vit les musulmans pouvoir voter à égalité avec les Européens (de ce fait, dès avant l'indépendance en 1962, une majorité des maires d'Algérie sont eux-mêmes des musulmans), ou que fut nommé le premier préfet musulman d'Algérie (Mahdi Belhaddad à Constantine). De Gaulle annonça en personne la mise en œuvre du plan de Constantine, dans cette ville, en septembre 1958 : ce plan prévoyait, sur cinq ans, la redistribution de 250 000 ha de terres, la construction de 200 000 logements et la création de 400 000 emplois[165].
Il laissa son Premier ministre, Michel Debré, vilipender comme « manœuvre communiste » le rapport accablant établi par le jeune Michel Rocard, et qui dénonçait l'entassement inhumain de deux millions de personnes civiles dans des « camps de regroupement ». Dès 1959, de Gaulle en revint aussi à une solution classique de répression militaire. À l'été 1959, l'opération « Jumelles », dite plan Challe, porta au FLN ses coups les plus rudes à travers tout le pays. Certes, de Gaulle réalisa rapidement qu'il n'était pas possible de résoudre le conflit par une simple victoire militaire, et à l'automne 1959 il commença à s'orienter vers une solution conduisant inéluctablement à l'indépendance de l'Algérie. Mais jusqu'à l'hiver 1961/62, il choisit tout de même de poursuivre la guerre, au prix de nombreuses victimes et, selon le journaliste Rémi Kauffer, d'un accroissement de l'usage de la torture. Jusqu'à la fin de 1961, la lutte contre le FLN est menée avec autant de vigueur, et même davantage, qu'avant. Selon Constantin Melnik, conseiller spécial de Michel Debré chargé de coordonner les services secrets, il y eut environ 500 assassinats politiques entre 1958 et 1961.
Il reste difficile de savoir quand de Gaulle comprit que l'indépendance était la seule solution pour sortir d'un conflit coûteux en hommes, en argent et en prestige international. D'autant plus qu'il perd le soutien de proches et d'anciens combattants luttant pour l'Algérie française. Édouard Lebas, à cet effet, écrit le 17 mars 1963 dans Combat : « Nous vivons depuis mai 1958 sur la plus grande duperie de l'histoire et depuis octobre 1962 sur la plus grande imposture. La cause du mal c'est la volonté tenace, bien que supérieurement camouflée, du Général de Gaulle. Il faut donc dénoncer à la masse, sans subterfuges et sans faux-fuyants, le responsable du mal dont meurent la République et la Liberté »[166]. En 1961, de Gaulle fit encore rédiger par Alain Peyrefitte un plan de partition de l'Algérie, sans doute en fait pour faire pression sur le FLN. Au même Alain Peyrefitte, il expliquait dès 1959 que « l'intégration » de l'Algérie à la France, défendue par les partisans de l'Algérie française, était une utopie : deux pays culturellement si éloignés et présentant un tel écart de niveau de vie n'avaient pas vocation à en former un seul. Sans compter qu'au vu de l'accroissement démographique des musulmans, ce serait ouvrir la porte à leur immigration massive en métropole, dépassant de fort loin la simple venue traditionnelle de populations étrangères appelées à se fondre dans le creuset français : « Mon village deviendrait Colombey-les-Deux-Mosquées ! »[167]
Dès le 16 septembre 1959, de Gaulle parle de « l'autodétermination » de l'Algérie. Comme pour les pays de l'empire colonial français qui viennent d’accéder à l’indépendance, le chef de l’État aurait pour stratégie d'installer une administration qui défendrait les intérêts politiques et économiques de la France[168].
En janvier 1960, le limogeage du général Jacques Massu, qui avait critiqué sa politique, provoque la rupture avec les Français d'Algérie et l'érection de barricades au centre d'Alger. Malgré ce climat insurrectionnel, de Gaulle abroge définitivement, par une ordonnance du 4 juin 1960, la peine de déportation[169]. En janvier 1961, un référendum valide cependant massivement sa politique des deux côtés de la Méditerranée.
Avec l'armée de conscription, il fait échec au putsch des généraux à Alger en avril 1961. Quatre jours suffisent à mettre en déroute le « quarteron de généraux à la retraite » stigmatisés dans un de ses plus célèbres discours. Cette attitude provoqua de fortes résistances dans certains groupes nationalistes et de Gaulle fut obligé de réprimer des soulèvements de pieds-noirs en Algérie.
Il est la cible d'organisations terroristes telles que l'Organisation armée secrète (OAS), qui le surnomme « la Grande Zohra ». La métropole devient alors l'objet de plusieurs vagues d'attentats commis par l'OAS. L'amiral Pierre Lacoste, ancien directeur de la DGSE, déclare en 1992, dans un entretien accordé au journal The Nation, que certains éléments du réseau Gladio étaient impliqués dans des activités terroristes contre le général de Gaulle et sa politique en Algérie[170],[171].
Dans la nuit du 17 au 18 octobre 1961, une manifestation, interdite par les autorités françaises, est organisée par le FLN. Les manifestants protestent contre le couvre-feu imposé en métropole aux ressortissants d'Afrique du Nord. Cette manifestation est férocement réprimée. Le préfet de police Maurice Papon couvre ses policiers et le gouvernement l'ensemble de ses fonctionnaires. Selon le rapport de l'avocat général Jean Geromini, remis le 5 mai 1999, il y aurait eu au moins 48 noyés pendant la nuit du 17 au 18 octobre, sans compter les personnes mortes des suites de leurs blessures ou de leurs conditions d'internement. Selon l'historien et éditorialiste Alain-Gérard Slama et Linda Amiri (laquelle a dépouillé les archives de la préfecture de police), le chiffre total est de l'ordre d'une centaine de victimes (L. Amiri compte 100 morts certains et 31 disparus). Les propos tenus par de Gaulle en Conseil des ministres quelques jours après le drame sont connus grâce aux notes prises par son ministre Louis Terrenoire, et publiées par Éric Rossel.
Quelques mois plus tard, lors d'une manifestation interdite le 8 février 1962, huit manifestants sont tués par les forces de police au métro Charonne et un autre meurt ensuite à l'hôpital. Selon l'historien Jean-Paul Brunet, Charles de Gaulle est « tout autant responsable de cette tragédie que le ministre de l'Intérieur Roger Frey, le préfet de police Maurice Papon, et toute la hiérarchie policière ». Une des raisons est, explique J.-P. Brunet, « l'autoritarisme » du Général. Selon l'historien Alain Dewerpe, directeur d'études à l'École des hautes études en sciences sociales, le massacre de Charonne n'est qu'une conséquence logique des « habitus de pouvoir » de De Gaulle et des gaullistes, dans la situation de la guerre d'Algérie.
Quant à l'organisation terroriste OAS, elle est réprimée par des moyens impitoyables : exécutions sommaires, tortures, polices parallèles, lesquelles n'hésitent pas à recruter des truands, comme Georges Boucheseiche et Jean Augé. La Cour de sûreté de l'État est créée en janvier 1963 pour en condamner les chefs, lesquels sont amnistiés quelques années plus tard (la Cour continue ensuite de juger des terroristes, jusqu'à sa suppression, en août 1981). En 1962, à la suite des accords d'Évian, un cessez-le-feu est proclamé en Algérie. Le général de Gaulle fait adopter par référendum l'indépendance de l'Algérie, effective en juillet 1962[172].
Très irrité par le ralliement massif des pieds-noirs à l'OAS, à l'heure où celle-ci lance une vague de terreur et de terre brûlée en Algérie, de Gaulle n'a aucun mot de compassion ni en public ni en privé pour le sort du million de Français rapatriés d'Algérie en juillet 1962 à la suite de la non-application des accords d'Évian par la partie algérienne.
Le lendemain de la signature des accords d'Évian, les supplétifs de l'armée française, les harkis, sont désarmés par la France, et abandonnés sur place. Le gouvernement s'oppose au rapatriement de la majorité d'entre eux, et fait interdiction aux officiers de l'armée de les aider à gagner la France, hors du cadre d'un plan de rapatriement général. Le 25 juillet 1962, en Conseil des ministres, alors que les massacres de pieds-noirs et harkis ont commencé, Charles de Gaulle s'oppose au repli des harkis en France[173]. Par la suite, plusieurs dizaines de milliers sont torturés et massacrés[174].
En avril 1962, le Premier ministre Michel Debré est remplacé par Georges Pompidou, et, en septembre de la même année, de Gaulle propose d'amender la Constitution afin de permettre au président d'être élu au suffrage universel direct, dans le but de renforcer sa légitimité à gouverner directement. La réforme de la Constitution, malgré l'opposition du Parlement, de la totalité de la gauche et d'une bonne partie de la droite, est aisément acceptée lors du référendum du 28 octobre 1962 avec 62,25 % de « oui », le président du sénat, à l'époque Gaston de Monnerville, ira même jusqu'à accuser de forfaiture Charles de Gaulle puis saisira le conseil constitutionnel afin de faire annuler le résultat du vote. Le conseil constitutionnel, par 7 voix contre 2 ( Coty et Auriol ) refuseront d'accéder à sa demande en se déclarant inapte ("Le Conseil constitutionnel n'a pas compétence pour se prononcer sur la demande susvisée du Président du Sénat.) .
En octobre, l'Assemblée nationale vote une motion de censure contre le gouvernement Pompidou, mais le Général refuse la démission que lui présente le Premier ministre et choisit de dissoudre l'Assemblée. Les nouvelles élections renforcent la majorité parlementaire gaulliste.
Un polytechnicien ingénieur de l'armement nommé Jean Bastien-Thiry, âgé de 35 ans, considère la politique algérienne du général de Gaulle comme une politique d'abandon et de trahison. Il conçoit donc, avec l'aide de personnes partageant son point de vue et appartenant à l'Organisation armée secrète (OAS), d'enlever de Gaulle, voire, si ce rapt se révèle impossible, de l’abattre. Un attentat est ainsi organisé au rond-point du Petit-Clamart le 22 août 1962. Il échoue, bien que la DS présidentielle montre ensuite, parmi les impacts (environ 150 balles tirées), une trace de balle passée latéralement à quelques centimètres des visages du couple présidentiel.
Dans la déclaration qu'il fait lors de l'ouverture de son procès en janvier 1963, Bastien-Thiry développe les motivations du complot basées essentiellement sur la politique algérienne du général de Gaulle. Il est condamné à mort le 4 mars 1963. Parce qu'il avait fait tirer sur une voiture occupée par une femme et parce que, contrairement aux autres membres du commando, il n'avait pas pris de risques directs, Bastien-Thiry n'est pas gracié par le général de Gaulle, comme l'ont été les autres membres du commando (tout comme d'ailleurs les autres membres de l'OAS, qui ont été pris). Une semaine après la fin de son procès, Bastien-Thiry est fusillé au fort d'Ivry.
En 1968, une première amnistie permet aux derniers responsables de l'OAS, aux centaines de partisans de l'Algérie française encore détenus, et à d'autres, exilés, comme Georges Bidault ou Jacques Soustelle, de rentrer en France. D'anciens activistes de l'Algérie française se rallient alors au gaullisme, en adhérant au SAC ou aux comités de défense de la République (CDR). De Gaulle déclare à Jacques Foccart le 17 juin 1968 :  Les autres condamnations pénales sont effacées par les lois d'amnistie de 1974 et 1987.
L'attentat du Petit-Clamart est celui qui a été le plus près de réussir. De nombreux autres attentats ont été organisés contre la personne du Général, parmi lesquels :
La télévision, pour la première fois dans l'histoire, joue un rôle très important dans une campagne ; malgré son refus de « jaspiner » dans « les étranges lucarnes », le Général se plie à cette nouvelle mode entre les deux tours. Cette campagne marque aussi l'apparition des sondages, qui mettent en évidence la baisse des intentions de vote en sa faveur avant le premier tour[178].
Lors du premier tour, de Gaulle arrive en tête avec 44,65 % des suffrages, devant notamment le candidat de la gauche unie, François Mitterrand (31,72 %), et Jean Lecanuet (15,57 %). Lorsque le ministre de l'Intérieur, Roger Frey, propose à de Gaulle de faire publier des photos de François Mitterrand aux côtés de Philippe Pétain pendant l'Occupation, il se voit opposer un refus, le président sortant refusant d'utiliser de telles méthodes[179]. Valéry Giscard d'Estaing fera de même que le général de Gaulle lors de l'élection présidentielle de 1981[180].
Charles de Gaulle est réélu président de la République le 19 décembre 1965, avec 55,20 % des suffrages exprimés. Le Général indique ultérieurement à quelques proches qu'il n'ira pas au bout de son mandat (devant s'achever en 1972) et qu'il se retirera à ses 80 ans[181],[182].
De Gaulle dut attendre la fin du conflit en Algérie pour lancer réellement sa politique étrangère. En effet, le « boulet algérien »[183] réduisait considérablement la marge de manœuvre française et, d'une façon ou d'une autre, il fallait avant toute chose mettre un terme à ce conflit. La politique de « l'indépendance nationale » est alors pleinement mise en application.
Sur le plan international, de Gaulle continua à promouvoir l'indépendance de la France : il refusa à deux reprises (en 1963 et en 1967) l'entrée du Royaume-Uni dans la CEE[184] ; il condamna dès 1964 l'aide militaire apportée par les États-Unis à la république du Viêt Nam (dite Viêt Nam du Sud) contre la rébellion communiste menée par le Viêt Cong (guérilla soutenue par le Nord-Viêt Nam), ainsi que la riposte israélienne au blocus du détroit de Tiran par l'Égypte, lors de la guerre des Six Jours en 1967. Il prit l'une de ses décisions les plus spectaculaires en 1966, lorsque la France se retira du commandement militaire intégré de l'OTAN, expulsant les bases américaines de son territoire.
En ce qui concerne l'Europe, de Gaulle était partisan d'une « Europe des nations » et des États, qui peuvent seuls répondre des nations, celles-ci devant conserver leur pleine souveraineté et leur personnalité historique et culturelle :  ; de Gaulle était franchement hostile à l'idée d'une Europe supranationale, c'est-à-dire celle prônée par Jean Monnet, une Europe avec un gouvernement fédéral composé des actuelles commissions, qui surplomberait des gouvernements provinciaux, lesquels ne s'occuperaient plus que des questions secondaires ; en 1962, le terme volapük[n 23] qu'il employa pour parler de la coopération européenne entraîna le départ du gouvernement des cinq ministres MRP.
C'est l'Europe qui fixe le cadre de son ambition, une Europe qui va même « de l'Atlantique à l'Oural », gommant d'un trait le provisoire rideau de fer. En effet, le pivot de la politique étrangère française est le rapprochement avec l'autre poids lourd du continent, l'Allemagne. Ainsi, de Gaulle tourne le dos aux « Anglo-Saxons ».
On pourrait en effet s'étonner de l'intransigeance gaullienne vis-à-vis du Royaume-Uni, tout particulièrement. Pour de Gaulle, comme pour Churchill d'ailleurs, le Royaume-Uni n'avait fait que son devoir en 1940, et il n'existait pas de « dette » française envers Londres liée à la Seconde Guerre mondiale. De Gaulle désapprouvait les relations privilégiées rapprochant le Royaume-Uni des États-Unis depuis la guerre, ainsi que la préférence économique impériale qui jouait entre celle-ci et les États du Commonwealth, rendant ainsi difficile son admission au sein de l'Europe. Aussi l'entrée d'un tel « cheval de Troie américain » au sein de l'Europe lui paraissait-elle non souhaitable. Les Britanniques attendront donc 1973 avant de rejoindre la communauté économique européenne (CEE).
La position de De Gaulle face au monde communiste était sans ambiguïté : il était totalement anticommuniste. Il prône la normalisation des relations avec ces régimes « transitoires » aux yeux de l'Histoire de façon à jouer le rôle de pivot entre les deux blocs. La reconnaissance de la république populaire de Chine dès le 27 janvier 1964 va dans ce sens. De même sa visite officielle en république populaire de Pologne (6-11 septembre 1967) fut un geste qui montrait que le président français considérait le peuple polonais dans son ancrage historique. La question allemande, et donc le tracé de la frontière occidentale de la Pologne, ont joué un grand rôle dans les discussions officielles. Malgré la domination exercée alors par l'URSS, de Gaulle fut accueilli spontanément par des foules enthousiastes. Il misait, comme il l'a dit devant la diète (Assemblée nationale) polonaise, sur un futur où la Pologne recouvrerait sa place d'État indépendant. Il s'agissait une fois de plus de son projet d'Europe continentale élargie[187]. Durant la Seconde Guerre mondiale, de Gaulle avait soutenu le mouvement royaliste tchetnik de Draza Mihailovic, dont il était un admirateur[188]. Tito, l'un des dirigeants des non-alignés, soutiendra fortement l'indépendance algérienne avec des livraisons massives d'armes au FLN via la Tunisie.
Les relations entre de Gaulle et les États-Unis sont assurément les plus épicées. Malgré quelques tensions vives, de Gaulle sera toujours au rendez-vous en cas de vrai coup dur : Berlin ou Cuba, notamment. En revanche, dès que les Américains entament le processus d'escalade, de Gaulle prend publiquement ses distances, notamment par son discours du 1er septembre 1966 à Phnom Penh vilipendant l'attitude américaine au Viêt Nam, théâtre d'opération que la France connaissait fort bien[189]. Ses communications privées sont espionnées par les États-Unis, mais aussi par le Royaume-Uni, qui le surveille à son domicile[190].
La notion gaullienne d'« une certaine idée de la France » se manifeste surtout en politique étrangère. De Gaulle puise une force dans sa connaissance de l'Histoire de France, qu'il a d'ailleurs enseignée à Saint-Cyr. Selon lui, le poids de cette Histoire donne à la France une position particulière dans le concert des nations. Convaincu que les relations internationales reposent avant tout sur les réalités nationales et les rapports entre États, il surnomme l'ONU « le machin » et refuse que la France participe au financement des opérations menées par les « casques bleus » contre la sécession katangaise au Congo ex-belge. Passablement irrité par l'attitude du Nigeria lors de l'explosion de Gerboise bleue[191], le troisième essai nucléaire français, en 1960, et souhaitant le « morcellement » de ce pays, comme il le raconte à son conseiller aux affaires africaines, Jacques Foccart[191], de Gaulle soutient la sécession du Biafra en 1967-68, qui fait un à deux millions de morts[191].
En Afrique francophone, il ne prend pas position face aux coups d'État qui se succèdent, mais apporte son soutien aux régimes en place quand il le juge nécessaire, faisant intervenir les troupes françaises au Gabon (1964) et au Tchad (1968).
Il entreprend un voyage de trois semaines en Amérique du Sud en 1964 au cours duquel il n'a de cesse de dénoncer les « hégémonies » des superpuissances. De Gaulle, qui visite dix pays, est acclamé par les foules, mais la tournée diplomatique aura peu de retombées concrètes et ne remet pas en cause l'emprise des États-Unis sur ce continent[192].
Convaincu de l'importance stratégique de l'arme nucléaire, de Gaulle poursuivit le développement en Algérie puis en Polynésie française de celle-ci, sous la protestation de l'opposition qui n'y voyait qu'une « bombinette ». La réponse de De Gaulle sera : « Dans dix ans, nous aurons de quoi tuer 80 millions de Russes. Eh bien je crois qu'on n'attaque pas volontiers des gens qui ont de quoi tuer 80 millions de Russes, même si on a soi-même de quoi tuer 800 millions de Français, à supposer qu'il y eût 800 millions de Français »[193],[194].
Le rôle des États-Unis dans cette affaire paraît étrange. Kennedy proposa à de Gaulle de lui donner des missiles Polaris, comme il l'avait fait avec le Royaume-Uni (accords de Nassau). Mais de Gaulle refusa, déclarant qu'il voulait que la France se bâtisse elle-même une armée. La question nucléaire empoisonna les relations franco-américaines durant toutes les années 1960. Il fallut attendre Richard Nixon pour trouver un premier président américain clairement « gaullien ». Nixon contourna d'abord les contraignantes législations américaines dans le domaine nucléaire avant d'ouvrir officiellement la voie de la collaboration nucléaire franco-américaine. Le gros du travail était déjà fait et les « bombinettes » françaises déjà fort efficaces.
Après avoir retiré du commandement de l'OTAN la flotte française de la Méditerranée (1959), puis celle de l'Atlantique et de la Manche, de Gaulle écrit le 7 mars 1966 au président américain Lyndon Johnson pour lui notifier la sortie de la France de l'OTAN : « la France se propose de recouvrer sur son territoire l'entier exercice de sa souveraineté, actuellement entamé par la présence permanente d'éléments militaires alliés ou par l'utilisation habituelle qui est faite de son ciel, de cesser sa participation aux commandements intégrés et de ne plus mettre de forces à la disposition de l'OTAN ». Tout en restant partenaire de l'Alliance atlantique, la France gaullienne se retire donc de « l'organisation militaire intégrée aux ordres des Américains », comme le confie de Gaulle à Peyrefitte[195]. Les troupes américaines présentes en France doivent évacuer leurs bases, et le quartier général de l'OTAN quitte Rocquencourt pour s'installer en Belgique.
Sur la recommandation de l'économiste Jacques Rueff qui voyait la conquête de l'espace et le conflit vietnamien déséquilibrer la balance des paiements des États-Unis, de Gaulle réclama à ces derniers la contrepartie en or d'une forte proportion des dollars détenus par la France. L'opération était légale, car le dollar était défini officiellement comme correspondant à 1/35 d'once d'or. Règlements internationaux obligent, les États-Unis durent obtempérer et de Gaulle fit procéder par la Marine nationale au rapatriement de la part de l'or de la Banque de France déposé à New York auprès de la Banque fédérale de réserve[196]. En 1971, les États-Unis mettront fin à la parité pour faire « flotter » le dollar. À la suite des chocs pétroliers de 1973 et de 1979, les cours de l'or s’envoleront : le conseil de Jacques Rueff était judicieux à long terme.
Conscient du danger que présente l'hégémonie du dollar pour le système monétaire international et l'économie mondiale d'une manière générale, hégémonie du dollar « qui entraîne les Américains à s'endetter, et à s'endetter gratuitement vis-à-vis de l'étranger, car ce qu'ils lui doivent, ils le paient […] avec des dollars qu'il ne tient qu'à eux d'émettre », de Gaulle est partisan d'un retour à l'étalon-or[197].
Lors d'une visite d'État au Canada en 1967 afin, officiellement, de prendre part aux festivités entourant l'Expo 67 comme l'y avait invité le Premier ministre québécois Daniel Johnson, de Gaulle provoqua l'indignation des autorités fédérales canadiennes, lorsqu'à Montréal, devant une foule de plus de 100 000 Québécois, il ponctua son discours d'un retentissant : « Vive Montréal, vive le Québec… vive le Québec libre ! », salué par une ovation générale. Cela déclencha une crise avec le gouvernement canadien. À la suite du discours de De Gaulle, qui contenait un certain nombre de clins d'œil, le Premier ministre canadien Lester B. Pearson répliqua sèchement à de Gaulle dans un discours livré le lendemain, déclarant que « les Canadiens n'ont pas besoin d'être libérés », et faisant savoir très clairement que de Gaulle n'était plus le bienvenu au Canada. Il repartit séance tenante pour la France, délaissant le croiseur qui l'avait amené, le Colbert. Le but de De Gaulle n'était pas de provoquer un « scandale » entre le Québec et le gouvernement fédéral canadien, mais plutôt de regonfler les « Français du Canada » face aux voisins Anglo-Saxons[réf. nécessaire].
Dans la perspective de la Seconde Guerre mondiale, cette déclaration fut ressentie comme injuste par les Canadiens anglophones qui avaient soutenu la France libre, alors que les Québécois francophones, soucieux de l'indépendance du Canada vis-à-vis du Royaume-Uni, étaient moins enthousiastes pour participer à l'effort de guerre. Des envoyés de la France libre, Élisabeth de Miribel et le capitaine de vaisseau Georges Thierry d'Argenlieu — dont le titre de supérieur majeur de la province des Carmes de Paris était censé lui valoir le respect des catholiques — tentèrent en 1941 de rallier les Canadiens à la cause du général de Gaulle.
Les réactions furent non seulement diplomatiques, mais aussi populaires. Par exemple, les habitants du boulevard de Gaulle, à Ottawa, obtinrent de la ville en décembre 1967 que leur rue fût rebaptisée boulevard Confédération, une décision qui ne fit toutefois pas l'unanimité[198],[199].
Le gouvernement d'Ottawa dut dès cette époque traiter avec une attention particulière les revendications du Québec qui, fort de cet encouragement qui laissait présager un soutien fort de la France si besoin, commença à parler de faire sécession.
De plus, lors de la conférence de presse du 27 novembre 1967 à l'Élysée, Charles de Gaulle justifia une fois de plus son geste d'éclat par un discours engagé, ponctué par un solennel « allons, allons, pour eux aussi, pour eux surtout, il faut que la France soit la France[200] ! »
Cette déclaration était cohérente avec la pensée du général de Gaulle, qui aurait déclaré à Alain Peyrefitte, en septembre 1965 : « L'avenir du Canada français, c'est l'indépendance. Il y aura une République française du Canada ». Selon Alain Peyrefitte, « sans préjuger de la forme que la souveraineté québécoise devait revêtir, de Gaulle, avec ce sens historique qui valut à la France son salut, s'en vint donc à Montréal, en juillet 1967, exhorter les Canadiens français à préserver leur identité française dont, sous Louis XV, l'indifférence des élites françaises avait fait si légèrement bon marché.  ne fut pas plus improvisé que l'appel du 18 juin 1940. L'appel à la liberté, lancé le 24 juillet, n'eut rien de fortuit[201]. »
Outre la réforme financière de 1958, la France bénéficie des « Trente Glorieuses » et de la croissance amorcée sous la IVe République. Les structures économiques sont modernisées, le niveau de vie s'accroît. Mais la croissance profite inégalement à tous, et un certain désenchantement apparaît face au blocage de la société. Les événements de Mai 1968 en sont le révélateur. Comme dans de nombreux pays, la contestation des étudiants se développe à partir de mars 1968. Les syndicats et les partis politiques de gauche profitent des manifestations étudiantes pour lancer une grève générale qui sera suivie par les ouvriers. Cette grève générale paralyse le pouvoir pendant le mois de mai.
De l'avis de ses propres partisans, de Gaulle a été complètement surpris par une crise qu'il ne prévoit pas et ne comprend pas. Indifférent aux revendications étudiantes et à la « crise de civilisation[202] » qu'elles révèlent, il ne voit là au mieux qu'un gigantesque chahut de jeunes qui ne veulent pas passer leurs examens, au pire une contestation de l'autorité de l'État à faire cesser sur-le-champ. Dans les premiers jours de mai, ses seules consignes sont de réprimer brutalement les manifestations étudiantes, contre l'avis de plusieurs de ses ministres qui conseillent l'apaisement.
Après la nuit des barricades du 10 mai au 11 mai 1968, de Gaulle, sceptique, laisse toutefois son Premier ministre Georges Pompidou, rentré d'un voyage en Iran et en Afghanistan[203], mener une nouvelle politique d'apaisement. Pompidou, qui a dû mettre sa démission dans la balance, veut éviter désormais les heurts, et parie sur l'essoufflement à terme du mouvement.
Du 14 au 18 mai, de Gaulle part en Roumanie. Or, en son absence, la grève générale se développe et des millions de grévistes paralysent la France, tandis que la Sorbonne et l'Odéon sont occupés sans réaction de la police. Seul aux commandes de l'État et de la majorité parlementaire, Pompidou paraît entre-temps devenu le vrai chef du pays.
À son retour anticipé de Roumanie le 18 au soir, de Gaulle déçoit jusqu'à des fidèles inconditionnels en apparaissant dépassé et flottant, sans cette vivacité et cette efficacité de réaction qui le caractérisent d'habitude. Il semble écartelé entre la prudence pompidolienne et la fermeté qu'il prêche lui-même. Il attend le 24 au soir pour parler en public, et pour n'annoncer que des mesures déjà éventées depuis plusieurs jours, qui ne répondent à aucune préoccupation de l'heure. « J'ai mis à côté », confesse-t-il aussitôt après avoir visionné son allocution. Le Général expose, dans cette allocution, qu'il entend que l'État doit rétablir l'ordre, maintenir la République. « La rue, c'est le désordre, la menace du totalitarisme, « la chienlit » »[204]. Le soir même, de violents incidents éclatent à Paris, on relèvera des centaines de blessés et plusieurs barricades érigées[réf. nécessaire].
Le 27 mai, les accords de Grenelle, passés entre le gouvernement Pompidou, les représentants des syndicats et du patronat, aboutissent à un train de mesures classiques[Quoi ?]. De Gaulle préside le Conseil des ministres qui ratifie aussitôt les accords, mais à la surprise de Pompidou et des chefs syndicaux, la base rejette les avancées de Grenelle, estimant que c'est la société entière qui est en cause. Les grèves continuent. Le 27, une manifestation au stade Charléty lance l'idée d'un gouvernement provisoire. Le jour même, François Mitterrand reprend cette solution et annonce sa candidature à la présidence de la République. La crise politique atteint son sommet.
La disparition soudaine et inexpliquée du chef de l'État, parti avec son épouse en hélicoptère le 29 mai pour une destination inconnue, provoque la stupeur et ouvre la voie à toutes les supputations. Il passe par Baden-Baden, où il est reçu par le général Massu[205][1]. Dès son retour à Paris le lendemain, son allocution radiodiffusée a le ton de la fermeté. Il y annonce la dissolution de l'Assemblée nationale. Elle est suivie d'une immense manifestation organisée par les gaullistes sur les Champs-Élysées[206].
De Gaulle était prêt à accepter certaines des revendications des manifestants. Il voulut faire approuver les réformes par référendum, mais Georges Pompidou, en mettant sa démission dans la balance, le persuada de plutôt dissoudre l'Assemblée nationale. De Gaulle l'annonça le 30 mai 1968, dans un discours radiodiffusé, comme l'appel du 18 Juin ou l'intervention de 1960 pendant les barricades d'Alger. Les phrases étaient courtes, chacune ou presque annonçait une décision :
La fin du discours mentionne au sujet d'une déclaration antérieure, et sans la citer, « l'ambition et la haine de politiciens au rancart » et affirme qu'après avoir été utilisés « ces personnages ne pèseraient pas plus que leur poids, qui ne serait pas lourd ». Mais le Général néglige les 44,5 % des voix qui se sont portées en 1965 sur Mitterrand au second tour de la présidentielle, ou encore le simple siège de sa majorité aux élections législatives de 1967.
Une manifestation fut organisée et fut créditée d'un million de participants selon les organisateurs, sept cent mille selon la préfecture de police. Les élections de juin 1968 furent un grand succès pour la droite qui obtient 354 des 487 sièges (du jamais vu dans l'histoire du parlementarisme français). Georges Pompidou fut remplacé par Maurice Couve de Murville au mois de juillet.
La campagne des législatives occupa les forces politiques, tandis que la reprise du travail se faisait progressivement. La reprise en main se fait parfois sans ménagement. Des Comités d'action civique, répondant à l'appel de De Gaulle, se constituent pour dresser des listes noires de grévistes et d'agitateurs notoires, et la police même renoue avec la brutalité des premiers jours de mai (quatre morts à déplorer en juin 1968).
La victoire des gaullistes aux élections législatives, bien que massive, n'a pas assez redynamisé le pouvoir. L'Assemblée nationale, plus à droite, est aussi plus frileuse face aux réformes pourtant voulues par le général de Gaulle (participation, régionalisation, réforme de l'Université…[207]). L'éviction du vrai vainqueur de la crise, Pompidou, a été mal comprise, et ce dernier fait désormais figure de recours et de successeur potentiel. De Gaulle n'est plus irremplaçable.
Il prononce son dernier discours public le 2 février 1969 ; il cite alors quelques vers en breton du poème Da Varzed Breiz (« Aux bardes de Bretagnes », de son oncle Charles[208]).
Dans un référendum portant sur le transfert de certains pouvoirs aux régions et la fusion du Sénat avec le Conseil économique et social[209], de Gaulle proposait d'introduire des représentants des organisations professionnelles et syndicales au sein des conseils régionaux. Mettant tout son poids dans le référendum, il annonça à l'avance son intention de démissionner en cas de victoire du « non ». Celui-ci, auquel s'était rallié Valéry Giscard d'Estaing, l'emporta par 52,41 % le 27 avril 1969. Quelques minutes après minuit, le 28 avril, un communiqué laconique tombe de Colombey-les-Deux-Églises : « Je cesse d'exercer mes fonctions de président de la République. Cette décision prend effet aujourd'hui à midi. »[210].
Ce communiqué est le dernier acte public de « l'homme du 18 Juin » : pour éviter d'être impliqué dans sa propre succession, il passe le temps de la campagne en Irlande où il arrive le 10 mai pour un séjour d'un mois. Treize jours à Sneem puis à Cashel où il vote par procuration ; ensuite il s'enferme à La Boisserie pour y écrire ses Mémoires d'espoir qui prendront la suite des Mémoires de guerre ; il y mène une existence retirée voire recluse.
Fidèle à ses principes concernant la séparation entre sa vie d'homme d'État et sa vie personnelle, il refuse sa retraite de général et d’ancien président de la République[211]. Sa veuve se contente jusqu’à la fin de la réversion de sa retraite de général de brigade à titre temporaire[212] obtenue grâce à un décret pris par le président Pompidou[211].
En juin 1970, il effectue un voyage en Espagne, durant lequel il fait une visite de courtoisie au général Franco[n 25], déclarant regretter n’avoir pu le rencontrer plus tôt du fait des circonstances internationales. Même si de Gaulle n'exerçait plus alors de charge publique, qu'un homme de son prestige aille rencontrer le dictateur espagnol suscite des critiques chez ses détracteurs.
Le 9 novembre 1970, comme à l'accoutumée, le Général entame une partie de patience dans la bibliothèque de la Boisserie. Il dit avoir mal au dos avant de s'écrouler à 19 h 2, victime d'une rupture d'anévrisme de l'aorte abdominale[213],[n 26] et meurt environ vingt minutes plus tard, avant même l'arrivée de son médecin le docteur Lacheny (venu de Bar-sur-Aube) et du curé de Colombey, l'abbé Claude Jaugey. La nouvelle n'est communiquée que le lendemain par une allocution télévisée du président de la République Georges Pompidou qui déclare que « la France est veuve[215],[216] ».
La mort de De Gaulle est l'occasion de prendre la mesure du rôle qu'il a joué dans l'histoire de France, ainsi que dans l'histoire de l'Europe et du monde. Ainsi, le lendemain du décès du général, le roi des Belges Baudouin vient, à titre privé, présenter ses condoléances à madame De Gaulle. Dans ses Mémoires de guerre, De Gaulle s'abstint de condamner le roi Léopold III de Belgique lors de la reddition de l'armée belge, en 1940, et le gouvernement belge d'Hubert Pierlot et Paul-Henri Spaak en exil à Londres fut le premier des gouvernements alliés à reconnaître la légitimité du gaullisme, malgré les pressions anglaises[140].
Les obsèques religieuses du Général ont lieu le 12 novembre 1970 à Colombey-les-Deux-Églises en présence de 50 000 personnes et d'une délégation des armées françaises, seule participation officielle autorisée par le Général dans son testament. L'homélie est alors prononcée par le prêtre et résistant Maurice Cordier[217]. À Paris, de nombreux chefs d'État étrangers sont rassemblés pour honorer sa mémoire à Notre-Dame, 70 000 personnes suivant la cérémonie depuis le parvis[218].
Seul l'hebdomadaire satirique Hara-Kiri osa un titre provocateur, dans son no 94, daté du 16 novembre 1970 : « Bal tragique à Colombey, un mort » (l'opinion était encore sous le choc de l'incendie d'un dancing qui avait causé la mort de 146 personnes une semaine plus tôt à Saint-Laurent-du-Pont) ; l'hebdomadaire fut interdit le lendemain.
Charles de Gaulle rédigea son testament en 1952, juste après les obsèques aux Invalides du maréchal Jean de Lattre de Tassigny, souhaitant éviter toute tentative de récupération politique et d'être trop lié à la IVe République. Il réaffirma à ses proches à plusieurs reprises les dispositions à prendre[219]. Ses dernières volontés[220], qu'il avait rédigées en trois exemplaires numérotés et actualisées, sont les suivantes :
« Je veux que mes obsèques aient lieu à Colombey-les-Deux-Églises. Si je meurs ailleurs, il faudra transporter mon corps chez moi, sans la moindre cérémonie publique.
Ma tombe sera celle où repose déjà ma fille Anne et où, un jour, reposera ma femme. Inscription : Charles de Gaulle (1890-…). Rien d’autre.
La cérémonie sera réglée par mon fils, ma fille, mon gendre, ma belle-fille, aidés par mon cabinet, de telle sorte qu'elle soit extrêmement simple. Je ne veux pas d'obsèques nationales. Ni président, ni ministres, ni bureaux d'assemblées, ni corps constitués[n 27]. Seules, les Armées françaises pourront participer officiellement, en tant que telles ; mais leur participation devra être de dimension très modeste, sans musiques, ni fanfares, ni sonneries[n 28].
Aucun discours ne devra être prononcé, ni à l’église ni ailleurs. Pas d'oraison funèbre au Parlement. Aucun emplacement réservé pendant la cérémonie, sinon à ma famille, à mes Compagnons membres de l'ordre de la Libération, au conseil municipal de Colombey. Les hommes et femmes de France et d'autres pays du monde pourront, s'ils le désirent, faire à ma mémoire l’honneur d'accompagner mon corps jusque sa dernière demeure. Mais c'est dans le silence que je souhaite qu'il y soit conduit. Je déclare refuser d'avance toute distinction, promotion, dignité, citation, décoration, qu'elle soit française ou étrangère. Si l'une quelconque m'était décernée, ce serait en violation de mes dernières volontés. »
— Testament de Charles de Gaulle, 16 janvier 1952Le 27 mai 2017, la croix de la tombe du Général est vandalisée par un individu, mais le socle est resté intact[221].
De 2012 à 2019, une étude conjointe est menée par les équipes du musée de la Légion d'honneur et musée de l'ordre de la Libération. L'ouvrage collectif qui en résulte est principalement utilisé ci-dessous[222].
Quelques semaines après sa mort, le 23 décembre 1970, est votée une loi exonérant de droits de mutation sa succession pour . La loi est présentée au Parlement par le secrétaire d'État à l'Économie et aux Finances, Jacques Chirac[311],[312].
Le nom de Charles de Gaulle a été donné à de nombreuses artères, des ponts ou des bâtiments importants des communes françaises : en 2007, l’Institut Charles-de-Gaulle dénombrait plus de 3 600 voies « de Gaulle »[313], les municipalités de droite ou du centre choisissant volontiers l’appellation militaire « Général-de-Gaulle », tandis que celles de gauche préféraient souvent la forme civile « Charles-de-Gaulle »[314] ; rapidement après sa mort, plusieurs villes communistes comptent d'ailleurs parmi les premières à l'honorer en nommant une rue, une place ou un boulevard en son honneur[315]. On peut citer notamment la place Charles-de-Gaulle (anciennement place de l’Étoile) et le pont Charles-de-Gaulle à Paris, l'avenue Charles-de-Gaulle à Saint-Priest, la place du Général-de-Gaulle à Lille, l’aéroport Roissy-Charles-de-Gaulle (ex-aéroport de Roissy) et le porte-avions nucléaire Charles de Gaulle. En septembre 2020, 4123 lieux portent le nom de l'ancien président[316]. Une variété de rose lui est dédiée en 1974[317]. À l'étranger, des avenues, places et rues lui sont dédiées à Beyrouth, Bucarest, Prague…
Le 4 avril 2005, lors d'une émission de France 2 diffusée en direct du Sénat, il est désigné par les téléspectateurs comme « le plus grand Français de tous les temps », devançant notamment Louis Pasteur, l'Abbé Pierre, Marie Curie, Coluche, Victor Hugo. Une partie des centristes, voire de la gauche, à l'image de Régis Debray, déclare aujourd'hui trouver en lui un inspirateur.
Selon un sondage effectué en 2005, dans le contexte du dixième anniversaire de la disparition de François Mitterrand, ce dernier, alors seul président de gauche de la Ve République, est considéré comme le meilleur président par 35 % des sondés, suivi par Charles de Gaulle (30 %) et Jacques Chirac (12 %), qui se réclame du gaullisme[318]. Un autre sondage réalisé par BVA quatre ans plus tard indique que 87 % des Français jugent positivement la présidence de Charles de Gaulle, le classant ainsi en première position de tous les présidents de la Ve République[319]. Un sondage réalisé par le même institut en 2013 va dans le même sens : avec 89 % d'opinions positives, de Gaulle apparaît comme étant le président préféré des Français, tandis que Mitterrand n'est qu'en cinquième position avec 55 %[320]. En novembre 2010, à l'occasion du 40e anniversaire de sa disparition, un sondage qualifie le général de Gaulle de « personnage le plus important de l'histoire de France » pour 44 % des sondés, devant Napoléon (14 %), Charlemagne (14 %), Jean Jaurès (12 %), Louis XIV (7 %) et Léon Blum (4 %)[321]. Une enquête réalisée par l'Ifop en avril 2011 indique que 45 % des Français considèrent le général de Gaulle comme celui ayant le plus changé la France, devant tous les autres présidents de la Ve République (François Mitterrand, Jacques Chirac, Nicolas Sarkozy, Valéry Giscard d'Estaing, puis Georges Pompidou)[322].
Des statues ont été érigées en sa mémoire aussi bien à Québec ou Londres qu'à Varsovie ou Moscou. La république populaire de Chine lui garde une forte reconnaissance publique pour l'avoir reconnue diplomatiquement en 1964. Israël ressentit d'autant plus durement ses déclarations fracassantes de 1967 que le culte populaire qui était voué à l'homme du 18 juin ne pouvait se comparer jusque-là, comme le rappelle Éric Roussel, qu'à celui du « Père de la nation » David Ben Gourion. Le monde arabe se souvient de ses critiques contre l'occupation de Gaza et de la Cisjordanie. Ben Bella rendit hommage à de Gaulle comme au plus valeureux adversaire du FLN : , mais qui finit par accepter l'indépendance algérienne. En effet, pour Ben Bella :  et [323]. À ceux qui lui reprochaient d'être resté un client de la France gaullienne, Léopold Sédar Senghor répliquait que peu de chefs d'État occidentaux pouvaient se vanter d'avoir risqué personnellement leur vie pour conduire une colonie à l'indépendance. Il n'est pas jusqu'au maître de Cuba, Fidel Castro, qui déclara devant les caméras avoir trouvé un modèle en de Gaulle à la lecture de ses Mémoires de guerre. L'Amérique latine ou le Viêt Nam apprécient encore le pourfendeur de la domination américaine, le Québec le contempteur de la prédominance anglophone.
Le 24 février 1990, l’administration des PTT émet un timbre-poste dans le cadre de l’« hommage à Charles de Gaulle 1890-1970 ». La dessinatrice du timbre est Huguette Sainson.
Statue à Londres.
Statue à Varsovie.
Statue à Nice.
Statue à Nantes sur le cours des 50-Otages.
Statue à Québec aux abords de l'Hôtel Le Concorde.
Statue devant l'hôtel Cosmos de Moscou, réalisée par le sculpteur d'origine géorgienne Zourab Tsereteli, érigée en 2005.
La Constitution de 1958 dure maintenant depuis plus d'un demi-siècle, avec des modifications. « L'homme de Londres » est entré dans un passé mythique où, pour les Français, il incarna à lui seul l'opposition au régime de Vichy.
Les années que l'économiste Jean Fourastié a nommées les Trente Glorieuses (1945-1975) ont laissé aux Français le souvenir d'une époque, sinon heureuse (deux guerres coloniales), au moins de croissance et de prospérité. , affirma Georges Pompidou lors de vœux usuels de Nouvel An aux Français. Or, la fin de cette période heureuse se trouve correspondre à peu près à celle de De Gaulle : difficile dans ces conditions de séparer objectivement ce qui est dû à l'homme et à son dauphin désigné de ce qui est dû au contexte économique.
Le général de Gaulle a planifié et modernisé la recherche et l'industrie par l'impulsion de l'État. C'est de son époque que datent le début des grands programmes qui ont fait la force de l'industrie française et qui trouvent leur aboutissement aujourd'hui dans de grands champions français ou européens : dans l'aéronautique, la Caravelle a donné naissance à Airbus Industrie ; dans l'industrie spatiale, la création du Centre national d'études spatiales (CNES) en 1961, le programme spatial français des « Pierres précieuses » et la fusée Diamant, premier lanceur construit en dehors des États-Unis et de l'URSS, ont abouti à la naissance d'Arianespace et de l'Agence spatiale européenne ; dans l'industrie nucléaire, la création du Commissariat à l'énergie atomique (CEA) en 1945 a permis à la France de contrôler l'ensemble de la filière nucléaire avec la société Areva ; dans l'industrie informatique[324], les objectifs du plan Calcul (1966) ne furent pas atteints, mais, notamment grâce à la création de l'IRIA (devenu INRIA) en 1967, la France est le seul pays européen qui ait réussi à conserver un constructeur informatique purement européen, Bull, qui fabrique aujourd'hui des superordinateurs et, rapproché avec Atos, forme un champion européen de l'informatique.
Bien des traits de sa personnalité avaient entraîné une sympathie des Français envers sa personne : d'abord son vocabulaire non conventionnel pour un homme politique de l'époque et de cet âge (« culbute », « chienlit »), ses boutades[325] (), son sens de la repartie (au cours d'une conférence de presse, à un journaliste lui demandant « Comment allez-vous ? » il répondit : [327]) ; à Louis Vallon, qui s'était écrié  au cours d'une réunion, au temps du RPF, de Gaulle répondit : , son mépris affiché des partis politiques, enfin, sa défiance envers une droite qui ne l'aimait pas et le lui fit voir en 1969, comme envers une gauche qui n'avait jamais vraiment soutenu le projet de participation des salariés aux bénéfices de leur entreprise qui lui était cher (conformément à sa politique directement inspirée du catholicisme social[71]). De Gaulle, c'était, dans un esprit très « Astérix », un de ces « petits qui ne se laissent pas avoir par les grands »[329]. On ne s'étonnera pas de sa déclaration selon laquelle son livre préféré était Cyrano de Bergerac. Et il fit un jour cette remarque ironique : [330],[331] ! »
Dès la Libération, les caricaturistes trouvent matière dans la grande taille de Charles de Gaulle, , constate Guillaume Doizy. À compter du retour au pouvoir du général en 1958, la plume des dessinateurs se polarise également sur la forme de son nez, en allongeant et métamorphosant l'appendice nasal à hauteur de l'opposition exprimée envers le chef de l'État[332].
Il est surnommé « Mongénéral » par Le Canard enchaîné, qui recourt parodiquement au déterminant possessif d'usage militaire, devenu  : « Mongouvernement », « MaFrance », etc. De la sorte, l'hebdomadaire satirique entend souligner , observe l'historien Laurent Martin[333].
Dans le film La Carapate (1978), l'avocat parvient à se faire accorder la grâce présidentielle pour son client par le président Charles de Gaulle.
Dans la série de bandes dessinées Jour J, la mort de Charles de Gaulle à différents moments de l'Histoire est un point de divergence conduisant à plusieurs uchronies. Dans L'Imagination au pouvoir ? (2011), sa mort dans un accident d'hélicoptère lors de sa fuite à Baden-Baden attise un peu plus la révolte de mai 68, menant à une guerre civile de deux ans puis à la victoire des idéaux soixante-huitards. Dans Paris brûle encore (2012), son assassinat lors des événements de mai 68, quand le palais de l'Élysée est attaqué par les manifestants, divise les armées françaises et le pays plonge alors dans huit ans de guerre civile et nucléaire, au cours desquelles Paris est détruite. Aussi, dans Le Crépuscule des damnés (2015), dernier volet d'une trilogie où la crise du 6 février 1934 a abouti au renversement de la République et l'instauration d'un régime fasciste, Charles de Gaulle finit assassiné, mais n'a jamais été le chef de la France libre puisqu'il n'y a pas eu de Seconde Guerre mondiale.
Charles de Gaulle est une série de bande dessinée historique et biographique, par Jean-Yves Le Naour (scénario), Claude Plumail (dessin) et Albertine Ralenti (couleurs), avec le concours de la Fondation Charles de Gaulle. Jean-Yves Ferri a également publié une BD, De Gaulle à la plage, qui a été adaptée pour la télévision par Arte.
Divers musées ou lieux de mémoire sont dédiés à Charles de Gaulle :
Charles de Gaulle, qui commence à écrire à l'âge de quinze ans, publie des articles et une nouvelle dans différentes revues entre 1908 et 1910 en utilisant le pseudonyme de Charles de Lugale[334]. Il est par la suite considéré comme un écrivain de talent[335]. L'écrivain et journaliste Claude Roy le salue, dans Libération, comme un des [336].
Dans les années 1920, Pétain, qui souhaite entrer à l'Académie française, fait appel à lui pour la rédaction d'un ouvrage, Histoire du soldat français, qui devait être publié sous le nom du maréchal[337]. Pétain n'en écrit que la partie sur la Première Guerre mondiale (La Guerre mondiale 1914-1918). À la suite de dissensions entre les deux hommes, le livre n'est jamais publié et de Gaulle reprend ses écrits pour la rédaction de l'essai La France et son armée, sorti en 1938[337].
En 1963, Charles de Gaulle fait partie des lauréats potentiels du prix Nobel de littérature[338] et ses Mémoires de guerre lui valent d'entrer dans la prestigieuse Bibliothèque de la Pléiade en 2000. Le troisième tome de ses Mémoires de guerre, Le Salut, 1944-1946, est inscrit au programme du baccalauréat littéraire en 2011 et en 2013[335].
« Il n’y avait rien de surprenant à ce que de Gaulle connût aussi bien l’histoire de l’Irlande. Par sa grand-mère maternelle, Joséphine Anne Marie Maillot, Charles de Gaulle descendait de ce clan MacCartan, originaire du district de Kinclarty dans le comté de Down, dont un cadet — Anthony MacCartan — par fidélité à la cause jacobite, avait débarqué de son Irlande natale pour prendre du service dans la Brigade irlandaise du roi de France et faire souche dans le Nord à l’orée du XVIIIe siècle. Parfois il y a confusion entre Kinclarty dans l'Irlande du Nord et Killarney, au sud-ouest. »
« M. De Gaulle, Charles André, Joseph, Capitaine commandant la 10e Compagnie du 33e Régiment d’Infanterie, a été nommé dans l'ordre de la Légion d'Honneur au grade de chevalier.
« À DOUAUMONT le 2 mars 1916 sous un effroyable bombardement, alors que l'ennemi avait percé la ligne et attaquait sa compagnie de toute part, a organisé après un corps à corps farouche, un îlot de résistance où tous se battirent jusqu'à ce que fussent dépensées les munitions, fracassés les fusils et tombés les défenseurs désarmés ; bien que très grièvement blessé d'un coup de baïonnette, a continué à être l'âme de la défense jusqu'à ce qu'il tombât inanimé sous l'action des gaz.
La présente nomination comporte l'attribution de la CROIX DE GUERRE avec PALME.
Le maréchal de France, commandant les armées de l'Est,
PÉTAIN » »
« Dès le début, il m'a paru nécessaire de différencier de façon apparente, les bâtiments de guerre de la France libre et ceux restés fidèles au gouvernement du Maréchal Pétain.
Un de mes premiers ordres — du 2 juillet, si j'ai bonne mémoire — précisa que les bâtiments des Forces Navales Françaises libres porteraient à la poupe les couleurs nationales françaises et à la proue un pavillon carré bleu, orné d'une Croix de Lorraine rouge. Et ce fût (sic) l'origine de l'insigne du Mouvement de la France libre.
Pourquoi j'ai choisi la Croix de Lorraine ? Parce qu'il fallait un emblème en opposition à la Croix Gammée et parce que j'ai voulu penser à mon père qui était Lorrain. »
Vous lisez un « bon article » labellisé en 2013.
Soliman Ier (turc ottoman : سلطان سليمان اول (Sultān Suleimān-i evvel) ; turc : I. Süleyman) est probablement né le 6 novembre 1494 à Trébizonde (Trabzon) dans l'actuelle Turquie et mort le 6 septembre 1566 à Szigetvár dans l'actuelle Hongrie. Fils de Sélim Ier Yavuz, il fut le dixième sultan de la dynastie ottomane et le soixante-quatorzième calife de l’islam de 1520 à sa mort en 1566. On le nomme Soliman le Magnifique en Occident et le Législateur en Orient (turc : Kanuni ; arabe : القانوني, al‐Qānūnī) en raison de sa reconstruction complète du système juridique ottoman.
Soliman devint l'un des monarques les plus éminents du XVIe siècle et présida à l'apogée de la puissance économique, militaire, politique et culturelle de l'Empire ottoman. Il mena ses armées à la conquête des bastions chrétiens de Belgrade, de Rhodes et de la Hongrie avant de devoir s'arrêter devant Vienne en 1529. Il annexa la plus grande partie du Moyen-Orient lors de ses guerres contre les Séfévides d'Iran ainsi que de larges portions de l'Afrique du Nord jusqu'en régence d'Alger. Sous son règne, la marine ottomane, menée notamment par le grand amiral Barberousse, domina la mer Méditerranée, la mer Rouge et le golfe Persique.
À la tête de son empire en pleine expansion, Soliman instaura des changements législatifs concernant la société, l'éducation, l'économie et le système judiciaire. Son code civil (appelé Kanun) fixa la forme de l'empire pour plusieurs siècles. Soliman était non seulement un poète et un orfèvre, mais également un mécène qui supervisa l'âge d'or de l'art, de la littérature et de l'architecture ottomanes. Il parlait quatre langues : le turc ottoman, l'arabe, le tchaghataï (un dialecte turc apparenté à l'ouïghour) et le persan.
En rupture avec les traditions ottomanes, Soliman épousa l'une des filles de son harem, Roxelane, qui devint Hürrem Sultan ; ses intrigues en tant que reine à la cour et son influence sur le sultan assurèrent sa renommée. Leur fils, Sélim II, succéda à Soliman à sa mort en 1566. Le règne de près de 46 ans de Soliman demeure le plus long de l'histoire de l'Empire ottoman.
Soliman est né à Trabzon au bord de la mer Noire, probablement le 6 novembre 1494[1],[Note 1]. Sa mère était la sultane validé Hafsa Sultan qui mourut en 1534. À l'âge de sept ans, il fut envoyé à l'école du palais de Topkapı à Constantinople (Istanbul) pour y étudier les sciences, l'histoire, la littérature, la théologie et les tactiques militaires. Il se lia d'amitié avec Pargalı Ibrahim Pacha, un esclave qui devint par la suite l'un de ses plus proches conseillers[2]. Le jeune Soliman, âgé de 18 ans, fut nommé gouverneur de Kefe (Théodosie) puis de Manisa avec un bref séjour à Adrianople[3]. À la mort de son père, Sélim Ier (1470/71-1520), Soliman accéda au trône en tant que dixième sultan ottoman. Seul fils survivant de Selim, il n'eut pas à affronter ses frères pour accéder au pouvoir contrairement à ses prédécesseurs.
Une première description de Soliman, quelques semaines après son sacre, est fournie par l'émissaire de Venise, Bartolomeo Contarini : [4]. Certains historiens avancent que dans sa jeunesse Soliman avait une admiration pour Alexandre le Grand[5],[6]. Il était influencé par la vision que portait Alexandre pour un empire mondial, qui s'étendrait de l'Est à l'Ouest, vision qui pourrait avoir encouragé Soliman à entreprendre ses futures campagnes militaires en Asie, en Afrique et en Europe, afin d'étendre l'Empire ottoman[7].
Dès la mort de son père, Soliman entama une série de conquêtes militaires et réprima une révolte menée par le gouverneur ottoman de Damas en 1521. Il prépara la conquête de Belgrade alors défendue par le Royaume de Hongrie et que son arrière grand-père Mehmed II avait échoué à prendre. La prise de la ville était indispensable pour éliminer les Hongrois qui, après les défaites des Serbes, des Bulgares, des Byzantins et des Albanais restaient l'unique réelle puissance pouvant s'opposer à l'avancée ottomane en Europe. Soliman encercla Belgrade et entama une série de bombardements depuis une île sur le Danube. Avec une garnison de 700 hommes et sans aucun soutien de la Hongrie, la ville tomba en août 1521[8].
La nouvelle de la chute de l'un des bastions de la Chrétienté sema la peur dans toute l'Europe. Comme l'ambassadeur du Saint-Empire romain germanique à Constantinople l'écrivit : [9].
La route de la Hongrie et de l'Autriche était maintenant ouverte, mais Soliman détourna son attention vers l'île méditerranéenne de Rhodes, le couvent des Hospitaliers de l'ordre de Saint-Jean de Jérusalem dont les activités de course en Asie Mineure et au Levant étaient une menace permanente pour les intérêts ottomans. À l'été 1522, Soliman détacha une flotte de 400 navires tout en menant personnellement une armée de 100 000 hommes en Asie Mineure face à l'île[10]. Après un siège de cinq mois, Rhodes capitula et Soliman, à la suite d'une négociation avec Philippe de Villiers de L'Isle-Adam, grand maître de l'ordre de Saint-Jean de Jérusalem, autorisa les Hospitaliers à quitter l'île, ces derniers finirent par s'installer à Malte[11].
Les relations entre la Hongrie et l'Empire ottoman continuaient de se détériorer et Soliman reprit sa campagne en Europe orientale. Le 29 août 1526, l'armée hongroise menée par Louis II de Hongrie (1506-1526) fut battue lors de la bataille de Mohács. Après cela, la résistance hongroise s'effondra et l'Empire ottoman devint la puissance dominante dans la région[12]. Croisant le corps sans vie du roi Louis II, Soliman se serait lamenté : [13]. Alors que Soliman menait campagne en Hongrie, des tribus turkmènes, menées par Kalender Çelebi, amorcèrent une révolte en Anatolie[14] qui fut bientôt réprimée par Pargalı Ibrahim Pacha
Certains nobles hongrois proposèrent que Ferdinand (1519-64), archiduc de l'Autriche voisine et lié à Louis II par mariage, devienne roi de Hongrie évoquant des accords antérieurs précisant que les Habsbourg prendraient le contrôle de la Hongrie si Louis II mourait sans héritier[15]. Cependant, d'autres nobles étaient partisans de Jean Zápolya, un noble soutenu par Soliman et donc rejeté par les puissances chrétiennes d'Europe. Sous Charles Quint et son frère Ferdinand, archiduc d'Autriche, les Habsbourg réoccupèrent Buda et la Hongrie. En conséquence, en 1529, Soliman remonta le Danube et reprit Buda avant d'assiéger Vienne. Avec une garnison renforcée de 16 000 hommes[16], les Autrichiens infligèrent à Soliman sa première défaite, semant les germes d'une rivalité entre les Ottomans et les Habsbourg qui dura jusqu'au XXe siècle[17]. Une nouvelle tentative pour prendre Buda en 1532 échoua également car Soliman dut se retirer avant d'atteindre la ville, repoussé par les défenseurs magyars et croates lors du siège de Güns (en). Dans les deux cas, l'armée ottomane avait été handicapée par le mauvais temps (la forçant à abandonner l'essentiel de son équipement de siège) et l'étirement excessif des lignes de ravitaillement[18]. L'échec de Soliman devant Vienne marqua ainsi l'apogée de la puissance ottomane et de son extension territoriale en Europe centrale[19].
Soliman ne tarda pas à prendre sa revanche : en 1538, la rapide campagne de Moldavie lui permit d'imposer sa domination à cette principauté roumaine et appuyer son candidat sur le trône de Moldavie le prince Ștefan Lăcusta contre le roi Petru Rareș qui s'est allié avec les Habsbourg et annexe à son empire le sud de la Bessarabie[20].
Peu après, des tensions en Hongrie lui fournirent l'opportunité de venger sa défaite devant Vienne. En 1541, les Habsbourg entrèrent une nouvelle fois en guerre avec les Ottomans et tentèrent d'assiéger Buda. Ils furent repoussés et plusieurs de leurs forteresses furent prises[21]. Ferdinand et son frère Charles Quint furent forcés de signer une humiliante trêve de cinq ans avec Soliman. Ferdinand renonçait à ses prétentions sur le trône de Hongrie et devait payer un tribut annuel pour les terres hongroises qu'il contrôlait. D'un point de vue plus symbolique, le traité faisait référence à Charles Quint, non en tant qu'« empereur » du Saint-Empire mais uniquement comme « roi d'Espagne »[22]. Avec l'affaiblissement de ses rivaux européens, Soliman s'était assuré un rôle de premier plan dans les affaires européennes.
Après avoir sécurisé ses frontières européennes, Soliman se tourna vers la menace posée par la dynastie chiite des Séfévides de Perse. Deux événements en particulier précipitèrent le conflit. Premièrement, le chah Tahmasp Ier avait fait assassiner le gouverneur de Bagdad loyal à Soliman pour le remplacer par un de ses partisans ; et deuxièmement, le gouverneur de Bitlis avait fait défection pour rejoindre les Séfévides[23]. En conséquence, Soliman ordonna en 1533 au grand vizir Pargalı Ibrahim Pacha de mener une armée en Asie. Il reprit Bitlis et occupa Tabriz sans rencontrer de véritable opposition. Après avoir rejoint Ibrahim en 1534, Soliman fit une poussée vers la Perse, mais il réalisa que le shah sacrifiait son territoire pour éviter une bataille rangée et harcelait l'armée ottomane alors qu'elle avançait en terrain difficile[24]. Néanmoins, Bagdad tomba l'année suivante et cela confirma Soliman en tant que chef de file du monde islamique et successeur légitime des califes abbassides[25].
Tentant de vaincre le shah une bonne fois pour toutes, Soliman se lança dans une seconde campagne en 1548-1549. Comme dans le conflit précédent, Tahmasp Ier évita toute confrontation avec l'armée ottomane et préféra se replier non sans avoir dévasté l'Arménie perse afin de priver les Ottomans d'abri durant le rude hiver caucasien[24]. Soliman abandonna la campagne mais conserva ses gains à Tabriz, en Arménie, dans la province de Van et en Géorgie[26]. En 1553, Soliman lança sa troisième et dernière campagne contre le shah. Après avoir perdu des territoires à Erzurum face au fils de son ennemi, Soliman riposta en reprenant la ville, en franchissant l'Euphrate et en dévastant des territoires de Perse. L'armée du shah continua sa stratégie de retraite, ce qui conduisit à une impasse, aucun camp ne semblant pouvoir prendre l'avantage. En 1554, un traité mit fin aux campagnes de Soliman dans la région. Les Perses conservaient Tabriz et leurs territoires du nord-ouest mais Soliman s'emparait de Bagdad, de la Mésopotamie, des embouchures du Tigre et de l'Euphrate et donc d'un débouché dans le golfe Persique[27]. Le Shah s'engageait également à cesser toutes ses incursions en territoire ottoman[26].
La marine ottomane parcourait l'océan Indien de manière régulière depuis l'an 1518. De grands amiraux turcs, tels Hadim Suleiman Pacha, Seydi Ali Reis ou Kurtoğlu Hızır Reis (en)[28], furent connus pour leurs voyages dans l'Empire moghol, dans les ports de Thatta, de Surate et de Janjira. De plus, l'empereur moghol Akbar échangea six documents avec Soliman, attestant des relations qu'entretenaient les deux principaux empires musulmans de l'époque[28],[29].
Dans l'océan Indien, Soliman mena plusieurs campagnes navales contre les Portugais qui s'étaient emparés du commerce avec la côte occidentale de l'Inde. Aden, au Yémen, fut prise par les Ottomans en 1538 afin de fournir de base navale contre les possessions portugaises sur la côte ouest indienne[30]. Faisant voile vers l'Inde, les Ottomans échouèrent à prendre Diu aux Portugais en septembre 1538 mais ils retournèrent à Aden, qu'ils fortifièrent avec 100 pièces d'artillerie[30],[31]. Depuis cette base, Hadim Suleiman Pacha parvint à prendre le contrôle de tout le Yémen, dont Sanaa[30]. Aden se souleva contre les Ottomans et fit appel aux Portugais. La ville fut reprise par l'amiral Piri Reis en 1548.
Fort de son contrôle indiscuté sur la mer Rouge, Soliman parvint à contrecarrer l'influence portugaise et à poursuivre un commerce important avec l'Empire moghol durant tout le XVIe siècle[32]. Son amiral Piri Reis mena une flotte ottomane qui s'empara de Mascate en 1552.
En 1564, Soliman reçut un émissaire d'Aceh (en actuelle Indonésie) demandant l'aide ottomane contre les Portugais. L'expédition vers le détroit de Malacca permit de fournir un soutien militaire considérable aux Acehnais[33].
Ayant consolidé ses conquêtes sur terre, Soliman apprit que l'amiral Andrea Doria, au service de Charles Quint, avait pris la forteresse de Koróni, en Morée (actuel Péloponnèse). La présence espagnole dans l'est de la Méditerranée inquiétait Soliman, qui y voyait une volonté de Charles Quint de s'attaquer à la domination ottomane[34].
Estimant nécessaire de renforcer la présence de sa marine dans la région, Soliman plaça à sa tête l'amiral Khayr ad-Din Barberousse, connu en Europe sous le nom de Barberousse. Ce dernier, chargé de reconstruire la flotte ottomane, parvint à faire en sorte que la marine de Soliman égale en nombre l'ensemble des flottes méditerranéennes coalisées[34]. En 1535, Charles Quint remporta toutefois une importante victoire contre les Ottomans à Tunis et la guerre avec Venise l'année suivante força Soliman à former une alliance avec François Ier contre Charles Quint[23]. En 1538, la flotte espagnole fut battue à Préveza par Barberousse, ce qui permit aux Ottomans de sécuriser leur contrôle de la Méditerranée orientale pendant 33 ans, jusqu'à la bataille de Lépante en 1571[35].
D'importants territoires d'Afrique du Nord furent annexés. Les États barbaresques de Tripolitaine, de Tunisie et d'Algérie devinrent des provinces autonomes de l'Empire et permirent de menacer Charles Quint, qui avait échoué à chasser les Ottomans en 1541[36]. La piraterie menée par les Barbaresques d'Afrique du Nord peut être interprétée dans le contexte des guerres contre l'Espagne. La marine ottomane contrôlait également la mer Rouge et le golfe Persique[35] jusqu'en 1554 lorsque ses navires furent battus par la marine portugaise. Les Portugais avaient pris Ormuz et le détroit du même nom en 1515 et continuèrent de rivaliser avec les forces de Soliman pour le contrôle d'Aden[37].
En 1542, François Ier chercha à renouveler l'alliance avec l'Empire ottoman pour lutter contre les Habsbourg. Soliman envoya 100 galères menées par Barberousse[38] pour aider les Français dans l'ouest de la Méditerranée. Barberousse pilla les côtes de Sicile et de Naples avant d'atteindre la France, où François Ier avait installé son état-major à Toulon[39]. Lors de la même campagne, Barberousse s'empara de Nice en 1543[39]. La trêve de Crépy-en-Laonnois, signée le 18 septembre 1544 entre François Ier et Charles Quint à l'issue de la neuvième guerre d'Italie, mit temporairement fin à l'alliance franco-ottomane[39].
Les Hospitaliers, qui s'étaient réfugiés à Malte après avoir été chassés de Rhodes, entreprirent de lutter contre la flotte des Ottomans, qui assemblèrent une large armée pour les déloger de l'île[40]. Les Ottomans débarquèrent en 1565 et entamèrent le siège du principal fort hospitalier le 18 mai. Le siège fit ensuite l'objet de nombreuses fresques par le peintre italien Matteo da Leccio. La bataille semblait être une répétition de la prise de Rhodes car la plupart des villes de Malte avaient été détruites et la moitié des chevaliers étaient morts au combat[40]. Cependant, des renforts espagnols obligèrent les Ottomans à lever le siège le 8 septembre. La bataille se solda par une victoire décisive des Hospitaliers, avec 30 000 Ottomans morts au combat[41],[40].
Si Soliman est appelé « le Magnifique » en Occident[42], il est désigné par « le Législateur » en Orient. Comme l'historien Patrick Balfour l'écrivit : [43]. La loi suprême de l'Empire était la charia, ou Loi sacrée qui, en tant que loi divine de l'islam, ne pouvait être modifiée par le sultan. Cependant, un domaine législatif appelé Kanun dépendait uniquement de la volonté de Soliman et couvrait le droit pénal, fiscal et foncier[44]. Il rassembla toutes les décisions faites par les neuf sultans ottomans précédents. Après avoir éliminé les doublons et choisi entre les textes contradictoires, il délivra un unique code légal qui ne violait pas les lois basiques de l'islam[45]. C'est à travers cette structure que Soliman, aidé par le grand mufti Ebussuud Efendi, chercha à réformer la législation pour l'adapter à l'évolution rapide de l'Empire. Lorsque le Kanun atteignit sa forme finale, le code des lois fut nommé kanun‐i Osmani ou les « lois ottomanes ». Ce code légal devait durer plus de 300 ans[46].
Soliman promulgua de nouvelles lois pénales et judiciaires limitant le nombre d'actes passibles de la peine de mort ou de la mutilation et établit une liste d'amendes correspondants à des délits définis. Dans le domaine fiscal, des taxes sur les animaux, les mines ou les profits du commerce furent levées. En plus des taxes, les fonctionnaires reconnus coupables de corruption pouvaient voir leurs terres confisquées par le sultan[47].
Soliman prêta une attention particulière aux souffrances des rayas, des sujets chrétiens travaillant sur les terres des sipahis. Son Kanune Raya, ou « Code des Rayas » réforma la loi concernant les taxes imposées à ceux-ci. Ce code améliora tellement leur condition que des serfs chrétiens émigrèrent dans les territoires ottomans pour en profiter[48]. Néanmoins, malgré les réformes de Soliman, la situation des Chrétiens resta marquée par la pratique du devchirmé. Cette dernière, en vigueur dans l'Empire ottoman depuis le XIVe siècle, consistait en un système de recrutement forcé, fondé sur la réquisition d'enfants et d'adolescents dans les populations chrétiennes pour les élever comme des Turcs musulmans, afin de les destiner à occuper de hauts postes dans l'administration, ou, pour la plus grande part, à faire partie des troupes d'élite ottomanes : les janissaires. Ainsi, plus de 30 000 janissaires issus de ce système servirent Soliman tout au long de son règne[49].
Soliman prit également des mesures pour protéger les sujets juifs de son empire. À la fin de l'année 1553 ou 1554, sur la suggestion de son médecin et dentiste préféré, le juif espagnol Moïse Hamon, le sultan émit un décret (firman) dénonçant les accusations antisémites de meurtre rituel[47].
Soliman s'intéressa également à l'éducation. Les écoles attachées aux mosquées et financées par des fondations religieuses offraient un système d'éducation largement gratuite aux garçons musulmans, comparable aux écoles épiscopales des États chrétiens de l'époque[50]. Dans sa capitale, Soliman accrut à 14 le nombre de mektebs (écoles primaires) qui apprenaient aux enfants à écrire et à lire. Le nombre de médersas (lycées) enseignant la philosophie, l'astronomie et l'astrologie passa à 8[50]. Il existait également des écoles supérieures dont les étudiants pouvaient devenir enseignants ou imams ; elles dépendaient étroitement du pouvoir politique ou religieux, contrairement aux universités européennes médiévales protégées par des privilèges. Les lieux d'éducation étaient souvent à proximité des mosquées et certains abritaient des dispensaires, des fontaines et des réfectoires ouverts au public[50].
Sous l'influence de Soliman, l'Empire ottoman entra dans un âge d'or culturel[51]. Des centaines de sociétés artistiques impériales (appelées Ehl-i Hiref, « communauté des Talentueux ») étaient administrées depuis le palais impérial de Topkapı. Après un apprentissage, les artistes et les artisans pouvaient monter en grade au sein de leur confrérie et recevaient des salaires très élevés. Les registres de salaires qui nous sont parvenus témoignent de l'étendue du mécénat artistique de Soliman ; le plus ancien de ces documents date de 1526 et recense 40 sociétés avec plus de 600 membres. Le Ehl-i Hiref attirait les artistes les plus talentueux de tout l'Empire, à la fois du monde islamique et des territoires conquis d'Europe. Le résultat est un mélange des cultures européennes, turques et islamiques[52]. Les artisans au service de la cour regroupaient des peintres, des fourreurs, des bijoutiers et des orfèvres. Alors que les précédents souverains avaient été influencés par la culture iranienne (le père de Soliman, Sélim Ier, écrivait des poèmes en persan), le mécénat artistique de Soliman a permis à l'Empire ottoman de construire son propre héritage artistique[53].
Soliman était lui-même un poète accompli, écrivant en persan et en turc sous le nom de Muhibbi (Amoureux). Lorsque son jeune fils Mehmed mourut en 1543, il composa un émouvant chronogramme pour commémorer l'année : Sans égal parmi les princes, mon sultan Mehmet[54],[55],[Note 2].
Parmi les grands poètes du règne de Soliman, on peut citer Fuzûlî et Bâkî. L'historien de la littérature E. J. W. Gibb observe [54]. Le verset le plus célèbre de Soliman est :
« Les gens considèrent la richesse et le pouvoir comme le plus grand des destins,
Mais dans ce monde un moment de santé est le meilleur des états.
Ce que les gens appellent souveraineté est une lutte temporelle et une guerre constante ;
La vénération de Dieu est le plus haut des trônes, le plus joyeux de tous les états[56]. »
Soliman est également renommé pour avoir soutenu une série de monumentaux développements architecturaux dans son empire. Le sultan chercha à transformer Constantinople en centre de la civilisation islamique avec une série de projets dont des ponts, des mosquées, des palais et divers établissements sociaux. Les plus grands d'entre eux furent bâtis par l'architecte en chef du sultan, Sinan, grâce auquel l'architecture ottomane atteignit son apogée[57]. Sinan devint responsable de plus de 300 monuments dans tout l'empire dont ses deux chefs-d'œuvre, la mosquée Süleymaniye de Constantinople et la mosquée Selimiye d'Adrianople qui fut achevée sous le règne de Sélim II, fils de Soliman[57]. Soliman fit restaurer également le dôme du Rocher et les murs de Jérusalem (qui forment aujourd'hui les murs de la vieille ville de Jérusalem) ainsi que la Kaaba de La Mecque[58].
Mahidevran fut la première grande amoureuse de Soliman le Magnifique. Ils ont ensemble 3 fils, dont un seul survivra : Şehzade Mustafa. 
Elle finira par se faire surpasser par Roxelane.
Soliman tomba amoureux de Roxelane, une fille de son harem originaire de Ruthénie, territoire faisant alors partie de la Pologne. Les diplomates occidentaux, constatant les commérages du palais sur elle, l'appelèrent « Russelazie » ou « Roxelane », en référence à ses origines slaves[59]. Fille d'un prêtre orthodoxe ruthène[27], elle devint esclave et gravit les échelons du harem pour devenir la favorite de Soliman.
Rompant avec deux siècles de traditions ottomanes[27], une ancienne concubine était devenue l'épouse légale du sultan, à la stupeur des observateurs du palais et de la ville[60]. Il autorisa également Hürrem Sultan (le nom officiel de Roxelane) à rester avec lui à la cour pour le reste de sa vie, brisant une tradition ottomane voulant que, lorsque les héritiers atteignaient leur majorité, ils soient envoyés avec leur génitrice pour gouverner une province reculée de l'Empire et n'en reviennent que pour occuper le trône impérial[61].
Sous son nom de plume, Muhibbi, Soliman composa ce poème pour Roxelane :
Trône de mon mihrab, ma richesse, mon amour, mon clair de lune.
Ma compagne intime, ma confidente, ma toute chose, mon seul et unique amour.
La plus belle parmi les admirables…
Mon printemps, source de toutes joies, source de lumière, mon étoile brillante, lumière de ma nuit…
Mon doux sucre, mon trésor, ma rose, la seule qui ne me désole pas dans ce monde…
Mon Constantinople, mon Caraman, le centre de mon Anatolie
Mon Badakhchan, mon Bagdad et mon Khorasan
Ma femme aux beaux cheveux et aux beaux sourcils, aux yeux langoureux et pleins de malice…
Je chanterais toujours tes louanges
Moi, amoureux au cœur tourmenté, Muhibbi aux yeux pleins de larmes, je suis heureux[62].
Pargalı Ibrahim était un ami d'enfance de Soliman. Ibrahim était initialement un Grec de confession orthodoxe, originaire de Parga, en Épire[63],[64]. Il fut éduqué à l'école du palais sous le système du devchirmé. Soliman le fit fauconnier royal puis le promut premier officier de la chambre royale[65]. Ibrahim Pacha devint grand vizir en 1523 et commandant en chef de toutes les armées. Soliman le nomma également beylerbey de Roumélie lui offrant l'autorité sur tous les territoires européens de l'Empire de même que sur les troupes y étant stationnées en temps de guerre. Selon un chroniqueur du XVIIe siècle, Ibrahim avait demandé à Soliman de ne pas le nommer à une position aussi haute, craignant pour sa sécurité ; ce à quoi Soliman répondit que sous son règne, peu importe les circonstances, il ne serait pas exécuté[66].
Cependant, Ibrahim perdit le soutien du sultan. Durant ses treize années en tant que grand vizir, sa promotion rapide et l'accumulation importante de ses richesses lui avait attiré l'inimitié de nombreux courtisans. Soliman reçut des rapports concernant l'insolence d'Ibrahim durant une campagne contre les Séfévides d'Iran, en particulier le fait qu'il ait adopté le titre de « sultan sérasker », ce qui était vu comme un affront envers Soliman[67].
La méfiance de Soliman fut accrue après une querelle entre Ibrahim et le ministre des finances Iskender Chelebi. La dispute se termina par la disgrâce d'Iskender sur des accusations de complot et ce dernier fut condamné à mort par Soliman sur les conseils d'Ibrahim. Avant sa mort, Iskender accusa Ibrahim de comploter contre le sultan[67]. Ces derniers mots convainquirent Soliman de la déloyauté d'Ibrahim et le 15 mars 1536, le corps sans vie d'Ibrahim fut découvert dans le palais de Topkapı[67].
C'est dans la nuit du 5 au 6 septembre 1566[71], soit la veille avant la victoire ottomane lors du siège de Szigetvár, que Soliman serait décédé, dans sa tente en terre hongroise. Sa santé s'était probablement dégradée du fait du long voyage entrepris depuis Constantinople[72], d'autant plus qu'il était extrêmement affaibli dès son départ de la capitale turque. On avait caché sa mort à l'armée pour qu'elle ne perde pas le moral et on avait ramené son corps à Constantinople dans une voiture close, prétextant qu'il était malade. Aucune proclamation officielle ne fut faite avant le 48e jour suivant son décès, date à laquelle le cortège approchait Belgrade où l'attendait son successeur, Sélim[73].
Cependant, la lutte de succession avait débuté bien des années auparavant. Soliman avait eu huit fils dont quatre vécurent jusque dans les années 1550 : Mustafa, Sélim, Bayezid et Cihangir. De ces derniers, seul Mustafa n'était pas le fils de Roxelane, mais de l'ancienne favorite de Soliman, Mahidevran. Mustafa était considéré comme le plus talentueux des frères et avait eu le soutien de Pargalı Ibrahim Pacha avant l'exécution de ce dernier en 1536. Guillaume Postel écrit ainsi en 1537 :  ; Ogier Ghislain de Busbecq parle de ses [74].
Roxelane savait que si Mustafa devenait sultan, ses fils seraient assassinés, et elle est généralement considérée comme ayant au minimum fait partie des intrigues concernant la nomination d'un successeur. Bien qu'elle soit la femme de Soliman, elle n'exerçait aucun rôle public officiel. Cela ne l'empêcha cependant pas de rassembler de nombreuses personnalités politiques. Comme l'Empire manquait de règles pour nommer un successeur, la succession impliquait habituellement la mort des princes concurrents afin d'éviter une guerre civile. Dans le but d'empêcher l'exécution de ses fils, Hürrem usa de son influence pour éliminer ceux qui soutenaient l'accession au trône de Mustafa[56].
Ainsi, dans les luttes de pouvoir apparemment à l'instigation de Hürrem[65], Soliman fit assassiner son grand vizir Ibrahim en 1536. En 1552, lorsque la campagne contre les Séfévides fut lancée avec à sa tête son beau-fils Rüstem Pacha, devenu grand vizir en 1544, les intrigues commencèrent contre Mustafa. Rüstem envoya l'un des hommes les plus respectés de Soliman pour rapporter que comme Soliman n'était pas à la tête de l'armée, les soldats pensaient que le temps était venu de mettre un plus jeune prince sur le trône ; dans le même temps il fit courir l'idée que Mustafa avait été réceptif à cette idée. Ulcéré par ce qu'il croyait être un plan de Mustafa pour s'emparer du trône, Soliman, en route vers le front perse, le convoqua dans sa tente à Ereğli pour qu'il .
Mustafa devait choisir, soit il apparaissait devant son père avec le risque d'être tué soit il refusait de venir et serait accusé de trahison. Finalement, il choisit de se rendre à l'invitation, confiant dans le fait que le soutien de l'armée le protégerait. Busbecq, qui avance avoir reçu un rapport d'un témoin, relate les derniers moments de Mustafa. Alors qu'il entrait dans sa tente, les eunuques de Soliman attaquèrent Mustafa qui se défendit vaillamment. Soliman, séparé de la lutte par de simples rideaux, assista à la scène. Mustafa fut étranglé avec une corde à arc[76].
Cihangir serait mort de chagrin quelques mois après le meurtre de son demi-frère[77]. Les deux frères survivants, Bayezid et Sélim, reçurent des commandements dans deux régions différentes de l'Empire. En quelques années, une guerre civile éclata entre les deux frères, chacun d'entre eux soutenu par ses troupes[78]. Avec l'aide de l'armée de son père, Sélim battit Bayezid à Konya en 1559 et ce dernier chercha refuge chez les Séfévides avec ses quatre fils. Le sultan demanda au Shah Tahmasp Ier que Bayezid soit extradé ou exécuté. En échange d'une importante quantité d'or, le Shah autorisa un bourreau turc à étrangler Bayezid et ses quatre fils en 1561[77]. L'accession au trône de Sélim était à présent dégagée.
Ce dernier succéda ainsi à son père, devenant le 11e sultan de l'Empire ottoman[72].
Lorsque Soliman décéda en Hongrie, les Ottomans érigèrent un petit tombeau sur place et y avaient inhumé son cœur. Des archéologues hongrois, fouillant un complexe funéraire turc (composé d'un sanctuaire, de casernes, d'un monastère), mis au jour près de Szigetvár (46° 04′ 46″ N, 17° 50′ 41″ E), affirment en 2016 que la tombe retrouvée est très probablement celle du sultan Soliman. Ils se fondent notamment sur le style des bâtiments de ce complexe, proche de celui du mausolée de Soliman à Istanbul[79],[80].
À la mort de Soliman, l'Empire ottoman était l'une des puissances les plus avancées au monde[81]. Soliman avait conquis les grandes villes musulmanes de La Mecque, Médine et Bagdad, de nombreuses provinces dans les Balkans (jusque dans la Croatie et l'Autriche actuelle) et la plus grande partie de l'Afrique du Nord. Son expansion en Europe avait donné aux Ottomans un rôle important dans la balance des forces européennes. En effet, la menace posée par les Ottomans était si prégnante sous le règne de Soliman que l'ambassadeur Busbecq avertit l'Europe : [82].
Même trente ans après sa mort, le « sultan Soliman » fut évoqué par l'auteur anglais William Shakespeare en tant que prodige militaire dans Le Marchand de Venise (Acte 2, Scène 1).
L'héritage de Soliman ne se limite pas simplement au domaine militaire. Le voyageur français Jean de Thévenot rapporte un siècle plus tard : [83]. Les réformes de l'administration et du système judiciaire qui lui valurent le surnom de « Législateur » assurèrent la survie de l'empire bien après sa mort, une réussite qui [84].
À travers son mécénat, Soliman présida également à l'âge d'or de l'Empire ottoman, représentant son apogée architecturale, culturelle, littéraire, théologique, artistique et philosophique[51],[85]. Aujourd'hui, l'horizon du Bosphore et de nombreuses villes de la Turquie moderne et des anciennes provinces ottomanes arbore toujours les travaux architecturaux de Sinan[86]. Parmi ceux-ci, la mosquée Süleymaniye, construite en 1557 par l'architecte à la demande du sultan, est la dernière demeure de Soliman et de Roxelane : ils y reposent dans deux mausolées séparés rattachés à la mosquée au sein de la capitale ottomane, désormais nommée Istanbul[86].
Soliman apparaît tout d’abord dans le court métrage Les Trois sultanes, réalisé par Adrien Caillard et sorti le 21 juin 1912 au cinéma. Dans celui-ci, Soliman est incarné par l’acteur français Georges Tréville[87].
La série télévisée Muhteşem Yüzyıl ou Le Siècle magnifique, diffusée de 2011 à 2014 sur la chaîne Show TV, s'inspire également de la vie de Soliman le Magnifique et de Roxelane, sa favorite, ainsi que de la vie de son harem. Dans la série, Soliman est interprété par l’acteur turc Halit Ergenç[88].
En 2012, un documentaire-fiction, intitulé Soliman le magnifique, lui est consacré dans le cadre de l'émission Secrets d'Histoire, présentée par Stéphane Bern. Le documentaire revient sur ses différentes conquêtes, ses trésors et ses mosquées, ainsi sur sa passion pour une esclave slave, Roxelane, qui élimine toutes ses rivales et devient l'épouse du sultan[89].
Soliman peut être incarné par le joueur en tant que dirigeant de la civilisation ottomane dans les jeux vidéo Civilization IV, Civilization V et Civilization VI.
Sur les autres projets Wikimedia : : document utilisé comme source pour la rédaction de cet article.
Pour les articles homonymes, voir Arbitre.
Cet article ne cite pas suffisamment ses sources (août 2011).
Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références » 
En pratique : Quelles sources sont attendues ? Comment ajouter mes sources ?
Le libre arbitre, parfois orthographié libre-arbitre[1], est la faculté qu’aurait l'être humain de se déterminer librement et par lui seul, à agir et à penser, par opposition au déterminisme ou au fatalisme, qui affirment que la volonté serait déterminée dans chacun de ses actes par des « forces » qui l’y obligent. « Se déterminer à » ou « être déterminé par » illustrent l’enjeu de l’antinomie du libre arbitre d'un côté et du destin ou de la « nécessité » de l'autre.
Cet article peut contenir un travail inédit ou des déclarations non vérifiées (16 août 2020).
Vous pouvez aider en ajoutant des références ou en supprimant le contenu inédit. Voir la page de discussion pour plus de détails.
L’expression française de « libre arbitre » correspond aux expressions anglaise Free will et allemande Willensfreiheit, qui présentent cependant le désavantage de dissoudre la notion d’arbitre ou de choix, essentielle au concept. « Libre arbitre » (liberum arbitrium en latin) est le plus souvent utilisé comme la contraction de l’expression technique : « libre arbitre de la volonté ».
De ce concept forgé par la théologie patristique latine, il n’est pas exagéré d’écrire qu'il fut développé pour préciser la responsabilité du mal, en l'imputant à la créature de Dieu. Ceci apparaît dans le traité De libero arbitrio (en) de saint Augustin (Augustin d'Hippone). Ce traité est une œuvre de jeunesse, commencée à Rome vers 388 (Livre I) alors qu'Augustin avait 34 ans (c'est-à-dire deux ans seulement après sa conversion), et achevée à Hippone entre 391 et 395 (livres II et III)[2]. Il décrit le dialogue d’Evodius et d’Augustin. Evodius pose le problème en des termes abrupts : « Dieu n’est-il pas l’auteur du mal ? ». Si le péché est l'œuvre des âmes et que celles-ci sont créées par Dieu, comment Dieu n’en serait-il pas, in fine, l’auteur ? Augustin répond sans équivoque que « Dieu a conféré à sa créature, avec le libre arbitre, la capacité de mal agir, et par-là même, la responsabilité du péché ».
Grâce au libre arbitre, Dieu reste impeccamineux (non coupable) : sa bonté ne saurait être tenue pour responsable d’aucun mal moral. Mais n’est-ce pas déplacer le problème sans le résoudre ? Pourquoi Dieu nous a-t-il conféré la capacité de pécher :
« d’où vient que nous agissons mal ? Si je ne me trompe, l’argumentation a montré que nous agissons ainsi par le libre arbitre de la volonté. Mais ce libre arbitre auquel nous devons notre faculté de pécher, nous en sommes convaincus, je me demande si celui qui nous a créés a bien fait de nous le donner. Il semble, en effet, que nous n’aurions pas été exposés à pécher si nous en avions été privés ; et il est à craindre que, de cette façon, Dieu aussi passe pour l’auteur de nos mauvaises actions (De libero arbitrio, I, 16, 35). »
La réponse d’Augustin est que la volonté est un bien, dont l’homme peut certes abuser, mais qui fait aussi la dignité de l’homme. Qui voudrait ne pas posséder de mains sous prétexte que celles-ci servent parfois à commettre des crimes ? Or, cela est plus vrai encore du libre arbitre : si on peut vivre moralement en étant privé de l’usage de ses bras, on ne saurait jamais accéder à la dignité de la vie morale sans libre arbitre :
« la volonté libre sans laquelle personne ne peut bien vivre, tu dois reconnaître et qu’elle est un bien, et qu’elle est un don de Dieu, et qu’il faut condamner ceux qui mésusent de ce bien plutôt que de dire de celui qui l’a donné qu’il n’aurait pas dû le donner (ibid., II, 18, 48). »
Mais le paradoxe d’Augustin, qui fait aussi sa richesse et qui explique pourquoi il a pu inspirer, au sein du christianisme, des théologies tellement divergentes, tient à la diversité de ses adversaires. S’il affirme, dans le traité De libero arbitrio, l’existence du libre arbitre contre les manichéens qui attribuaient au divin la responsabilité du mal, il tend, contre les pélagiens, à en minimiser le rôle dans l'œuvre du salut, sous prétexte que l’homme a, par le péché originel, perdu l’usage de cette faculté : amissa libertas, nulla libertas (« liberté perdue, liberté nulle »). Seule la grâce, gratuitement octroyée par Dieu, peut alors accomplir l'œuvre du salut. C'est ainsi que le 16e concile de Carthage en 418 affirme les doctrines du péché originel et de la grâce salvifique, approuvées par le pape Zosime.
Cette position paradoxale fait que les Réformateurs et les catholiques pourront, sans contradiction, se revendiquer d’Augustin dans les controverses au sujet du rôle respectif de la grâce et du libre arbitre dans l'œuvre du salut.
La scolastique a considérablement réélaboré ce concept inventé par Saint Augustin, en s’appuyant sur Aristote. Les Grecs ignoraient le libre arbitre, n’ayant pas la notion de volonté mais plutôt celle d’acte volontaire, étudiée au troisième livre de l’Éthique à Nicomaque.
Dans ce livre, Aristote définit le volontaire par l’union de deux facultés : la spontanéité du désir (agir par soi-même), dont le contraire est la contrainte, et l’intentionnalité de la connaissance (agir en fonction d'une cause et en connaissant cette cause), dont le contraire est l’ignorance. Ainsi, j’agis volontairement quand :
Le volontaire suppose ainsi l’union de la spontanéité et de l’intentionnalité ; il est la condition de la responsabilité morale de l’individu (je ne saurais être tenu pour responsable du fait d’avoir quitté mon pays quand j’ai été enlevé par des agresseurs auxquels il m’était matériellement impossible d’échapper, ou quand j’ai franchi par mégarde une frontière qui n’était pas clairement signalée, en ayant eu l’intention de rester sur le territoire national). Ces analyses aristotéliciennes ont été fondamentales pour l’élaboration scolastique du concept de libre arbitre. Les théologiens chrétiens retiendront d’Aristote la notion de libre arbitre comme associant la volonté (spontanéité) et la raison (intentionnalité), et comme fondant la responsabilité de l’individu devant les lois morales, pénales et divines.
La scolastique définit traditionnellement le liberum arbitrium comme facultas voluntatis et rationis (faculté de la volonté et la raison : cf. Saint Thomas d'Aquin, Somme théologique, I, q. 83, a.2, obj. 2). Cette expression est exacte si elle désigne la collaboration de ces deux facultés dans la genèse de l’acte libre, mais erronée en un sens plus technique. À proprement parler, le libre arbitre est une puissance de la volonté (ibid., q. 83, a. 3) ; mieux, elle est la volonté elle-même en tant que la volonté opère des choix. Le libre arbitre, en son essence, n’est autre que la volonté dans la libre disposition d’elle-même ; vouloir, c’est décider librement, et c’est donc être libre. L’acte libre répond au schéma suivant : la volonté éprouve le désir d’un bien (appétition), qui constitue la fin de l’action ; elle sollicite la raison à délibérer sur les moyens de parvenir à ce bien (délibération), mais c’est à elle qu’appartient de choisir le moyen qui lui semble le plus approprié (electio en latin, qui signifie choix) pour parvenir à cette fin, de mouvoir le corps pour mettre en œuvre ces moyens (l’action à proprement parler), et de jouir du bien obtenu (fruition). C’est donc la volonté (plus que la raison) qui joue le rôle moteur et elle ne parviendrait à rien sans le concours de la raison. Dans ce schéma de l’action, le libre arbitre se manifeste tout particulièrement dans le choix, que Thomas d’Aquin définissait comme l'actus proprius (l’acte éminent ou l’acte propre) du liberum arbitrium.
Thomas d’Aquin entend prouver la réalité du libre arbitre par deux moyens.
« L’homme possède le libre arbitre ; ou alors les conseils, les exhortations, les préceptes, les interdictions, les récompenses et les châtiments seraient vains (Thomas d’Aquin, Somme théologique, I, q. 83, a. 1, rép.). »
Le concept de libre arbitre a fait l’objet de trois catégories de critiques, l’une théologique (attribuer à l’homme un libre arbitre, n’est-ce pas nier ou du moins, minimiser le rôle de la grâce divine dans l'œuvre du salut ?), l’autre philosophique (le libre arbitre ne revient-il pas à nier l’influence des motifs ou des mobiles qui déterminent nos choix et nos actions ?), et la dernière d'ordre soit psychanalytique (le libre arbitre n'est possible que si l'on est en mesure de dominer son inconscient) soit de ce que l'on appelle les sciences humaines. La première critique est motivée par le « prédestinationisme » : elle aboutit aux querelles autour de la prédestination caractéristiques de la Réforme dans sa version calviniste. La seconde est motivée par le « nécessitarisme » (mais aussi, dans une mesure plus complexe le « rationalisme »), le fatalisme et le déterminisme.
En sociologie, beaucoup de coercitions (celles de Durkheim) s'opposent à l'idée du libre arbitre. Elles sont de plusieurs sortes :
Ces critiques sont caractéristiques pour une sociologie du dix-neuvième siècle, une discipline qui cherche ses méthodes dans les sciences naturelles, à savoir elle veut expliquer une société par des « lois » sociales.
Le libre arbitre est l’une des deux réponses envisageables à la question du salut (sotériologie) telle qu’élaborée par les théologiens de la Renaissance[3]. L’autre réponse est la prédestination chez Martin Luther, voire la « double prédestination » chez Jean Calvin, théologiens qui par opposition au libre arbitre, défendirent l'un et l'autre la thèse du serf arbitre.
Plus largement, la question du libre arbitre tente de situer le rôle de la volonté humaine dans la conduite d’une vie bonne (susceptible de mener au salut) face à un Dieu conçu comme tout puissant. De cette façon, la question du libre arbitre traverse les trois monothéismes et les réponses que chacun d’entre eux donne méritent l’examen.
Avec l’humanisme, Érasme et Luther partagent le goût de la lecture et du commentaire de la Bible avec le rejet de la glose scolastique. Luther est un « jusqu’au-boutiste » tandis qu’Érasme est un modérateur. Luther espère avoir le soutien d’Érasme, dont l’autorité morale est alors considérable, dans sa querelle contre l’autorité ecclésiastique. Mais les deux hommes vont s’opposer sur le concept de libre arbitre. Érasme soutient le libre arbitre, c’est-à-dire la responsabilité de l’homme devant Dieu concernant ses actes. Au contraire, se fondant notamment sur le dogme du péché originel, le moine augustinien Luther défend la prédestination, c’est-à-dire le serf arbitre et la justification par la foi, chère à Paul de Tarse. Alors, Érasme et Luther perdent toute modération dans leur polémique. Tandis que le frère Martin, en 1519 se disait « admirateur convaincu » d’Érasme, il en viendra à qualifier celui-ci de « venimeux polémiste », de « pourceau d’Épicure* », d’écrivain « ridicule, étourdi, sacrilège, bavard, sophiste, ignorant ».
(* ) Épicure philosophe hédoniste est représenté suivi d’un porc par ses adeptes. Cet animal, sous l’influence biblique sera pris en mauvaise part.
La critique philosophique du libre arbitre tient au rôle des motifs (raisons de choisir) dans la détermination du choix et, par conséquent, de l’action. Suis-je vraiment libre de choisir entre deux objets (et deux fins), l’un qui représente un grand bien, et l’autre, un moindre bien ? De deux choses l’une.
Pour remédier à ce problème, la doctrine de la seconde scolastique a inventé le concept de liberté d’indifférence, dont l'Âne de Buridan est un exemple célèbre. Soit un individu appelé à choisir entre deux biens identiques, et donc indifférents. Il y a ici une équivalence des motifs : rien ne le détermine à préférer l’un à l’autre. Or, la volonté éprouve qu’elle est douée de spontanéité : même en ce cas, elle peut se déterminer à choisir. L’acte ne trouve pas alors son explication dans les motifs, ni par conséquent dans les objets, mais dans le sujet lui-même en tant qu’il est doué d’une capacité à agir arbitrairement. Le concept de liberté d’indifférence établirait, avec la spontanéité de la volonté, la réalité du libre arbitre. Par extension, la liberté d’indifférence s’applique aussi aux cas où il n’y a pas d’équivalence des motifs : je puis fort bien préférer un moindre bien à un plus grand bien, prouvant ainsi que je suis le seul sujet ou la seule cause de mes actes.
Si le thomisme attribue le libre arbitre à Adam, dans le jardin d’Éden, principalement pour lui imputer l'origine du mal par la désobéissance à l'ordre donné par Dieu de ne pas manger le fruit défendu de l'arbre de la connaissance du bien et du mal, qui le rend responsable du péché originel (Genèse, chapitre 3), d'autres philosophes voient les choses d’un œil différent, selon que leur réflexion se situe avant la révolution cartésienne ou après.
Selon Maïmonide :
« Par la raison, l’homme distingue le vrai du faux et ceci a lieu dans toutes choses intelligibles »
— Guide des Égarés, 1re partie, chap. 2, VerdierLe bon et le mauvais, le beau et le laid ne ressortent pas de l’intelligible, du rationnel, mais de l’opinion, du probable.
Tant qu’Adam possédait parfaitement et complètement la connaissance de toutes choses connues et intelligibles, il n’y avait en lui aucune faculté qui s’appliquât aux opinions probables et même il ne les comprenait pas (ibidem). Le bien et le mal n’existaient même pas ; seules existaient les choses intelligibles et nécessaires. La perte de cette connaissance parfaite de toutes choses intelligibles dont lui faisait bénéfice sa fusion avec Dieu fait accéder Adam à un état nouveau, un monde différent :
Pour Spinoza, le libre arbitre est une totale illusion qui vient de ce que l'homme a conscience de ses actions mais non des causes qui le déterminent à agir. En effet, l'homme n'est pas un « empire dans un empire » mais une partie de la substance infinie qu'il appelle Dieu ou la nature.
Cependant, l'homme dispose bien d'une liberté dans la mesure où il comprend avec sa raison pourquoi il agit. Est donc libre celui qui sait qu'il n'a pas de libre arbitre et qui agit par la seule nécessité de sa nature, sans être contraint par des causes extérieures qui causent en lui des passions.
« Si les hommes naissaient libres, et tant qu’ils seraient libres, ils ne formeraient aucun concept du bien et du mal […] [car] Celui-là est libre qui est conduit par la seule raison et qui n'a, par conséquent que des idées adéquates »
— Éthique IV, proposition 68, SpinozaL'homme libre n'a donc aucun concept du bien et du mal lequel est le résultat d'idées inadéquates et confuses, non plus que d'un bien qui lui serait corrélé. Spinoza définit le bien au début de la partie IV de l'Éthique :
« Ce que nous savons avec certitude nous être utile »
— Éthique IV, définition 1, SpinozaRapprochant cette définition de sa Préface et des propositions 26 et 27, son éthique nous renverrait à une éthique des vertus plutôt qu'à un utilitarisme.
Toutefois, observant que les hommes ne sont que des parties de la nature, il en déduit que cette hypothèse d’une liberté de l’homme dès la naissance est fausse. Les parties de la nature sont soumises à toutes les déterminations de celle-ci, et elles sont extérieures à l’homme. Il considère donc que le sentiment de liberté de l’homme résulte du fait qu’il n’a connaissance que des causes immédiates des évènements rencontrés. Il rejette alors le libre arbitre, parlant plutôt de « libre nécessité » (Lettre à Schuller).
Spinoza commente alors ainsi l’épisode du jardin d'Éden.
« C’est cette détermination que semblent signifier les paroles de Moïse dans la fameuse histoire du premier homme […] cette liberté originaire impossible quand Moïse raconte que Dieu interdit à l’homme libre de manger le fruit de la connaissance du bien et du mal et que, dès qu’il en mangerait, il craindrait la mort plus qu’il ne désirerait la vie »
— Éthique IV, proposition 68, scolie, SpinozaCet article provoque une controverse de neutralité (voir la discussion) (novembre 2016).
Considérez-le avec précaution. (Questions courantes)
Les deux disciplines scientifiques qui semblent les plus à même de pouvoir apporter des éléments à la question du libre arbitre sont la physique (qui étudie les lois de la nature) et les neurosciences (qui étudient le fonctionnement du système nerveux et donc du cerveau, organe décisionnel). La physique permet de mieux comprendre la notion de déterminisme tandis que les neurosciences touchent vraiment au libre arbitre.
Aujourd’hui, la physique moderne élimine la connaissance des causes sans faire de l’indétermination quantique la preuve d’un hasard essentiel. La connaissance des causes, même limitée aux causes efficientes disparaît des explications au profit de lois mathématiques prédictives parce que probabilistes et calculables.
« la croyance en la relation de cause à effets, c’est la superstition »
— Tractatus logico-philosophicus, 5.1361, Gallimard, Ludwig WittgensteinEncore que, jusqu’ici, cette affirmation n’est généralisable qu’aux sciences dures où le fortuit désigne ce qui intervient non seulement sans cause finale ou efficiente mais surtout sans loi probabiliste calculable. L’indéterminisme quantique représente la prise en compte des limites de la connaissance : celle d’une limite infranchissable en pratique comme en théorie en ce qui concerne la réalité en soi. Contrairement à la réalité en soi de Kant, cette indétermination ne dégage pas l’espace non-phénoménal d’une liberté : les lois probabilistes s’appliquent au niveau des phénomènes observables. En ce qui concerne le non observable, c’est l’équation de Schrödinger qui en rend compte.
On pense généralement que la croyance dans le libre arbitre fonde à elle seule une éthique de la responsabilité. La psychanalyse considère que la plupart de nos actes dépendent plus de notre inconscient que de notre volonté consciente[réf. nécessaire]. Ce savoir aboutit au paradoxe que les criminels sexuels sont à la fois des criminels susceptibles de rendre des comptes à la justice du fait de leur responsabilité et des malades, commandés par leur inconscient et leurs hormones qui doivent être soignés. La jurisprudence fait entrer ce paradoxe dans son arsenal avec l’injonction thérapeutique où le suivi médical devient une peine.
Dans cette limitation, on rencontre l’intuition de Nietzsche quand décrivant l’éternel retour, il a l’intuition d’une volonté créatrice déterminée par le passé qu’elle tente de justifier :
« Je leur ai enseigné toutes mes pensées et toutes mes aspirations : à réunir et à joindre tout ce qui chez l’homme n’est que fragment et énigme et lugubre hasard, en poète, en devineur d’énigme et rédempteur du hasard. Je leur ai appris à être créateur de l’avenir et à sauver, en créant, tout ce qui fut. Sauver le passé dans l’homme et transformer tout ce qui était jusqu’à ce que la volonté dise : « Mais c’est ainsi que je voudrais que ce fût. Mais c’est ainsi que je le voudrais » »
— Ainsi parlait Zarathoustra, III, 3 - Des vieilles et des nouvelles tables - Mercure de France, NietzscheDans son ouvrage majeur, The Organisation of Behaviour: a Neuropsychological Theory, Hebb argumente fortement en défaveur d'un quelconque libre arbitre. Il explique notamment qu'il n'existe aucune force qui influencerait les neurones et les ferait faire ce qu'ils ne feraient pas sinon. Il exprime aussi une nécessité de cohérence dans les différents champs de la science en disant qu'il est impossible d'être déterministe en physique et d'être mystique en biologie.
Dans le cas où l’hypothèse d’Everett serait fondée — hypothèse selon laquelle existeraient des univers parallèles, ce qui n’est pas établi — tous les futurs possibles (ou plus exactement un nombre de futurs possibles ayant la constante de Planck en dénominateur) à chaque moment de l’univers en chaque lieu se produiraient effectivement : il n’y a pas de hasard quantique ; si une particule semble devoir choisir au hasard entre deux directions, en réalité il existerait un univers dans lequel la particule prend à gauche et un autre dans lequel elle prend à droite.
Sans qu’il soit possible de se prononcer sur la validité de l’hypothèse d’Everett, on peut examiner à titre d’expérience de pensée en quels termes elle influencerait la question du libre arbitre si elle était exacte : dans la mesure où tous les futurs possibles (possibles selon les lois de la physique quantique, ce qui ne signifie donc pas tous les futurs imaginables) se produisent et où chaque observateur situé dans l’un de ces univers improprement nommés parallèles a l’impression d’être le seul, les paradoxes liés au libre arbitre sont levés en niant l'unicité de l'observateur dans le futur (mais non dans son passé actuel, d'où l'asymétrie de ces deux domaines de temps). Une telle négation n'est pas propre à cette thèse et se retrouve chez quelques philosophes contemporains, dont Daniel Dennett, (La conscience expliquée). Voir David Deutsch.
En 2006, les deux mathématiciens John Conway et Simon Kochen ont démontré un théorème appelé « théorème du libre arbitre »[4]. Ils y définissent le libre arbitre d'une entité A comme la capacité qu'aurait A de prendre des décisions non définies par une fonction (au sens mathématique du terme) de l'information accessible à A, c'est-à-dire des observations disponibles dans le « cône de passé » de A.
Le théorème dit alors que si un expérimentateur dispose de ce libre arbitre (dans le sens ainsi exposé), alors les particules élémentaires qui le composent en disposent aussi. Par contraposée, cela signifie que si les particules n'ont pas de "libre arbitre", alors les humains n'en possèdent pas non plus.
Les philosophes[Lesquels ?] considèrent généralement que les expérimentateurs ont assez de « libre arbitre » pour choisir la manière dont ils organisent leurs expériences d'une manière qui n'est pas déterminée par l'histoire passée. Le théorème en déduit que si cela est vrai la réponse des particules n'est pas non plus déterminée par l'histoire passée.
Cet article peut contenir un travail inédit ou des déclarations non vérifiées (17 août 2020).
Conway et Kochen commencent par démontrer que si l'on accepte un axiome appelé Spin - admis en physique quantique car conforme aux résultats de l'expérience - alors une certaine quantité mesurée par ces physiciens ne peut pas pré-exister avant l'expérience, c'est-à-dire qu'elle ne peut être inscrite dans la structure de la particule étudiée. Cela invaliderait donc une conception « réaliste » de l'univers.
On pourrait certes supposer cette quantité instantanément « calculée » à partir de l'information disponible dans l'univers accessible à ces particules juste avant la mesure; toutefois le théorème dit que ce n'est pas le cas si les expérimentateurs disposent d'un libre arbitre et que l'on accepte deux autres axiomes appelés Fin et Twin, eux aussi simples et admis par les physiciens.
Le théorème élimine en ce cas les notions de « variables cachées » (par exemple celle de David Bohm) supposant aux particules des propriétés non directement observables (par exemple position dans une ou plusieurs dimensions surnuméraires) ne se manifestant que lors de la « réduction du paquet d'onde » qui suit une mesure.
Ce théorème du libre arbitre établirait les variables cachées comme inconsistantes avec la relativité restreinte sans faire appel à la mécanique quantique, puisqu'il s'agit d'un simple raisonnement mathématique n'utilisant aucune propriété physique des particules.
Descartes et après lui Laplace imaginaient possible de décrire l'univers comme évolution d'un système à partir d'un état initial et selon des lois déterministes, c'est-à-dire invariables dans l'espace et le temps. Le raisonnement suivi dans la démonstration de Conway et Kochen montre pourtant, sans hypothèse de libre arbitre mais juste les trois axiomes Fin, Twin, Spin, qu'aucune théorie utilisant des lois indépendantes de l'espace et du temps ne peut prédire ne serait-ce que le résultat de certaines mesures de spin sur des particules.
Cela n'invalide pas pour autant le déterminisme : si l'univers est entièrement déterministe, alors il n'y a pas de libre arbitre chez l'homme et le théorème ne s'applique pas. Mais s'il existe un indéterminisme (un libre arbitre) chez les humains, il doit en exister aussi un pour les particules élémentaires.
D'autant que l'existence d'un indéterminisme au niveau des particules élémentaire n'implique pas une indétermination chez l’être humain.
Dans une de ses conférences consacrée au thème du voyage temporel, l'astronome Sean Carroll explique que le concept de libre arbitre n'est qu'une approximation et qu'il est en théorie tout à fait compatible avec le déterminisme. Ainsi, il compare la notion de déterminisme à un garnement qui prétendrait connaître le futur mais refuserait toujours de le dévoiler à l'avance[réf. nécessaire]:
Selon Caroll, le déterminisme n'est donc pas incompatible avec le libre arbitre puisqu'aussi longtemps que nous ignorons ce que nous ferons dans le futur, l'éventail des possibles reste au moins théoriquement réalisable, de telle sorte qu'un futur non-déterministe nous parait tout à fait équivalent.
Sur les autres projets Wikimedia :Notions opposées
Cette page contient des caractères spéciaux ou non latins. Si certains caractères de cet article s’affichent mal (carrés vides, points d’interrogation, etc.), consultez la page d’aide Unicode.
Le bouddhisme est une religion et une philosophie dont les origines se situent en Inde aux VIe – Ve siècles av. J.-C. à la suite de l'éveil de Siddhartha Gautama à Bodhgaya dans le Bihar et de la diffusion de son enseignement.
Les notions de dieu et de divinité dans le bouddhisme sont particulières : bien que le bouddhisme soit souvent perçu comme une religion sans dieu créateur[n 1], cette notion étant absente de la plupart des formes du bouddhisme[n 2], la vénération et le culte du Bouddha historique Siddhartha Gautama en tant que bhagavat jouent un rôle important dans le Theravāda tout comme dans le Mahāyāna, qui voient en ce personnage un être éveillé doté d’un triple corps[n 3].
Le bouddhisme, à travers ses différentes écoles, présente un ensemble ramifié de pratiques méditatives, de rituels religieux (prières, offrandes), de pratiques éthiques, de théories psychologiques, philosophiques, cosmogoniques et cosmologiques, abordées dans la perspective de la bodhi, « l'éveil ». À l'instar du jaïnisme, le bouddhisme est à l'origine une tradition shramana, et non brahmanique comme l'est l'hindouisme[n 4].
En 2018, on compte (mais le chiffre doit être pris avec prudence) quelque 623 millions de bouddhistes dans le monde[1], ce qui fait du bouddhisme la quatrième religion mondiale, derrière (par ordre décroissant) le christianisme, l'islam et l'hindouisme. Toutefois, il pourrait passer de 7 % à quelque 5 % de la population mondiale vers 2060, du fait d'un taux de fécondité relativement bas et d'un nombre de conversions pas assez important[2]. L'historien des religions Odon Vallet relève d'ailleurs que c'est , en raison, notamment, des persécutions menées contre le bouddhisme par les régimes communistes en Chine et en Indochine[3].
Le bouddhisme fait l'objet de critiques de la part de religieux non bouddhistes et de scientifiques.
Originellement, en sanskrit, pour parler de la doctrine du Bouddha, on utilise le plus souvent l'appellation buddhadharma (ou, en pali, buddhadamma), mots signifiant « dharma [enseignement] du Bouddha », à côté d'autres appellations, parmi lesquelles dharmavinaya (enseignement et discipline [vinaya]) et śāsana (enseignements), et par la suite, la traduction de ces termes dans les langues (chinois, japonais, coréen, vietnamien...) des pays où le bouddhisme s'est diffusé et implanté[4],[5].
Le mot « bouddhisme », au sens de « système religieux fondé par le Bouddha en Inde », est un néologisme apparu dans les langues européennes au début du XIXe siècle — et d'abord en anglais, langue dans laquelle on trouve la première occurrence de Boudhism en 1800 ou 1801 puis Buddhism en 1816, mot créé sur Buddha avec ajout du suffixe -ism[6],[7],[8]. C'est dans des revues savantes qu'on le rencontre d'abord, revues elles-mêmes créées à la suite de l'intérêt croissant de l'Empire britannique et de l'Empire français pour l'Orient[9].
En France, c'est vers la fin du XVIIIe siècle qu'apparaissent des termes pour signifier les doctrines propres au bouddhisme: on aura ainsi d'abord budsdoisme (1780), puis bouddhisme (1823)[10]. Michel-Jean-François Ozeray est un des premiers à utiliser en français le mot bouddisme (sic) en 1817[11],[12]. Bouddhisme devient courant dans les langues européennes vers 1830[13],[n 5].
Cette création d'un nouveau mot ne signifie pas que la réalité qu'il recouvre ait été découverte simultanément. À titre d'exemple, deux œuvres médiévales ont permis d'entendre parler un peu, sinon du bouddhisme, en tout cas de Sidhartha Gautama, le bouddha historique: la Vie des saints Barlaam et Joasaph et le chapitre 168 de la Description du monde de Marco Polo, intitulé « Description de l'île de Ceylan »[14].
Reprenant le terme d'« idées reçues » employé par l'historien des religions Bernard Faure dans un titre d'ouvrage éponyme, et constatant [15], on peut s'arrêter sur un certain nombre de ces idées reçues pour appréhender le sujet, comme le fait d'envisager le bouddhisme comme une pratique monolithique ; ou de considérer qu'il s'agirait d'une doctrine essentiellement philosophique et rationnelle, auquel cas, les rituels, la magie, les exorcismes ou encore l'ésotérisme n'y auraient pas leur place[16].
Bien souvent, le bouddhisme est vu comme une sorte de monolithe, et en France essentiellement sous la forme du bouddhisme tibétain, avec le bouddhisme Theravada, ainsi que le bouddhisme zen, tandis que d'autres écoles comme la Terre pure, le Shingon ou le Tendai sont très peu voire pas du tout connues[17]. Et les Occidentaux peuvent penser avoir affaire à des formes du bouddhisme originel (en particulier pour le Theravada), alors que ces formes que nous connaissons aujourd'hui ont toutes traversé les siècles et connu donc d'importantes évolutions. On peut aussi se heurter aux différences dans les pratiques et les croyances entre bouddhistes occidentaux et bouddhistes d'origine asiatique[18].
La longue histoire du bouddhisme, faite de rencontres et de confrontations avec d'autres religions, de réflexions et de controverses au sein des communautés bouddhistes, a abouti à la constitution de nombreuses variations, potentiellement très différentes les unes des autres. 
De grands regroupements ont pu être opérés. Peter Harvey, avec d'autres, met en avant  où le bouddhisme demeure courant : un « bouddhisme du Sud », autour du Theravada, au Sri-Lanka, en Birmanie, en Thaïlande, au Cambodge, au Laos, et dans leur voisinage ; un « bouddhisme de l'Est », autour du Mahayana dans son développement chinois, en Chine, en Corée, au Japon et au Vietnam ; et un « bouddhisme du Nord » dans la région de tradition tibétaine, autour du Mantrayana, au Tibet, en Mongolie, dans l'Himalaya, les régions orientales de la Chine[20],[21].
Plusieurs auteurs parlent à ce propos de  ou de [22]. Faure considère que, , et que par conséquent, c'est bien des bouddhismes qu'il convient de parler. Richard H. Robinson (en), Willard L. Johnson et Ṭhānissaro Bhikkhu (en) proposent qu'il serait plus approprié de concevoir le bouddhisme comme une « famille de religions », autour de ces trois grands ensembles, ayant chacune sa propre intégrité[24]. Harvey, tout en reconnaissant que la métaphore de la famille est pertinente, a souligné que le fait de voir les trois ensembles comme des « mondes » distincts risquait de faire minimiser l'importance des différentes connexions qui existent au sein du « réseau » formé par le bouddhisme, qui lient ses différentes composantes[25].
Dans ces conditions, la tendance peut être de s'en tenir à quelques idées et représentations simples, qui devraient, pense-t-on, être partagées par les membres de tous les courants bouddhistes, depuis les origines. L'unité des différentes traditions bouddhistes est alors assurée par un , qui consiste en  La quête du bouddhisme « originel » a occupé une grande place aux débuts de la bouddhologie, aboutissant à l'image d'un bouddhisme initial rationnel et antiritualiste, qui aurait ensuite dégénéré dans des formes plus ritualistes et superstitieuses, suivant un schéma de pensée de « déclin de la Loi » déjà présent dans les écrits bouddhistes[27], qui a suscité dans diverses communautés bouddhistes un mouvement de retour aux écritures fondatrices (ce qui a pu être qualifié de « protestantisation du bouddhisme »)[28]. Il reste néanmoins impossible pour les spécialistes de s'entendre sur le profil qu'aurait eu le bouddhisme originel, en l'absence de sources écrites remontant à cette époque (les écrits les plus anciens sur la vie et les enseignements de Bouddha qui soient connus auraient été codifiés au plus tôt au Ier siècle av. J.-C.)[29].
Concernant les études sur le bouddhisme actuel, Robinson, Johnson et Bhikkhu identifient des spécialistes qui essaient de définir un « bouddhisme idéal », en procédant de différentes manières, ce qu'ils définissent comme une approche « essentialiste », puisqu'elle recherche l'« essence » du bouddhisme qui est commune à toutes les traditions[24]. B. Faure souligne que le plus souvent est présenté une sorte de bouddhisme censé être « pur », libre de toute « superstition », qui serait arrivé intact dans l'Occident contemporain, après avoir traversé les siècles et les cultures. Or, insiste B. Faure, le bouddhisme est une invention relativement récente, né à la suite de réformes entreprises dans différents pays d'Asie au contact avec l'Occident, à quoi vient s'ajouter un développement moderne connu sous le nom de « néo-bouddhisme », qui, selon Faure, ne garde du bouddhisme traditionnel que des éléments doctrinaux et de pratiques arbitraires[30]. D'autres ont considéré que la recherche d'un bouddhisme « pur » relevait de la gageure car une telle chose n'aurait jamais existé[31],[32].
À l'opposé s'est développée une approche « inclusionniste », qui part des croyances et pratiques de ceux qui se définissent comme bouddhistes[24]. Selon cette seconde posture,  et , et l'historien ou sociologue des religions ne doit pas tenter de prendre parti sur la doctrine. Cette approche a plus tendance à mettre en avant la diversité des pratiques bouddhistes[33], mais elle porte aussi en germe le risque de mettre en avant certaines formes de bouddhisme plutôt que d'autres[24].
Constatant également la difficulté qu’il y a à isoler une « essence » du bouddhisme, certains spécialistes proposent de leur côté d'envisager le bouddhisme comme un « système », complexe par sa diversité, dynamique, ayant des limites poreuses avec les autres religions et idéologies qu'il rencontre[34],[35].
En effet, dans tous les pays où il a pris pied, le bouddhisme a pu coexister avec les autres religions et courants de pensée présents (Brahmanisme/Hindouisme dans le monde indien en Asie du sud-est, Confucianisme et Taoïsme en Chine, Shinto au Japon, Bön au Tibet, etc.), car il se focalise sur le développement spirituel[36]. Plusieurs chercheurs ont souligné qu'il ne s’intéresse pas à tous les domaines couverts par les activités rituelles, ce qui explique que les dieux de ces religions aient pu être vénérés par des bouddhistes, du moment qu’ils étaient invoqués pour des affaires terrestres. En revanche, dès lors qu’il s’agit d’affaires concernant ce qui est transcendant, de leurs préoccupations au moment de la mort, ils se tournent exclusivement vers les enseignements de Bouddha[37],[38]. Selon Williams, Tribe et Wynne, [39].
Avant l’époque moderne, la plupart des Bouddhistes n’ont pas tenté de distinguer ce qui est proprement bouddhiste de ce qui ne l’est pas. Les spécialistes du bouddhisme parlent souvent de « religion populaire » pour les formes de croyances et de pratiques qui ne sont pas spécifiquement bouddhistes mais peuvent être pratiquées par des personnes désignées comme Bouddhistes. Cela regroupe notamment les cultes de divinités locales, les rites de type chamanistique, ainsi que les cultes domestiques, notamment ancestraux. L’emploi de cette notion est controversé, car cela revient là encore à chercher à isoler un bouddhisme « pur » ou « authentique », artificiel, tout en reléguant et dépréciant les autres croyances et pratiques renvoyées dans la catégorie péjorative du « populaire »[40].
Le bouddhisme est-il une religion, une philosophie, les deux, ou encore autre chose ? Le Petit Robert le qualifie de « doctrine religieuse », et le Petit Larousse de religion et philosophie. Autant dire qu'il est difficile de classer ce terme, inventé par les Occidentaux au début du XIXe siècle[15] et que la question suscite la perplexité[41]. En Occident en particulier, beaucoup se basent sur l'absence d'un Dieu éternel, créateur et personnel tel qu'on le trouve dans les monothéismes pour voir dans le bouddhisme une philosophie. Par ailleurs, le mot « religion » est un terme apparu en Occident que l'on appliquerait abusivement à des pratiques et doctrines de l'Inde comme l'hindouisme et le bouddhisme[42]. Terme difficile, voire impossible à définir — du moins n'y a-t-il pas de réel consensus entre spécialistes sur ce qu'on qualifie de « religieux »[43].
Vincent Goossaert, en s’intéressant aux raisons pour lesquelles les personnes se posent la question et choisissent une dénomination plutôt qu’une autre, considère que :  ; selon lui, ces jugements renvoient, 
André Bareau souligne que l'amour de la discussion, de la spéculation intellectuelle pure que l'on dit propres à la Grèce sont tout aussi développés en Inde[45] et David Seyfort Ruegg (en) affirme[46] qu'. Il est indéniable qu'il existe un « bouddhisme philosophique » ou une « philosophie bouddhiste », et que plusieurs docteurs ont produit des réflexions et débats philosophiques de très haut niveau, par exemple Nagarjuna, Vasubandhu en Inde, Fazang en Chine, Kukai et Dôgen au Japon[47],[48]. En cela ils ont pu être comparés aux penseurs de la philosophie chrétienne et de la philosophie juive[49]. Cependant, des spécialistes estiment que le bouddhisme peut bien être considéré comme une philosophie, selon la définition que l'on retient, par exemple M. Siderits en prenant la définition de philosophie comme [50] et D. S. Wright avec la définition d', ce qui ne correspond cependant pas selon lui à l'acception moderne de la philosophie en Occident, plus portée sur la logique et la raison[51]. En effet les œuvres de la philosophie bouddhiste ne s'inscrivent pas dans le cadre de la raison universelle mais restent vouées au but final de la délivrance bouddhique, et pour Faure [52],[53].
Selon Lionel Obadia,  Mais, relèvent certains spécialistes, contrairement à d'autres systèmes religieux, le bouddhisme ne s'appuie pas sur une révélation divine[56],[57], ni sur un Dieu suprême créateur, ni sur des Écritures sacrées, autant d'éléments qui caractérisent communément la « religion » en Occident[58].
Philippe Cornu[59], tout en soulignant qu', appelle cependant à . Car, dit-il :  Le bouddhisme est parfois classé parmi les religions dharmiques[60],[61].
Cependant, relève B. Faure[n 6], le bouddhisme, , a été très souvent considéré par l'orientalisme occidental (né au début du XIXe siècle) avant tout comme une philosophie, les savants européens et américains rejetant les aspects religieux que sont les éléments de rituel[n 7], de mythologie ou de métaphysique[63]. Ce discours fut repris par les élites autochtones, qui cherchèrent à mettre de côté les éléments de la tradition au profit des seuls aspects rationnels philosophiques, psychologiques ou éthiques du bouddhisme[64]. Démarche artificielle qui aboutit à [62]. Car nier les aspects rituels revient à créer un bouddhisme idéalisé qui masque des réalités sociologiques évidentes témoignant de la religiosité dans le bouddhisme en Asie (offrandes, lampes devant les autels, pèlerinages vers des lieux saints, rites funéraires, etc.) et de ce fait selon J.-N. Robert celui-ci a bien le caractère de religion, [65].
Pour Damien Keown (en), se demander s'il est une religion, une philosophie, une manière de vivre ou un code d’éthique oblige à repenser ces catégories, et aussi la signification de « religion ». À faire de la croyance en Dieu l’essence de la religion, on exclut le bouddhisme de cette catégorie. En revanche, avec une définition plus large et complexe — que Keown emprunte à Ninian Smart — intégrant plusieurs « dimensions » (pratique et rituelle, expérimentale et émotionnelle, narrative et mythique, doctrinale et philosophique, éthique et légale, sociale et institutionnelle, matérielle), le bouddhisme est bien, selon lui, une religion[66].
Pour plusieurs chercheurs, le bouddhisme est à la fois une religion et une philosophie[67],[68],[69],[70]. Une pareille affirmation nécessite de reconsidérer ces catégories. Selon M. Siderits, on peut affirmer à la fois qu'il est une philosophie et une religion, sinon cela reviendrait à séparer strictement foi et raison, division que la majorité des bouddhistes rejetterait[71] et qui est en outre propre à l'Occident[70].
Le bouddhisme est également souvent considéré en Occident comme une « spiritualité », ce qui est une autre manière de rejeter la dénomination de « religion », cette fois-ci en mettant en avant l'expérience personnelle plus que la doctrine ou les pratiques[72]. Pour des raisons similaires, le terme de « sagesse » est lui aussi employé[73].
Le bouddhisme a aussi pu être présenté comme [74].
Le contexte culturel de l'Inde du nord à l'époque du Bouddha est marqué par la domination traditionnelle du Védisme, et de sa classe sacerdotale, celle des Brahmanes, qui défend l'autorité des textes sacrés, les Védas, et dispose du monopole sur l'accomplissement des rites, notamment sacrificiels. Mais son autorité est contestée par des groupes de religieux et penseurs, dont les plus radicaux tournent le dos aux traditions védiques, les shramanas, personnages qui ont quitté leur foyer pour mener une vie d'ascèse errante. Les différents penseurs de l'époque ont développé courants originaux se démarquant plus ou moins du védisme. Ce contexte donne notamment naissance aux textes appelés Upanishads, amenés à devenir le fondement de la religion hindoue. Ils sont difficiles à dater précisément, mais il est clair qu'ils ont été élaborés sur une longue période, certains étant antérieurs à l'époque de Bouddha, mais beaucoup lui sont postérieurs. D'autres figures développent des courants spécifiques, comme le Jaïnisme fondé par Mahavira, contemporain du Bouddha, ou l'Ajivika. L'enseignement de Bouddha s'inscrit dans ce contexte et il interagit régulièrement avec des ascètes errants[75],[76],[77].
Au-delà d'un nombre important de divergences, ces nouveaux courants partagent une cosmologie spécifique, qui se met en place à partir des Upanishads les plus anciennes (vers 600-400 av. J.-C.), et rompt avec l'approche des Védas. Selon ces idées communes, les êtres vivants passent par un cycle de renaissances (en sanskrit, saṃsāra), disposent d'une sorte d'âme ou essence individuelle (ātman), qui existe continuellement entre leurs différentes vies, et que leurs conditions de vie sont la conséquence des actes (karma) accomplis durant leurs existences passées et présente. Progressivement apparaît l'idée que le but ultime est la libération (mokṣa) du cycle des réincarnations[78]. L'enseignement du Bouddha prend place dans cette période. De ce fait le Bouddhisme est marqué par ces réflexions, mais il pourrait également les avoir influencé (une partie des Upanishads majeures étant manifestement postérieures à son apparition)[75].
Le bouddhisme est issu des enseignements de Siddhartha Gautama (« l'éveillé »), considéré comme le Bouddha historique.
La vie de Bouddha est documentée par un ensemble de textes, dont les plus anciens ont été mis par écrit vers le Ier siècle de notre ère, soit environ cinq siècles après son nirvana. Ils reposent sur une tradition orale voire des textes plus anciens, disparus depuis, ne présentent chacun qu'un exposé partiel de sa vie et contiennent de nombreux éléments « merveilleux ». De ce fait, si l'existence du Bouddha « historique » n'est pas contestée, la fiabilité de ces sources pour reconstituer sa vie « réelle » est discutée, même si elles sont importantes pour leur valeur exemplaire auprès des fidèles[79]. Mais il est généralement considéré qu'elles présentent suffisamment de points communs pour permettre de dessiner une biographie relativement fiable dans les grandes lignes[75],[80].
Les dates de vie du Bouddha selon la tradition bouddhique vont d'environ 560 à 480 av. J.-C., mais les études actuelles la placent environ un siècle plus tard, avec un nirvana situé quelque part entre 420 et 350 av. J.-C.[81],[82],[83].
Le futur Bouddha, appelé Siddharta[84] (« Celui qui a réalisé son but »[85]) dans certains textes en sanskrit, est né dans le pays de Magadha, dans le clan des Shakya, parmi la lignée des descendants de Gautama[86] (ou Gotama). Cela explique qu'il soit aussi appelé dans les textes Siddharta Gautama, ou Shakyamuni, le « Sage des Shakyas » (plutôt dans la tradition mahayana)[87]. Il a un statut social important, son père Shuddhodana étant un personnage éminent dans le pays des Shakyas. Vers l'âge de 29 ans, bien que marié et jeune père (ou en passe de le devenir), Siddharta est insatisfait par cette vie plaisante et quitte sa famille pour devenir un ascète. Non convaincu par l'enseignement que lui prodiguent plusieurs maîtres et les pratiques ascétiques, il se tourne vers la « voie moyenne » qui renvoie dos-à-dos aussi bien l'opulence que l'ascétisme. Puis il connaît l'« Éveil » sept années après avoir quitté son foyer, ce qui lui confère la condition d’« Éveillé », Bouddha. Il se met ensuite à dispenser ses enseignements, en commençant par son premier sermon, prononcé selon la tradition dans le parc aux Daims de Bénarès devant ceux qui devaient devenir les premiers membres de la communauté bouddhiste. Il y énonce les Quatre nobles vérités, fondements de la doctrine bouddhiste. Il acquiert une réputation importante, et constitue progressivement une communauté de disciples, posant les bases de la discipline bouddhique[88],[89].
Après 45 ans d'enseignements, sa vie s'achève à l'âge de 80 ans, âge auquel survient son nirvana (ou parinirvāṇa) selon la tradition bouddhiste[90].
La tradition bouddhiste relative à la vie de Bouddha, que ce soit par les textes ou les nombreuses images qui se sont développées dans leur sillage, mettent en avant divers épisodes de la vie du personnage fondateur, servant à le glorifier et à avoir une valeur exemplaire pour les Bouddhistes. Ils concernent en particulier les moments-clefs de sa vie : sa conception et sa naissance, son « grand départ » du foyer, son Éveil et le début de son enseignement (la « mise en branle de la roue de la Loi »), puis son nirvana[91],[92],[93]. Un ensemble de récits relate également ses nombreuses vies passées (Jatakas), annonciatrices de son accès au statut de Bouddha[94],[95],[96].
Bouddha est la figure majeure de tous les courants du Bouddhisme, quand bien même on ne le considérait pas comme le seul Bouddha ayant existé. Il est le fondateur, l'exemple par excellence, celui qui est parvenu à l'illumination dans cette période cosmique, puis a dispensé son savoir, montrant ainsi la Voie à suivre. Selon une formule courante prononcée au début de rituels bouddhistes, il est le premier des Trois Joyaux dans lesquels les Bouddhistes prennent refuge[97],[98], celui qui par son enseignement a permis les deux autres, le dharma et la samgha[99],[100].
Bouddha est un objet de vénération de la part des Bouddhistes, aussi bien de façon individuelle (par des offrandes, des prières) que collective (par des fêtes, notamment les célébrations de sa naissance). Même s'il n'est plus présent dans le monde, il est considéré qu'une partie de sa puissance réside dans ses reliques et ses images, ce qui explique notamment le développement de pèlerinages autour de ses reliques et des lieux des épisodes marquants de sa vie[101].
Les sources sur l'évolution de la communauté bouddhiste après le départ de son fondateur sont lacunaires. L'enseignement du Bouddha est d'abord transmis par oral[102]. Il apparaît que la transmission des enseignements du maître fait dès le début l'objet de débats, la tradition retenant la tenue de trois « conciles », le dernier étant organisé par le grand roi Ashoka (v. 273-232 av. J.-C.) de la dynastie des Maurya. Celui-ci passe pour avoir été un fervent bouddhiste, et semble avoir joué un rôle déterminant dans la dissémination de cette religion, devenant le modèle du monarque bouddhiste. Une première rédaction et une organisation du corpus de textes bouddhistes semblerait avoir eu lieu dès cette époque, avec l'apparition des « Trois corbeilles »[103],[104].
L'archéologie et l'étude des inscriptions antiques indique que la communauté bouddhiste s'étend et se structure au moins à partir des IIIe – IIe siècle av. J.-C., et acquiert d'importantes ressources. Des communautés monastiques se retrouvent dans de nombreuses parties du sous-continent indien, et différents groupes bouddhistes distincts sont apparus, les nikāya, au nombre de dix-huit selon la tradition, mais manifestement plus nombreuses dans les faits (une trentaine d'après les travaux des historiens). Les différences doctrinales entre ces groupes ne semblent pas très prononcés, mais elles sont mal documentées. De ces écoles, seule le Theravada devait survivre et se répandre[105],[106]. Puis dans le courant des premiers siècles de notre ère se développe le « Grand Véhicule », Mahayana, qui s'impose comme un courant très dynamique en Inde, au moins à partir du Ve siècle[107],[108],[109]. Vers la même période, une nouvelle émanation du bouddhisme se produit à partir du Mahayana, le Tantrisme, ou « Véhicule du Diamant » (Vajrayana). Il a connu un certain essor en Inde, dans le milieu monastique où il a séduit une frange de l'élite spirituelle, mais a surtout connu le succès au nord, au Tibet (et également en Chine et au Japon)[110],[111].
Tout au long de son histoire, le bouddhisme indien est resté marqué par la diversité : ni le Theravada ni le Mahayana n'y ont atteint une prééminence, et au moins quatre anciennes écoles ont survécu à leurs côtés. Du reste les courants hindouistes, revivifiés par des approches dévotionnelles, sont nettement plus populaires. Après plusieurs siècles de déclin, les monastères bouddhistes du XIIe siècle ressemblent à des tours d'ivoire coupées du reste de la société. La destruction des derniers importants centres bouddhistes lors des conquêtes turques au tournant du XIIIe siècle semble marquer le coup de grâce du bouddhisme indien, qui s'éteint peu après[112].
Le bouddhisme survécut néanmoins dans des régions situées aux marges du monde indien. Sri Lanka dispose probablement de la plus ancienne tradition bouddhiste encore existante, puisque l'implantation de la religion sur l'île remonterait au IIIe siècle av. J.-C. C'est une contrée cruciale pour le succès du Theravada : c'est sur l'île que le canon pali aurait été recopié et compilé vers le Ier siècle, c'est là qu'a été actif un des plus grands auteurs de commentaires des écrits de ce courant, Buddhaghosa (Ve siècle). Mais le Mahayana et le Tantrisme y sont aussi bien représentés durant l'époque pré-moderne. Les cours des rois d'Anurâdhapura et de Polonnâruvâ fournissent un appui important au bouddhisme. Après une période de stagnation, la pensée bouddhiste redevient dynamique sur l'île durant l'époque coloniale, avec la mise en relation avec les religions occidentales qui entraînent d'importantes évolutions (dont la constitution d'un courant surnommé « Protestantisme bouddhiste »). Les cultes hindouistes sont également restés très importants sur l'île. Au XXe siècle les différences religieuses se politisent et s'ethnicisent, dans le contexte de tensions et de conflits internes à l'île : le bouddhisme (theravada) est vu comme la religion des Cinghalais, et la culture de l'élite dominante, tandis que l'Hindouisme est celle des Tamouls, qui sont dans une position de dominés[113].
Le Népal est une autre région du monde indien où le bouddhisme subsiste. Dans les vallées du sud du pays, les Newars comprennent une communauté bouddhiste, rattachée au Mahayana. Dans les régions hautes du nord en revanche, le bouddhisme est dans la mouvance tibétaine, qui s'est également renforcé dans la région de Katmandou après la venue de réfugiés Tibétains. Le Theravada a fait son apparition au milieu du XXe siècle, sous la forme d'approches modernistes influencées par le Sri Lanka[114].
Dans l'Inde indépendante, le bouddhisme connaît un nouvel essor dans le sillage de la conversion de Bhimrao Ramji Ambedkar (1891-1956), un Intouchable qui tourne le dos à l'Hindouisme en raison de son traitement de son groupe social. Avec lui, des centaines de milliers d'Intouchables se convertissent également. Il s'agit officiellement d'une forme de Theravada, mais elle présente de nombreuses originalités[115].
Situées à la jonction de l'Asie centrale, les régions du nord-ouest du monde indien comprennent des foyers importants du bouddhisme antique, notamment la Cachemire et le Gandhara (dans l'actuel Afghanistan). Cette dernière région joue un rôle important dans le développement de l'imagerie bouddhiste, puisque c'est là qu'apparaissent les premières représentations figurées du Bouddha, sous l'influence de l'art grec (art gréco-bouddhiste). Plusieurs rois importants appuient le bouddhisme, les traditions bouddhistes (dont la fiabilité est discutée) commémorant les conversions de grandes figures tels l'indo-grec Ménandre et le kouchan Kanishka. D'importants monastères se constituent dans des sites de l'actuel Afghanistan, comme celui de Bamiyan fameux pour ses Bouddhas monumentaux aujourd'hui détruits. Le Bouddhisme disparaît progressivement de ces régions durant l'époque des premiers royaumes musulmans de la région, entre 700 et 1000, quand ces régions deviennent majoritairement musulmanes[116],[117],[118].
Les voies de la Route de la soie, cruciales pour les échanges matériels et culturels à la fin de l'Antiquité et durant le Moyen-Âge, deviennent un axe de diffusion du bouddhisme. La religion se répand, des monastères se constituent dans différentes cités marchandes, notamment dans le bassin du Tarim (Kashgar, Khotan, Loulan, Kizil, Dunhuang), adaptant l'art bouddhiste dans la région. Les études bouddhistes se développent, concernant le Mahayana et d'autres écoles, et certains des plus brillants moines qui sont nés et formés dans ces régions sont des acteurs majeurs du développement du bouddhisme en Chine (Dharmaraksa, Kumarajiva). Les royaumes turcs, notamment celui des Ouïghours, développent également une culture bouddhiste. La conquête de la région par des royaumes musulmans à partir du Xe siècle entraîne progressivement la disparition du bouddhisme dans ces régions au profit de l'Islam[119],[120].,[121].
Selon un récit semi-légendaire, le bouddhisme est introduit à Luoyang, la capitale de la dynastie des Han postérieurs, en 67 de notre ère. Que cela soit vrai ou pas, il faut attendre la période des Six Dynasties (220-581) pour que le bouddhisme se développe en Chine. La première phase consiste en une introduction de la doctrine et des règles monastiques, depuis l'Asie centrale, grâce à la traduction de textes bouddhistes initiée par des moines venus de ces pays (notamment Kumarajiva, 344-412). Ils y forment des disciples qui s'emparent de cette religion, qui connaît une popularité croissante, au point de devenir l'un des trois principaux systèmes de pensée de l'Empire du milieu, aux côtés du Confucianisme et du Taoïsme. C'est la seule religion étrangère à avoir connu un tel succès dans le monde chinois. De nombreux monastères sont fondés, ils acquièrent d'importantes richesses, de nombreux membres de l'élite chinoise, y compris des empereurs, deviennent de fervents bouddhistes. Des moines chinois voyagent à leur tour jusqu'en Inde, pour y rechercher des textes (Xuanzang, 602-664, Yijing, 635-713)[122],[123].
Le bouddhisme qui s'implante en Chine est pour l'essentiel du Mahayana. Progressivement un bouddhisme proprement sinisé se développe, notamment à la suite de débats et emprunts avec le confucianisme et le taoïsme. Le courant de la Terre pure du Bouddha Amitabha connaît rapidement un succès à l'époque médiévale, à la suite de Huiyuan (334-416). Le Sutra du Lotus connaît un également grand succès, par le biais de l'école Tiantai fondée au VIe siècle par Zhiyi (538-597). L'école Huayan, fondée par Fazang (643-712) se repose quant à elle sur le Sutra de la guirlande (de fleurs). L'émergence du Chan, issu de l'école de la méditation, dont le fondateur supposé est Bodhidharma, conclut la période faste de développement d'écoles bouddhistes chinoises[124]. 
Après avoir connu un apogée au début de la dynastie Tang (618-907), les monastères bouddhistes subissent une importante persécution de la part du pouvoir impérial dans les années 842-845. Cette période marque un tournant dans l'histoire du bouddhisme chinois, dont l'influence en sort affaiblie. Les siècles suivants sont couramment présentés comme un déclin du bouddhisme, qui n'a dès lors plus de position dominante parmi l'élite impériale (sauf durant la domination mongole de 1272-1368) mais cette religion connaît plusieurs phases d'éclat (notamment sous les Song), et reste très importante dans la société et la culture chinoises[125],[126]. 
Les troubles que connaît la Chine durant l'époque contemporaine affectent les institutions bouddhistes, malgré des tentatives de revitalisation au début du XXe siècle. Le régime communiste qui domine la Chine continentale depuis 1949, peu favorable aux religions, impose son contrôle sur les monastères bouddhistes, et cherche à supprimer la religion durant la Révolution culturelle. Depuis les années 1970 le contexte est plus favorable à la reprise du culte bouddhiste. Sur l'île de Taïwan, le bouddhisme est également une religion majeure, ainsi que dans les communautés de la diaspora chinoise (Bouddhisme à Taïwan)[127].
Le bouddhisme à la chinoise se diffuse vers l'est, dans des pays traditionnellement sous l'influence de l'Empire du Milieu. 
La Corée d'abord, au contact direct de la Chine, dont les premiers moines bouddhistes sont formés en Chine. Le bouddhisme prospère sous la dynastie Goryeo (918-1392). Les écoles Huayan, Chan et Tiantai se développent dans la Péninsule, mais aussi une école locale, Jogye, dérivée du Chan et fondée par Jinul (1158-1210). Sous les Joseon (1392-1910) le bouddhisme perd la faveur des élites, qui se tournent vers le confucianisme, et les monastères bouddhistes se replient dans les provinces reculées où ils se consacrent plus à la pratique qu'à l'étude[128]. 
C'est depuis la Corée que le bouddhisme prend pied au Japon à partir du milieu du VIe siècle, dans les cercles de l'élite impériale. Durant l'époque de Nara (710-784) plusieurs écoles bouddhistes se développent (Six écoles de la Capitale du Sud), autour de monastères fondés par la famille impériale ou les lignages les plus éminents. L'époque de Heian (794-1185) voit ensuite le développement du Tendai (variante locale du Tiantai) et du Shingon (école des mantras). Des expéditions sont diligentées en Chine afin de ramener des textes bouddhistes. De puissants monastères sont fondés près de la capitale, le bouddhisme prenant alors surtout pied dans la noblesse. De nouvelles écoles se développent durant l'époque de Kamakura (1185-1333). Les plus répandues sont les courants de la Terre pure : le Jodo-shu fondé par Honen, le Jodo-shinshu fondé par Shinran, et le Jishu fondé par Ippen. La secte du Lotus de Nichiren développe une approche plus radicale. Le Zen, variante japonaise du Chan chinois, qui comprend deux écoles (le Rinzai fondé par Eisai et le Soto fondé par Dogen), est l'autre grand courant qui se développe à cette période[129],[130]. Les cultes présents au Japon avant l'introduction du bouddhisme (ce qui est dénommé Shinto à l'époque moderne) sont combinés aux cultes bouddhistes, et ce syncrétisme est justifié théologiquement (honji suijaku)[131]. Durant l'époque d'Edo (1600-1868) le bouddhisme devient une sorte de religion d’État, mais dans le contexte nationaliste de l'ère Meiji (1868-1911) il est réprimé en raison de son origine étrangère, en même temps qu'est constituée une religion nationale, le Shinto, à partir des cultes traditionnels du Japon expurgés autant que faire se peut des éléments bouddhistes. Après la fin du régime nationaliste en 1945, le bouddhisme japonais traditionnel ne reprend pas son importance passée, mais émergent des nouvelles formes de religion empruntant aussi bien au bouddhisme qu'au shintoïsme (Shinshūkyō)[132].
Les pays d'Asie du sud-est sont sous forte influence indienne durant le Ier millénaire de notre ère, et de ce fait ils adoptent les religions indiennes, bouddhisme et hindouisme (notamment le shivaisme), souvent entremêlées, et surimposées sur leurs propres traditions (souvent désignées comme « animistes »). Cela crée un environnement religieux marqué par l'éclectisme[133]. Cette diversité vaut du reste pour le bouddhisme, qui se diffuse dans ces régions sous différentes formes, avant tout le Theravada et le Mahayana. S'il est souvent difficile de caractériser précisément la religion pratiquée dans la société, les monastères bouddhistes se rencontrent dans les principaux royaumes anciens de ces pays[134] : les royaumes môns de Birmanie[135], l'empire khmer dont le cœur est dans l'actuel Cambodge (Angkor)[136], le royaume du Champa dans le Vietnam central[137], le royaume de Sriwijaya dont le centre est à Sumatra. L'impressionnant sanctuaire de Borobodur, érigé par on ne sait qui sur l'île de Java aux VIIIe – IXe siècle, est la manifestation la plus éloquente de l'expansion du bouddhisme vers le sud-est[138],[139]. 
Dans les premiers siècles du IIe millénaire le bouddhisme Theravada est adopté par plusieurs des royaumes majeurs d'Asie du sud-est, qui sont en relations avec le foyer sri-lankais de cette tradition, et en font une religion officielle servant leur pouvoir. Cela concerne la Birmanie, le Cambodge, et aussi les royaumes thaï qui apparaissent à partir du XIIIe siècle (Sukhothaï, Ayutthaya)[140],[141]. Au-delà du Theravada officiel, le bouddhisme pratiqué dans ces pays garde néanmoins sont caractère éclectique, conservant divers aspects du mahayana et des religions indigènes, et aussi des dévotions hindouistes. Au Vietnam le bouddhisme chinois, mahayana, est très influent, en particulier au nord[142]. Dans la péninsule malaise et en Indonésie l'Islam est devenu la religion dominante et le bouddhisme a quasiment disparu[134],[143]. 
Au Cambodge le bouddhisme a connu une importante répression sous le Khmer Rouge, et connu une reprise lente depuis la fin du régime en 1979[144]. En Thaïlande le pouvoir royal est le garant du bouddhisme theravada et de ses monastères, même si la relation entre les deux a pu être houleuse. Cette religion est vue comme un symbole et un élément de l'identité nationale et de l'unité du royaume[145],[146]. Au Myanmar une situation semblable s'observe, le bouddhisme theravada ayant le statut de religion d’État, placé sous la coupe de la junte militaire qui dirige le pays depuis 1962[147].
Selon la tradition tibétaine, le bouddhisme est introduit dans le pays au VIIe siècle par un de ses plus grands rois, Songtsen Gampo (v. 618-650). Quoi qu'il en soit, les puissants rois tibétains du siècle suivant ont embrassé le bouddhisme, d'inspiration indienne plutôt que chinoise, et de grands monastères sont érigés. Avec le temps c'est la tradition tantrique, vajrayana, qui devient dominante, aux dépens du Mahayana, mais le bouddhisme tibétain est éclaté entre plusieurs courants. Au XIe siècle la venue du moine bengalais Atisha (m. 1054) donne un nouvel essor aux études bouddhistes. Alors que le pays connaît une grande fragmentation politique, les monastères consolident leur puissance, les ordres monastiques tibétains se structurent et un canon bouddhiste tibétain est élaboré. Les chefs de l'ordre Sakyapa établissent des relations privilégiées avec la dynastie des Mongols (dynastie Yuan de Chine, les successeurs de Gengis Khan) et acquièrent une importance politique et religieuse majeure, pour un temps, jusqu'au déclin politique mongol. Le courant des Gelugpa (les Bonnets rouges) est fondé par Tsongkhapa (1357-1419), qui met l'accent sur l'étude, et dont les monastères prennent une grande importance, notamment en tant que lieux d'études, mais aussi en tant que lieu de pouvoir temporel. Les chefs de l'ordre se succèdent par réincarnation Les relations avec les dynasties mongoles restent fortes, et au XVIe siècle, le nouveau maître des tribus mongoles, Altan Khan, intervient dans les affaires tibétaines et décerne le titre de Dalaï-lama (« maître [vaste comme] l'océan »), réincarnation du bodhisattva Avalokitesvara, au chef de l'ordre des Gelugpa. Ce courant devient la première autorité religieuse et politique du Tibet sous la direction de Lobsang Gyatso (1617-1682), qui fait de Lhassa la capitale du pays, avec pour centre le palais du Potala. Les autres ordres monastiques déclinent, parfois à la suite d'une répression[148],[149].
Le bouddhisme tibétain exerce un grand rayonnement dans les pays des steppes de l'Asie centrale, où de grands monastères sont constitués sur le modèle tibétain, avec des abbés se succédant par réincarnation. Les Mandchous qui dominent la Chine durant la dynastie Qing (1644-1911) accordent leurs faveurs au bouddhisme tibétain, qui s'implante dans leur capitale, Pékin (temple de Yonghe)[150],[151].
L'arrivée au pouvoir des régimes communistes s'accompagnent de tentatives d'éradication du monachisme bouddhiste dans ces pays. En république de Mongolie (intérieure), la répression se solde par l'élimination de milliers de moines, l'exil d'autres, et le bouddhisme ne reprend pied dans le pays qu'à partir de la chute du bloc communiste après 1991. Dans les régions de Mongolie extérieure, dirigées par la Chine communiste, les monastères sont contrôlés par le pouvoir comme ceux des autres provinces[152]. 
Au Tibet, l'invasion chinoise en 1950 entraîne l'exil du Dalaï-lama, entraînant avec lui plusieurs dizaines de milliers de personnes. Le bouddhisme tibétain vit depuis en partie en exil, préservant ses traditions et rencontrant un certains succès en Occident. Au Tibet même, la période de la révolution culturelle (1966-1976) s'accompagne de la destruction des institutions monastiques et d'une grande quantité d'écrits et images bouddhistes. Les monastères rouvrent après cette période, mais sont placés sous étroit contrôle par le pouvoir[153].
Le bouddhisme est une voie individuelle dont le but est l'éveil, par l'extinction du désir égotique et de l'illusion, causes de la souffrance de l'homme. L'éveil est une base à l'action altruiste.
Pour les theravādins, l'éveil est la compréhension parfaite et la réalisation des quatre nobles vérités (voir plus bas) ; il s'agit de se réveiller du cauchemar des renaissances successives (saṃsāra).
L'homme éveillé atteint le nirvāṇa (l'illumination), et échappe complètement à la souffrance lors de sa mort (appelée parinirvâna, dissolution complète des cinq agrégats). Le cycle des renaissances et des morts est donc brisé.
Pour les adeptes du Mahāyāna en revanche, l'éveil est la sagesse personnelle et est utilisée pour venir en aide à autrui, par le biais du transfert de mérites et la prise de conscience de sa propre nature de Bouddha (la nature essentielle de tout être possédant une conscience, de tout être vivant[154]).
Il en convient que, le mahāyāna laisse aux bodhisattvas (ceux qui sont éveillés) la possibilité de se maintenir dans le monde sans toutefois produire de karma, par compassion pour les êtres vivants, qu'ils vont alors guider à leur tour vers l'éveil.
Le terme , d'invention occidentale, est communément utilisé pour désigner, d'une façon quelque peu approximative, le , soit buddhadharma en sanskrit, buddhaśāsana en pali, fójiào en chinois, bukkyō en japonais, nang pa sangs rgyas pa'i chos en tibétain[155],[156].
Le Dharma (ou « Loi ») est l'ensemble des enseignements donnés par le Bouddha, qui forment le Canon pali. Toutefois, le terme est polysémique, et il peut signifier « ce qui est établi », « la loi naturelle », « la loi juridique », « le devoir », « l'enseignement » voire « l'essence de toute chose » ou « l'ensemble des normes et lois, sociales, politiques, familiales, personnelles, naturelles ou cosmiques ».
On utilise souvent aussi le mot pali śāsana. En sanskrit, le mot signifie « enseignements, système religieux dans un lieu et à une époque donnés (angl. dispensation) », au sens des enseignements spécifiquement conçus historiquement comme une religion institutionnalisée, ce qu'en Occident on appelle « bouddhisme »[157]. Dans les commentaires pali, ce mot peut sur trois types d'enseignement — dans l'ordre, ceux sur « l'étude des écritures », sur « la pratique » et sur « la réalisation », ceux qui laisse entendre que les textes sont le fondement de l'enseignement du Bouddha, et que sans eux il ne peut y avoir de pratique de l'octuple chemin, et donc pas non plus de réalisation[157].
« La mise en mouvement de la roue de la Loi », le Dharmacakra Pravartana Sūtra, est le premier sermon du Bouddha, donné après qu'il eut atteint l'éveil[158].
Dans le bouddhisme, « prendre refuge dans les trois joyaux », le Bouddha, le Dharma (l'ensemble des enseignements) et le Sangha (l'ensemble des pratiquants, voir plus bas), est une cérémonie par laquelle on devient bouddhiste.
Les quatre nobles vérités indiquent ce qu'il est essentiel de savoir pour un bouddhiste. Elles énoncent le problème de l'existence, son diagnostic et le traitement jugé adéquat :
Les trois caractéristiques ou marques de l'existence, trilakshana[159] (du sanskrit : lakṣaṇa ; pali : lakkhaṇa ; [160]) sont :
Ces trois caractéristiques de l'existence conditionnée se retrouvent dans les quatre sceaux de la philosophie bouddhiste[162]. Elles sont valides en tout temps et en tout lieux, et peuvent être appréhendées par une vision directe de la réalité. Le nirvāṇa, n'étant pas conditionné, échappe aux caractéristiques de souffrance et d'impermanence (il est cependant impersonnel, il n'y a donc « personne » en nirvāṇa).
Le bouddhisme considère qu'il existe trois poisons pour l'esprit :
Certaines écoles en ajoutent deux : la jalousie et l'orgueil.
Selon le Bouddha, les causes de la souffrance humaine peuvent être trouvées dans l'incapacité à voir correctement la réalité. Cette ignorance, et les illusions qu'elle entraîne, conduisent à l'avidité, au désir de posséder davantage que les autres, à l'attachement et à la haine pour des personnes ou des choses.
Sa philosophie affirme que la souffrance naît du désir ou de l'envie. C'est en s'en libérant qu'il serait parvenu au nirvāṇa.
À cause des trois poisons et de l'interdépendance, les hommes sont assujettis au Saṃsāra (le cycle des renaissances). Le « monde » (Loka) dans lequel ils renaîtront après leur mort dépendra de leur karma, c'est-à-dire de leurs actions passées. Cette renaissance ne fait donc que prolonger indéfiniment la souffrance (« la fatigue de remplir les cimetières » dit l'Assu Sutta[163]). Conformément à la philosophie bouddhiste, ce n'est ni le même, ni un autre qui renaît. Ce n'est donc pas, comme dans le principe de la réincarnation, une âme immortelle qui se « réincarne ». En effet, la notion de réincarnation implique l’existence d’une âme immortelle qui entre et sort d’un corps et entre à nouveau dans un autre, mais, selon la croyance bouddhiste, il n’existe rien de tel.
Le Bouddha propose de se réveiller de ce cauchemar, de chasser la confusion et l'illusion pour être illuminé par la réalité. Ainsi, la souffrance et le cycle karmique seraient brisés. Il définit le « but ultime » de son enseignement comme étant « la délivrance », le « dénouement », « la libération de la souffrance » ou nirvāṇa.
Les douze liens interdépendants décomposent le cycle des renaissances selon des liens conditionnés dépendant l'un de l'autre.
Les huit membres du noble sentier octuple (ariyāṭṭaṅgika magga) sont :
Au lieu de « juste » on lit parfois « complet » ou « total ».
Les quatre conduites ou sentiments pieux (brahmavihāra en sanskrit et pali) sont aussi appelés les Quatre Incommensurables car ils pourraient être développés indéfiniment. Cultivées sans l'intention de mener tous les êtres à la libération ultime, ces quatre intentions conduisent à une renaissance dans le monde céleste de Brahmā ; développées avec le désir de mener tous les êtres à la libération ultime, les quatre conduites deviennent alors « incommensurables » et conduisent à « l'éveil parfait ».
Il existe plusieurs méditations (bhāvanā) pouvant développer ces quatre  :
Dans le Theravāda, la vacuité (Śūnyatā) signifie qu'aucune chose n'a d'existence propre[164] (les choses n'existent que par interdépendance). Durant la méditation, la pratique de vipassanā est la contemplation de cette vacuité.
Mais le concept de vacuité, exposé par la littérature dite de la prajnaparamita, et Nāgārjuna, prend un autre sens avec le Madhyamaka. Le Madhyamaka reconnaît l'enseignement de l'interdépendance mais il considère cette roue de la vie elle-même comme vacuité.
Le Canon pāli désigne trois corps de Gautama Bouddha :
Le concept prend de l'importance dans l'école Sarvāstivādin. Mais il acquiert par la suite une signification fort différente.
En effet, dans le Mahāyāna, les trois corps, manifestations d'un bouddha, ne sont pas des entités séparées mais des expressions de l'ainsité (tathatā) qui est une. Ils y sont respectivement :
Dans le bouddhisme, l’éthique est basée sur le fait que les actions du corps, de la parole et de l’esprit ont des conséquences pour nous-mêmes et pour ce qui nous entoure, les autres comme notre environnement. Il existe deux sortes d’actions : les actions kusala (mot pali signifiant sain, habile, favorable, positif) et les actions akusala (malsain, malhabile, défavorable, négatif).
L’éthique bouddhiste propose donc à l'être humain de prendre conscience des états d’esprit dans lesquels il se trouve et à partir desquels il agit, parle, pense et à devenir ainsi responsable tant de ses états d’esprit que des conséquences de ses actions. La pratique de l'éthique est donc une purification du corps, de la parole et de l'esprit.
Elle se décline sous forme de préceptes (pali : sīla) — les cinq préceptes et les dix préceptes sont les plus fréquemment rencontrés — qui ne sont pas des règles absolues mais des principes, des guides de comportement éthique. L'application de certains d'entre eux varie selon les personnes mais aussi selon les traditions.
Ces préceptes sont le plus souvent présentés sous une forme négative en tant qu'entraînement à ne pas faire quelque chose, mais les textes canoniques font aussi référence à leur formulation positive en tant qu'entraînement à faire le contraire. Ces derniers se déclinent en trois groupes qui sont :
Le Saṅgha est la communauté de ceux qui suivent l'enseignement du Bouddha. C'est un des trois lieux de refuge. On distingue le « Noble Saṅgha » (sanskrit Arya Saṅgha) constitué des êtres ayant atteint un haut niveau de libération et le Saṅgha ordinaire, comportant tous les êtres suivant la voie du Bouddha. Le terme est communément utilisé pour désigner des réunions bouddhistes.
Toutes les méditations bouddhistes ont pour but le développement de la « conscience éveillée » ou « conscience sans ego », en utilisant la concentration comme un outil. Mais le bouddhisme comprend de nombreuses voies différentes, qui peuvent toutes être rattachées à ses trois principales branches :
Buddhānusmṛti (en) est une pratique, commune à plusieurs écoles, prenant le Bouddha comme objet de méditation.
Le bouddhisme ancien, appelé parfois bouddhisme hīnayāna (terme sanskrit signifiant « petit véhicule ») par des tenants du grand véhicule, regroupe plusieurs écoles, dont une seule a survécu jusqu'à nos jours, le bouddhisme theravãda. Si plusieurs classifications sont débattues, bouddhistes et chercheurs s'accordent grosso modo à reconnaître dans le bouddhisme dix-huit écoles anciennes.
Le bouddhisme theravāda (en pāli « doctrine des Anciens », sanskrit sthaviravāda) est la forme de bouddhisme dominante en Asie du Sud et du Sud-Est (Sri Lanka, Thaïlande, Cambodge, Birmanie, Laos, parties du Viêt Nam), parmi les Chinois d’Indonésie et de Malaisie ainsi que chez certaines ethnies du sud-ouest de la Chine. Son implantation en Occident est plus récente que celle des courants zen ou vajrayāna.
Comme son nom l’indique, il se veut l’héritier de la doctrine originelle du Bouddha. À cet égard, il est apparenté aux courants définis comme hīnayāna (« petit véhicule ») par le bouddhisme mahāyāna apparu au début de l’ère chrétienne. Hinayāna et theravāda sont des termes souvent employés l’un pour l’autre, malgré les objections de nombreux pratiquants du theravāda. La « doctrine des Anciens » s'appuie sur un canon rédigé en pāli nommé Triple corbeille ou Tipitaka, comprenant de nombreux textes basés sur les paroles du Bouddha, recueillies par ses contemporains mais retranscrites bien plus tard.
 Mahāyāna est un terme sanskrit (महायान) signifiant « grand véhicule ». Le bouddhisme mahāyāna apparaît vers le début de l’ère chrétienne dans l'Empire kouchan et dans le nord de l’Inde, d’où il se répand rapidement au Tarim et en Chine, avant de se diffuser dans le reste de l’Extrême-Orient.
Le Madhyamaka, Chittamatra, Chán (Son en Corée, Zen au Japon), la Terre pure, et le bouddhisme de Nichiren sont des écoles du bouddhisme mahāyāna.
Le vajrayāna est une forme de bouddhisme, nommée aussi bouddhisme tantrique, dont la compréhension peut se faire de façon intuitive ou bien nécessite la maîtrise du mahāyāna et du hīnayāna. Il contient des éléments qui l'apparentent à l'hindouisme et particulièrement au shivaïsme cachemirien. Au Tibet, le vajrayāna et le bön, religion locale, se sont influencés réciproquement.
Son nom sanskrit signifie « véhicule », yāna, de vajra, c'est-à-dire de « diamant » (indestructible et brillant comme l'ultime réalité), et de « foudre » (destructrice de l’ignorance et rapidité fulgurante). On appelle aussi ce véhicule mantrayāna et tantrayāna, puisqu’il fait appel aux mantras et tantras; on trouve aussi le nom guhyayāna « véhicule secret », donc ésotérique (en chinois mìzōng 密宗 et en japonais mikkyō).
Il est surtout pratiqué de nos jours dans la région himalayenne (Tibet, Népal, Sikkim, Bhoutan, aux confins ouest et au nord de la Chine, au nord de l’Inde) et aussi au Japon depuis le VIIe siècle à travers les écoles du Shugendo, du Shingon et du Tendai. C'est la forme de bouddhisme qui caractérise le plus le bouddhisme tibétain. On le trouve aussi en Mongolie et dans quelques régions de la fédération de Russie (oblasts d’Amour et de Tchita, républiques de Touva, de Bouriatie et de Kalmoukie, Kraï de Khabarovsk), ainsi qu'au Japon (Shingon et Tendai, voir Bouddhisme au Japon). Bien que différent d'origine, le Bön tibétain est presque à tous égards un vajrayāna non-bouddhiste.
L'expression bouddhisme tibétain renvoie au bouddhisme vajrayāna qui s'est développé au Tibet. Il y a actuellement quatre écoles principales : Nyingmapa, Kagyüpa, Sakyapa, Gelugpa. Cette dernière est la plus connue en Occident, car le dalaï-lama en est un membre éminent.
Pour les bouddhistes, la mort fait partie du cycle de la vie. Les proches qui restent aux côtés du défunt lors de ses derniers instants n’expriment aucune douleur afin qu’il puisse se séparer de ce monde avec sérénité.
Dans la tradition tibétaine, le corps du défunt ne peut pas être touché durant trois jours et demi, afin que le processus ne soit pas affecté lorsque la conscience quitte le corps. Durant 49 jours après le décès, soit le temps pour que le défunt puisse renaître sous une nouvelle forme, les bouddhistes font des rituels tous les sept jours, dont des prières et des offrandes.
Le zen naît au Japon par l'héritage du chan chinois et du son coréen et s'implante par Bodhidharma 28e patriarche descendant de Bouddha[167] et ce notamment en corrélation de temples ou dojo voués à la pratique des arts martiaux.
Après un voyage d’étude en Chine Eisai (1141-1215) va apporter au Japon la pratique du chan, bouddhisme zen issu de l'école Rinzai[168]. En 1191, il revient au Japon. Dès lors il se heurte aux écoles du bouddhisme japonais apparues aux VIIIe et IXe siècles au sein de l’aristocratie japonaise (tel l’école Tendai, Shingon ou encore celle de la terre pure). En 1199 il quitte donc Kyoto pour la ville de Kamakura où le Shogun et les membres de sa caste de samuraïs accueillent avec enthousiasme ses enseignements zen orientés vers les arts-martiaux. Hôjô Masako, la veuve du Shogun Minamoto no Yoritomo donne à Eisai une autorisation pour construire le premier centre zen à Kamakura le temple Jufuku-ji.
Dès lors Bodhidharma (達磨) appelé Daruma (だるま) (qui vient de Dharma) s'inscrit au cœur de la caste bushido[169]. Ainsi dès les débuts de la période Edo et des 250 ans de paix mis en place par le Shogunat Tokugawa[170], la voie du sabre suivie par les castes de samouraïs s’est forgée plus encore vers le bouddhisme issu du Daruma. Takuan Soho (1573-1645) prélat de la secte Rinzai[171] (auteur notamment de l’Esprit Indomptable, Écrits d’un maître zen à un maître de sabre) côtoya et influença considérablement Yagyu Munenori (Heiho kadensho) et Miyamoto Musashi (Traité des cinq anneaux) le plus célèbre samouraï du Japon aujourd’hui appartenant au trésor national japonais, artiste et philosophe qui représenta à plusieurs reprises le Daruma. Ainsi le Traité des cinq roues apparenté au cinq éléments, godai ((五大) terre, eau, air, feu, vide ou éther) qui jalonnent le bouddhisme est rappelé sur tout le territoire japonais par le gorintō (« stūpa à cinq anneaux »)[172].
Depuis les années 1970, comme dans d'autres pays, le bouddhisme s'est développé en France de façon spectaculaire[173]. Plusieurs maîtres de diverses traditions y ont fondé des centres : Ryotan Tokuda, Taisen Deshimaru ou encore Thich Nhat Hanh pour le Zen et Kalou Rinpoché, Guendune Rinpoché, Dilgo Khyentse Rinpoché, Drubpon Tharchin Rinpoché pour le bouddhisme tibétain.
Arnaud Desjardins a également contribué à faire connaître les enseignements du bouddhisme en France. Plusieurs organisations bouddhistes sont reconnues comme congrégations religieuses par le Bureau central des cultes qui dépend du Ministère de l'Intérieur, selon la loi du 9 décembre 1905 relative à la séparation des Églises et de l'État. À l'instar des religions établies en France depuis plus longtemps, le bouddhisme a également aujourd'hui ses émissions à la télévision.
Selon l'Union bouddhiste de France, il y avait en 1986 environ 800 000 bouddhistes en France dont les trois-quarts seraient d'origine asiatique. Une enquête plus récente, publiée par TNS Sofres, en avril 2007, avance un chiffre de 500 000 adeptes du bouddhisme (âgés de plus de 15 ans), représentant 1 % de la population française de cette tranche d'âge. En 1999, le sociologue Frédéric Lenoir avait estimé à cinq millions « les sympathisants » bouddhistes français[173].
Pagode Chua Tinh Tam, mahayana vietnamien à Sèvres.
La pagode du bois de Vincennes à Paris, principalement partagée entre pratiquants du vajrayana tibétain et du theravada cambodgien.
Stupa de l'Institut Karma Ling, pratiquant le vajrayana tibétain, en Savoie.
Temple bouddhique Linh Son, chan sino-vietnamien, à Joinville-le-Pont.
Bouddhisme et philosophie occidentale entretiennent de nombreux liens. Plusieurs penseurs européens, notamment Arthur Schopenhauer et Friedrich Nietzsche, ont été inspirés par la pensée bouddhiste (et hindoue, par les Upanishad)[174],[175], qu'ils ont néanmoins appréhendée avec un certain nombre de préjugés[176].
Des religieux non bouddhistes et des scientifiques émettent des critiques à l'encontre du bouddhisme.
Les jaïns, dont la religion est basée sur l'existence de l'âme ou atman, considèrent que le bouddhisme ne respecte pas la non-violence (ahimsa) : en effet, un fidèle bouddhiste ne doit pas commettre de violence lui-même mais peut, par exemple, manger de la chair d'un animal tué par un autre ; cette attitude est condamnée par le jaïnisme, qui promeut une non-violence obligatoire pour ses disciples, exigeant de s'abstenir de la violence de neuf façons : par la pensée, par la parole et par le corps et, à chaque fois, soit personnellement (krita), soit en le commandant à d'autres (kârita), soit en consentant à son exécution par d'autres (anumodita)[177].
Si les différentes branches du bouddhisme et de l'hindouisme considèrent que la compassion (karuna) est une vertu cardinale (commune autant aux gens vivant dans la société qu'à ceux qui ont renoncé au monde)[178], il n'en reste pas moins qu'il y a des divergences métaphysiques entre le « bouddhisme » et l'« hindouisme » (différences qui n'étaient pas originellement si prononcées[n 8]) ; ainsi, le bouddhisme s'est vu critiqué par les philosophies hindoues Vaisheshika et Nyâya : « Le Vaisheshika-sutra semble s'opposer radicalement au bouddhisme par sa conception réaliste et substantialiste du cosmos et de l'homme »[179], et la philosophie Nyâya considère la notion bouddhiste d'anatman (non-Soi) comme étant illogique (par exemple, se remémorer d'un objet est impossible s'il n'y a pas un âtman (Soi connaisseur) permanent) et que la Totalité est une réalité alors que le bouddhisme affirme l'inverse :
« Tandis que le Bouddhisme pense que le tout n'existe pas, que les parties seules existent — mais pas en l'état de parties ! — alors que la doctrine védique est que le tout est plus ou moins différent de la somme des parties »
— Michel Angot, Le Nyâya-sûtra de Gautama Akshpâda, et Le Nyâya-Bhâshya d'Akshapâda Pakshilasvâmin[180].Akshapâda Pakshilasvâmin, dans son Nyâya-Bhâshya, a réfuté les thèses de la vacuité (Śūnyatā), de l'impermanence (Anitya) et du non-Soi (Anātman).
Dans son ouvrage L'infini dans la paume de la main[181], l'astrophysicien Trinh Xuan Thuan évoque deux points de discorde entre la vision bouddhiste et la vision scientifique du monde.
Il explique que l'univers décrit par le bouddhisme est un univers cyclique qui n'a ni commencement ni fin et serait donc traversé d'une série sans fin de big bang et big crunch. Or l'avènement d'un big crunch n'est pas confirmé par les données actuelles de la science qui établissent que l'univers ne contient pas assez de matière pour le générer[réf. nécessaire]. Le modèle actuel est au contraire celui d'une expansion infinie de l'univers[réf. nécessaire] ce qui est en contradiction avec la conception d'un univers cyclique.
Dans ce même ouvrage Trinh Xuan Thuan évoque le concept bouddhiste de flots de consciences coexistants avec l'univers matériel de tout temps. Il explique que pour beaucoup de neurobiologistes la conscience est une propriété émergente de la matière vivante qui aurait passé un certain seuil de complexité. Le fait que la conscience ait pu préexister à la matière ou en dehors de celle-ci n'est pas prouvé.
Cet article concerne les individus mâles de l'espèce humaine. Pour l'être humain, voir Homo sapiens. Pour les autres significations, voir Homme (homonymie).
Cet article ne cite pas suffisamment ses sources (janvier 2019).
Un homme est un être humain adulte de sexe ou de genre masculin. Conformément à son étymologie latine, le mot est aussi employé pour désigner de manière générale l’espèce Homo sapiens, parfois distingué par l'emploi d'une majuscule (). 
En latin classique, le mot « homo » désigne l’être humain[1], tandis que « vir » désigne l'individu masculin[1]. À partir de l'époque impériale, l'usage militaire du terme homo fait qu'il tend progressivement à remplacer vir pour prendre aussi le sens d'« être humain du sexe masculin »[réf. souhaitée]. Toutefois, l'usage général demeure, et se retrouve par exemple dans la Déclaration des droits de l'homme et du citoyen de 1789, qui concerne tous les citoyens sans distinction[réf. nécessaire]. Depuis le XXe siècle, cet usage général est le plus souvent distingué par l'emploi d'une majuscule : .
Le mot latin homo dériverait du thème indo-européen ghem-, ghom-, ghm-, qui serait le principal nom de la terre. L'être humain serait donc sorti de la terre, de l'humus, mot qui provient de la même racine[2].
Le mot homme désigne en français à la fois l'être humain[3],[4],[5], considéré de façon générale, et l'être humain masculin ayant acquis une certaine maturité physique et morale, le mari ou bien l'individu considéré par rapport à son activité[3],[6]. Par distinction, l'homme prépubère est appelé « garçon », tandis que l'individu féminin adulte est appelé « femme », ou « fille » quand elle est enfant.
Les études de genre, en différenciant le sexe du genre, définissent l’homme selon que la personne s’identifie et se genre au masculin[7],[8].
Chez les mammifères, les hormones qui influencent le développement et la différenciation sexuelle sont les androgènes, (principalement la testostérone, qui stimule le développement ultérieur de l'ovaire). Dans l'embryon sexuellement indifférencié, la testostérone stimule le développement du canal de Wolff, le pénis et la fermeture des replis labioscrotaux. Une autre hormone importante dans la différenciation sexuelle est l'hormone anti-müllerienne, qui inhibe le développement du conduit de Müller.
Dans l'humanité, le sexe d'un individu est généralement déterminé au moment de la fécondation par le matériel génétique porté par le spermatozoïde. Si un spermatozoïde porteur d'un chromosome X féconde l'ovule, la progéniture sera généralement femelle (porteuse de chromosomes XX) ; si un spermatozoïde porteur d'un chromosome Y féconde l'ovule, la progéniture sera typiquement mâle (XY). Les personnes dont l'anatomie ou le maquillage chromosomique diffèrent de ce modèle sont appelées intersexes.
C'est ce que l'on appelle le système de détermination du sexe XY, qui est typique de la plupart des mammifères. D'autres systèmes de détermination du sexe sont recensés, dont certains sont non génétiques.
Les caractères sexuels primaires indiquent le type de gamète qui sera produit : l'ovaire produit des ovules chez la femelle, les testicules produisent des spermatozoïdes chez le mâle. Les caractéristiques sexuelles secondaires regroupent toutes les autres distinctions sexuelles qui jouent des rôles indirects dans l'unification du sperme et des ovules. Il s'agit de tout ce qui est issu des spécificités masculines et féminines, du tractus génital, jusqu'au plumage brillant des oiseaux mâles ou de la pilosité faciale humaine, en passant par les caractéristiques comportementales telles que la cour.
Les hommes ont en moyenne une taille, une masse et  une puissance musculaire supérieures à celle des femmes. Leur système pileux est en général plus développé : leur caractère sexuel secondaire le plus visible est d'ailleurs souvent la barbe. Les hommes ont en général une espérance de vie plus courte que celle des femmes.
Les humains présentent un dimorphisme sexuel pour de nombreuses caractéristiques, dont beaucoup n'ont pas de lien direct avec la capacité de reproduction, bien que la plupart de ces caractéristiques aient un rôle dans l'attirance sexuelle. La plupart des expressions du dimorphisme sexuel chez l'humain se trouve au niveau de la taille, du poids et de la structure du corps, bien qu'il y ait toujours des exemples qui ne suivent pas le modèle global. Par exemple, les hommes ont tendance à être plus grands que les femmes, mais beaucoup de personnes des deux sexes sont dans la gamme moyenne pour l'espèce.
Parmi les caractères sexuels secondaires acquis par les garçons devenant hommes, on peut citer :
Les approches de la psychologie masculine, du fonctionnement psychique de l'homme sont multiples.
Par exemple, parmi les disciplines ayant décrit la construction de la masculinité, une des approches est la psychologie analytique. Selon cette théorie, l'intégration des caractères masculins se fait par l'acceptation de sa féminité par l'homme. Tout comme la femme qui doit accepter son masculin. Ce processus se fait avec difficulté pour les deux genres. Ainsi pour le célèbre psychanalyste Carl Gustav Jung, l'homme a sa part de féminité, qui se nomme l'Anima.
Comme le fait remarquer Richard von Krafft-Ebing dans Psychopathia sexualis (1886) la sexualité des individus masculins est sujette à un nombre très grand de déclinaisons et de comportements. Krafft-Ebing propose que le conditionnement social et le développement de chaque individu se combinent pour produire des variations importantes dans l'expression de la sexualité et des fantasmes. Les hommes semblent associer la sexualité à des affects parfois durables, voire à de l'attachement pour certains partenaires (environ 73 % des hommes en France déclarent être en couple). La présence de ces affects n'est cependant pas systématique, certains hommes prétendant que leurs affects et leur sexualité constituent, dans leur perception, des choses distinctes.
Les études sur la prostitution montrent que « l'immense majorité des clients de la prostitution sont des hommes »[10],[11]. En France, selon les chiffres de la proposition de loi de 2013 visant à sanctionner les clients de prostituées, 99 % des clients sont des hommes alors que 85 % des personnes prostituées sont des femmes[12].
Selon un rapport de 2014 de l'Office des Nations unies contre la drogue et le crime (ONUDC), sur les 437 000 personnes assassinées dans le monde en 2012, 95 % des auteurs étaient des hommes qui représentaient aussi 80 % des victimes. Quand les homicides ont lieu dans le cadre de violences familiales, soit dans 15 % des cas, 70 % des victimes sont des femmes. À l'inverse des femmes, qui ont un plus grand risque d'être tuées par des connaissances, les hommes sont majoritairement tués par des inconnus. Un meurtre sur sept commis dans le monde est celui d'un très jeune homme[13].
Selon un rapport de 2010 de l'Office des Nations unies contre la drogue et le crime et une étude de l'Organisation mondiale de la santé de 2017, la plupart des viols sont commis par des hommes et la plupart des victimes sont des femmes, la même proportion existant pour les violences conjugales et les agressions sexuelles[14],[15].
Selon l'Office des Nations unies contre les drogues et le crime (ONUDC), la grande majorité des individus détenus en prison dans le monde sont des hommes qui représentent 93% de la population carcérale. De 2000 à 2020, leur nombre a proportionnellement moins augmenté que celui des femmes, avec une hausse de 25% contre 33% pour les femmes[16].
La masculinité prend ses racines dans la génétique (voir différences sexuelles chez l'humain)[17],[18]. Par conséquent, bien que la masculinité soit différente selon les cultures, ses définitions comportent quelques éléments communs[19]. Parfois, les universitaires du genre utiliseront l'expression « masculinité hégémonique »  pour distinguer la forme dominante par rapport aux variantes[20].
Le machisme est une forme de culture masculine. Selon le sociologue Alfredo Mirandé, professeur à l'université de Californie à Riverside[21], il inclut l'affirmation de soi ou la défense de ses droits, de sa responsabilité, de son désintéressement, de son code d'éthique, de sa sincérité et de son respect[22].
Le taux de masculinité est la proportion d'hommes dans la population totale des deux sexes. L'expression taux de masculinité est issue de la démographie.
En se limitant aux nouveau-nés, on observe un taux de surmasculinité à la naissance compris entre 1,03 et 1,07. En France, le taux de surmasculinité à la naissance est de 1,05.
L'anglicisme Sex ratio (SR), est défini comme le rapport du nombre d'hommes au nombre de femmes, pour une tranche d'âge donnée. Le taux de surmasculinité à la naissance est donc le Sex ratio à l'âge 0, ou SR0.
Pour des raisons biologiques et sociologiques, il est déjà bien différent pour les enfants de moins de 5 ans (le SR5). À partir de 30 ans (entre 35[23] et 49 ans[24] en France, contre 25 en 1950), la prépondérance s'inverse et le nombre de femmes l'emporte généralement sur le nombre d'hommes (huit centenaires sur dix sont des femmes), malgré de notables disparités régionales.
La fonction de Pape, chef de l'Église catholique est réservée aux hommes. Certaines fonctions de l'Église catholique (pape, cardinal ou évêque) ne sont accessibles qu'aux hommes. La prêtrise est également exclusivement masculine.
Les hommes sont parfois donnés prioritaires pour le poste de monarque dans les pays utilisant les règles de la primogéniture agnatique.
Le mot socialisme recouvre un ensemble très divers de courants de pensée et de mouvements politiques[1], dont le point commun est de rechercher une organisation sociale et économique plus juste. Le but originel du socialisme est d'obtenir l'égalité sociale, ou du moins une réduction des inégalités[2] et, notamment pour les courants d'inspirations marxiste et anarchiste, d'établir une société sans classes sociales. Plus largement, le socialisme peut être défini comme une tendance politique, historiquement marquée à gauche, dont le principe de base est l'aspiration à un monde meilleur, fondé sur une organisation sociale harmonieuse et sur la lutte contre les injustices. Selon les contextes, le mot socialisme ou l'adjectif socialiste peuvent qualifier une idéologie, un parti politique, un régime politique ou une organisation sociale. La notion de socialisme s'exprime également par une forme spécifique de morale sociale laïque et non-religieuse véhiculant des valeurs morales individuelles et collectives.
Le mot socialisme entre dans le langage courant à partir des années 1820, dans le contexte de la révolution industrielle et de l'urbanisation qui l'accompagne : il désigne alors un ensemble de revendications et d'idées visant à améliorer le sort des ouvriers, et plus largement de la population, via le remplacement du capitalisme par une société plus juste. L'idée socialiste, sous de multiples formes, se développe au long du XIXe siècle et donne naissance dans le monde entier à des partis politiques s'en réclamant sous diverses dénominations (socialiste, mais également social-démocrate, travailliste, etc.)[3].
Au tournant du XXe siècle, le marxisme supplante progressivement l'anarchisme puis l'approche dite du  : le courant de pensée marxiste se veut porteur d'une forme  de socialisme, fondé sur une analyse du capitalisme, du dépassement de celui-ci par le biais de la lutte des classes et du passage à la propriété sociale des moyens de production[4]. Dans les dernières années du siècle, une partie du socialisme européen s'oriente cependant dans les faits vers le réformisme. À la fin de la Première Guerre mondiale, la famille politique socialiste connaît un schisme avec la naissance du courant communiste, qui continue de se réclamer du socialisme en affirmant le ramener à la tradition révolutionnaire. Les partis socialistes connaissent dans le monde entier des scissions au cours des années 1920 ; ils se trouvent dès lors en compétition avec des partis communistes qui se réclament du  (ou [5]) appliqué par l'Union des républiques socialistes soviétiques (URSS), cette dernière s’étant auto-proclamée [6],[7].
La plupart des partis socialistes européens accélèrent, notamment après la Seconde Guerre mondiale, leur évolution vers un réformisme éloigné du marxisme, tandis que les régimes communistes alignés sur l'URSS, et qui se disent eux-mêmes socialistes, se multiplient dans le monde. Le socialisme démocratique, c'est-à-dire un socialisme converti à la démocratie libérale et respectueux du jeu parlementaire, représente aujourd'hui la tendance majoritaire des partis socialistes, qui n'envisagent plus la rupture avec l'économie de marché. La notion de socialisme démocratique est par ailleurs désormais associée à celle de social-démocratie qui tend, notamment en Europe, à en devenir un synonyme[8]. Outre les diversités liées à ses variations idéologiques, le socialisme connaît également de nombreux avatars liés aux contextes géographiques et culturels, comme le socialisme arabe ou le socialisme africain.
La mouvance socialiste, prise au sens large, demeure aujourd'hui l'une des plus importantes de la vie politique mondiale, bien que le mot socialisme continue de recouvrir un ensemble de réalités, de pratiques politiques, et de formes de pensée extrêmement diverses et parfois contradictoires entre elles, allant des partis travaillistes aux diverses variétés de [1], en passant par les partis et régimes communistes actuels. La majorité des principaux partis se réclamant du socialisme se réunit, au niveau international, au sein de l'Internationale socialiste et, au niveau européen, au sein du Parti socialiste européen. Ces organisations n'ont cependant pas le monopole de l'usage de l'appellation socialiste.
Socialisme et socialiste sont des termes qui, en raison de leur connotation, ont été revendiqués et diffusés depuis leur création dans de nombreux contextes, et ont acquis une mosaïque de significations différentes, bien que des lignes directrices communes s'en dégagent. Le socialisme naît d'une philosophie de l'histoire occidentale, qui repose sur l'idée de progrès, c'est-à-dire de la transformation du monde dans un sens positif[3] : dans son acception la plus large, il condamne les inégalités sociales et l’exploitation de l’homme par l’homme[9] et défend le progrès social[10]. Le Grand Larousse encyclopédique définit le socialisme comme une [11]. 
Les universitaires Georges Bourgin et Pierre Rimbert le présentent comme « une forme de société dont les bases fondamentales sont les suivantes : propriété sociale des instruments de production ; gestion démocratique de ces instruments ; orientation de la production en vue de satisfaire les besoins individuels et collectifs des hommes »[12]. Sur le plan économique, le mot socialisme désigne à l'origine un ensemble de doctrines fondées sur la propriété collective — ou  — des moyens de production, ou du moins la critique de la propriété privée de ceux-ci[13],[14],[15], par opposition à la vision capitaliste. Élie Halévy résume le socialisme par la possibilité de  : ainsi défini, le socialisme est vu comme un système de valeurs opposées à celles du libéralisme. Dès le XIXe siècle, cependant, des écoles de pensée ont tenté de concilier libéralisme et socialisme, en conciliant les valeurs de la solidarité avec les principes de la liberté, dans le cadre d'une relation critique avec le libéralisme économique[16].
Sur le plan politique, le socialisme s'affirme en Europe au XIXe siècle, en même temps que le libéralisme et l'aspiration démocratique. Si le socialisme se veut porteur d'une démocratie universelle, socialisme et démocratie ne sont cependant pas synonymes sur les plans politique, économique et social. En effet, de par ses contraintes particulières, le socialisme peut se trouver en contradiction avec la démocratie[17].
La revendication commune de la filiation socialiste par des tendances politiques souvent opposées entre elles a accentué la polysémie du terme, mais aussi favorisé les confusions. Ainsi, le fait que les sociaux-démocrates et les communistes se réclament du socialisme a permis aux adversaires des mouvements socialistes de pratiquer, au XXe siècle, des amalgames entre le réformisme socialiste et le communisme révolutionnaire et autoritaire[18].
Bertrand Russell décrit en 1918  comme étant [19].
Karl Marx et Friedrich Engels utilisent alternativement les mots  et  pour désigner la société sans classes qui naîtra après la révolution et le renversement du capitalisme[20] : Marx ne définit pas avec précision ce que serait une société socialiste post-révolutionnaire : il se borne à des formules générales ou abstraites, indiquant qu'elle sera basée sur la liberté et le développement humain[21]. Le Manifeste du Parti communiste parle ainsi d'« une association où le libre développement de chacun est la condition du libre développement de tous ». Émile Durkheim définit le socialisme comme étant, sur le plan économique, , et visant à instaurer une forme d'organisation sociale dont l'amélioration du sort des travailleurs ne sera que l'une des conséquences[22].
Selon Durkheim, le socialisme, relevant d'une certaine sensibilité morale, est d'abord un idéal et non le produit d'une démarche scientifique : pour lui, le socialisme se caractérise surtout par . Joseph Schumpeter donne du socialisme une définition strictement économique et assimilée aux conceptions marxistes, en le décrivant comme [23]. Le socialisme, sous cette acception, s'oppose donc au libéralisme économique classique en ce qu'il ne croit pas au laissez-faire et à l'autorégulation du système économique par la seule recherche de l'intérêt personnel et par la liberté individuelle, dont la quête ne suffirait pas à aboutir à l'harmonie des intérêts. Les degrés de régulation dans l'économie sont cependant nombreux : le collectivisme économique est longtemps apparu comme une condition sine qua non du socialisme, mais l'appellation de socialiste (ou de social-démocrate) est aujourd'hui revendiquée par des courants qui acceptent l'économie de marché et prônent un contrôle de l'économie qui n'irait pas jusqu'à la collectivisation[22].
L'historien Albert Samuel définit le socialisme, au sens large, comme [23].
Le théoricien Francesco Merlino donne quant à lui une définition avant tout philosophique du socialisme, distinguant le , c'est-à-dire les tentatives concrètes de mise en œuvre, du , c'est-à-dire celui des théoriciens : pour lui, l'essence du socialisme est à chercher dans une aspiration à l'égalité des conditions et au bien-être pour tous. Dans cette optique, le socialisme se définit donc moins par le biais de telle ou telle théorie que comme une aspiration à la dignité et à la justice sociale[24].
Sur le plan politique, le terme de socialisme désigne l'ensemble des pratiques mises en œuvre par les partis et autres groupements qui s'en réclament. Si le socialisme ne se prête pas à une définition unique, il se distingue par un système de valeurs dont le principe central est que les relations collectives et la justice sociale doivent l'emporter sur les actions et les intérêts individuels : en ce sens, il constitue une réaction contre la logique libérale apparue au XVIIIe siècle, tout en constituant le produit du contexte économique et politique né de la révolution industrielle[1]. Selon les pays et en fonction de son implantation au cours de l'histoire, le courant socialiste a pu être représenté par des partis utilisant des appellations comme social-démocrate - le terme social-démocratie est devenu, dans certains pays, notamment ceux de culture allemande ou scandinave, un synonyme de socialisme au sens d'organisation partisane - ou travailliste[22].
L'expression socialisme démocratique est utilisée par opposition aux formes  de socialisme, mais l'expression a pu recouvrir des sens différents selon les contextes historiques[8]. L'appellation socialiste a été par ailleurs employée au fil des décennies par des tendances aussi diverses que l'anarchisme socialiste ou le socialisme libéral, voire le national-socialisme (ou nazisme). Le courant socialiste est par définition complexe, divers et contradictoire : la qualité de socialiste est revendiquée par des courants parfois très opposés les uns aux autres, y compris par des mouvements qui emploient le mot  en le détournant de son sens originel, et que l'historien Gilles Candar qualifie d'[25]. L'adjectif socialiste, bien qu'étant couramment associé de nos jours à des partis modérés, continue ainsi d'être utilisée par des mouvements d'extrême gauche : au Royaume-Uni, par exemple, la qualité de socialiste est revendiquée aussi bien par un parti de centre gauche comme le Parti travailliste[26] que par un parti trotskiste comme le Parti socialiste des travailleurs. Le socialisme apparaît également, dans certains cas, dans le vocabulaire de mouvements d'extrême droite ou de dictatures militaires. En ce qui concerne les régimes politiques, l'application d'une politique socialiste a été et est toujours revendiquée aussi bien par des gouvernements librement élus et respectant les règles de la démocratie parlementaire, que par des pays classés comme dictatoriaux, voire totalitaires.
Au sein de la famille de pensée socialiste elle-même, le mot socialisme prend des sens multiples. Dans une optique radicale, voire révolutionnaire, le socialisme ne peut être que l'abolition complète du capitalisme et son remplacement par une société socialiste. Dans son acception marxiste, puis léniniste, puis marxiste-léniniste, le socialisme correspond à une forme de société mise en place via la lutte des classes et caractérisée par une dictature du prolétariat, étape que Marx conçoit comme une phase transitoire de dictature révolutionnaire destinée à mettre à bas le pouvoir de la bourgeoisie fondé sur le capitalisme. Sur le plan économique, la dictature du prolétariat se traduit par la suppression de la propriété privée des moyens de production (bien que la consommation des biens de consommation reste privée)[27]. Cette phase, dite , correspondrait à celle de l'instauration d'un socialisme d'État, soit en l'occurrence d'un collectivisme économique, via un processus dit de socialisation des biens[28]. Après cette phase de socialisme et de dictature du prolétariat, considérée par Lénine comme étant celle où le pouvoir d'État subsiste sous la forme d'un [29], la société passe ensuite à la phase, dite , du communisme intégral, soit celui de la société sans classes et du dépérissement de l'État, appelé à disparaître tout à fait (cette disparition étant conçue comme un processus naturel, en opposition à la conception anarchiste qui préconise la fin de l'État comme effet d'une décision volontaire). La société fonctionne alors selon l'adage [28]. Dans l'optique de la théorie marxiste, la société communiste, dont l'avènement est considéré comme inéluctable, constitue donc le dernier stade de l'évolution d'une société socialiste, où l'homme devient un , soit un homme  libéré de l'aliénation[30].
Dans le vocabulaire communiste, le mot socialisme est utilisé pour désigner la réalité effective de l'État et de la société en URSS, puis après 1945 dans les autres régimes communistes. Selon le discours officiel pratiqué en Union soviétique — puis dans les régimes dits de  — le stade du socialisme est considéré comme atteint au moment de la disparition de l'économie capitaliste et des couches sociales qui en sont la base. La question du socialisme devient dès lors celle de son , c'est-à-dire du renforcement du secteur collectif de l'économie[31].
Les régimes politiques couramment appelés communistes se sont présentés en conséquence comme appliquant un , ou étant parvenus au stade du , leur modèle étant l'URSS, proclamée [7]. Les différents régimes communistes ont pu ainsi, à divers stades de leur histoire, proclamer que le stade du socialisme — ou de la , objectif considéré comme plus réaliste que celui du communisme intégral — était atteint (en accompagnant souvent cette annonce d'un changement de nom officiel ou d'une modification de la constitution), ce discours étant destiné à légitimer le maintien des partis communistes au pouvoir[32].
Les sens prêtés au mot socialisme ont varié dans l'histoire et varient encore selon les contextes culturels et politiques. En 1831, le mot  tend à désigner une doctrine morale à forte connotation religieuse, qui voit en l'homme un être social, pour progressivement s'accoler avec  : à la fin des années 1830, le mot désigne de manière courante une doctrine qui vise à résoudre la question sociale due à la paupérisation massive de la classe ouvrière[7]. À compter de la seconde moitié du XIXe siècle et des progrès de l'influence du marxisme, le socialisme se définit essentiellement par l'aspiration à une société égalitaire, qui résulterait de l'organisation de la production et de la substitution de la propriété sociale à la propriété individuelle ou capitaliste : dans cette optique, une société nouvelle, censée représenter le règne de la démocratie véritable, pourra alors être réalisée[3].
Jadis associé à des positions radicales, le terme  est aujourd'hui utilisé en Europe, dans son acception la plus courante, pour désigner une famille politique de gauche modérée[8]. A contrario, dans certains pays comme les États-Unis, le mot conserve une tonalité plus nettement orientée à gauche[33].
Dans les textes de Marx et d'Engels, le terme de socialisme est utilisé pour désigner l'ensemble des doctrines critiquant la société capitaliste. Dans les écrits postérieurs au Manifeste du Parti communiste, le mot est surtout employé pour qualifier les courants, idéologies et mouvements politiques de la classe ouvrière, soit le mouvement ouvrier. Engels utilise le mot socialisme pour désigner, de manière plus précise, la prise de conscience par la classe ouvrière des oppositions de classes et des tares du capitalisme. Ce n'est qu'à partir de l'époque de l'Internationale ouvrière que le mot est employé pour signifier l'organisation sociale fondée sur l'appropriation collective des moyens de production, sous une forme étatique et/ou coopérative. Lénine et les bolcheviks reprennent cette acception pour l'identifier avec ce que Marx appelle la  de la société communiste : dans cette optique, le socialisme devient l'organisation sociale de transition entre le capitalisme et le communisme. Dès lors qu'un  est créé, le socialisme, dans l'optique de Lénine, s'identifie à l'existence même de cet état[31].
La tendance léniniste du socialisme - comme par ailleurs celle, bien moins répandue, se réclamant du luxemburgisme - est couramment désignée, après la révolution d'Octobre et plus précisément à partir de 1918, sous le nom de communisme, tout en continuant à se réclamer du socialisme. La tendance politique généralement désignée, dans les démocraties occidentales, sous le nom de socialisme - au sens de socialisme non communiste - évolue progressivement après la Seconde Guerre mondiale vers des positions de centre gauche : le socialisme est dès lors perçu comme une tendance politique réformiste tendant à corriger les inégalités inhérentes au libéralisme économique, sans s'opposer radicalement au principe de l'économie de marché. Le terme de social-démocratie, utilisé comme appellation par une partie des mouvements socialistes, en vient à désigner des positions réformistes et modérées, caractérisées par l'usage du compromis et non plus par la logique révolutionnaire. Le socialisme est perçu, dans cette optique, comme une correction des injustices, notamment par le biais de systèmes de protection sociale : la social-démocratie d'Europe du Nord se distingue dès les années 1930 par des réformes en faveur de l'État-providence. Dans la seconde partie du XXe siècle, les partis socialistes tendent dans leur majorité à s'éloigner des conceptions marxistes et à s'intégrer à la société libérale-capitaliste, non pour la renverser ou la remplacer mais pour la réformer de l'intérieur[34],[35]. Divers courants socialistes, non communistes, ont conservé un discours plus radical : ces courants se sont notamment exprimés dans l'entre-deux-guerres dans une organisation comme le Bureau international pour l'unité socialiste révolutionnaire ou après-guerre dans la tendance du socialisme autogestionnaire.Le socialisme démocratique, héritier de la tradition réformiste, constitue de nos jours la tendance majoritaire des partis socialistes dans les pays développés, où elle est devenue la forme de socialisme la plus couramment associée à l'adjectif socialiste, étant parfois utilisé comme un synonyme de social-démocratie[8]. En 1999, Lionel Jospin, alors Premier ministre de la France, publie un texte dans lequel il définit le  - qu'il désigne, en s'en revendiquant, du nom de social-démocratie - comme représentant, non plus un  mais , soit . Il considère que si le socialisme accepte l'économie de marché , en revanche il se doit de refuser la  car le marché en lui-même ne produit pas de valeurs ni de sens : jugeant que la question de l'appropriation collective des moyens de production ne définit plus en soi le socialisme, il plaide pour un  se définissant non par ses modes d'action mais par le maintien de valeurs de . La social-démocratie doit être, aux yeux de Jospin, [36].
L'idée d'une organisation harmonieuse de la société remonte à la haute Antiquité, bien avant l'apparition du mot  lui-même. Des ancêtres lointains et indirects du socialisme — bien que l'emploi du mot soit très anachronique — comme du communisme — dans son sens premier de société sans propriété privée — se trouvent sur plusieurs continents : en Grèce chez Platon, qui imagine dans La République et Les Lois des modes idéaux d'organisation de la cité (Platon ne prône pas l'égalité sociale - sa cité idéale de La République étant au contraire strictement hiérarchisée - mais l'harmonie. Au sein de l'élite sociale envisagée par Platon règnerait une communauté absolue de biens matériels)[37] ; en Asie dans certains courants de pensée du confucianisme et de l'islam qui envisagent une société reposant sur un mode d'organisation fraternel ; enfin, dans la doctrine chrétienne, qui prône le partage des biens matériels[38]. On retrouve des revendications fortement égalitaristes dans les hérésies chrétiennes, comme dans la mouvance millénariste[39]. Le courant de pensée utopiste, qui se développe à partir de la Renaissance, tend lui aussi vers l'idée d'une société harmonieuse, qui surmonterait les inégalités sociales en supprimant notamment la propriété privée. Thomas More, dans son livre Utopia (1516), crée le modèle du genre : un narrateur visite un pays merveilleux où règnent la fraternité et l'égalité et le fait découvrir au lecteur[40]. La littérature utopique, dans laquelle la satire et la critique sociales sont présentées par le biais de la fiction, connaît une longue postérité, dont La Cité du Soleil du moine Tommaso Campanella est l'un des exemples les plus connus[41]. Les utopies peuvent varier dans leur contenu, mais toutes se caractérisent par l'harmonie, l'équilibre et les mécanismes de régulation. Dans presque toutes, la tendance dominante est l'égalité sociale, accomplie par la disparition de la propriété privée, celle-ci étant considérée comme la cause du malheur des hommes[42]. Signe de cette filiation philosophique, de nombreuses associations socialistes ou socialisantes du XIXe siècle se nomment Utopie ou Utopia[43].
La critique sociale continue de se développer dans la pensée occidentale, le corollaire de l'utopie étant la protestation morale et sociale contre le monde tel qu'il est, et contre son principe inégalitaire[44]. Cette école de pensée atteint un apogée au XVIIIe siècle à l'époque des Lumières : Jean-Jacques Rousseau, dans son Discours sur l'origine et les fondements de l'inégalité parmi les hommes, Étienne-Gabriel Morelly dans Code de la nature, ou Le véritable esprit de ses lois de tout temps négligé ou méconnu, traitent des questions de la propriété et de l'égalité[45]. Morelly, plus radical que Rousseau, prône une organisation à la fois égalitaire et autoritaire de la société, où le mariage et le travail seraient obligatoires. Parmi les autres auteurs français des Lumières, le curé Meslier dénonce avec vigueur l'injustice de son temps, dont il voit la cause dans l'institution de la propriété privée ; le moine Dom Deschamps condamne également la propriété privée et prône un univers à la fois égalitaire et uniforme. Si Morelly a eu de l'influence de son vivant, Dom Deschamps et le curé Meslier ont laissé des écrits essentiellement posthumes : ces auteurs font figure de précurseurs d'un socialisme [46].
Dans la seconde moitié du XVIIIe siècle, des clercs catholiques forgent sur le latin socialis le terme socialistae pour dénoncer la doctrine de l’école allemande du droit naturel qui visait, à la suite de Grotius et Pufendorf, à fonder l’ordre social non plus sur la Révélation divine mais sur un instinct humain de « sociabilité » ; à la fin du siècle, dans l'aire germanophone, le terme a perdu sa charge péjorative pour désigner simplement, dans les manuels d’études juridiques, Pufendorf et ses élèves[47]. Le mot socialisme apparaît dans la langue française à la même époque, sans entrer pour autant immédiatement dans le langage commun. Étymologiquement, le terme dérive du mot latin socius, nom commun signifiant compagnon, camarade, associé, allié, confédéré et adjectif signifiant joint, uni, associé, allié, mis en commun, partagé[48]. Le mot socius dérive du verbe sequi : suivre. Sa première utilisation remonte à l'abbé Sieyès qui dans les années 1780 évoque un « traité du socialisme » devant parler « du but que se propose l’homme en société et des moyens qu’il a d’y parvenir ». Dans cette utilisation éphémère, le mot signifie alors « science de la société »[49]. En 1795, Gracchus Babeuf utilise le mot  pour désigner une personne membre de la collectivité sociale ; Charles Fourier reprend ce terme au sens d'associé de la [7]. En 1803, on retrouve le mot  sous la plume d'un auteur italien, Giacomo Giuliani, à qui sa paternité a parfois été attribuée[25] : dans le sens que lui accorde Giuliani, le terme désigne la défense de la propriété privée, facteur d'ordre dans la société humaine, tandis que l' est l'individualisme révolutionnaire qui dissout les rapports sociaux[50]. Ce n'est qu'après 1820 que le mot socialisme  réellement dans diverses langues, entrant progressivement dans le langage courant et prenant sa signification contemporaine[51] : en 1822[52], l'expression  apparaît dans un courrier qu'Edward Cooper, un partisan de Robert Owen, envoie à ce dernier. Owenistes et fouriéristes, en contact fréquent à partir des années 1830, adoptent l'appellation[53], peut-être à la suite d'une visite d'Owen à Fourier en 1837[54]. Le mot , au sens de doctrine visant à résoudre la question sociale, est définitivement entré dans le langage courant à la fin de la décennie[7].
Le socialisme, en tant qu'engagement militant, trouve une partie de ses racines dans la Révolution française, qui introduit dans l'ordre des faits la coupure historique et rend possible le passage de l'utopie à l'action, pour construire un monde nouveau, supposément harmonieux et fraternel. La révolution française devient un paradigme dont s'inspirent par la suite les révolutions suivantes. Karl Marx écrit par la suite que le socialisme allemand a pour lui la force de la théorie, et le socialisme français la tradition révolutionnaire. L'historien Michel Winock estime que c'est grâce à la révolution que le socialisme . Michel Winock distingue également trois apports décisifs de la Révolution française à la réflexion socialiste, sur le plan des idées comme sur celui de la pratique : pour ce qui est de l'exercice du pouvoir, avec le gouvernement révolutionnaire sous la Convention montagnarde et les mesures d'exception du Comité de salut public que sont la Terreur et l'instauration d'une dictature  en raison des circonstances ; pour ce qui est de l'usage du contre-pouvoir avec la pression  exercée par les sans-culottes et l'attachement de ceux-ci à la démocratie directe, bien que leur idéologie se résume à une forme d'. Enfin, une technique de prise de pouvoir est amenée, fût-ce au niveau théorique, par Gracchus Babeuf lors de la  en 1796 : Babeuf confie la direction de l'insurrection à un état-major secret, soit à une minorité révolutionnaire, sa conception annonçant celles de Blanqui et de Lénine. Les conceptions égalitaristes de Babeuf annoncent également, de manière directe, l'idéologie communiste[55].
Durant les premières décennies du XIXe siècle, la révolution industrielle bouleverse l'Europe occidentale, modifiant paysages, modes de vie et cultures. La transformation progressive des économies et des sociétés entraîne le développement en milieu urbain d'une classe ouvrière, vivant dans des conditions souvent difficiles. L'école de pensée socialiste se développe en réponse à cette situation, d'abord par la recherche de nouveaux modes d'organisation sociale destinés à résoudre les problèmes de l'époque[56]. Le mouvement socialiste naît durant la première moitié du siècle. Dans les années 1820 et 1830, le mot  rentre progressivement dans le vocabulaire politique courant, dans diverses langues. À l'origine, cependant, le mouvement socialiste est distinct du mouvement ouvrier : si une doctrine socialiste, ou plutôt un ensemble de doctrines, émerge par le biais d'une multitude d'écrits d'auteurs se reconnaissant comme socialistes, le mouvement ouvrier lui-même ne peut être qualifié de socialiste que dans la mesure où il reprend à son compte les objectifs, ou certains objectifs, des doctrines socialistes. Il existe par ailleurs des mouvements ouvriers sans finalité socialiste, comme des mouvements socialistes sans ouvriers. Ce n'est que dans la deuxième moitié du siècle que le courant de pensée socialiste se constitue progressivement en partis politiques et en mouvements internationaux[57]. Cette première époque du socialisme correspond à ce qui est par la suite désigné par les marxistes du nom de socialisme utopique, par opposition au socialisme scientifique qu'eux-mêmes estiment représenter. Certains socialistes préfèrent par ailleurs se désigner comme , le terme désignant une opposition à la propriété privée. La plupart du temps, les socialistes accompagnent les mouvements démocratiques qui réclament l'instauration du suffrage universel, comme le chartisme au Royaume-Uni ou les républicains en France sous la Restauration et la Monarchie de Juillet[58].
Trois pays européens jouent un rôle fondamental dans l'apparition de la mouvance socialiste : la France contribue à l'émergence du socialisme par le biais de sa tradition révolutionnaire ; le Royaume-Uni, par la puissance de son mouvement industriel, aux profondes conséquences sociales, contribue à la naissance du mouvement ouvrier en tant que force organisée ; l'Allemagne, qui n'est pas encore à l'époque un État unifié, apporte au mouvement un cadre philosophique, grâce aux travaux de nombreux penseurs de langue germanique[59].
Les idées socialistes se développent au Royaume-Uni alors que le pays connaît, en avance sur le reste de l'Europe, une forte industrialisation ainsi que le développement d'un capitalisme très dynamique : l'idéologie accompagne les transformations de l'économie britannique au lieu de les précéder. Robert Owen, chef d'entreprise aux idées humanistes et théoricien politique, fait partie des auteurs qui réfléchissent sur les bouleversements des rapports de production et des relations de travail. Le socialisme ambitionne de proposer, face aux aspects  du capitalisme, un idéal de liberté et de communauté harmonieuse : les théoriciens socialistes qui se font connaître à l'époque au Royaume-Uni ne sont pas des intellectuels mais, dans leur majorité, des membres de la classe dirigeante, qui souhaitent améliorer le sort du peuple et se réclament souvent d'exemples issus du passé national, comme les niveleurs. Les œuvres d'auteurs britanniques de la fin du XVIIIe siècle, comme William Godwin, William Ogilvie (en) ou Thomas Paine, ainsi que les idées du jacobinisme, contribuent à nourrir le courant[60]. En 1812, Thomas Spence fonde avec quelques-uns de ses partisans une société philanthropique qui apparaît comme la première, et modeste, organisation  britannique. Les disciples de Spence, qui meurt en 1814, se situent dans la lignée du jacobinisme, à mi-chemin entre l'action légale et les rêves de soulèvement[61].
Avec Robert Owen, le socialisme britannique gagne en notoriété et en influence : ce sont d'ailleurs des disciples d'Owen qui, en 1822, introduisent le mot socialism dans la langue anglaise[51]. Auteur très prolifique, bénéficiant de son vivant d'une renommée sans commune mesure avec celle des autres théoriciens socialistes de l'époque, Owen préconise, pour résoudre les problèmes nés de l'individualisme capitaliste, une nouvelle organisation de la société via la constitution de communautés - des  de 500 à 2 000 personnes, formés de groupes égalitaires d'ouvriers et de cultivateurs organisant leur auto-suffisance sur le modèle coopératif. Dans les années 1820, Owen, dont la démarche et le langage se font volontiers messianistes, fonde plusieurs communautés de ce type, dont la plus célèbre est celle de New Harmony, aux États-Unis. L'échec de ces projets n'empêche pas leur inspirateur de bénéficier d'une grande renommée : entre les années 1820 et 1840, l' compte de nombreux disciples, tant chez les intellectuels que chez les ouvriers[62]. Différents philosophes et économistes britanniques, comme John N. Gray, William Thompson ou Thomas Hodgskin, se livrent à la même époque à une critique de l'économie capitaliste[63].
Le Royaume-Uni se distingue également, dans le contexte de son industrialisation avancée, par le développement d'un mouvement ouvrier, qui dès la fin du XVIIIe siècle constitue une ébauche du syndicalisme. À partir de 1829, le syndicalisme se développe rapidement en Grande-Bretagne, à la faveur d'une pointe de prospérité. À la même époque, le mouvement ouvrier britannique rencontre le socialisme avec l'adoption de l'idéologie oweniste, qui se traduit notamment par la constitution d'entreprises coopératives selon les principes d'Owen. Robert Owen lui-même joue un rôle important dans l'éclosion d'un syndicalisme de masse au Royaume-Uni. Le chartisme, entre 1836 et 1848, rencontre un très fort écho parmi les travailleurs britanniques, en mêlant au cours de son histoire réformisme et tentations radicales[64].
C'est également au Royaume-Uni que se développe un courant de pensée libéral qui aspire à concilier le libéralisme avec les idées socialistes : John Stuart Mill, intéressé dans les années 1830 par le saint-simonisme, s'éloigne du libéralisme économique classique pour prôner une société dans laquelle le progrès économique ne serait pas une fin en soi et qui viserait la justice sociale via une équitable répartition des richesses et du travail, ainsi qu'une organisation autogestionnaire des travailleurs qui prendraient eux-mêmes en charge leur destin dans des coopératives. Les idées de Mill trouvent ensuite une continuation dans le courant dit du social-libéralisme, qui reprend le concept de socialisme libéral et constitue la première école de pensée libérale à s'ouvrir aux idées socialistes[65].
En France, l'industrialisation s'effectue plus lentement qu'au Royaume-Uni. Le nombre des ouvriers s'accroît, mais la classe ouvrière française n'a rien d'homogène. Faute de réel mouvement ouvrier organisé, la France est avant tout, sous la Restauration, un lieu d' : de nombreux intellectuels se livrent à une critique du libéralisme économique par le biais de pensées  très variées[66].
Le comte Claude Henri de Saint-Simon développe dans les années 1820 une doctrine matérialiste prônant d'atteindre l'âge d'or par le progrès économique, lequel serait assuré par un gouvernement de la bourgeoisie qui aurait pour but l'amélioration du sort de la classe la plus pauvre[67]. Le socialisme prôné par Saint-Simon est un socialisme élitiste, où le savoir est détenu par une élite sociale à laquelle les autres sociétaires ne sauraient prétendre, même si l'ignorance de ces derniers est destinée à reculer avec le temps[68]. Après la mort de Saint-Simon en 1825, sa doctrine, baptisée saint-simonisme, est perpétuée par ses disciples, dont certains évoluent vers une logique sectaire. Parmi les personnalités les plus marquantes issues du saint-simonisme, on note Saint-Amand Bazard, Barthélemy Prosper Enfantin et Olinde Rodrigues : les saint-simoniens réfléchissent notamment au problème des crises économiques, causées selon eux par l'appropriation privée des capitaux, qui engendre non seulement l', mais aussi l'[69]. C'est également un saint-simonien, Pierre Leroux, qui réintroduit en 1831 le mot socialisme dans la langue française, cette fois dans son sens moderne[70] ; Leroux s'en attribue d'ailleurs la paternité[71]. En mars 1834, il emploie le néologisme dans un texte intitulé De l'individualisme et du socialisme, publié dans la Revue encyclopédique. Pierre Leroux, dont la pensée est d'inspiration avant tout religieuse, définit le mot comme un , forgé par opposition au concept d'individualisme[72]. Pour lui, le socialisme, directement rattaché à la Révolution française, est « la doctrine qui ne sacrifiera aucun des termes de la formule Liberté, Égalité, Fraternité »[73],[74].
Charles Fourier se livre pour sa part à une critique virulente de la , qu'il qualifie d' : à ses yeux, le morcellement de la propriété et le  mènent au désordre et à une société qui ne peut être maintenue que par la force. Fourier préconise le passage au stade de l', par l'association des producteurs et la réorganisation de la société selon un principe communautaire. Dans le projet de Fourier, les hommes formeront, au sein de Phalanstères, des communautés sans distinction de race et de sexe, où le travail sera réparti selon une logique d'harmonie et de plaisir. L'œuvre de Fourier, très personnelle voire singulière, diffère sur plusieurs points des autres écoles de pensée socialistes. En effet, Fourier ne préconise pas l'égalité totale au sein des Phalanstères où subsisterait une forme de hiérarchie, et ne formule pas d'idée très claire sur la notion de propriété ; s'il déplore la misère des ouvriers, il ne raisonne pas en termes d'antagonisme des classes sociales, dont il préconise au contraire la solidarité. Parmi ses contributions à l'école de pensée socialiste, on note des réflexions sur l'égalité des sexes, la nécessité de vaincre l'antagonisme entre les villes et les campagnes, et celui entre travail manuel et travail intellectuel[75]. Hostile à la Révolution française, Fourier condamne toute forme de violence et conçoit le politique comme déconnecté de la réalité sociale. Il se distingue également par ses conceptions en matière d'érotisme, et prône à cet égard au sein des Phalanstères l'assouvissement de tous les désirs, y compris les plus originaux[76]. Si la pensée de Fourier a moins d'influence que celle de Saint-Simon, elle donne lieu, entre 1832 et 1848, à diverses tentatives de vie communautaire, plusieurs étant menées par son disciple Victor Considerant[77].
Entre 1830 et 1848, la prise de conscience de la dureté de la condition ouvrière entraîne un foisonnement de doctrines socialistes. Celles-ci sont imprégnées aussi bien des traditions républicaines et démocratiques que des idéaux de charité, religieuse ou non. Le socialisme français accompagne le courant républicaniste, mais ne se confond pas avec lui et s'inscrit au contraire dans une tendance plus globale d'aspiration à des réformes sociales, au droit au travail et au suffrage universel[78]. Étienne Cabet, essayiste chrétien, apparaît comme le chef de la principale école de pensée  en France, la communauté des biens matériels étant à ses yeux la seule application possible de l'enseignement de Jésus-Christ : dans son livre Voyage en Icarie, il décrit, dans la lignée de More et de Campanella, une société idéale, fondée sur l'égalité et l'absence de propriété privée. À la fin des années 1840, Cabet et ses disciples passent de la théorie à la pratique en se lançant, aux États-Unis, dans l'aventure de diverses communautés . D'autres théoriciens comme Richard Lahautière, Théodore Dézamy, Jean-Jacques Pillot ou Albert Laponneraye, qui se distinguent du communisme chrétien de Cabet par une filiation plus marquée envers les traditions révolutionnaires et la pensée de Babeuf, prônent également la communauté des biens matériels. Lahautière anime en 1840 à Belleville un  qui contribue à populariser le terme en France[79]. Les héritages intellectuels de Saint-Simon et de Fourier influencent la plupart des théoriciens socialistes français, dont Victor Considerant, qui s'emploie à synthétiser la doctrine de Fourier, Pierre Leroux qui, influencé tout à la fois par Saint-Simon et Fourier, développe une critique sociale mêlée d'utopisme et de mysticisme, ou l'économiste Constantin Pecqueur, dissident du saint-simonisme dont la pensée s'imprègne par la suite de proudhonisme et de christianisme. Lamennais associe des idées socialistes à un paternalisme évangélique et à la vision messianique d'une société régénérée. Philippe Buchez, autre représentant du courant du socialisme chrétien, envisage de résoudre le problème de la misère des travailleurs par l'association ouvrière : fondateur avec Bazard de la charbonnerie française, il entend faire une synthèse entre le socialisme, le christianisme et la Révolution française, qu'il considère comme découlant directement des principes chrétiens. Louis Blanc, journaliste et écrivain très actif, s'inspire des pensées socialistes qui l'ont précédé pour envisager des solutions à la misère ouvrière. Dans son livre Organisation du travail (1839), il prône une réorganisation du monde du travail au sein d' annonçant les principes de l'autogestion, ainsi que l'évolution progressive de la société vers l'égalitarisme : dans sa conception, les aspirations sociales ne peuvent être satisfaites que par le biais d'une intervention rationnelle de l'État, qui seule garantirait le passage à une société plus fraternelle[80],[81]. Le socialisme de Buchez ou de Blanc s'inscrit en partie dans la lignée de Saint-Simon en ce que ces auteurs admettent la révolution industrielle, mais à condition qu'elle s'opère sous le contrôle de l'État et au service du peuple[82].
Saint-simonien dans sa jeunesse, Auguste Blanqui apparaît pour sa part comme un  de Babeuf, non seulement pour sa vision de la société mais également pour sa conception du coup d'État révolutionnaire. Son programme est explicite et prévoit l'application d'une dictature du prolétariat, où le peuple serait armé au sein d'une milice nationale[83]. Blanqui - qui donne son nom au courant politique du blanquisme - envisage une société où règnerait une stricte égalité des conditions, par l'abolition de l'héritage, et où les biens seraient répartis en fonction des besoins de chacun[84]. Une fois la révolution réalisée, il s'agirait ensuite de réaliser progressivement une société communiste en luttant contre l' et contre les religions. Le  de Blanqui est avant tout une affaire d'avant-garde révolutionnaire et de sociétés secrètes. Participant à divers mouvements, Blanqui fonde en 1837 avec Armand Barbès et Martin Bernard la Société des saisons, qui tente en mai 1839 une insurrection à Paris. Le soulèvement échoue et ses meneurs sont arrêtés : Blanqui paie son militantisme de multiples emprisonnements et de longues années de prison, qui lui valent le surnom de [83].
Pierre-Joseph Proudhon, essayiste prolifique, s'impose progressivement comme l'une des personnalités les plus marquantes du socialisme français de l'époque[85]. Anarchiste, partisan de la réalisation d'un [86], Proudhon apparaît comme l'un des principaux tenants du [87]. Dans son ouvrage Qu'est-ce que la propriété ?, il contribue à populariser l'adage  : pour lui comme pour d'autres, la propriété est le fondement de l'injustice sociale. Proudhon n'en accepte pas pour autant la propriété étatique et s'oppose à toute forme de collectivisme centralisé. S'il construit de manière déductive un discours critique, Proudhon n'avance cependant pas, avant 1848, de solutions positives[85]. Hostile à la plupart des autres écoles socialistes, il s'oppose notamment au  de Louis Blanc et se distingue des fouriéristes et des saint-simoniens par son refus virulent de l'endoctrinement et de l'autorité[88].
La diffusion de ces théories va bien au-delà des cercles restreints d'idéologues et d'utopistes. Des circuits alternatifs permettent de toucher certaines franges des classes populaires. Étienne Cabet parvient ainsi à publier secrètement ses brochures avec l'appoint d'ouvriers typographes lyonnais. Ceux-ci ne jouent d'ailleurs pas un rôle passif : en septembre 1842 ils refusent de publier un de ses textes, jugé trop critique envers Théodore Dézamy[89]. Parallèlement, des bibliothèques officieuses se constituent spontanément dans plusieurs villes industrielles, en dépit de la répression gouvernementale. Elles constituent à la fois des lieux de lecture et de réunion où des idéologues viennent exposer leurs vues auprès d'un public issu de la classe moyenne et de la classe ouvrière[90]. Certains ouvriers emportent et lisent des brochures sur leur lieu de travail. Cette lecture est loin d'être révérencieuse ; elle donne lieu à une appréciation critique qui contribue à l'enrichissement des échanges théoriques[91].
Au début du XIXe siècle, dans les États de langue allemande membres de la Confédération germanique — durant la période dite du vormärz — la classe ouvrière est principalement employée dans l'artisanat et non dans l'industrie. Si la pensée utopique s'est diffusée en Allemagne comme dans le reste de l'Europe, la mouvance socialiste se développe par la suite essentiellement dans les milieux intellectuels, par le biais de contacts avec l'étranger. Les thèmes de la lutte des classes et de l'amélioration du sort des ouvriers se retrouvent dans les œuvres des écrivains exilés Ludwig Börne et Heinrich Heine - ce dernier étant influencé par le saint-simonisme - qui contribuent indirectement à diffuser en Allemagne les idées associées au socialisme. Les théories de Fourier et de Babeuf trouvent des disciples allemands. Georg Büchner, via ses contacts en France avec la Société des droits de l'homme, s'initie à l'idéologie babouviste. Franz Xaver von Baader, philosophe mystique, contribue à sensibiliser l'opinion catholique au sort des prolétaires. Lorenz von Stein publie en 1842 l'ouvrage Le Socialisme et le communisme dans la France contemporaine, qui permet au lectorat de langue allemande de se familiariser en profondeur avec les idées françaises[92].
Les premiers embryons de groupes socialistes allemands se constituent dans le milieu des associations de compagnons vivant à l'étranger, dont certains ont été influencés par le mouvement Jeune-Allemagne, d'inspiration mazzinienne. Des exilés politiques, en contact étroit avec les écoles de pensée françaises, fondent en 1834 une société secrète inspirée du carbonarisme, la Ligue des bannis. En 1836, ce groupe laisse la place à une nouvelle organisation clandestine, la Ligue des justes, dont Wilhelm Weitling est le principal idéologue. Ne comptant que quelques centaines de membres, la Ligue a néanmoins des ramifications dans plusieurs pays[93]. Le socialisme de la Ligue est empreint de nombreux éléments chrétiens qui révèlent son caractère transitoire : en 1841, Weitling interprète ainsi la communion comme l'acte communiste par excellence, le partage du repas étant conçu comme la métaphore du partage des biens[94]. Ce socialisme christianisant est rapidement dépassé par des idéologies plus radicales. Dès 1842, Engels souligne le caractère paradoxal de ce communisme qui, pour se légitimer, se croit obligé de se référer à la Bible[95]. L'un des membres de la Ligue, Karl Schapper, réorganise la section londonienne du mouvement et anime la Société communiste de formation ouvrière (Kommunisticher Arbeiterbildungsverein), qui constitue une foyer actif de militantisme socialiste. Il finit par condamner la direction de Weitling : [96]. Tout au long des années 1840, la Ligue se sécularise et incorpore des catégories et des concepts nouveaux élaborés par des milieux intellectuels athées : au milieu de la décennie, la tendance  tend à décliner, alors que le débat dominant parmi les émigrés politiques allemands devient principalement celui entre croyants et athées[93].
La pensée socialiste allemande présente pour spécificité de découler pour partie d'un débat philosophique. À la fin des années 1830, un désaccord profond émerge entre les disciples de Hegel. L'enjeu est initialement théologique : menant jusqu'à leur terme les présupposés de la Phénoménologie de l'esprit, les Jeunes hégéliens remettent en cause la notion chrétienne de personne[97]. Pour David Strauss et Ludwig Feuerbach, le soi n'a pas véritablement d'existence en tant que tel : ce n'est qu'une construction sociale. Cette posture philosophique possède d'emblée une connotation politique[98]. En tant que déterminant, la société ne saurait rester figée. À l'instar de la conscience humaine, elle est appelée à évoluer, à se réformer jusqu'à parvenir à un degré d'organisation toujours plus juste, démocratique et rationnel. Contrairement à ce que pensent les hégéliens de droite, la société prussienne ne représente pas la fin de l'histoire. Elle ne constitue que l'étape d'un processus de libéralisation toujours en cours[99].
À partir des années 1840, cette gauche hégélienne se radicalise : la promotion d'une libéralisation graduelle ne parvient pas à s'imposer face à un État prussien rétif à tout changement[99]. Moses Hess, futur théoricien du sionisme, soucieux de transformer la philosophie de Hegel en philosophie de l'action, développe une pensée socialiste préconisant l'instauration d'une société sans propriété privée, qui serait une . La tendance de Hess — que Marx surnomme par la suite, de manière ironique, le  — se répand en Allemagne vers 1844, en particulier dans les provinces occidentales, témoignant de l'intérêt des milieux intellectuels pour les questions sociales. Mais la littérature issue de ce courant est trop abstraite et inactuelle pour avoir une influence politique directe[100]. En 1843, Karl Marx décide d'opérer une liaison entre l'hégélianisme de gauche et son propre engagement socialiste[98].
En février 1847, Karl Marx et Friedrich Engels adhèrent à la Ligue des justes. On assiste de fait à la réunion des deux principales sources de la pensée socialiste allemande : celle, intellectuelle, des hégéliens de gauche et celle, institutionnelle, des associations de compagnonnage[101]. En conséquence, la Ligue des justes change d'appellation et devient la Ligue des communistes. La première devise de la Ligue, , est remplacée par . Marx et Engels parviennent à imposer leurs conceptions révolutionnaires face aux doctrines utopistes : ils sont chargés d'écrire la profession de foi de l'organisation, le Manifeste du Parti communiste, après qu'un premier projet de Hess a été refusé. Ayant réfuté le , le socialisme  de Proudhon, comme le  de Hess, Marx et Engels exposent leur conception de la lutte des classes, les communistes devant constituer l' des partis ouvriers et soutenir le prolétariat dans sa lutte contre la classe dominante, qu'il s'agisse, selon les pays, de la bourgeoisie ou de la noblesse. La classe ouvrière à laquelle Marx s'adresse n'est cependant encore qu'en gestation en Allemagne et la Ligue des communistes, si elle dispose de sections dans divers pays, ne compte que peu de membres (environ 500). Le Manifeste du Parti communiste, s'il est promis à une longue postérité, n'a pas d'influence immédiate au moment de sa parution[102].
L'ensemble des situations révolutionnaires que connaît le continent européen en 1848-1849 donne aux mouvements socialistes l'occasion de participer à un bouleversement politique et de se faire connaître du plus grand nombre. En France et en Allemagne notamment, le socialisme apparaît alors comme un  sur la scène politique[103]. Il cesse de représenter l'aile gauche du mouvement libéral et démocratique pour s'imposer comme une idéologie autonome, disposant de ses valeurs politiques propres[104]. À cette époque, le terme social-démocrate apparaît pour désigner ceux qui ajoutent, à la revendication de la démocratie politique - soit l'instauration du suffrage universel - la revendication , soit l'amélioration de la condition ouvrière[105].
Au moment de la révolution de février 1848 en France, les autorités de la nouvelle République française tout juste proclamée répondent à une pétition ouvrière demandant le droit au travail en créant la Commission du gouvernement pour les Travailleurs, dite Commission du Luxembourg, chargée de prendre des mesures pour améliorer la condition ouvrière : Louis Blanc en est le président et l'ouvrier Alexandre Martin, dit , le vice-président. La Commission comprend des représentants des ouvriers et du patronat et tente, avec plus ou moins de succès, d'arbitrer les conflits entre patrons et ouvriers. Le 27 février, le chantier des ateliers nationaux — inspiré du projet d' de Louis Blanc — est lancé afin de fournir du travail aux ouvriers. Un projet de loi reprenant les idées exprimées par Blanc dans Organisation du travail, est rédigé pour réglementer l'économie du pays en fonction des aspirations socialistes du moment. Les doctrines socialistes se répandent en France par l'intermédiaire des clubs politiques - où s'expriment les théoriciens comme Blanqui, Dézamy, Toussenel, Barbès, Raspail ou Cabet - et des journaux - Raspail, Lamennais et Proudhon dirigent chacun leur propre publication. L'élection de l'Assemblée constituante, en avril, marque un premier recul des socialistes : Philippe Buchez en prend la présidence et Louis Blanc est élu député, mais la  est écrasée à Paris et les socialistes obtiennent nettement moins de députés que les républicains modérés et les orléanistes, et autant que les légitimistes. Louis Blanc et Albert abandonnent la Commission du Luxembourg et la Commission exécutive repousse le projet de Blanc de créer un ministère du Progrès et du Travail. Lors des élections partielles de juin, Pierre Leroux et Pierre-Joseph Proudhon deviennent députés. Le socialisme français, qui se distingue encore mal du démocratisme radical, recule encore après le désastre des ateliers nationaux : maladroitement conçus d'après une traduction grossière des idées de Louis Blanc, les ateliers sont un échec, l'aggravation du chômage provoquant en leur sein un afflux des ouvriers désœuvrés[106]. La fermeture des ateliers entraîne une insurrection, connue sous le nom des journées de Juin : les violences, tout en discréditant les théories de Louis Blanc[107], brisent les liens qui s'étaient noués entre l'idée républicaine et le socialisme. En juillet, le projet de réforme financière et sociale de Proudhon n'obtient que deux voix à l'assemblée. Lors de l'élection présidentielle de décembre 1848, le  Ledru-Rollin et le socialiste Raspail sont largement devancés par Bonaparte et Cavaignac, Raspail n'obtenant pour sa part que 0,51 % des suffrages. Le socialisme français, dont l'idéologie se brouille de plus en plus, est mis en nette minorité par le Parti de l'Ordre lors des élections législatives de 1849. Le coup d'État de 1851 réalisé par Louis-Napoléon Bonaparte réduit ensuite au silence l'essentiel de la génération des socialistes français de 1848. La plupart, comme Lamennais, Leroux, Blanc, Buchez, Cabet, Considerant ou Pecqueur, cessent toute activité politique ; certains choisissent l'exil. Blanqui - malgré son emprisonnement puis son exil sous le Second Empire - et Proudhon continuent au contraire à écrire et parviennent à maintenir, voire à consolider leur influence[106].
En Allemagne, la révolution de mars 1848 est accompagnée de violentes grèves, qui marquent le début d'un mouvement ouvrier organisé. L'écrivain Stephan Born, militant de la Ligue des communistes, contribue à la coordination des associations ouvrières et parvient à éviter leur basculement dans l'anarchie. Il amène à la naissance d'un  berlinois puis, en septembre de la Arbeiterverbrüderung () qui devient rapidement l'organisation ouvrière la plus importante d'Europe continentale. Marx, qui se trouve alors en France, envisage à long terme l'engagement du monde ouvrier dans la lutte révolutionnaire contre la bourgeoisie, et entend y préparer le prolétariat. Se rendant à Mayence, il publie les , tentative d'adapter les principes du Manifeste à la réalité du moment. Trouvant le mouvement ouvrier peu consistant politiquement, il décide ensuite de la mise en sommeil de la Ligue des communistes : établi à Cologne, il prend le parti de privilégier l'action par voie de presse pour rassembler les forces progressistes et crée dans ce but la Nouvelle Gazette Rhénane. Dans ce journal, Marx développe un programme de révolution européenne et met ses espoirs dans une guerre avec la Russie pour délivrer les nations sujettes. Il tente également de faire de l'association ouvrière de Cologne le fer de lance du mouvement révolutionnaire ; sa pensée se diffuse en dehors de la province rhénane et plusieurs associations de travailleurs adoptent l'idéologie du Manifeste. Des journalistes diffusent, de manière plus ou moins précise, l'idéologie de la Ligue. Marx est finalement frappé par un arrêté d'expulsion le 19 mai 1849 et son journal doit cesser de paraître. La défaite des mouvements révolutionnaires en Allemagne au printemps 1849 entraîne l'exil des démocrates, parmi lesquels les socialistes allemands : Engels, après avoir participé au soulèvement dans la Bade et le Palatinat, parvient à fuir en Suisse. Marx, réfugié à Londres, croit d'abord à une nouvelle flambée révolutionnaire provoquée par une crise économique, puis se résout en 1850, sur la base de ses propres études, à ne plus espérer à court terme de crise majeure ni de victoire du socialisme. La Ligue des communistes se reconstitue avec difficulté en Allemagne et continue de toucher des milieux intellectuels par la diffusion de pamphlets clandestins, mais elle se dissout à la fin 1852 après l'arrestation de plusieurs de ses membres. Marx, toujours en exil, met un terme à ses activités révolutionnaires pour se consacrer à ses travaux d'économie politique. En Allemagne, les idées de 1848 continuent d'être diffusée au sein des associations de travailleurs : en 1854, le Bundestag de la Confédération germanique finit par intimer aux gouvernements de tous les États allemands de dissoudre les organisations ouvrières. Les idées socialistes, bien que réduites à la clandestinité, continuent néanmoins d'être diffusées au sein des milieux ouvriers allemands[108].
Le déclin, puis l'échec, des mouvements révolutionnaires de 1848 aboutissent à un rapprochement entre l'opposition démocratique modérée et les institutions monarchiques et autocratiques. Certains démocrates s'effraient en effet de la radicalisation des associations ouvrières ; inversement, les autorités conservatrices constatent que l'exercice d'une répression continue n'a pas empêché la résurgence du spectre révolutionnaire plus d'un demi-siècle après la Révolution française. Ce rapprochement contribue à exclure les socialistes du jeu politique pendant plus d'une décennie : [109]. Cette exclusion a d'importantes conséquences à long terme. Définitivement mis à l'écart de la gauche républicaine, les socialismes européens se réorganisent sur des bases autonomes. Ernest Labrousse souligne ainsi qu'après une période 1815-1851 consacrée aux spéculations idéologiques, la période 1851-1870 est marquée par la constitution de structures politiques et sociales élaborées[110].
Parmi les socialistes français encore actifs, Blanqui approfondit son socialisme sur le plan doctrinal, mais continue de le concevoir comme la résultante d'une révolution qui serait menée par une minorité décidée. Opposé à cette conception d'un , Proudhon développe une pensée complexe, qui lui vaut plus tard d'être présenté par Pierre Kropotkine comme l'un des . Pour le  Proudhon d'après 1848, la solution pour rejeter à la fois les gouvernements bourgeois et le communisme étatique se trouve dans ce qu'il appelle l', ou mutuellisme, c'est-à-dire non pas une justice redistributive exercée d'en haut, mais une justice , fondée sur des rapports contractuels entre chacun et tous. Le mutuellisme apparaît alors comme une revanche de la société sur l'État. Proudhon se veut révolutionnaire, mais hostile au jacobinisme et au terrorisme : il conçoit la révolution comme devant être exercée non pas par une avant-garde révolutionnaire mais , soit par la constitution d'un partenariat volontaire entre producteurs et consommateurs, selon le principe de l'échange réciproque de produits. Proudhon préconise la création de compagnies ouvrières, autogérées et associées en fédérations nationales, qui soustrairont les citoyens à l'exploitation ; le fédéralisme politique permettra de concrétiser le mutuellisme économique en réduisant au minimum le rôle de l'État et en permettant au prolétariat, constitué en mouvement à part, d'échapper au contrôle de la bourgeoisie. Si Proudhon considère la propriété comme injuste, il ne prône pas pour autant l'expropriation générale - s'opposant sur ce point aux  de l'époque - et respecte la , la propriété devant cependant être subordonnée au nouveau système économique : l'œuvre de Proudhon n'explique cependant pas comment l'hérédité peut se concilier avec l'égalité[88],[111],[112]. L'influence de Proudhon est vaste : ses idées nourrissent plus tard les théories anarchistes et socialistes de Bakounine ou de Herzen, le syndicalisme révolutionnaire de Fernand Pelloutier, l'action des bourses du travail, et sont même par la suite récupérées par une partie de l'extrême droite[113]. Le philosophe français François Huet tente pour sa part de promouvoir une forme française de  qui concilierait socialisme, libéralisme et christianisme. Le socialisme chrétien de Huet, exposé notamment dans son ouvrage Le Règne social du christianisme (1852), s'oppose tant au socialisme anarchiste individualiste qu'au communisme qui nie l'individualité et prône une société fondée à la fois sur les libertés économiques et sur une solidarité assurée par l'État, qui garantirait l'égalité des chances en abolissant les inégalités de classe via un partage des richesses. Professeur à l'Université de Gand, Huet joue un rôle dans la diffusion du socialisme en Belgique : certains de ses étudiants fondent une société consacrée à sa pensée, qui compte parmi ses membres Émile de Laveleye, futur théoricien important du socialisme belge[114].
Au Royaume-Uni, le mouvement socialiste chrétien, animé par des personnalités comme Frederick Denison Maurice, John Malcolm Ludlow ou Charles Kingsley, s'oppose à la fois au chartisme, qu'il juge démagogique, et au système capitaliste. Le socialisme chrétien, qui prône une réforme spirituelle et sociale de la société et de l'économie, est principalement actif entre 1848 et 1854 avant d'évoluer principalement vers l'activité de diverses fondations éducatives et coopératives[115].
Les anciens militants chartistes britanniques se durcissent idéologiquement et se rapprochent plus nettement du socialisme. George Julian Harney répand dans ses articles et son action militante un socialisme inspiré directement de Marx, avec qui il finit cependant par se brouiller, lui reprochant son dogmatisme et son intolérance. Ernest Charles Jones (en), autre militant issu du chartisme, garde plus longtemps la faveur de Marx et s'emploie, sans grand succès, à diffuser l'internationalisme révolutionnaire dans le monde ouvrier britannique. James Bronterre O'Brien (en) prêche pour sa part un socialisme modéré et réformiste, purgé de ses excès révolutionnaires et qui annonce le socialisme constitutionnel de type travailliste. Les thèmes d'un socialisme démocratique acquièrent une audience dans la classe ouvrière britannique grâce notamment à leur diffusion dans le Reynolds Weekly Newspaper, journal à fort tirage publié par George William McArthur Reynolds. Entre 1850 et 1875, un important mouvement syndical se développe par ailleurs au Royaume-Uni, avec un intérêt marqué pour l'internationalisme. Le syndicalisme britannique, qui se fédère à partir de 1868 au sein du Trades Union Congress, se montre cependant peu enclin aux luttes idéologiques et révolutionnaires, évoluant au contraire vers un réformisme partisan de la paix sociale[116].
En Italie, le socialisme se diffuse progressivement et ne prend réellement corps en tant que force politique organisée qu'à partir de 1872. Mais bien avant cette date, les écoles de pensée socialistes acquièrent de l'influence dans l'Italie du risorgimento, très influencée par la vie intellectuelle française : si Giuseppe Mazzini lui-même n'est pas socialiste, bien que des éléments de sa pensée sociale dérivent en grande partie de Saint-Simon et de Fourier, son action a une grande influence sur le socialisme italien, qui nait parmi les dissidents du mazzinisme. Le révolutionnaire Carlo Pisacane ou le philosophe Giuseppe Ferrari, influencé par Proudhon, comptent à des titres très divers parmi les premières figures du socialisme italien[117].
Dans la Russie impériale - qui se distingue des autres monarchies européennes par un absolutisme particulièrement peu réceptif aux évolutions démocratiques et dont la classe ouvrière ne bénéficie, dans la seconde moitié du siècle, d'aucune des avancées sociales que connaissent les prolétariats des autres pays - le socialisme apparaît essentiellement au sein de cercles d'intellectuels. Dans les années 1830, Alexandre Herzen et Nikolaï Ogarev introduisent en Russie les idées socialistes, qui trouvent difficilement des moyens d'expression sous un régime autocratique. Jusqu'en 1905, le mouvement socialiste russe est presque toujours clandestin et animé en grande partie par des exilés politiques. Herzen, réfugié à Londres dans les années 1850, diffuse clandestinement en Russie les revues L'étoile polaire puis, avec Ogarev, La Cloche. Dans ses publications, il attaque l'absolutisme tsariste et prône un socialisme paysan, dans la perspective d'un communisme essentiellement libéral[118].
En Allemagne, le mouvement socialiste réapparaît au tournant des années 1860 après avoir été condamné par les autorités conservatrices à une décennie de clandestinité[119]. Il bénéficie indirectement des luttes politiques pour l'unité de la nation allemande et, surtout, de la faiblesse du mouvement libéral et démocratique. Des représentants modérés du mouvement ouvrier allemand comme Johann Jacoby ou Friedrich Lange, ne parviennent pas à faire la jonction avec le groupe parlementaire national-libéral ; ce dernier se rallie en effet progressivement aux institutions de la monarchie prussienne. L'espace politique, resté libre, est rapidement occupé par plusieurs socialismes concurrents[119].
En 1863, Ferdinand Lassalle fonde le premier parti socialiste allemand, l'Association générale des travailleurs allemands (Allgemeiner Deutscher Arbeiterverein, abrégé en ADAV)[120]. Avocat brillant, Lassalle se fait connaître en publiant plusieurs pamphlets politiques qui font grand bruit à l'époque. Son socialisme se veut éclectique et combine en particulier la lutte des classes et la lutte pour l'unité nationale. L'instauration d'un État allemand unique lui paraît en effet un prérequis indispensable à l'édification d'une société émancipée. L'État serait alors garant d'une économie socialisée, gérée par de multiples coopératives décentralisée où le travailleur percevrait l'intégralité de la valeur de sa production. Lassalle prône un socialisme d'État, l'État étant pour lui non un instrument de la domination de classe, mais un outil de justice sociale[121]. Ces conceptions théoriques ont d'importantes conséquences pratiques. Pour l'ADAV, le renversement de la société bourgeoise ne dépend pas d'une révolution violente mais d'une succession de réformes graduelles. Loin de se couper du jeu politique institutionnel, le parti tente de l'utiliser. Lassalle prône ainsi un rapprochement avec Bismarck. Le chancelier accueille assez favorablement ces initiatives : selon lui, Lassalle est [122]. Les conceptions de Lassalle, qui envisage de passer au socialisme en favorisant les coopératives de production ouvrière grâce à l'aide de l'État, sont par ailleurs vivement critiquées par Marx et Engels, qui considèrent que la socialisation des moyens de production ne saurait être l'œuvre que d'un État prolétarien, c'est-à-dire du prolétariat lui-même[123].
En 1864, Lassalle meurt prématurément, à la suite d'un duel lié à une affaire sentimentale. Même si elle demeure une force politique importante, l'ADAV ne parvient pas à se remettre de la disparition de son chef[122]. D'autres socialistes allemands comme Wilhelm Liebknecht, disciple de Marx ayant rapidement rompu avec Lassalle, ou August Bebel, reprochent par ailleurs à l'ADAV de diviser les forces démocratiques en Allemagne en s'appuyant exclusivement sur les ouvriers : en septembre 1865 est créé le Parti populaire allemand (Deutsche Volkspartei) qui milite pour le suffrage universel mais se garde d'aliéner l'électorat bourgeois et limite son programme de réformes économiques aux coopératives de production[124].
À la même époque naît l'Association internationale des travailleurs (AIT, surnommée a posteriori ), créée à Londres le 28 septembre 1864, au cours d'un meeting réunissant des délégués d'associations ouvrières françaises et des représentants des trade-unions britanniques. Karl Marx, invité en dernière minute, est présent à la tribune mais sans prendre la parole. L'AIT apparaît initialement comme le simple résultat d'une convergence d'intérêts des syndicalistes français et britanniques qui souhaitent s'unir pour répondre à la conjoncture sociale et politique : son comité central provisoire réunit des Britanniques et des Français, mais aussi des émigrés allemands, italiens et suisses. Sur le plan idéologique, les partisans de Marx y côtoient des libéraux, des proudhoniens, des anciens chartistes, des trade-unionistes et des socialistes de toutes obédiences. Marx joue cependant un rôle décisif au sein du sous-comité chargé d'élaborer les statuts provisoires et la déclaration de principe : son futur adversaire, le suisse James Guillaume, lui reprochera de prendre une importance démesurée et de détourner l'AIT à son profit idéologique. L'Internationale développe bientôt son implantation dans divers autres pays européens, comme la Belgique ou l'Espagne. À la fin des années 1860, des grèves éclatent en France : le gouvernement impérial en attribue indument la paternité à l'AIT et fait réprimer les mouvements par l'armée, tandis que des militants socialistes sont arrêtés. Malgré la répression, ces mouvements contribuent à faire affluer les membres ouvriers. L'Internationale connaît son apogée au tournant de la décennie 1870[125] ; elle est cependant parcourue de conflits qui opposent Marx aux disciples de Proudhon (ce dernier étant mort en janvier 1865) et, par la suite, à l'anarchiste russe Mikhaïl Bakounine, qui rejoint l'AIT en 1868. Marx préconise l'organisation politique du prolétariat de chaque pays sous la forme d'un , qui favoriserait par ses luttes la prise du pouvoir des travailleurs ; ces derniers appliqueraient alors une dictature du prolétariat provisoire avant d'en arriver à la société sans classes. Les proudhoniens, au contraire, suivent une ligne résolument anti-étatique et refusent la lutte politique, voyant l'avenir dans le mutuellisme et les coopératives. Les disciples de Proudhon sont mis en nette minorité dès 1867 et l'influence du proudhonisme tend ensuite à décliner au sein de la nouvelle génération de militants[126].
La pensée marxiste gagne en influence dans la mouvance socialiste : Karl Marx construit une œuvre philosophique fondée sur une analyse à visée scientifique des réalités historiques, sociales et économiques et sur une vision de l'histoire dont la lutte des classes serait le moteur. En 1867, il publie le livre premier du Capital, qui n'a pas immédiatement de retentissement particulier bien que l'AIT recommande sa lecture, mais qui influence profondément à moyen terme la pensée socialiste en fournissant une somme de l'analyse critique du capitalisme. Les livres deux et trois sont complétés et publiés par Engels après la mort de Marx, Karl Kautsky se chargeant par la suite de mettre en forme d'autres ébauches de ce dernier[127]. Par opposition au courant de pensée dit du  (ou ) de Saint-Simon, Fourier, ou Proudhon, le socialisme de Marx est désigné par ses disciples, dont Engels lui-même, du nom de [128],[129]. Dans l'optique du socialisme scientifique, qui s'appuie sur le matérialisme historique, l'histoire est déclarée objet d'une science exacte, soumise à des lois de transformation issues de la nécessité pour les humains de produire la vie par le travail et l'échange[7].
Au Royaume-Uni, qui en fut le premier pilier, l'AIT est un échec dès 1867-1868 : les trade-unionistes n'acceptent pas les conceptions de Marx quant au remplacement de la lutte économique par une lutte politique dans le cadre d'un parti. Le syndicalisme britannique adopte une orientation très nettement réformiste et s'éloigne de l'AIT[130].
À la même époque, dans plusieurs autres pays européens, l'Internationale se développe, notamment dans le sillage de l'enthousiasme révolutionnaire né de l'épisode de la Commune de Paris. Si la Commune — œuvre de tendances politiques disparates — ne constitue à proprement parler une révolution socialiste, on trouve en son sein des membres de l'AIT comme Eugène Varlin, Charles Longuet (futur gendre de Marx) ou Benoît Malon. Les socialistes de la Commune comptent parmi eux des blanquistes, des proudhoniens, ainsi que des marxistes, ceux-ci étant nettement minoritaires et ne jouant pas un rôle essentiel. La révolution parisienne de 1871, puis son écrasement lors de la semaine sanglante, constituent pour le socialisme français des faits déterminants : le mouvement ouvrier français et le socialisme se trouvent unifiés par l'expérience de la lutte ; plusieurs personnalités marquantes du socialisme français, comme Édouard Vaillant, Jean Allemane ou Jules Guesde, participent à la Commune, cette expérience contribuant à forger leur engagement politique ultérieur. La récupération de la mémoire de la Commune donne cependant lieu, dans les décennies qui suivent, à des interprétations parfois conflictuelles de l'événement[131],[132].
L'épisode de la Commune de Paris a en outre un grand retentissement dans toute l'Europe : en donnant le sentiment que la révolution sociale est possible, il influe sur les contextes politiques de divers pays. En Belgique, les organisations ouvrières s'épanouissent. En Italie, l'achèvement du processus d'unification du pays, le déclin du mazzinisme — après notamment que Mazzini ait condamné la Commune de Paris — le soutien de Giuseppe Garibaldi pour qui le socialisme est le , contribuent à faire naître de nombreuses sections italiennes de l'AIT. En Espagne, la question de la succession au trône, la reprise du conflit carliste puis la proclamation de la République en 1873 favorisent l'engagement révolutionnaire. En Hongrie et en Russie, les mouvements d'inspiration socialiste, jusque-là surtout limités au milieu des émigrés politiques, se développent après la Commune : les idées de l'AIT contribuent directement à former les Narodniki () russes, influencés par les idées de Herzen[133].
Malgré ces progrès de l'idéal socialiste, l'Internationale demeure profondément divisée entre d'une part Bakounine et ses disciples, libertaires et tenants d'un socialisme  et d'autre part les  menés par Marx. Bakounine fait par ailleurs une recrue de choix en ralliant à lui James Guillaume. Au congrès de Bâle en 1869, les partisans de Marx sont mis en minorité. Si les deux factions semblent se réconcilier temporairement dans leurs jugements sur la Commune de Paris, le conflit reprend dès la conférence internationale de Londres en septembre 1871 : Marx fait voter une résolution stipulant que la conquête du pouvoir politique doit être le premier devoir de la classe ouvrière. L'année suivante, lors du congrès de La Haye, Bakounine et Guillaume sont exclus par la majorité des participants. Le siège du Conseil général est transféré hors d'Europe, à New York, ce qui équivaut à une liquidation déguisée de l'Association internationale des travailleurs. L'Internationale s'étiole aux États-Unis et, en juillet 1876, la conférence de Philadelphie prononce la dissolution du Conseil général. De leur côté, Bakounine et Guillaume réorganisent dès 1872 les  mais l'Internationale dissidente, qui fonctionne sur la base de l'autonomie fédérale et prône la grève générale comme moyen d'émancipation du prolétariat, ne réunit que de petites minorités et cesse rapidement d'exister. Bakounine lui-même s'en éloigne dès 1874 et meurt deux ans plus tard. Plusieurs communards en exil, comme Benoît Malon et Jules Guesde, l'abandonnent à leur tour. L'Internationale anti-autoritaire tient son dernier congrès en 1877 et sa fédération jurassienne tient le sien en 1880[130],[134].
Les divisions, puis la fin, de l'Internationale ne portent pas de coup d'arrêt aux progrès des idées socialistes : à la fin des années 1860, August Bebel et Wilhelm Liebknecht cherchent à construire un  dont la Fédération des Associations ouvrières allemandes constituerait le noyau ; ils s'emploient à constituer des coopératives syndicales internationalistes, indépendantes du Parti populaire mais orientées dans l'esprit de l'Internationale. En août 1869, lors du congrès de la Fédération à Eisenach, le Parti social-démocrate des travailleurs (Sozialdemokratische Arbeiterpartei, SDAP, également traduit par Parti travailliste social-démocrate) est créé[135].
Lors de la guerre de 1870, Bebel et Liebknecht, suivis par les députés lassalliens, s'opposent au vote des crédits militaires, ce qui leur vaut d'être arrêtés par le gouvernement de Bismarck et inculpés de haute trahison. La Commune de Paris suscite l'enthousiasme des socialistes allemands, tant chez les lassalliens que chez les . Le contexte de l'unité allemande rend plus pressants les appels à l'unification des familles socialistes allemandes, que réclament notamment les syndicalistes internationalistes lors de leur congrès de 1872[136],[137]. S'il ne connaît pas encore l'unité sur le plan politique, le socialisme se diffuse dans les milieux intellectuels : le terme de  désigne ainsi les travaux de différents universitaires allemands, comme Gustav von Schmoller ou Adolph Wagner, qui s'emploient à relier économie et morale pour faire de l'économie politique un instrument de réorganisation sociale[138].
En 1874, l'Empire allemand est dans un contexte économique difficile, après des années de prospérité, ce qui entraine des manifestations ouvrières et un progrès électoral des socialistes ; en juin 1874, la justice allemande obtient l'interdiction provisoire de l'ADAV et de la section berlinoise du SDAP. Dans ce contexte, des négociations en vue de la fusion des mouvements s'ouvrent en février 1875 à Gotha et aboutissent à un texte de compromis d'inspiration marxiste mais faisant une large place aux idées lassalliennes. Le congrès de Gotha débouche, le 27 mai 1875, sur l'adoption d'un nouveau programme pour le SDAP, tandis que l'ADAV disparait définitivement. Marx et Engels se montrent mécontents de ce programme, qui leur paraît opportuniste et antiscientifique, mais ne rompent pas pour autant avec le parti social-démocrate. Marx rédige à cette occasion le texte Critique du programme de Gotha, connu uniquement à l'époque de quelques initiés, dans lequel il dénonce les idées lassalliennes et affine la notion de dictature révolutionnaire du prolétariat. En 1878, à la suite de deux tentatives d'attentats anarchistes contre l'Empereur qu'il attribue aux sociaux-démocrates, Bismarck fait voter une loi d'exception contre les socialistes, dont les organisations sont interdites. Des personnalités socialistes peuvent néanmoins continuer à se faire élire députés au Reichstag, de manière individuelle[139].
Le départ de Bismarck, en 1890, est rapidement suivi de l'abrogation de la législation antisocialiste : les syndicats se développent et le parti allemand, qui prend en 1890 son nom définitif de Parti social-démocrate d'Allemagne (Sozialdemokratische Partei Deutschlands, SPD), gagne de nombreux députés. Le SPD apparaît bientôt comme le modèle des autres partis européens, grâce à l'alliance étroite entre parti et syndicats : les dirigeants syndicaux sont ainsi souvent des élus du parti. Autour du double noyau parti-syndicats se constitue un réseau d'organisations parallèles — coopératives de consommation, sociétés d'éducation... — qui constituent bientôt un  ouvrière dans l'Empire allemand. Les progrès électoraux du SPD sont désormais constants : en 1912, il compte plus d'un million d'adhérents et devient le premier parti du Reichstag, avec 35 % des suffrages et 110 députés[136],[137].
Dans le reste de l'Europe, le dernier quart du XIXe siècle est accompagné d'un essor décisif des organisations ouvrières : le mouvement socialiste est désormais principalement organisé sous forme de partis politiques[140] ; entre les années 1870 et 1890, de nombreux partis socialistes apparaissent dans l'ensemble des pays européens[141]. Le Parti socialiste ouvrier espagnol (PSOE), fondé en 1879, n'est au départ qu'un modeste groupe madrilène mais se structure véritablement à partir de 1888 ; cette même année, Pablo Iglesias Posse, fondateur du PSOE, crée également l'Union générale des travailleurs, syndicat allié du parti et qui constitue dans les faits la force la plus importante. Le mouvement socialiste espagnol se développe dans un contexte politique tendu, marqué par une forte présence anarcho-syndicaliste et révolutionnaire[142].
Aux Pays-Bas, la Ligue sociale-démocrate naît en 1881[143] ; elle connaît une scission quand une minorité, qui souhaite créer un parti social-démocrate sur le modèle allemand, s'oppose à la majorité anti-étatiste conduite par Ferdinand Domela Nieuwenhuis et fonde le Parti social-démocrate des ouvriers[144]. Le Parti ouvrier belge, apparu en 1885, compte parmi ses fondateurs Louis Bertrand et César De Paepe[145].
En Autriche-Hongrie, le congrès d'unification du Parti ouvrier social-démocrate autrichien (Sozialdemokratische Arbeiterpartei, SDAP) se tient en décembre 1888 ; le parti est dominé jusqu'en 1918 par la personnalité de son fondateur Victor Adler. Aux premières élections au suffrage universel en 1907, le parti réalise une percée immédiate. L'imprégnation marxiste de la social-démocratie autrichienne se traduit par la naissance d'un courant de pensée pragmatique et d'une grande vitalité intellectuelle, baptisé du nom d'austromarxisme, au sein duquel les travaux d'Otto Bauer se caractérisent par une réhabilitation de l'idée de Nation[146]. Toujours en Autriche-Hongrie, le Parti social-démocrate de Hongrie est créé en 1890 avec le soutien du parti autrichien, mais doit bientôt affronter plusieurs autres partis socialistes concurrents[147].
En Suisse, la présence durant le XIXe siècle de nombreux exilés politiques allemands, notamment après les révolutions de 1848, favorise la diffusion des idées socialistes et du syndicalisme, donnant naissance à la fin du siècle au Parti socialiste suisse (en allemand : Sozialdemokratische Partei der Schweiz, en italien, Partito Socialista Svizzero)[148]. En 1889 est fondé, sur l'entente de 69 associations ouvrières, le Parti social-démocrate suédois des travailleurs, qui met rapidement l'accent sur la primauté de l'action parlementaire et la conquête du suffrage universel, qui ne peut être obtenu que par une alliance avec les partis bourgeois de gauche. Dès le premier congrès du parti suédois, la perspective de la révolution violente est écartée et, si la référence marxiste n'est initialement pas absente, elle demeure lettre morte[149],[150]. Le Parti travailliste norvégien est fondé en 1887[151]. Le Parti social-démocrate du Danemark est fondé dès 1871 comme section de l'Internationale, mais ne réalise de percée électorale qu'à partir de 1883 : en 1898, les socialistes danois reçoivent l'appui de diverses organisations syndicales[152].
Filippo Turati compte en 1892 parmi les fondateurs du Parti des travailleurs italiens, qui devient l'année suivante le Parti socialiste des travailleurs italiens, avant d'être reformé en 1895 sous le nom de Parti socialiste italien : dans ses toutes premières années, le parti italien attire dans ses rangs les faisceaux siciliens, qui mettent alors la Sicile dans un état quasi-insurrectionnel[153]. Le Parti socialiste polonais (PPS), qui milite pour l'indépendance de la Pologne par rapport à l'Empire russe, apparaît en 1892 ; il doit affronter dès l'année suivante la concurrence d'un autre parti socialiste, la Social-Démocratie du Royaume de Pologne (SDKP), fondée par Rosa Luxemburg et Leo Jogiches. Contrairement au PPS, la SDKP est opposée à l'indépendance de la Pologne, considérant que la classe ouvrière n'a pas à être divisée par le nationalisme[154].
En France, durant les premières décennies de la Troisième République, la mouvance socialiste demeure très divisée et le reste plus longtemps qu'en Allemagne. À l'extrême-gauche de la tendance républicaniste, le socialisme exerce un attrait qui conduit à ce que ce courant soit baptisé . Le terme de radical-socialisme apparaît en 1881, lorsqu'il est utilisé par le comité de soutien de Georges Clemenceau. Des mouvements apparaissent comme l'Alliance socialiste républicaine dirigée par Stephen Pichon, un proche de Clemenceau, qui ambitionne ainsi de se rapprocher des militants socialistes parisiens tout en se situant dans une ligne nettement anti-marxiste et plutôt dans la tradition blanquiste. La greffe ne prend cependant pas et dans les décennies suivantes, malgré leur utilisation de l'adjectif socialiste, les radicaux-socialistes demeurent, sur le plan partisan, une famille politique distincte des socialistes proprement dits[155]. En 1901, les courants du radicalisme donnent naissance au Parti républicain, radical et radical-socialiste[155]. Les radicaux se distinguent notamment des socialistes par leur attachement à la propriété privée : le Parti radical, s'il prône dans son programme officiel, adopté en 1907, la disparition du salariat et la lutte contre la , fait de la défense de la propriété individuelle l'un des piliers de son idéologie. Le radicalisme exalte avant tout les travailleurs indépendants, attirant un électorat au sein de la paysannerie propriétaire ou des classes moyennes[156]. Dans le paysage politique français, le radical-socialisme est avant tout identifié à la défense de la République, de la laïcité, et des  contre les . Le Parti radical, dont l'électorat est avant tout provincial voire rural, est avant tout le parti des , compris comme l'ensemble des petits-bourgeois et des prolétaires : il prône non pas la lutte, mais la collaboration des classes[157]. Des radicaux continuent par la suite de se réclamer explicitement du socialisme, à l'image d'Alfred Naquet qui, dans son ouvrage Socialisme collectiviste et socialisme libéral (1890), théorise un  alternatif à celui préconisé par le marxisme, qu'il estime liberticide[158],[150]. Les radicaux français sont par ailleurs rapidement associés au décalage entre leur discours très orienté à gauche et leur gestion modérée des affaires publiques, qui ne se traduit que par de modestes progrès sociaux[159].
Dans les années 1880-1890, les socialistes français, dont le poids politique est tout d'abord assez faible, se répartissent entre différents groupes, syndicats et cercles derrière des personnalités comme Jules Guesde et le gendre de Marx Paul Lafargue (dirigeants du Parti ouvrier français, dont le programme, rédigé par Guesde, obtient l'imprimatur de Marx en personne[160]), Édouard Vaillant (Parti socialiste révolutionnaire), Paul Brousse (Fédération des travailleurs socialistes de France) ou Jean Allemane (Parti ouvrier socialiste révolutionnaire). Les différentes tendances socialistes françaises rivalisent entre elles, parfois violemment, entre autres sur la question des syndicats. Les guesdistes, de tendance marxiste, souhaiteraient subordonner le syndicat au parti, tandis que les allemanistes, attachés à un gouvernement par la base et au principe du mandat impératif, prônent le contraire[161]. L'anticapitalisme des socialistes français entraîne par ailleurs des passerelles avec certains courants antisémites : Édouard Drumont se présente volontiers à l'époque comme socialiste, et multiplie les contacts avec les guesdistes ou les socialistes indépendants, contribuant également à populariser l'idée de la  chez les disciples de Blanqui ou de Proudhon[162].
En 1892 est créée la Fédération des Bourses du travail, animée par Fernand Pelloutier qui, opposé au socialisme des , prône la grève générale comme instrument politique et pose les bases du syndicalisme révolutionnaire. En 1893, les socialistes français font leur première percée électorale et obtiennent un peu moins d'une cinquantaine de députés : ce succès entraîne un affaiblissement de la ligne révolutionnaire au profit du socialisme de parti. Le manque d'unité du mouvement ouvrier en France et le faible niveau théorique des marxistes français — le socialisme demeurant en France fortement attaché aux traditions républicaines — fait que le socialisme français ne représente, aux côtés notamment des radicaux, que l'un des éléments de la gauche, au contraire des sociaux-démocrates de l'Europe du centre et du nord[161]. Les guesdistes doivent quitter la Fédération des syndicats en 1894 et doivent se résoudre en 1895 à voir apparaître une Confédération générale du travail (CGT) qu'ils ne pourront jamais contrôler. En 1896, un programme minimum commun semble accepté par les différents groupes socialistes à la suite notamment des efforts d'Alexandre Millerand pour poser les jalons d'un rassemblement, mais les péripéties et conflits liés à l'affaire Dreyfus repoussent encore l'unification des socialistes français. L'entrée, en 1899, de Millerand dans le gouvernement de Pierre Waldeck-Rousseau, où il siège aux côtés du , le général Galliffet, provoque l'indignation des guesdistes. L'unité est à nouveau repoussée, bien que Millerand ait pu mettre à profit sa présence au gouvernement pour obtenir la réduction progressive de la journée de travail à dix heures. La famille socialiste française demeure divisée par les oppositions entre des dirigeants comme Jules Guesde et Jean Jaurès : au contraire de Guesde, Jaurès soutient le gouvernement de Waldeck-Rousseau puis celui de son successeur, le radical Émile Combes, arrivé au pouvoir au moment de la victoire du bloc des gauches[163]. L'alliance des socialistes et des radicaux au sein du bloc des gauches souffre cependant de l'indifférence manifestée par Combes à l'égard de la question sociale : les socialistes doivent rapidement renoncer à le soutenir[164].
Le Parti socialiste français est fondé en 1902 par la fusion des socialistes indépendants, de la Fédération des travailleurs socialistes de France et du Parti ouvrier socialiste révolutionnaire, mais ce n'est qu'en avril 1905, lors du congrès du Globe, qu'est réalisée l'union de la majorité des socialistes français, réclamée par l'Internationale, au sein de la Section française de l'Internationale ouvrière (SFIO). Jaurès salue à cette occasion le . Une petite minorité de socialistes indépendants continue cependant de se tenir à l'écart de la SFIO. En décembre 1905, Jaurès et Aristide Briand obtiennent le vote de la loi de séparation des Églises et de l'État[165],[163].
Au Royaume-Uni existent diverses écoles de pensée socialistes, qui ne donnent pas lieu dans l'immédiat à un mouvement politique d'envergure. En 1884 naît la Fabian Society, cercle de réflexion sans réelle unité idéologique mais prônant un socialisme d'État qui consisterait en la garantie par l'État de l'égalité et de la sécurité : le socialisme fabien, dans sa première version, ne doit rien au marxisme et constitue un type de [166]. À compter des toutes premières années du XXe siècle apparaît au Royaume-Uni une forme de parti social-démocrate, comparable au modèle allemand par l'alliance entre le parti et les syndicats, à ceci près que les syndicats sont antérieurs au parti et lui donnent naissance : en outre, le terme utilisé n'est pas « social-démocratie » mais « labour movement » — soit mouvement ouvrier — dont la contraction labourism est traduite en français par travaillisme. La volonté des syndicats britanniques d'avoir une représentation au Parlement est à l'origine du mouvement travailliste : en 1893 est fondé le Parti travailliste indépendant, dirigé par James Keir Hardie, sur un programme collectiviste et démocratique. Mais l'opinion publique britannique se montre peu réceptive au socialisme et le parti connaît un revers électoral décourageant en 1895. Ce n'est qu'en 1899 que les syndicalistes statuent, à leur congrès annuel, sur la nécessité d'avoir un relais politique : en 1900 est créé, lors d'un congrès spécial, le Comité pour la représentation du travail, qui devient en 1906 le Parti travailliste (Labour Party), d'abord dirigé par Hardie, puis par Ramsay MacDonald[167]. Le Parti travailliste britannique n'a comme vocation, lors de sa création, que d'harmoniser le rôle des syndicats pour parvenir à une transformation de la société, et non d'avoir une identité politique propre[168].
Le social-libéralisme inspiré des idées de John Stuart Mill se développe par ailleurs au début du XXe siècle dans les milieux intellectuels britanniques : Leonard Trelawny Hobhouse, notamment, se fait dans son livre Liberalism (paru en 1911) le théoricien d'un  concevant la liberté comme celle de tous les membres de la communauté, et qui ferait donc une place plus large à l'intervention étatique. Il envisage une forme de socialisme libéral, dans lequel l'État ne devrait pas contraindre l'initiative mais au contraire, en corrigeant les injustices, garantir les conditions de l'initiative individuelle. Le socialisme libéral, tel que le conçoivent des auteurs comme Hobhouse et John Atkinson Hobson, se situe à l'opposé, aussi bien du socialisme hostile à l'initiative et à la propriété privées, que du libéralisme dogmatique : il a pour principe de respecter les initiatives venues d'en bas et le développement personnel de chacun[169].
L'évolution du socialisme européen et une partie de ses débats ont lieu dans le cadre d'une nouvelle organisation internationale, appelée Internationale ouvrière (dite également  et ), fondée à Paris en 1889 : l'année du centenaire de la Révolution française, les représentants de divers pays européens décident de rassembler un grand congrès ouvrier international, dont la préparation est confiée à la Fédération des travailleurs socialistes de France de Paul Brousse : mais la rivalité entre les possibilistes de Brousse et les marxistes de Jules Guesde aboutit à la tenue de deux congrès séparés. Le congrès  décide, entre autres, de faire du 1er mai — future Fête des travailleurs — la date d'une manifestation mondiale pour la réduction légale de la journée du travail. En 1891 se tient à Bruxelles un seul et unique congrès, les socialistes belges ayant réussi à faire fusionner les deux courants issus des congrès de Paris. En 1896, le Congrès de Londres tourne à l'opposition frontale entre d'une part les tenants de la conquête du pouvoir politique via une logique de parti et de l'autre les partisans de l'autonomie ouvrière, autrement dit les antiparlementaires, syndicalistes révolutionnaires et libertaires. Les anarchistes sont ensuite exclus de l'Internationale, l'avantage allant aux tenants du socialisme de parti, alors que la démocratie parlementaire tend à s'affirmer en Europe. Jusqu'en 1900, l'Internationale ouvrière existe essentiellement par le biais de ses congrès ; à compter de cette date, un Bureau socialiste international (en) (BSI) est institué, un secrétariat permanent fixé à Bruxelles assurant la continuité. À partir de 1905, le Belge Camille Huysmans devient le secrétaire du BSI, qui dispose désormais de ses propres publications[170]. L'Internationale ouvrière conserve une grande diversité idéologique, bien que le théoricien marxiste allemand Karl Kautsky soit parfois présenté comme [171].
En Europe, la Russie connaît un contexte particulier du fait de la clandestinité imposée aux idées socialistes : à la fin des années 1860, et plus encore après la Commune de Paris, les courants socialistes russes sont renforcés par le biais de la section russe de l'Association internationale des travailleurs. Les tendances bakouniniennes et populistes dominent alors au sein du socialisme russe : en 1873-1874, les Narodnikis () tentent une  consistant à se rapprocher de la population des campagnes pour leur amener les idées nouvelles. Mal accueillis par les paysans, bientôt traqués par les autorités, les Narodniks connaissent un échec sans appel : certains rescapés de la  fondent une nouvelle organisation, mieux organisée, Terre et Liberté (Земля и воля : Zemlia i Volia). Certains militants se tournent vers les attentats, comme Véra Zassoulitch, qui tire sur le gouverneur de Saint-Pétersbourg ; d'autres, comme Gueorgui Plekhanov, répudient l'action violente. Dans le dernier tiers du XIXe siècle, la Russie connaît une série d'attentats de la part d'organisations révolutionnaires, comme Narodnaïa Volia, qui assassine en 1881 le tsar Alexandre II. Entretemps, le marxisme commence à se diffuser dans les cercles intellectuels et militants : en 1872 paraît la traduction en russe du Capital, ouvrage que la censure tsariste avait jugé peu accessible au grand public et par conséquent inoffensif. Les idées de Marx se répandent rapidement parmi les révolutionnaires russes qui, déçus par leurs rapports avec la paysannerie, tournent maintenant leurs espoirs vers la classe ouvrière. Marx lui-même, qui voyait naguère dans la Russie un pays trop peu industrialisé pour y voir l'apparition d'une avant-garde révolutionnaire, suit désormais avec intérêt la progression de ses thèses en Russie. Gueorgui Plekhanov, exilé en Suisse, se fait le principal diffuseur en Russie des idées de Marx : en 1883, il fonde à Genève, avec Pavel Axelrod et Véra Zassoulitch, le groupe Libération du Travail qui s'emploie à diffuser sur le sol russe des ouvrages marxistes. En 1899, le Parti ouvrier social-démocrate de Russie (POSDR) est formé à Minsk lors d'une réunion clandestine. Le POSDR attire les militants révolutionnaires mais, du fait des conditions de clandestinité, fonctionne pour l'essentiel grâce au réseau des exilés : le journal Iskra, publié à l'étranger, tient dans les faits le rôle de  du parti[172],[173].
Les idées socialistes se diffusent par ailleurs hors d'Europe : aux États-Unis, les doctrines socialistes françaises, britanniques et, surtout, allemandes, gagnent des émules dans la première moitié du XIXe siècle. En 1876, quelques jours après la dissolution de l'Association internationale des travailleurs sur le sol américain, naît le Workingmen's Party of the United States. Créé par la fusion de l'AIT avec deux groupes lassalliens, le parti compte environ 2 500 membres et prend l'année suivante le nom de Parti ouvrier socialiste d'Amérique. Les socialistes américains participent ensuite à la Fédération américaine du travail mais leurs espoirs de gagner les syndicats américains à la cause socialiste sont déçus. Le socialisme, aux États-Unis, touche aussi bien des milieux ouvriers qu'intellectuels, via notamment les œuvres d'écrivains comme Edward Bellamy : la diffusion des idées socialistes variant beaucoup quant à son degré comme à la nature des idées, en fonction des contextes géographiques et culturels américains. Un courant du socialisme chrétien, d'inspiration évangélique, se développe notamment dans le pays. Le Social Democratic Party of America (en) est créé en 1898 ; il donne naissance trois ans plus tard au Parti socialiste d'Amérique, en fusionnant avec un groupe d'anciens militants du Parti ouvrier. Le socialisme américain ne parvient pas obtenir de représentation parlementaire importante et demeure globalement marginal ; il se distingue par un enracinement dans la tradition populiste et le christianisme évangélique[174].
Dans l'Empire britannique, les idées travaillistes s'implantent progressivement et de manière inégale avec l'apparition des syndicats, au Canada ou en Afrique du Sud. Si, au Canada, le socialisme ne progresse guère, le processus d'implantation de l'activité syndicale qui donne naissance au Parti travailliste britannique a les mêmes résultats dans d'autres dominions britanniques : le Parti travailliste australien apparaît en 1891, avant même son homologue du Royaume-Uni ; le Parti travailliste néo-zélandais et le Parti travailliste sud-africain naissent en 1910[175].
En Asie et dans le monde arabe, les contacts avec l'occident favorisent la pénétration des idées socialistes dans les milieux intellectuels. Le Japon est le seul pays asiatique dont les organisations possèdent des liens effectifs avec la IIe Internationale : de nombreuses organisations socialistes japonaises, à l'existence souvent éphémère, se succèdent entre la fin du XIXe siècle et les premières décennies du XXe siècle[176]. En Chine, les idées socialistes se diffusent, notamment au début du XXe siècle, chez des militants politiques et des intellectuels dont beaucoup sont alors exilés en Occident ou au Japon. Les influences socialistes se retrouvent ainsi chez certains réformateurs partisans de Kang Youwei ou chez des nationalistes républicains membres de l'entourage de Sun Yat-sen[177]. En Perse, les idées socialistes se développent, à la faveur de la révolution constitutionnelle, entre 1906 et 1911[178]. Le courant socialiste exerce également une influence sur le mouvement sioniste[179], notamment sur l'organisation des premiers kibboutz[180].
En Amérique latine, les idées socialistes sont peu répandues jusque dans la deuxième moitié du XIXe siècle et touchent surtout quelques élites comme les saint-simoniens argentins. L'immigration en provenance d'Europe favorise ensuite la pénétration du socialisme en Amérique du Sud, où les thèses socialistes se diffusent dans les milieux syndicaux. Au Mexique, elles voisinent avec l'anarchisme et le christianisme social[181],[182]. Les associations de travailleurs apparaissent à partir de la fin des années 1850. Après 1871, le retentissement mondial de la Commune de Paris contribue à attirer des travailleurs sud-américains vers l'AIT. Entre la disparition de la Première Internationale et l'apparition de la Deuxième, le socialisme libertaire se développe en Amérique latine par le biais de l'immigration italienne dans différents pays (Argentine, Brésil, Uruguay). Errico Malatesta, installé en Argentine, contribue à diffuser l'anarcho-communisme en créant en 1885 le journal bilingue La Question sociale. En 1896 est créé le Parti socialiste argentin, dont l'organisation et le programme inspirent les socialistes de divers pays. Au Brésil, les idées socialistes se diffusent surtout dans les milieux anarcho-syndicalistes : des premiers partis socialistes brésiliens apparaissent au tournant du XXe siècle, mais leur audience demeure faible. En Uruguay, en Colombie, au Pérou, au Chili, les idées socialistes se diffusent également par l'intermédiaire des milieux syndicaux[183].
Malgré ses progrès sur tous les continents, le socialisme demeure partout éloigné des affaires de l'État, et ce jusqu'au début du XXe siècle. À cette époque, dans la quasi-totalité des pays du monde, les socialistes n'ont pas encore eu à affronter l'épreuve du pouvoir, ni la confrontation de leurs idéaux avec les impératifs d'une politique nationale. Seule l'Australie connaît avant la Première Guerre mondiale un éphémère gouvernement d'inspiration socialiste, quand le Parti travailliste australien occupe le pouvoir durant quelques mois en 1904[184].
Le socialisme européen est marqué, dès la fin du XIXe siècle, par l'opposition entre les tenants d'une ligne révolutionnaire et les partisans d'un  idéologique qui se traduirait par une évolution vers le réformisme. La social-démocratie allemande, pénétrée au départ par les idées de Lassalle au même titre que celles de Marx, adopte ensuite progressivement le marxisme comme doctrine officielle. Après la mort de Marx en 1883, Engels tient jusqu'à son propre décès en 1895 un rôle de figure tutélaire du SPD. Le parti fonctionne cependant sur l'ambiguïté entre un discours révolutionnaire et une pratique politique réformiste. Eduard Bernstein, exécuteur testamentaire d'Engels, tente de mettre en accord la théorie et la pratique de la social-démocratie ; constatant l'erreur des prédictions catastrophistes de Marx quant à l'évolution du capitalisme, il publie entre 1896 et 1898 dans la revue Die Neue Zeit une série d'articles défendant la thèse selon laquelle la transformation socialiste de la société devient possible par le parachèvement et l'élargissement des institutions politiques et économiques qui existent déjà[185].
Provoquant la  (reformismusstreit), Bernstein se livre rien moins qu'à une remise en cause du marxisme, dont il récuse le dogme matérialiste, et se porte en faux contre les thèses de la prolétarisation accrue de la société et de l'inéluctabilité de l'effondrement du capitalisme. Bernstein prône un socialisme éthique, où l'État, démocratisé par l'instauration du suffrage universel, se fait l'instrument de l'intérêt général sans être pour autant le dispensateur de toutes choses[186]. Pour lui, la social-démocratie doit cesser de se penser comme le parti du prolétariat pour devenir un vaste parti populaire et démocratique englobant les classes moyennes, et proposer simplement des réformes visant à une plus grande justice sociale[187]. Ces thèses suscitent une opposition très forte de la part des marxistes orthodoxes comme August Bebel et Karl Kautsky et sont mises en minorité en 1899 lors du congrès du SPD à Hanovre[188]. Kautsky s'oppose à Bernstein en ce qu'il continue de prôner la révolution, soit la transformation totale de la société : il n'en est pas moins, lui aussi, hostile à la violence, qui ne lui paraît nullement indispensable à la révolution, laquelle peut à ses yeux être réalisée par la voie majoritaire[187].
Malgré la condamnation de sa ligne, Bernstein n'est pas exclu du parti et continue d'y jouer un rôle important. En dépit de cette défaite apparente du réformisme, le SPD évolue vers un abandon de la ligne révolutionnaire. À la veille de la Première Guerre mondiale, le Parti social-démocrate, présidé à partir de 1913 par Friedrich Ebert, tient une ligne pragmatique, en accord avec le mouvement syndical : les acquis sociaux de la social-démocratie sont défendus sans plus songer à la révolution. Toujours officiellement marxiste, le SPD se contente désormais d'accroître son audience électorale dans une Allemagne dont la législation sociale est désormais avancée. Face à cette évolution réformiste, le courant d'extrême gauche du SPD, incarné notamment par les jungen - jeunes - (Rosa Luxemburg, Karl Liebknecht - fils de Wilhelm - , Clara Zetkin...) souhaite en finir avec la séparation entre discours révolutionnaire et pratique révolutionnaire, et prône la  comme moyen d'action politique[188]. Ce courant a tiré parti de la querelle réformiste pour étayer ses positionnements idéologiques. Dans Réforme ou révolution, Rosa Luxemburg se distingue ainsi des marxistes orthodoxes en établissant que le marxisme n'est pas scientifique mais partisan : c'est justement parce qu'il prend parti qu'il parvient à s'émanciper des catégories scientifiques infondées de la société bourgeoise[189]. Pour elle, le prolétariat doit prendre en charge sa propre destinée et les partis politiques ne doivent pas ambitionner de tenir un rôle dirigeant, mais au contraire se contenter d'un rôle d'éclaireur, et céder le pouvoir à la classe ouvrière une fois la révolution réalisée[190].
Malgré les travaux d'auteurs comme Georges Sorel, la théorie marxiste ne pénètre que modérément en France, où le socialisme en est faiblement imprégné : le guesdisme, principal courant français inspiré des idées de Marx et Engels, souhaite suivre l'exemple de la social-démocratie allemande, mais ne propose qu'un marxisme dogmatique, sans guère d'innovations théoriques. La plupart des guesdistes, à commencer par Guesde lui-même, ne maîtrisent pas l'allemand, voire l'anglais, et n'ont qu'une connaissance indirecte ou partielle des œuvres de Marx, dont la traduction en langue française est alors très incomplète. Assimilant avec difficulté le matérialisme historique et le matérialisme dialectique, beaucoup de partisans français du  se limitent à utiliser des formules marxistes sans fournir d'analyse approfondie du capitalisme français[191]. Par ailleurs, la SFIO a quelque difficulté à s'imposer en tant que parti ouvrier, ses chefs étant pour la plupart des parlementaires : la tendance ouvriériste, révolutionnaire et antiparlementaire s'exprime surtout en dehors du parti — voire parfois contre ce dernier — et au sein de la CGT[192].
Jean Jaurès, dont la personnalité tend progressivement à dominer la SFIO, intègre l'analyse marxiste du capitalisme mais demeure attaché à l'humanisme républicain : partisan d'un , il développe l'idée d'une progression vers le socialisme comme un achèvement des principes républicains et récuse la conception marxiste de l'État comme expression d'une classe sociale[192]. Si Jaurès se réclame du courant des  et des  au sens de partisans de la propriété collective des moyens de production, sa pensée fait également une large place à l'individualisme : pour Jaurès, si la nation doit être détentrice des moyens de production, elle doit déléguer ceux-ci à des coopératives et à des syndicats où l'initiative individuelle serait essentielle[193]. Jaurès réussit la synthèse en France de la démocratie et du socialisme, de la réforme et de la révolution, du patriotisme et de l'internationalisme, mais est davantage un homme de compromis qu'un bâtisseur de doctrine, sa conception du socialisme apparaissant surtout comme un humanisme progressiste et idéaliste, vécu à la manière d'un engagement religieux[194].
La social-démocratie suédoise s'éloigne d'emblée de la voie révolutionnaire[149]. En Norvège, l'évolution vers le réformisme du parti travailliste est au contraire combattue par une minorité de gauche très active[151]. Dans le Grand-duché de Finlande, État autonome de l'Empire russe, le Parti social-démocrate profite du contexte de la révolution russe de 1905 pour obtenir une réforme du système électoral : la Diète finlandaise vote une nouvelle constitution et l'adoption du suffrage universel. Mais dès le nouveau parlement instauré, les antagonismes sociaux réapparaissent : les sociaux-démocrates finlandais tiennent des positions radicales et refusent toute collaboration avec la bourgeoisie, même indépendantiste et anti-russe. Lors des élections de 1907, après une campagne particulièrement violente, les sociaux-démocrates obtiennent 80 sièges sur 200 à la chambre[195].
Aux Pays-Bas, le Parti social-démocrate des ouvriers (SDAP) est également divisé entre modérés et révolutionnaires : une partie de la jeune génération des militants reproche ses compromissions au dirigeant du parti, Pieter Jelles Troelstra (en). Henriette Roland Holst, Anton Pannekoek ou Herman Gorter comptent parmi les personnalités marquantes de l'aile révolutionnaire, qui s'exprime notamment dans le journal De Tribune. En 1909, les  scissionnent et créent le Sociaal-Democratische Partij (SDP)[196].
Le Parti socialiste italien, quant à lui, voit la lutte en son sein entre réformistes et révolutionnaires tourner au net avantage de ces derniers, au début du XXe siècle[153]. La pensée marxiste domine nettement le mouvement socialiste en Italie et un intellectuel comme Francesco Merlino, qui prône un dépassement de la pensée de Marx et le passage à un , demeure marginal face à des marxistes comme Arturo Labriola[197]. Dans les années qui précèdent la Première Guerre mondiale, Benito Mussolini, alors proche du syndicalisme révolutionnaire et représentant de l'extrême gauche du PSI, devient l'un des principaux dirigeants du parti[198].
Les sociaux-démocrates russes, réprimés par les autorités tsaristes, demeurent quant à eux dans une optique révolutionnaire. Un autre parti socialiste apparaît au début du XXe siècle en Russie, avec le Parti socialiste révolutionnaire (ou S-R), dirigé notamment par Viktor Tchernov et qui se réclame directement des populistes[199]. Contrairement au sociaux-démocrates russes, les S-R sont un parti axé avant tout sur la défense de la paysannerie, dont ils considèrent qu'elle tient un rôle historique privilégié en Russie. Le parti S-R se divise cependant entre tenants de la défense catégorielle de la paysannerie et partisans de l'action individuelle et terroriste[200].
En 1902, Vladimir Oulianov dit , militant du Parti ouvrier social-démocrate de Russie, publie le traité politique Que faire ?, dans lequel il prône la prise du pouvoir par une organisation strictement centralisée de . Dès le second congrès du POSDR, tenu en juillet 1903 à Bruxelles, le parti se divise entre les partisans de Lénine et ceux de Julius Martov. La motion de Lénine est d'abord mise en minorité par celle de Martov mais le départ du congrès des délégués des courants du Bund et des  permettent ensuite à Lénine d'obtenir la majorité et d'affermir le contrôle de sa tendance sur le comité central et le journal du parti. Cet épisode aboutit à ce que les partisans de Lénine soient désormais surnommés bolcheviks () et ceux de Martov mencheviks (). Le milieu socialiste russe demeure divisé et parcouru de conflits incessants. À l'occasion de la révolution de 1905, bolcheviks et mencheviks retournent en Russie pour participer au soulèvement populaire, qui a vu l'apparition en Russie de conseils ouvriers (en russe : Soviets). Léon Trotski devient en octobre vice-président du soviet de Saint-Petersbourg ; Lénine, arrivé en novembre, prône l'instauration d'un gouvernement des travailleurs. Mais après la publication par le tsar Nicolas II du manifeste d'octobre, l'opposition est divisée et le mouvement s'essouffle. L'instauration, en Russie, d'une ébauche de régime parlementaire permet la légalisation des organisations socialistes : mencheviks et bolcheviks obtiennent des élus à la Douma d'État, ce qui entraîne de vifs débats au sein des bolcheviks. Repartis en exil, les dirigeants socialistes russes comme Lénine et Martov ne renoncent pas à leurs ambitions révolutionnaires, bien que celles-ci apparaissent désormais peu réalisables. Au sein de l'Internationale ouvrière, la division permanente du parti russe suscite l'inquiétude : Rosa Luxemburg et Karl Kautsky, notamment, s'opposent à la politique suivie par Lénine et le Bureau socialiste international adopte une résolution condamnant les bolcheviks[172],[201],[202],[203].
À la fin du XIXe siècle et au début du XXe siècle, les dirigeants de l'Internationale ouvrière évoquent fréquemment le risque de guerre et les actions à mener dans cette éventualité. Jean Jaurès, résolument pacifiste, préconise d'empêcher la guerre par une grève générale conduite à un niveau international. Mais rien n'est dit sur l'attitude des socialistes si un conflit finit par éclater[204]. Le déclenchement en 1914 de la Première Guerre mondiale constitue pour l'Internationale ouvrière, qui se trouve alors au sommet de son audience et de sa puissance, un désastre à la fois politique et moral[205] : l'Internationale n'a mis en œuvre aucune stratégie pour empêcher la guerre, les principaux partis socialistes s'étant au contraire ralliés aux politiques de défense nationale à l'approche du conflit. Jaurès, l'un des principaux opposants socialistes français à la guerre, est assassiné le 31 juillet 1914, trois jours avant le début du conflit[206]. En France, trois dirigeants socialistes, Jules Guesde, Marcel Sembat et Albert Thomas, deviennent membres de gouvernements d' ; en Belgique, Émile Vandervelde participe au gouvernement alors que le pays est envahi ; en Allemagne et en Autriche-Hongrie, les sociaux-démocrates demeurent écartés des affaires mais soutiennent la politique de leurs gouvernements respectifs, au nom du  (équivalent allemand de l'expression )[204]. En décembre 1914, le siège du Bureau socialiste international est transféré à La Haye, en pays neutre, mais Camille Huysmans met l'organisation en veilleuse[207].
Durant la guerre mondiale, trois tendances s'affirment progressivement au sein de la mouvance socialiste. Une tendance  prône le soutien à l'effort de guerre : cette tendance est majoritaire dans la plupart des pays belligérants jusqu'en 1918. Elle domine notamment chez les socialistes français et allemands, tandis que les Italiens se distinguent en adoptant l'attitude contraire : Mussolini est ainsi exclu du parti pour son soutien à l'entrée en guerre de l'Italie[198].
Une tendance  milite en faveur d'une paix de compromis sans annexions : elle est notamment représentée en France par Jean Longuet (fils de Charles et petit-fils de Karl Marx) et en Allemagne par Karl Kautsky et Eduard Bernstein. Enfin, une tendance  disparate - qui compte des personnalités comme Lénine, Karl Liebknecht ou Rosa Luxemburg - considère que, contre la guerre, la  doit être mise à l'ordre du jour. Lénine préconise, plus particulièrement, le  : les travailleurs sont censés lutter contre leur propre gouvernement, sans craindre l'éventualité de précipiter sa défaite militaire, qui favorisera au contraire la révolution. Les socialistes minoritaires s'organisent durant le conflit, passé le choc de l'entrée en guerre : en 1915, sur l'initiative du Parti socialiste italien, la conférence de Zimmerwald réunit, en Suisse, 38 participants représentants 11 pays. Les pacifistes comme les  y sont représentés. Lénine, qui assiste à la conférence, propose la rupture avec les sociaux-patriotes et le lancement d'une troisième Internationale, mais il n'est suivi que par 5 délégués. Léon Trotski rédige le Manifeste de Zimmerwald, approuvé par le congrès, qui condamne la guerre et l'Union sacrée sans pour autant appeler à la violence ni à la révolution. Durant le conflit, le rapport entre les trois tendances se modifie, à mesure que l'opposition à l'Union sacrée se renforce devant un conflit prolongé et particulièrement meurtrier ; les thèses de Lénine gagnent du terrain à mesure que la  se renforce. En France, en 1918, la majorité de la SFIO s'est rangée aux thèses de Jean Longuet : le parti doit quitter le gouvernement en 1917[208],[209],[210].
En Allemagne, le SPD scissionne, les centristes et spartakistes étant exclus et fondant en avril 1917 le Parti social-démocrate indépendant d'Allemagne (USPD), qui souhaite revenir à la tradition de non-collaboration révolutionnaire : l'USPD est un ensemble de courants disparates, réunissant réformistes et radicaux ; la Ligue spartakiste constitue en son sein une tendance d'extrême-gauche autonome[211].
Le conflit mondial bouleverse les destinées du mouvement socialiste mondial en provoquant la chute de l'Empire russe : le régime tsariste, déconsidéré par son autocratie et par les défaites sur le Front de l'Est, est renversé par la révolution de février 1917. Un gouvernement provisoire, auquel participe le Parti socialiste révolutionnaire en la personne d'Aleksandr Kerenski (chef du gouvernement à partir de juillet) est mis sur pied. Mais les dirigeants bolcheviks et mencheviks, revenus sur le sol russe, ne tardent pas à entretenir une agitation révolutionnaire. Lors de la révolution d'Octobre, les bolcheviks prennent le pouvoir et, alliés dans un premier temps aux socialistes-révolutionnaires de gauche (scission des S-R), mettent en place le régime de la Russie soviétique[212]. Dans sa brochure La Révolution prolétarienne et le renégat Kautsky, publiée en 1918, Lénine défend ses conceptions de la révolution et de la dictature du prolétariat : il s'en prend violemment à Kautsky — dont il a dénoncé pendant la guerre l'opportunisme politique — et, plus largement, au réformisme et à l'idée d'une révolution socialiste non violente. La même année, les bolcheviks se rebaptisent du nom de Parti communiste, destiné selon Lénine à souligner leur identité révolutionnaire[213].
En Allemagne, l'Empire tombe lors de la révolution de novembre 1918. Friedrich Ebert, chef du SPD, devient chef du gouvernement puis le premier président de la République. Alors que la Ligue spartakiste réclame un gouvernement dirigé par les conseils d'ouvriers et de soldats apparus peu avant le renversement de l'Empire, le SPD souhaite éviter au pays les affres de la révolution russe et, soutenu par l'assemblée des délégués des conseils d'ouvriers et de soldats, engage l'Allemagne sur une voie réformiste. La Ligue spartakiste se constitue en Parti communiste d'Allemagne (KPD). La situation ne tarde pas à tourner à l'affrontement entre modérés et révolutionnaires : en janvier 1919, la tentative d'insurrection des spartakistes est écrasée par les autorités, le ministre SPD Gustav Noske s'appuyant sur les corps francs pour organiser la répression. Deux des principaux leaders spartakistes, Karl Liebknecht et Rosa Luxemburg, sont assassinés par des militaires[214].
Dans la Finlande nouvellement indépendante, une partie de l'appareil du Parti social-démocrate de Finlande mène, durant l'année 1918, une guerre civile contre les forces politique du Sénat conservateur. Un gouvernement révolutionnaire, dirigé par Kullervo Manner, est proclamé, mais les  sociaux-démocrates sont finalement vaincus par les  conservateurs. Le parti finlandais connaît alors une scission : les révolutionnaires, réfugiés sur le sol de la Russie soviétique, rebaptisent leur tendance Parti communiste de Finlande, tandis que ceux qui n'ont pas participé à la guerre civile conservent le nom de Parti social-démocrate et s'intègrent à la vie politique de leur pays[215].
En mars 1919 les bolcheviks organisent une conférence internationale au cours de laquelle est constituée l'Internationale communiste (Komintern), soit la Troisième Internationale que Lénine appelait de ses vœux pour remplacer la Deuxième Internationale discréditée par son attitude durant la guerre. Des partis communistes apparaissent dans la plupart des pays, souvent par scission des partis socialistes, une partie des cadres socialistes étant sensibles aux thèses léninistes et au rayonnement d'une révolution réussie[216].
En Italie, le courant  (ce mot étant à l'origine une mauvaise traduction de [217]) prend en septembre 1918 le contrôle du Parti socialiste italien, adhère dès mars 1919 à l'Internationale communiste[218] et mène durant deux ans une politique d'agitation (le biennio rosso, soit les ) qui échoue faute de direction politique. En 1921, le PSI se divise sur la question du maintien ou non dans le Komintern : les partisans de l'intégration à la Troisième Internationale font sécession pour donner naissance au Parti communiste d'Italie. L'année suivante, c'est l'aile réformiste du PSI qui quitte le parti pour former le Parti socialiste unitaire. La scission de la gauche italienne intervient au pire moment, le fascisme étant alors en pleine ascension[219] : fondée par Benito Mussolini - devenu, après son exclusion du PSI, farouchement anti-socialiste - cette nouvelle mouvance politique mêle dans son idéologie des emprunts au socialisme révolutionnaire et au nationalisme radical[220]. En 1926, quatre ans après son arrivée au pouvoir, Mussolini promulgue une série de  qui mettent en place un régime dictatorial. Les partis socialiste et communiste sont interdits ; de nombreux cadres et militants socialistes italiens, dont leur chef historique Filippo Turati, sont réduits à l'exil[219].
En France, L-O Frossard et Marcel Cachin, envoyés par la SFIO en Russie soviétique, en reviennent conquis par les idées du nouveau régime : en décembre 1920 se tient le congrès de Tours de la SFIO, au cours duquel une majorité de délégués de la SFIO choisit d'adhérer à l'Internationale communiste, fondant la Section française de l'Internationale communiste, qui se rebaptise ensuite Parti communiste français. Léon Blum s'oppose quant à lui résolument aux thèses léninistes qui lui apparaissent comme un nouvel avatar du blanquisme, soit de la prise du pouvoir non par les masses mais par une minorité organisée entraînant ces dernières. Une minorité de délégués, conduite par Blum et Jean Longuet, refonde aussitôt la SFIO[221],[222]. Blum, lors de son intervention au congrès de Tours, condamne sévèrement les pratiques des bolcheviks qui lui apparaissent non comme la dictature du prolétariat, mais comme celle d'un petit groupe. Pour Blum, les thèses de Marx, dont il juge la métaphysique  et la doctrine économique caduque, ne sauraient constituer le fondement du socialisme : récusant le matérialisme historique et le déterminisme sociologique, Blum conçoit le socialisme comme devant assurer  en installant [223]. Pour lui, si une phase transitoire de dictature du prolétariat est inévitable après une prise du pouvoir victorieuse et dans un contexte de vacance de la légalité, la révolution doit se comprendre comme la transformation de la société, et non comme la prise du pouvoir elle-même : en outre, une révolution n'a pas à être cruelle et sanglante, Blum ne la concevant pas comme une insurrection ou une guerre civile[224]. Devenu le  de la SFIO, il estime de son devoir de  dans laquelle il espère voir la famille socialiste française se réunifier. Mais au cours des années 1920, les deux  du socialisme français demeurent séparés par une hostilité grandissante. La SFIO reprend rapidement l'avantage par rapport au PCF, qui s'enferme dans des positions sectaires. Avec le reflux de la vague révolutionnaire en Europe, les communistes français perdent de nombreux militants : bien que gagnant certains bastions électoraux, ils sont nettement distancés par les socialistes lors des élections de 1924. Fait unique jusque-là, un parti formé d'exclus et de dissidents parvient à se reconstituer sur des bases solides et à dominer largement l'adversaire[221],[222].
En Norvège, le Parti travailliste adhère en 1919 à l'Internationale communiste mais rompt avec elle quatre ans plus tard, provoquant la scission du Parti communiste norvégien[151]. Dans l'Allemagne de Weimar, le SPD perd un tiers de ses sièges lors des élections de 1920 et doit quitter la chancellerie ; il demeure cependant le parti le plus important en nombre de militants et revient au pouvoir en 1928. Tout en continuant d'utiliser un vocabulaire radical et des références marxistes, le SPD a abandonné toute perspective révolutionnaire ; il doit affronter sur sa gauche le KPD, devenu un parti de masse et qui lui est irrémédiablement hostile depuis les évènements de 1919[225],[226].
Durant l'entre-deux-guerres, la révolution d'Octobre et la création de l'Internationale communiste ont de profondes conséquences sur la mouvance socialiste et sur le mouvement ouvrier en général, qui se trouvent profondément divisés[227]. L'URSS, via le Komintern, contrôle les partis communistes européens qu'elle entreprend de  — soit de réorganiser à sa convenance — à partir de 1924. Des dissidences communistes, comme la gauche communiste hostile dès le début des années 1920 à l'autoritarisme léniniste (création de l'Internationale communiste ouvrière en 1921), l'Opposition communiste internationale créée en 1930, ou le trotskisme qui se fédère à partir de 1938 au sein de la Quatrième Internationale, ne parviennent pas à concurrencer l'hégémonie de l'URSS, dont Joseph Staline devient le principal dirigeant après la mort de Lénine. Le mouvement communiste mondial stalinien — dont l'idéologie officielle prend dans les années 1930 le nom de marxisme-léninisme — apparaît comme un système centralisé où le Parti communiste soviétique joue un rôle dominant[228].
Face à l'Internationale communiste, les socialistes doivent mieux définir leur identité propre, afin de conjuguer démocratie et émancipation sociale[229] : le socialisme démocratique affirme dès lors progressivement sa singularité face au communisme[8], malgré d'importantes divisions en son sein. Au sortir de la Première Guerre mondiale, l'Internationale ouvrière est en effet très affaiblie : en 1921, plusieurs partis, dont la SFIO et le Parti social-démocrate d'Autriche, la quittent pour constituer une nouvelle Internationale, l'Union des partis socialistes pour l'action internationale, dite . En 1922, une conférence censée réunir les trois Internationales échoue, confirmant le fossé entre les familles socialistes. Lors du congrès de Hambourg de 1923, l'Internationale ouvrière et l'Union des partis socialistes pour l'action internationale fusionnent pour donner naissance à l'Internationale ouvrière socialiste, qui regroupe la majorité des partis socialistes européens. Une grande partie des cadres de la nouvelle Internationale - dont Léon Blum lui-même - s'affirment encore révolutionnaires et partisans d'une dictature du prolétariat organisée de manière démocratique. Toutefois, Blum insiste sur la nécessité de transformer la société par des moyens légaux et ne conçoit l'arrivée au pouvoir que par le biais du processus électoral : tout en continuant d'affirmer leurs crédos marxistes, les partis socialistes continuent dans les faits d'évoluer, dans leur ensemble, vers le réformisme, ce qui prend l'aspect d'une victoire différée des thèses d'Eduard Bernstein[229],[8].
Un théoricien marxiste comme Karl Kautsky reconnaît de plus en plus, dans les années 1920-1930, la nécessité de la démocratie politique[229],[8] : Kautsky consacre de nombreux écrits à l'analyse de la révolution russe, qui lui apparaît comme l'archétype de la révolution violente, et du gouvernement par la violence. Si les sociaux-démocrates envisagent toujours une violence défensive pour garantir les acquis de la révolution, ils n'en condamnent pas moins la révolution violente et le  russe. Otto Bauer, chef et théoricien du parti autrichien, condamne fermement l'expérience russe, considérant que  et que la dictature . Dans cette optique, le parti doit donc attendre d'avoir conquis la majorité pour mettre en œuvre son programme de transformation radicale et, en attendant, proportionner ses objectifs au rapport de force politique[230].
Les partis socialistes doivent faire face, outre la concurrence des partis communistes, à la Grande Dépression née du krach de 1929, ainsi qu'à la montée du fascisme et des idéologies apparentées, dont la plus puissante est le national-socialisme (Nazionalsozialismus) en Allemagne[184].
En France, la SFIO participe à la coalition du cartel des gauches, qui remporte les élections de 1924 : elle s'abstient cependant de contribuer aux gouvernements qui se succèdent jusqu'à l'échec du cartel en 1926. Des partis socialistes, sociaux-démocrates et travaillistes accèdent au pouvoir dans plusieurs pays européens, et dans des contextes politiques très différents les uns des autres, mais qui vont tous dans le sens de l'évolution vers le réformisme. Outre l'Allemagne, où Friedrich Ebert est président du Reich jusqu'à sa mort en 1925 et où le SPD tient à nouveau la chancellerie de 1928 à 1930, le Royaume-Uni connaît son premier gouvernement travailliste avec Ramsay MacDonald, premier ministre de janvier à novembre 1924. Revenu au pouvoir en 1929, MacDonald doit gérer les effets de la crise économique mondiale et adopte en 1931 une politique économique orthodoxe, qui le conduit à rompre avec son parti[229].
En Scandinavie (Suède, Norvège, Danemark), les partis sociaux-démocrates accèdent au pouvoir dans les années 1920-1930 et contribuent à la mise en place d'un système d'État-providence, plus tard désigné du nom de . En 1924, Thorvald Stauning, chef du Parti social-démocrate, devient premier ministre du Danemark et jette les bases d'un État-providence, mais ne dispose pas d'une majorité absolue : les sociaux-démocrates danois doivent participer à des gouvernements de coalition et affronter l'opposition syndicale. En 1920, Hjalmar Branting prend la tête du premier gouvernement social-démocrate en Suède, mais à c'est à partir de 1932, quand Per Albin Hansson devient chef du gouvernement, que les sociaux-démocrates suédois entament une très longue période au pouvoir, quasi-ininterrompue jusqu'en 1976. Ayant pris depuis longtemps ses distances avec le marxisme et le socialisme révolutionnaire, la social-démocratie suédoise bénéficie d'une osmose entre parti et syndicats et instaure un État-providence sans équivalent, modelant la société sans doute la plus égalitaire de l'ensemble des pays industriels[231].
L'expérience suédoise se distingue d'emblée en dissociant le socialisme de la socialisation des moyens de production, et en rendant compatibles la politique sociale et l'efficacité économique[232]. En 1935, le Parti travailliste norvégien, ayant rompu avec le bolchevisme, accède au pouvoir quand Johan Nygaardsvold devient chef du gouvernement : l'évolution du parti vers le réformisme est accélérée par sa pratique des affaires publiques et les travaillistes norvégiens bénéficient de la même osmose avec les syndicats que les sociaux-démocrates suédois pour bâtir leur propre Welfare State[231].
Le socialisme italien, réduit à la clandestinité ou à l'exil, continue néanmoins de se développer dans la mouvance de l'antifascisme. En 1930, Carlo Rosselli publie à Paris l'ouvrage Socialisme libéral, qui constitue un texte théorique du mouvement antifasciste Giustizia e Libertà et dans lequel il prône un dépassement du marxisme et la convergence entre un libéralisme sensibilisé à la question sociale, ainsi qu'un socialisme à la fois non autoritaire et non utopiste[233].
Dans le monde arabe, encore pour l'essentiel colonisé, les idées socialistes pénètrent de plusieurs manières différentes : dans des milieux d'occidentaux expatriés et des colons, parmi des minorités musulmanes dont les revendications se marient volontiers avec le communisme et enfin au sein de mouvements de jeunesse au sein desquels le socialisme se mêle au nationalisme[234]. Des intellectuels marxistes arabes et des partis communistes locaux s'intéressent aux contradictions sociales de l'Orient arabe et aux problèmes du sous-développement et de l'impérialisme[235]. Aux Indes britanniques, le Parti du Congrès indépendantiste adopte des thématiques socialistes, son chef Nehru définissant pour sa part le socialisme comme un contrôle par l'État des ressources et des moyens de production. Nehru, influencé par le socialisme fabien et opposé à l'idée d'une révolution violente, se situe dans la ligne du socialisme démocratique : le socialisme indien, revendiqué par des partis opposés, connaît cependant, dans l'entre-deux-guerres et après, d'importantes variations quant à son identité[236]. Au Mexique, le  fait durant les années 1920 son apparition dans le vocabulaire du régime politique, au discours laïque et socialisant, en place depuis la révolution de 1910[237].
Indépendamment de la social-démocratie, des partis socialistes démocratiques et du  revendiqué par les communistes, le vocable  continue d'être utilisé par les mouvements et dans les contextes les plus divers. En Allemagne, le Parti ouvrier allemand (DAP), fondé en 1919, se réclame d'un « socialisme germanique » mal défini, qui a pour but d'amener les ouvriers allemands vers le nationalisme, tout en les détournant de l'internationalisme marxiste[238]. Le socialisme et l'anticapitalisme continuent ensuite de faire partie du discours du Parti national-socialiste des travailleurs allemands (NSDAP), qui succède au DAP : ces thèmes se retrouvent essentiellement au sein de l'aile gauche de ce nouveau parti[239]. L'idéologie du NSDAP, conçue par Adolf Hitler, prend le nom officiel de national-socialisme (couramment abrégé en nazisme) : le national-socialisme n'entretient cependant aucune relation avec les mouvements socialistes et communistes, auxquels l'oppose une hostilité radicale, et qui sont interdits après l'arrivée au pouvoir d'Hitler. L'idéologie hitlérienne a en commun avec le fascisme mussolinien de réaliser des emprunts à la fois au nationalisme radical et au socialisme révolutionnaire. Le nazisme se distingue cependant du fascisme par un racisme et un eugénisme fondamentaux[220] et repose avant tout sur une doctrine nationaliste, pangermaniste, militariste et antisémite. Hitler définit pour sa part dès 1922 le  nazi comme un dévouement inconditionnel à la nation[240]. Pour lui, le véritable , dont le nom a selon lui été indûment  par la  du marxisme, est en réalité une , qui donne à l'État la mission de  et vise à susciter [241]. Le nazisme s'oppose au « socialisme international », c'est-à-dire aux mouvements socialistes internationalistes, d'inspiration marxiste ou non, dont la vocation « antipatriotique » est à l'opposé du nationalisme pangermaniste hitlérien ; il les rejoint cependant dans l'opposition à certaines formes de capitalisme et dans la mise en avant de l'intérêt général[242] au moyen d'une économie dirigiste subordonnée aux intérêts du peuple[243]. Dans le projet nazi, la lutte des classes serait évitée par l'union des classes sociales au sein d'une  (Volksgemeinschaft) soudée par la solidarité nationale et raciale[238] ; Joseph Goebbels présente ainsi le national-socialisme comme , en ce qu'il permettrait aux classes de vivre ensemble au lieu de les dresser les unes contre les autres[244]. Le national-socialisme se distingue également par l'adoption du Führerprinzip, qui consacre de manière officielle le primat absolu de l'autorité personnelle du chef — soit en l'espèce Adolf Hitler lui-même — en matière de gouvernement. Dès l'été 1930, l'aile gauche du NSDAP, menée notamment par Otto Strasser, est évincée de la direction du parti. Ceci contribue à éliminer les aspects  — au sens traditionnel du terme — du nazisme[245] ; le mot socialisme continue cependant de faire partie du vocabulaire nazi[238]. L'historien Ian Kershaw range pour sa part le nazisme parmi les  et considère que la  proposée par Hitler ne repose sur aucun concept socialiste moderne, mais au contraire sur une forme primaire de darwinisme social et d'idées impérialistes héritées du XIXe siècle[246].
Face à la montée du nazisme et des autres fascismes, les partis socialistes européens se montrent désorientés et incapables d'une action commune. L'Internationale ouvrière socialiste assimile tout d'abord l'ensemble des phénomènes fascistes à un phénomène de réaction issu de la Guerre mondiale[247]. Le SPD espère un temps préserver sa légalité avant d'être dissout en 1933, la SFIO veut empêcher l'Allemagne de se réarmer et les Scandinaves souhaitent préserver leur neutralité. Le pacifisme des socialistes, marqués par les ravages de la Première Guerre mondiale, l'emporte sur les autres considérations : au sein de la famille socialiste, les partisans de la fermeté face à l'Allemagne sont les sociaux-démocrates autrichiens, les travaillistes britanniques ainsi qu'une minorité de la SFIO (dont Léon Blum, Jean Longuet et Jean Zyromski)[248]. Les partis communistes suivent quant à eux une ligne  dictée par le Komintern et consistant à renvoyer dos à dos les fascistes et les  socialistes. Cette politique a des résultats désastreux en Allemagne, aboutissant à laisser le champ libre à l'accession au pouvoir d'Hitler : sociaux-démocrates et communistes sont interdits comme tous les autres partis[249]. En Autriche, le Parti social-démocrate est interdit : la situation politique débouche sur la brève guerre civile de février 1934, à l'issue de laquelle le Front patriotique  devient parti unique, tandis que les socialistes autrichiens sont réprimés. L'absence de stratégie cohérente de l'Internationale ouvrière socialiste conduit dans plusieurs pays européens à une résurgence du socialisme révolutionnaire. Une vague de scissions a lieu dans les partis socialistes européens : des nouveaux partis comme le Parti socialiste ouvrier d'Allemagne ou le Parti socialiste indépendant (en)néerlandais apparaissent, signe de la crise du socialisme européen due à son impuissance face au danger. Des mouvements comme le Parti travailliste indépendant britannique (qui se détache du Parti travailliste) le Parti socialiste ouvrier d'Allemagne, ou le Parti ouvrier d'unification marxiste (scission du Parti communiste d'Espagne) se fédèrent à partir de 1932 au sein d'une nouvelle internationale, le Bureau international pour l'unité socialiste révolutionnaire (dit )[250].
Parallèlement se développe, en France et en Belgique, une nouvelle ligne  connue sous le nom de néo-socialisme ; celle-ci se fait l'écho d'une controverse doctrinale ancienne, d'autant plus aigüe au moment où le socialisme peine à s'affirmer devant le danger nazi. Des socialistes s'interrogent à nouveau sur l'identité politique du socialisme, la place du marxisme, la dimension révolutionnaire et la prise en compte de l'évolution sociétale. Henri De Man, membre du Parti ouvrier belge, publie en 1926 le livre Au-delà du marxisme, dans lequel il analyse le décalage entre discours révolutionnaire et pratique politique et donne au facteur subjectif une importance primordiale par rapport à l'économique, considérant que le socialisme doit aider le peuple à . Émile Vandervelde, chef du POB, critique vivement les thèses d'Henri De Man, considérant que l'exploitation capitaliste n'a pas disparu, mais ne prend pas en compte ses remarques sur l'évolution sociale du prolétariat et des classes moyennes. Les idées de De Man influencent en France Marcel Déat, étoile montante de la SFIO, qui se place dans une optique résolument réformiste de rassemblement anticapitaliste avec les classes moyennes. Combattues par Léon Blum et les partisans de ce dernier, les thèses de Déat sont au contraire soutenues par d'autres socialistes comme Adrien Marquet qui prône les devises . Les néo-socialistes français finissent par quitter la SFIO pour fonder le Parti socialiste de France-Union Jean Jaurès, qui donne ensuite naissance à l'Union socialiste républicaine. De Man et Déat évoluent ensuite tous deux, à l'approche du conflit mondial, vers un pacifisme de plus en plus affirmé. Déat finit par s'éloigner quant à lui du mouvement socialiste, son parcours l'amenant ensuite vers le fascisme et la collaboration durant l'occupation allemande[251].
L'échec manifeste de la ligne  conduit l'Internationale communiste à adopter, à partir de 1935, une politique de , c'est-à-dire d'alliance de toutes les forces antifascistes[249]. Les socialistes acceptent ces alliances, qui mettent fin aux attaques des communistes contre eux. En Espagne, le Front populaire constitué par le Parti socialiste ouvrier espagnol, le Parti communiste d'Espagne et divers autres mouvements dont des nationalistes galiciens et catalans, remporte les élections générales de février 1936. Le climat politique est particulièrement tendu en Espagne, les socialistes espagnols étant divisés entre modérés et radicaux : Francisco Largo Caballero, chef du PSOE et de l'Union générale des travailleurs, passé depuis 1934 du réformisme gouvernemental à l'affirmation révolutionnaire, multiplie les surenchères, ce qui lui vaut le surnom de [252],[253].
En France, le Front populaire, alliance défensive formée par la SFIO, le PCF et le Parti républicain, radical et radical-socialiste, remporte les élections législatives de 1936. Léon Blum devient président du conseil des ministres et met en œuvre une série de réformes qui modifient en profondeur la société française : les accords Matignon concernant le monde du travail, la semaine de 40 heures et les congés payés[254]. Blum parvient en outre à faire triompher sa ligne modérée ; il fait accepter à l'extrême-gauche de la SFIO — notamment la Gauche révolutionnaire de Marceau Pivert — qu'un changement de régime, en l'absence des conditions requises et d'un mandat des électeurs en ce sens, n'est pas à l'ordre du jour. Il réussit dès lors à maintenir le mythe de la conquête du pouvoir tout en faisant admettre la réalité de son exercice tandis qu'il s'en tient, dans la continuité de la pensée de Jaurès, à une théorie de l'État démocratique[255].
Un troisième front populaire arrive au pouvoir durant la période, cette fois hors d'Europe : au Chili, le Front populaire, formé notamment par le Parti socialiste, le Parti communiste et le Parti radical, gouverne à partir de 1938, sous la présidence de Pedro Aguirre Cerda. Le Parti socialiste du Chili est divisé en diverses tendances, allant du socialisme modéré à l'anarcho-syndicalisme en passant par divers marxismes révolutionnaires : il s'affirme néanmoins dans son ensemble  et partisan d'une dictature du prolétariat temporaire. Les ministres socialistes sortent cependant du gouvernement dès 1940 et l'entente des socialistes chiliens avec les communistes se brise à l'occasion de la polémique sur les causes de la guerre mondiale et sur la politique suivie par l'URSS en 1939-40[256].
En Espagne, la situation débouche sur une guerre civile après le soulèvement nationaliste de juillet 1936. Le gouvernement de Léon Blum, devant les crimes commis durant la terreur rouge, fait initialement taire ses sympathies pour les républicains et adopte une politique de non-intervention. Ce choix illustre la difficulté pour les socialistes de tenter d'allier antifascisme et pacifisme. Blum défend ensuite une politique de fermeté face au danger fasciste en Europe, mais il est écarté du pouvoir dès 1937. Le conflit espagnol, prélude à la Seconde Guerre mondiale, s'achève par la victoire des nationalistes espagnols : les partis socialiste et communiste espagnols sont interdits[254],[257].
Au cours de la Seconde Guerre mondiale, l'Internationale ouvrière socialiste se disloque : Camille Huysmans, réfugié à Londres[258], tente vainement de la maintenir à flot. Les socialistes européens, face à l'approche du conflit puis durant celui-ci, sont très divisés : certains, comme Henri De Man et le secrétaire général de la SFIO Paul Faure, défendent avec force le pacifisme. Une partie d'entre eux, durant l'occupation allemande de leur pays, conservent les mêmes priorités pacifistes qui leur avaient fait approuver les accords de Munich et vont parfois jusqu'à glisser dans la collaboration. Durant l'occupation, la SFIO se reconstitue clandestinement et est progressivement réorganisée par les socialistes partisans de la résistance; ces derniers gardent le contact avec Léon Blum emprisonné. Le soutien des socialistes au général de Gaulle contribue à fonder la légitimité de la France libre[259].
Dans l'ensemble des pays européens, des socialistes participent aux mouvements de résistance de l'intérieur et de l'extérieur. Le Parti socialiste italien, exilé depuis plus de vingt ans, prend part à la résistance contre les Allemands et les fascistes, intégrant le Comité de libération nationale créé en septembre 1943. Dans le milieu antifasciste italien se développe un courant distinct de celui de Giustizia e Libertà, intitulé , dont le principal théoricien est Guido Calogero. Le libéralsocialisme, dont les thèses sont fixées dans deux manifestes publiés en 1940 et 1941, ambitionne de créer une synthèse de la  du libéralisme et du marxisme, en promouvant un projet de société fondé sur la démocratie et l'économie mixte, où l'État assurerait un rôle de régulateur, en laissant une large part de contrôle à la société civile. Les deux courants socialistes libéraux convergent au sein du Parti d'action[260]. En France, la SFIO participe au Conseil national de la Résistance. Les socialistes doivent cependant compter avec la présence des communistes : ceux-ci, profondément déstabilisés en 1939 par le pacte germano-soviétique, ont massivement rejoint la résistance après l'invasion de l'URSS par l'Allemagne en juin 1941, jusqu'à atteindre, dans certains pays, une position nettement majoritaire au sein des mouvements de libération nationale[261].
La période de l'après-Seconde Guerre mondiale est marquée par le déclenchement de la guerre froide, capitale pour l'évolution du socialisme européen. La prise de contrôle par l'URSS des pays d'Europe de l'Est s'accélère en 1947-1948, avec pour conséquence l'apparition d'une série de régimes communistes se réclamant du  marxiste-léniniste. Les autres partis politiques, dont les partis socialistes, sont dissous, absorbés ou inféodés par les partis communistes au pouvoir, parfois avec la complicité d'une partie de leurs propres cadres. En Asie, des régimes communistes apparaissent également avec la proclamation de la république populaire de Chine présidée par Mao Zedong, et la naissance de la Corée du Nord dirigée par Kim Il-sung ; les partis chinois et nord-coréen donnent respectivement naissance au maoïsme et au juche, variations de la doctrine communiste. Des conflits armés éclatent dans le contexte des premières années de la guerre froide, comme la guerre civile grecque en Europe, ou la guerre de Corée et la guerre d'Indochine en Asie. En Europe, en Asie, en Afrique et aux Amériques, un ensemble de régimes communistes à parti unique — le titisme constituant en Yougoslavie, à partir de 1948, une dissidence du stalinisme — se réclame du socialisme et du marxisme, ce qui impose aux partis relevant du socialisme démocratique de redéfinir leur identité. Malgré la déstalinisation et les différentes phases de la détente, les régimes communistes demeurent des systèmes autoritaires, fondés sur le règne du Parti : la domination du modèle soviétique est réaffirmée lors de l'écrasement en 1956 de l'insurrection de Budapest et celui, en 1968, du printemps de Prague qui espérait amener à l'avènement, à l'Est, d'un [262].
Les partis socialistes d'Europe occidentale, confrontés à cet essor du communisme, poursuivent à des rythmes divers leur évolution vers le réformisme. L'Internationale socialiste est fondée en 1951 dans une optique d'opposition résolue à la forme de socialisme revendiquée par le bloc de l'Est : la position des socialistes européens vis-à-vis des communistes passe cependant de l'opposition frontale à la recherche de la coexistence pacifique et de la coopération. Entre les années 1950 et 1990, les partis socialistes européens connaissent une évolution ultérieure en se convertissant, à des rythmes et des degrés divers, à une approche réformiste de l'économie de marché. Dans la majorité des pays européens, les socialistes et les sociaux-démocrates constituent la force dominante à gauche et au centre gauche. Les thèmes marxisants sont abandonnés au profit d'un discours centré sur la liberté et la démocratie[263].
Si, dans certains pays, notamment en France et en Italie, des références marxistes demeurent longtemps présentes[264], la plupart des partis européens, substituent dans leurs discours les thèses de Keynes et de Schumpeter à celles de Marx[265]. Cette mue politique se fait cependant au prix d'une certaine déperdition de substance idéologique[266]. Reconnus comme des partis de gouvernement, les mouvements socialistes participent, après des divisions initiales, à la construction de l'Union européenne. Parallèlement, à partir des années 1950, l'Internationale socialiste abandonne son optique européo-centrée pour se tourner vers les autres continents[267]. Des formes de socialisme, distinctes de l'approche européenne et très différentes les unes des autres, se développent par ailleurs en Asie, en Afrique et en Amérique latine[268].
La référence au socialisme demeure également présente à l'extrême gauche. Des intellectuels, comme Cornelius Castoriadis et Claude Lefort — animateurs de la revue Socialisme ou barbarie et du groupe du même nom — ou Guy Debord, se réclament du socialisme et du communisme de conseils, tout en dénonçant à la fois le stalinisme et le socialisme réformiste. En outre, la référence au socialisme continue d'être utilisée par certains mouvements d'extrême droite apparentés au néofascisme : le courant nationaliste révolutionnaire se veut ainsi  et partisan d'un [269].
Au sortir de la Seconde Guerre mondiale, les situations des socialistes européens sont très diverses. Au Royaume-Uni, alors que le conflit dure encore en Asie, le Parti travailliste gagne à la surprise générale les élections de juillet 1945, Clement Attlee devenant premier ministre. Le cabinet travailliste obtient la nationalisation de la sidérurgie mais doit faire face à des difficultés accrues, et réaliser des économies sur le système de santé. Les problèmes intérieurs et la tension internationale née notamment de la guerre de Corée contribuent à provoquer la défaite des travaillistes aux élections de 1951[270]. En Europe continentale, la position des socialistes varie en fonction de leur implantation nationale, de leur participation à l'effort de guerre ou à la résistance et de la position des partis communistes nationaux. En Autriche, alors que le pays entame sa reconstruction, le Parti socialiste forme avec les conservateurs du Parti populaire une coalition qui dure jusqu'en 1966 ; aux Pays-Bas, le Parti social-démocrate des ouvriers fusionne avec d'autres partis pour donner naissance au Parti travailliste ; en Belgique, le Parti ouvrier belge laisse la place au Parti socialiste belge. Dans des pays comme la Belgique ou les Pays-Bas, les socialistes demeurent majoritaires à gauche, tandis qu'en France, en Italie ou en Tchécoslovaquie, les communistes bénéficient de leur participation à la résistance pour acquérir une position de premier plan sur le plan électoral[271].
En Italie, le Parti socialiste - rebaptisé temporairement Parti socialiste italien d'unité prolétarienne - dont Rodolfo Morandi (it) et Pietro Nenni sont les principaux dirigeants, s'allie au Parti communiste contre la Démocratie chrétienne. Le Front démocratique populaire, coalition formée par le PCI et le PSI, est cependant nettement battu par la DC lors des élections de 1948. L'alliance PSI-PCI provoque la scission du Parti socialiste des travailleurs italiens (rebaptisé ensuite Parti socialiste démocrate italien) qui regroupe, autour de Giuseppe Saragat, les socialistes hostiles à la coalition avec les communistes et favorables à l'alliance de l'Italie avec les États-Unis. La politique du PSI varie grandement avec les années et, à partir de 1955, les socialistes italiens, sans rompre définitivement avec les communistes, se rapprochent de la Démocratie chrétienne et de leurs  du PSDI. La déstalinisation et l'insurrection de Budapest accélèrent la rupture des socialistes italiens avec leurs alliés communistes ; en 1959, Nenni condamne publiquement le centralisme démocratique. Si la Démocratie chrétienne a, jusque dans les années 1980, le monopole de la présidence du conseil, les socialistes italiens conservent leurs positions électorales. Le PSI reste cependant très divisé, entre l'aile gauche qui souhaiterait relancer l'alliance avec les communistes et les tenants d'une social-démocratie à l'italienne qui souhaitent au contraire renouer des liens avec le PSDI[272]. Par ailleurs, malgré la forte prégnance du marxisme sur la gauche italienne et la disparition du Parti d'action en 1947 à la suite de son échec électoral, un courant de pensée apparenté au socialisme libéral continue d'exister en Italie ; le philosophe Norberto Bobbio, issu du courant Libéralsocialiste, réfléchit sur les rapports entre socialisme, libéralisme et démocratie, préconisant l'alliance de ces trois systèmes de valeur et soulignant la nécessité absolue de la rencontre entre socialisme et démocratie[273],[274].
En France, la SFIO est distancée par le PCF lors des élections de 1945 : le parti socialiste tente alors un retour aux sources doctrinales et Daniel Mayer, socialiste modéré proche de Léon Blum, est remplacé au secrétariat général par Guy Mollet. Le parti s'éloigne alors de la redéfinition  tentée par Blum et en revient aux références marxistes orthodoxes. Située politiquement entre le PCF et le MRP, la SFIO est dans une position électorale inconfortable durant la Quatrième République. Un socialiste, Vincent Auriol, occupe néanmoins le poste honorifique de président de la République française entre 1947 et 1954[259],[275]. Malgré des tentatives comme celle d'André Philip pour promouvoir un socialisme d'inspiration au moins partiellement libérale, le courant du socialisme libéral tend quant à lui à marquer le pas en France dans la seconde moitié du XXe siècle, faute d'un véritable renouvellement théorique qui irait au-delà du seul positionnement antimarxiste[276].
Face à la prise de pouvoir communiste en Europe de l'Est et à une situation internationale de plus en plus tendue, les partis socialistes européens entreprennent de s'organiser. L'initiative vient en grande partie des Britanniques, certains membres travaillistes du gouvernement Churchill ayant évoqué durant la guerre la reconstitution d'une nouvelle internationale. Une première conférence internationale a lieu en mai 1946 à Clacton-on-Sea à l'initiative du Parti travailliste britannique pour aborder différentes questions, dont celle de la réinsertion du Parti social-démocrate d'Allemagne dans le courant socialiste. En juin 1947, une nouvelle conférence a lieu à Zurich, où Kurt Schumacher, dirigeant du SPD, appelle les socialistes à s'unir pour aider à l'existence d'un . En novembre de la même année, le SPD est réadmis parmi les partis socialistes européens. Un Comité international de la conférence socialiste (Committee of the International Socialist Conference, COMISCO) est créé à Londres pour assurer la liaison entre les partis. Les travaux du COMISCO (qui existe jusqu'en 1956) aboutissent en 1951 à la naissance d'une nouvelle organisation, l'Internationale socialiste, lors d'un congrès à Francfort-sur-le-Main, en Allemagne de l'Ouest. La nouvelle Internationale condamne explicitement le communisme et se rallie, dans le cadre de la guerre froide, au camp occidental et à l'alliance avec les États-Unis ; les partis socialistes européens évoluent vers l'adoption définitive d'une ligne socialiste démocratique. Le contexte de la guerre froide, de la lutte contre le communisme et de l'alliance américaine pousse les socialistes au compromis : les espoirs de transformation sociale nés de la libération de l'Europe sont renvoyés à plus tard[277],[278],[279].
En 1959, lors de son congrès extraordinaire à Bad Godesberg, le SPD consacre l'abandon de toute référence au marxisme et à la lutte des classes[280], le programme du parti mentionnant parmi ses références l', l' et la . L'ensemble des partis socialistes se convertit aux principes économiques du keynésianisme et de l'économie mixte[281]. Dans tous les pays, l'électorat des partis socialistes évolue et se fait de moins en moins ouvrier et attire de plus en plus les votes et les adhésions des classes moyennes. Le parti socialiste ne s'adresse désormais plus au seul prolétariat : son spectre s'élargit aux intellectuels, aux salariés, aux employés du commerce et de l'industrie, ainsi qu'à une partie des agriculteurs et des ouvriers ; dans divers pays, le socialisme démocratique se rapproche désormais, sur le plan idéologique, des milieux d'inspiration chrétienne, comme en France la revue Esprit[282]. Abandonnant la notion d'appropriation collective des moyens de production, même à titre d'objectif lointain, les partis socialistes continuent cependant à défendre une conception globale de l'ordre social et des valeurs qui doivent le fonder[283].
Au Royaume-Uni, les travaillistes mettent à profit leur période dans l'opposition pour redéfinir leurs positions : le chef du Labour, Hugh Gaitskell, souhaite rompre avec un programme qui juxtapose les réformes sociales et les nationalisations sans tracer de limites précises. Les  travaillistes définissent l'idéal socialiste comme  mais considèrent que, cet idéal étant lointain, la société a avant tout besoin  : la tâche essentielle du socialisme est donc la réalisation d'une société plus égalitaire dans les revenus et l'éducation. La révision politique du Labour demeure cependant partielle, car butant sur la clause IV du programme travailliste, qui prévoit  et que la gauche du parti impose de conserver. Mis en minorité au sein du Parti, Hugh Gaitskell fait cependant voter en 1960 une déclaration qui établit qu'une plus grande extension de la propriété publique ne pourra être décidée qu'au cas par cas, selon les circonstances[284].
En Italie, la Démocratie chrétienne réalise en février 1962 une ouverture à gauche quand le Parti social-démocrate italien entre dans le gouvernement d'Amintore Fanfani. Le PSI de Nenni décide ensuite de franchir le pas de l'alliance avec la DC et, en novembre 1963, les socialistes font leur entrée dans le gouvernement d'Aldo Moro. L'ensemble de la gauche italienne non communiste est désormais alliée avec la Démocratie chrétienne[285]. Les socialistes italiens hostiles à l'alliance avec la DC quittent alors le PSI et fondent un parti qui reprend l'ancien nom de Parti socialiste italien d'unité prolétarienne. En 1964, Giuseppe Saragat, fondateur du PSDI, accède au poste, essentiellement honorifique, de président de la République italienne. En 1966, le PSI et le PSDI se réunifient sous le nom de Parti socialiste unifié (PSU). L'alliance des socialistes avec la Démocratie chrétienne aboutit cependant à leur faire perdre en crédibilité auprès de l'électorat de gauche, qui se tourne désormais davantage vers le Parti communiste. Mais le PSU, qui souffre sur sa gauche de la concurrence du PSIUP, connaît un échec aux élections de 1968 et, l'année suivante, le PSDI se sépare à nouveau du PSI[272].
Sur la défensive par rapport aux régimes communistes, l'Internationale socialiste est en outre divisée dans les premiers temps sur la question de la construction européenne, que soutient la SFIO mais, initialement, pas le Parti travailliste britannique ni le SPD ouest-allemand[267]. Mais des dirigeants socialistes, comme le Belge Paul-Henri Spaak, l'italien Giuseppe Saragat, le français Guy Mollet - qui, avec Christian Pineau, négocie le traité de Rome en 1957 - comptent parmi les artisans majeurs de la construction européenne[263] (les travaillistes britanniques étant nettement plus réticents)[286]. Les socialistes d'Europe continentale comptent, avec les démocrates-chrétiens, parmi les acteurs majeurs de la construction de la Communauté économique européenne[287].
Durant la majeure partie des années 1950 et 1960, les partis socialistes européens sont éloignés du pouvoir dans la moitié de l'Europe de l'Ouest[267] : dans les pays où les partis socialistes ne souffrent pas de la concurrence des partis communistes, le socialisme démocratique enregistre au contraire des progrès notables, comme au Royaume-Uni où Harold Wilson gouverne entre 1964 et 1970, ou en Scandinavie où les sociaux-démocrates gouvernent la majeure partie du temps. En Suède, Tage Erlander est chef du gouvernement de 1946 à 1969, suivi par Olof Palme de 1969 à 1976. En Norvège, les travaillistes gouvernent sans interruption de 1945 à 1963 puis alternent au pouvoir avec les autres partis. Au Danemark, les sociaux-démocrates gouvernent de 1953 à 1968[288]. Au Royaume-Uni, Harold Wilson met à profit l'acquis  et, tout en renationalisant la sidérurgie, applique un programme qui vise essentiellement la modernisation de la société britannique[284] : les travaillistes britanniques abolissent ainsi la peine de mort, légalisent l'avortement et dépénalisent l'homosexualité, mettant ainsi à bas une partie des règles sociales héritées de l'époque victorienne[289]. En Autriche, le Parti socialiste d'Autriche (SPÖ, ex-SDAP), reste marqué par sa tradition de l'austromarxisme et conserve des liens étroits avec les syndicats. Après une longue participation au gouvernement de coalition de l'après-guerre, les socialistes autrichiens obtiennent pour la première fois la majorité absolue en 1970 : Bruno Kreisky devient chancelier et conserve ce poste jusqu'en 1983, gouvernant selon une pratique du compromis sur les plans économique et social pour appliquer des politiques directement inspirées du modèle suédois[290],[146].
En France, la politique menée par Guy Mollet à la tête de la SFIO, puis du gouvernement en 1956-1957, contribue à affaiblir à long terme le parti, du fait de la dichotomie entre un discours marxiste orthodoxe très marqué à gauche et une pratique politique modérée et réformiste (voire, selon ses détracteurs, opportuniste). Le socialisme français apparaît handicapé, de manière durable, par le décalage entre l'élaboration de plans stratégiques pour accéder au pouvoir et l'absence de plans réalistes pour l'exercice de celui-ci, le tout étant aggravé par un manque de substance idéologique. L'échec de la SFIO dans le contexte de la décolonisation prend un tour tragique : le gouvernement Mollet connaît des succès avec le vote de la loi-cadre Defferre et la gestion des situations au Maroc et en Tunisie, mais ceux-ci sont occultés par les désastres de la participation à l'expédition de Suez et de la politique menée dans le cadre de la guerre d'Algérie qui crée un malaise durable dans les rangs socialistes français. En 1958, la SFIO connaît une scission lors de la création du Parti socialiste autonome (PSA), dirigé par Édouard Depreux ; le PSA fusionne deux ans plus tard avec d'autres groupes dissidents pour former le Parti socialiste unifié (PSU), qui ambitionne de se situer politiquement entre socialistes et communistes et d'incarner un [267],[291],[292]. Pierre Mendès France, en rupture avec les radicaux, participe au PSA, puis au PSU[293]. Alors que la SFIO ne se montre guère ouverte au débat d'idées, le PSU se veut, avec la CFDT, le tenant de la , résolument antitotalitaire : ce courant contribue notamment, après Mai 68, à vulgariser le projet autogestionnaire[275].
La SFIO connaît un long dépérissement durant les années 1960, se regroupant entre 1965 et 1968 au sein de la Fédération de la gauche démocrate et socialiste, un rassemblement de partis de centre-gauche dirigé par François Mitterrand. Le déclin de la SFIO débouche sur l'échec cuisant de la candidature de Gaston Defferre lors de l'élection présidentielle de 1969, où le candidat socialiste n'obtient que 5,01 % des suffrages. Un mois plus tard, la SFIO cesse d'exister, pour laisser la place au Parti socialiste, d'abord dirigé par Alain Savary. En juin 1971, lors du congrès d'Épinay, Mitterrand prend le contrôle du Parti socialiste, qu'il entreprend de réorganiser. Le PS rassemble progressivement une grande partie de la gauche non communiste : en 1978, la moitié des adhérents du parti l'ont rejoint après le congrès d'Épinay. Le PSU conserve son indépendance mais perd son principal dirigeant, Michel Rocard, qui rejoint le PS en 1974. Michel Rocard continue ensuite de se positionner en tenant de la  non-marxiste, et prône un  à la française qui verrait le PS s'adosser à la CFDT et au mouvement associatif[294]. Il continue également de plaider pour l'autogestion, dans une perspective à la fois  et non-violente[295]. Une autre tendance du PS, le Centre d'études, de recherches et d'éducation socialiste (CERES) dirigée par Jean-Pierre Chevènement, prône au contraire un gauchissement du PS dans une optique  afin d'occuper le terrain du PCF[294] et de prévenir une . Au terme de révolution est désormais préféré l'expression , dont le sens peut par ailleurs varier[296].
Au fil du temps, l'attitude du socialisme européen face à la guerre froide évolue, dans le contexte de la période de relative détente qui suit la mort de Staline en 1953. La répression de l'insurrection hongroise en 1956 suscite un mouvement d'indignation dans les rangs socialistes, mais celui-ci ne dure pas ; les partis préfèrent par la suite employer la voie diplomatique dans le cadre de la recherche de la sécurité collective en Europe. Sans coopérer politiquement avec les communistes, les socialistes recherchent une coopération diplomatique avec le bloc de l'Est, pour favoriser la coexistence pacifique. Willy Brandt, dirigeant du SPD et maire de Berlin-Ouest au moment de la construction du mur de Berlin, est l'une des figures majeures de cette nouvelle stratégie des partis socialistes européens : devenu chancelier fédéral de l'Allemagne de l'Ouest en 1969, Brandt poursuit une politique d'ouverture vers l'Est (dite ) qui amène en 1972 à la signature du traité fondamental par lequel Allemagne de l'Ouest et Allemagne de l'Est nouent des rapports sans pour autant se reconnaître[267],[278]. La détente des relations avec les pays de l'Est touche les autres partis de l'Internationale, comme les PS belge et le français, dont des délégations effectuent des visites en URSS en 1975[297]. En 1976, Brandt est élu à la tête de l'Internationale socialiste, qu'il s'emploie à réorganiser pour la faire fonctionner de manière plus démocratique[278]. L'IS entreprend de se  pour tourner une plus grande part de son attention vers les partis du tiers monde. À mesure que les partis socialistes accédant au pouvoir sont de plus en plus nombreux, les congrès de l'Internationale socialiste ressemblent de plus en plus à des rencontres de chefs d'État ou de gouvernement[286].
Les années 1970 voient par ailleurs s'amorcer un processus d'infléchissement idéologique général au sein des partis européens. La crise économique consécutive aux chocs pétroliers, face à laquelle le keynésianisme et l'État-providence apparaissent impuissants, entraîne une crise de l'identité sociale-démocrate traditionnelle ; les partis sont amenés progressivement à se convertir au libéralisme économique, évolution qui ne fera que s'accentuer au cours de la décennie suivante[298].
La stratégie de coopération avec l'Est initiée par Willy Brandt porte ses fruits, dans la mesure où le monde communiste connaît à l'époque une série de morcellements et que la perspective de la construction européenne rééquilibre les rapports de force sur le plan politique : les partis communistes se rapprochent des partis socialistes, à l'image du Parti communiste français qui signe en 1972 avec le Parti socialiste un programme commun, rejoint par les radicaux de gauche. Le programme commun, qui prévoit toujours la , frôle la victoire à l'élection présidentielle de 1974. L'alliance entre socialistes et communistes français se défait après 1978 mais en 1981, François Mitterrand remporte l'élection présidentielle, devenant le premier président de la République socialiste de l'histoire de la Cinquième République ; son premier mandat se signale entre autres par une réforme du droit du travail via les lois Auroux. Les communistes intègrent le gouvernement français dans le cadre de l'Union de la gauche. Mais en 1983, l'adoption du tournant de la rigueur en 1983 marque un infléchissement de la politique socialiste en France, dans le contexte d'une évolution de plus en plus marquée vers le réformisme et l'acceptation de l'économie de marché. Le PCF, qui connaît un essoufflement électoral, quitte le gouvernement en 1984, mais cette rupture de l'Union de la gauche n'enraye pas son déclin. Le Parti socialiste devient la force dominante de la gauche française au détriment du PCF ; mais si le PS s'impose en tant que parti de gouvernement, il le fait au prix d'une conversion au libéralisme économique qui entraîne un certain trouble de son identité politique. Le PS apparaît dès lors comme un , bénéficiaire mécanique de l'alternance politique mais très éloigné de perspectives de réformes profondes de la société. Les chercheurs Alain Bergounioux et Gérard Grunberg jugent que l'exercice du pouvoir par le Parti socialiste français a entraîné . Les thèmes de l'autogestion, de la planification et des nationalisations, tombent progressivement en désuétude. Battus aux élections législatives de 1986, les socialistes français reviennent au gouvernement en 1988 mais sont ensuite victimes de rivalités internes qui s'étalent notamment au grand jour lors du Congrès de Rennes de 1990 ; ils subissent une défaite sans appel aux élections de 1993, qui sanctionnent moins l'idée de socialisme que la politique menée par le PS[299],[300],[301]. Le socialisme français, cependant, bénéficie malgré ses divisions d'une certaine stabilité, du fait de la domination très large du PS sur la gauche française : le parti ne subit à l'époque qu'une seule scission, en 1993, lors de la formation par Jean-Pierre Chevènement du Mouvement des citoyens, en réaction contre l'engagement pro-européen du PS[263].
En Italie, les scores du Parti communiste italien déclinent à partir des élections de 1979[299] tandis que le Parti socialiste italien regagne progressivement le terrain électoral perdu, en rompant ses derniers liens avec le marxisme. Bettino Craxi devient premier secrétaire du PSI en 1976 et l'oriente résolument vers la voie d'une forme de socialisme libéral : les socialistes italiens contribuent à pousser les communistes en crise dans leurs retranchements idéologiques. Craxi, président du conseil des ministres de 1983 à 1987, ambitionne de faire du PSI la force dominante du centre-gauche italien en s'inspirant ouvertement du modèle français, mais sans faire preuve d'une cohérence doctrinale particulière : les socialistes italiens se font les avocats du libéralisme au nord de l'Italie et de l'interventionnisme au sud du pays. En Espagne, le Parti socialiste ouvrier espagnol est à nouveau autorisé après la fin du franquisme et accompagne la transition démocratique. Son chef, Felipe González, rompt avec le marxisme dès 1979 et devient chef du gouvernement en 1982. Au Portugal, après la révolution des œillets et le retour à la démocratie, la constitution de 1976 inclut dans son préambule une référence au socialisme en proclamant la volonté d'[302]. Le Parti socialiste portugais accède au pouvoir en 1983 et abandonne toute référence au marxisme, son chef Mário Soares étant encouragé en ce sens par les sociaux-démocrates allemands : les socialistes portugais mènent une politique d'austérité afin de rejoindre la communauté européenne[303].
La Grèce connaît, jusque dans les années 1970, une situation particulière du fait de l'absence d'un parti social-démocrate fort : la gauche grecque est alors dominée par un Parti communiste à l'orientation strictement pro-soviétique. Ce n'est qu'en 1974, après la chute de la dictature des colonels, qu'est créé le Mouvement socialiste panhellénique (PASOK) : le PS grec s'emploie cependant à faire concurrence aux communistes en reprenant une partie de leurs thèmes et de leurs discours , ce qui lui permet de devenir rapidement le premier parti d'opposition. En 1981, Andréas Papandréou, chef du PASOK, devient premier ministre ; il continue alors de se situer dans une mouvance de gauche aux accents nationalistes et populistes, très liée au secteur public. Progressivement, dans le courant de la décennie 1980, les socialistes grecs adoptent un discours plus modéré, à mesure que la Grèce doit suivre des politiques d'austérité[303],[304].
Au Royaume-Uni, le Parti travailliste, battu aux élections de 1979, adopte une tactique d'opposition frontale face au premier ministre conservateur Margaret Thatcher : dirigé par Michael Foot et Tony Benn (ce dernier subissant la pression de la tendance trotskiste du parti), le Labour adopte des positions radicalement orientées à gauche qui lui valent une défaite retentissante lors des élections de 1983. Neil Kinnock, nouveau chef des travaillistes, revient à ensuite un discours de gauche plus traditionnel[303] : il s'emploie notamment à réduire l'influence de la Militant tendency, la minorité trotskyste du Labour[305],[306]. Mais Kinnock échoue à nouveau aux élections de 1987, puis à celles de 1992 : sous les leaderships de John Smith puis de Tony Blair, le Parti travailliste britannique s'engage alors plus avant dans la voie du révisionnisme politique[303].
Le socialisme progresse après la Seconde Guerre mondiale hors du continent européen, mais sous des formes très variées et parfois fort éloignées de celles qu'il peut connaître en Europe. Dans le tiers monde, les invocations du socialisme prennent parfois des formes vagues, confuses ou contradictoires, en rupture fréquente avec la ligne prônée par l'Internationale socialiste. Celle-ci, à la recherche de partenaires dans les pays du tiers-monde, doit composer avec des tendances très diversses, allant des partis traditionnels de notables locaux aux anciens mouvements de guérilla convertis à la démocratie. Si l'Internationale socialiste réussit son expansion hors d'Europe, ce succès ne s'accompagne pas d'une position internationale cohérente. Au sein ou en dehors de l'IS, l'identité  est au fil des décennies revendiquée à des titres et dans des contextes très variés, par des dirigeants politiques extrêmement différents les uns des autres comme Léopold Sédar Senghor au Sénégal, Ahmed Sékou Touré en Guinée, Jawaharlal Nehru et sa fille Indira Gandhi en Inde, Habib Bourguiba puis Zine el-Abidine Ben Ali en Tunisie[307], Fidel Castro à Cuba, Gamal Abdel Nasser en Égypte, Saddam Hussein en Irak[308], Mouammar Kadhafi en Libye, Hafez el-Assad puis Bachar el-Assad en Syrie[309], Norodom Sihanouk au Cambodge, Ne Win en Birmanie, Salvador Allende au Chili, Daniel Ortega au Nicaragua[310], Michael Manley à la Jamaïque[311], Thomas Sankara au Burkina Faso, ou Walter Lini à Vanuatu[268].
Au Proche-Orient, David Ben Gourion, chef du parti socialiste et sioniste Mapaï créé en 1930 en Palestine mandataire, fait partie en 1948 des fondateurs de l'État d'Israël. Très présent dans les kibboutz, le socialisme est alors particulièrement influent en Israël, tant en politique que sur le plan socio-économique : le sionisme travailliste, courant dominant de la gauche israélienne, reste au pouvoir sans interruption jusqu'en 1977. Le Mapaï est remplacé en 1968 par le Parti travailliste israélien[312].
Dans le monde arabe, où le communisme éprouve des difficultés à s'imposer (un seul régime marxiste-léniniste, le Yémen du Sud, voit le jour en 1967)[313], divers militants et intellectuels développent des visions non marxistes du socialisme, qu'ils visent à adapter aux réalités arabes. Le syrien Michel Aflak participe en 1947 à la fondation du Parti Baas arabe socialiste, mouvement aux objectifs panarabes qui devient l'une des principales incarnations du nationalisme arabe. Aflak théorise un socialisme arabe, qu'il conçoit comme le moyen technique d'organiser la société arabe afin que chacun puisse s'épanouir . Le socialisme arabe, qui prend avec le temps des formes multiples, devient l'un des éléments constitutifs des différentes tendances du nationalisme arabe ; il ne vise cependant pas l'égalité entre les individus par la répartition des richesses et défend au contraire la propriété privée. Tel que le Parti Baas d'Aflak, le socialisme a vocation à être réalisé par la participation du peuple à la construction nationale et à la marche de l'État, par le biais d'assemblées nationales et locales, ainsi que par la liberté de la presse. Sans viser l'égalitarisme, le socialisme envisagé par le Baas vise à . L'arabisme du Parti Baas est explicitement présenté comme étant, en soi, un socialisme. Selon le discours officiel du Baas, [314]. Des divisions se font néanmoins jour progressivement au sein du Baas, la tendance syrienne étant plus influencée par le marxisme tandis que la tendance irakienne se veut plus fidèle à la vision originelle d'Aflak[315].
En Égypte, Gamal Abdel Nasser se réclame de l'application du socialisme arabe, qu'il consacre en 1962 par une Charte d'union nationale panarabe : l'Union socialiste arabe devient le parti unique égyptien. Le programme de nationalisations de Nasser et ses rapports conflictuels avec l'Occident et Israël le conduisent à se rapprocher un temps de l'URSS. Mais, après sa défaite en 1967 lors de la guerre des Six Jours, le régime nassériste doit réfréner ses ambitions panarabes et se replie sur l'Égypte et sa culture islamique[316]. Par ailleurs, un courant d'idées se réclamant du socialisme se développe dans les milieux islamistes, et plus précisément au sein des Frères musulmans[317] : l'égyptien Sayyid Qutb théorise le rôle primordial de l'islam en tant que force opposée au matérialisme et à l'injustice sociale ; le syrien Moustapha Siba’i traduit à son tour le socialisme arabe prôné par Nasser en termes de . En se fondant sur la tendance de l'islam à lutter contre l'injustice sociale et l'accumulation de capitaux par certains privilégiés, le socialisme islamique est considéré par ses partisans comme remontant aux sources de l'enseignement des prophètes : selon cette vision, le socialisme doit conduire à la solidarité entre les différentes catégories sociales, et non à la guerre entre les classes prônée par le communisme[318]. Dans l'optique des Frères musulmans, l'État islamique moderne, arbitre et gestionnaire, veille à la redistribution des richesses et contraint les récalcitrants à payer la taxe-aumône. L'expression , telle que la définit Nasser lui-même, vise originellement à la fois à se démarquer du socialisme islamique des Frères musulmans et du socialisme marxiste : mais par la suite, le nassérisme est partagé entre deux orientations, l'une se réclamant davantage d'un socialisme arabe pénétré d'un marxisme tiers-mondiste, et l'autre influencée par le socialisme islamique, à mesure que les références religieuses sont à nouveau privilégiées par le pouvoir en place[317].
Le libyen Mouammar Kadhafi, arrivé au pouvoir en 1969, se proclame par la suite seul et authentique héritier du nassérisme, et se réclame à la fois du socialisme arabe et du socialisme islamique qu'il présente comme étant une seule et même doctrine. Si Nasser veillait à l'association des valeurs de fraternité arabes et islamiques, Kadhafi systématise les références religieuses[317]. Dans les années 1970, Kadhafi met au point sa propre doctrine politique, baptisée du nom de troisième théorie universelle et conçue comme une  entre le capitalisme et le communisme. Le dirigeant libyen prône, sur le plan politique, la pratique de la démocratie directe et, sur le plan économique, un  défini comme une répartition  des produits de la nature. L'application de la doctrine kadhafiste, promue idéologie officielle de la Grande Jamahiriya arabe libyenne populaire et socialiste et parfois comparée à certaines formes de , a conduit à l'interdiction en Libye des professions libérales et du petit commerce et à la prise en main des entreprises par des  selon des principes officiels d'autogestion et d'abolition du salariat. La pratique de la  par le régime de Kadhafi débouche dans les faits sur un régime dictatorial et autocratique : à partir de 1979, le dirigeant libyen gouverne en dehors de tout cadre légal ou constitutionnel[319],[320].
D'autres tendances du socialisme arabe, moins hétérodoxes que celle de Kadhafi, arrivent au pouvoir dans plusieurs pays du Moyen-Orient et d'Afrique du Nord, mais toutes conduisent à la mise en place de régimes autoritaires. Le chercheur Olivier Carré souligne que les régimes politiques baasistes, en Irak sous les présidences d'Ahmad Hassan al-Bakr et surtout de Saddam Hussein, en Syrie sous celle de Hafez el-Assad, ont rapidement évolué vers des systèmes dictatoriaux qui, tout en continuant à se réclamer du socialisme, apparaissent [321]. En Irak, le socialisme est défini par une économie étatisée, défendue par un nationalisme jaloux ; en Syrie, le régime élargit le secteur public mais s'appuie autant sur le capitalisme national, fortement soutenu par l'État, que sur les coopératives[322].
Dans le Maghreb décolonisé, des régimes se réclamant du socialisme se mettent en place en Tunisie et en Algérie : en Tunisie, le Néo-Destour, parti du président Habib Bourguiba, devient en 1964 le Parti socialiste destourien ; en Algérie, le Front de libération nationale se réclame d'un socialisme indissociable d'une idéologie nationaliste et tiers-mondiste où la structure d'État est défendue en tant que corps social[323].
En Turquie, le courant dit du socialisme turc apparaît dans les années 1960, dans le contexte de libéralisation politique qui suit le coup d'État de 1960, à la suite duquel l'armée rend le pouvoir aux civils. Mehmet Ali Aybar, dirigeant du Parti des travailleurs de Turquie, théorise un socialisme propre à la Turquie, qui se caractériserait par son caractère  (le pouvoir étant atteint exclusivement par des voies légales et parlementaires),  (c'est-à-dire fondé sur une alliance de l'ensemble des classes laborieuses et des intellectuels ) et  (c'est-à-dire attaché à la défense de la souveraineté de la Turquie et dégagé de toute influence étrangère). Malgré la présence de références marxistes dans le discours d'Aybar, le socialisme turc se veut résolument distinct du marxisme-léninisme et opposé au totalitarisme. Le concept de socialisme turc est bientôt adopté par des groupes concurrents de celui d'Aybar : des intellectuels de gauche turcs, réunis autour du journal Yön, fondent la Société pour la culture socialiste, qui se réclame d'un socialisme turc nettement identifié à l'héritage du kémalisme, moins anticapitaliste que la tendance d'Aybar, et qui s'appuierait davantage sur les classes moyennes plus que sur les classes laborieuses. Le putsch de 1980 met un coup d'arrêt aux activités de l'ensemble des groupes de gauche[324].
En Afrique noire, après la décolonisation, des partis se réclamant à titres divers du socialisme arrivent au pouvoir, en sus des régimes communistes comme ceux de l'Angola, du Mozambique, de la Somalie, de l'Éthiopie, du Bénin ou du Congo-Brazzaville. En Afrique francophone, diverses tendances issues du Rassemblement démocratique africain s'identifient comme socialistes, au Sénégal, au Mali et en Guinée. Le président sénégalais, Léopold Sédar Senghor théorise en 1961 une , où des influences marxistes se marient avec celles de Pierre Teilhard de Chardin. Pour Senghor, la victoire de la négritude se confond avec celle du socialisme, conçu comme un  non communiste dont la lutte des classes est exclue car contraire à la tradition africaine d'unanimité et de conciliation. En Guinée, Ahmed Sékou Touré instaure un régime autoritaire fondé sur une planification ultra-centralisée, proche des conceptions staliniennes. En Afrique anglophone, les influences socialistes, davantage marquées par le socialisme chrétien et par un panafricanisme radical, sont revendiquées par des régimes comme ceux de Kwame Nkrumah au Ghana et de Julius Nyerere en Tanzanie. Nyerere théorise un socialisme africain spécifique au continent qui, dans le cadre de communautés villageoises autogérées et égalitaires - les villages Ujamaa - respecterait les traditions africaines et lutterait contre les inégalités, tout en revenant aux sources de l'esprit communautaire ébranlé par l'individualisme de l'époque coloniale. L'existence d'un socialisme spécifiquement africain ne fait cependant l'objet d'aucun consensus chez les socialistes du continent. Dans des anciennes colonies portugaises et françaises se mettent par ailleurs en place, dans les années 1970, des régimes autoritaires influencés par le marxisme soviétique[325].
En Asie, des dirigeants comme Soekarno en Indonésie, Nehru en Inde ou Norodom Sihanouk au Cambodge se réclament à des degrés divers du socialisme[268]. Sihanouk, qui exerce jusqu'à son renversement en 1970 un pouvoir proche de l'autocratie sur le Royaume du Cambodge, se rapproche dans les années 1960 des pays communistes dans le cadre de ses efforts pour éviter la tutelle américaine et met en place un programme de nationalisations et de réformes sociales, qu'il baptise du nom de [326]. En Birmanie, le général Ne Win impose comme idéologie officielle, à partir de 1962, un mélange d'idées marxistes et de principes bouddhistes, présenté comme la  ; le Parti du programme socialiste birman (en) est proclamé parti unique[327]. La période de la Voie birmane vers le socialisme débouche sur un désastre économique pour le pays et prend fin au moment des évènements de 1988[328].
Au Japon, le Parti socialiste japonais se situe dans une posture d' face au Parti libéral-démocrate qui détient une position hégémonique après 1955 : éloigné de toute perspective d'accès au pouvoir, le PSJ, très lié aux syndicats les plus à gauche, campe sur des positions radicales voire . Il adopte en 1964 un programme rejetant toute tentation réformiste et prônant une révolution socialiste pacifique mais inspirée des principes marxistes-léninistes. Le Parti socialiste japonais bénéficie d'une  en tant que parti d'opposition permanent et remporte 32,9 % des voix en 1958 : ses résultats électoraux déclinent ensuite jusqu'à atteindre 19,3 % en 1979. Devenu en 1996 le Parti social-démocrate et politiquement recentré, l'ex-PSJ ne parvient pas à profiter des difficultés du Parti libéral-démocrate, le leadership au centre-gauche lui échappant au profit du Parti démocrate du Japon[329].
En 1972, la constitution du Sri Lanka dispose le principe d'une , entendue comme une société fondée sur le plein-emploi, l'égalité des citoyens et le développement du pays[330]. Dans sa constitution de 1978, le pays prend comme nom officiel République démocratique socialiste du Sri Lanka : le socialisme sri lankais, tel qu'il est pratiqué par l'administration et les acteurs politiques du pays, est compris comme une forme d'État-providence garantissant un niveau de service public relativement élevé pour un pays en voie de développement[331].
En Inde, durant la période de l'état d'urgence, Indira Gandhi fait adopter un programme en vingt points comportant un ensemble de réformes sociales et affirmant une ligne politique socialiste démocratique ; en 1976, la constitution est modifiée pour inclure dans son préambule des références au socialisme, entendu comme une recherche du bien-être commun. L'Inde se présente désormais officiellement comme une [236],[332].
Sur le continent américain, le socialisme connaît des fortunes diverses. Aux États-Unis, la Guerre froide, et notamment la période du maccarthysme au début des années 1950, contribuent à marginaliser le courant socialiste : déjà très minoritaire, il est désormais amalgamé au communisme dans une partie de l'opinion publique[333].
L'influence des idées socialistes aux États-Unis est historiquement faible, notamment en comparaison des pays européens, au point d'avoir été considérée par les sociologues et les historiens comme constitutive de l’« exception américaine ». Pour le politologue conservateur Seymour Martin Lipset, cette particularité tenait à plusieurs faits majeurs : « la nature du système politique américain (l’hégémonie de deux partis, un seul tour pour l’élection présidentielle, un collège électoral privilégiant le vote des États et le suffrage universel indirect, etc.) ; une classe ouvrière hétérogène (fruit des vagues successives d’immigration) ; l’absence historique de toute alliance solide et durable entre les partis politiques et les syndicats ; et, enfin, un attachement « culturel » à des valeurs individualistes contraires aux idées socialistes ». Le sociologue Werner Sombart soutient que les travailleurs américains, parce qu’ils aspiraient à s’affranchir de leur classe, ne concevaient pas l’idée que celle-ci pourrait les accompagner dans leur ascension sociale : ils raisonnaient en termes d’amélioration individuelle, et non d’action collective[334].
Au début du XXe siècle cependant, le Parti socialiste d'Amérique eu une certaine influence, son candidat à l'élection présidentielle de 1912, Eugene Victor Debs, obtenant près d'un million de voix (6 % du total). Mais les quelques succès du parti restèrent sans lendemain. Après l’entrée des États-Unis dans la Première Guerre mondiale (1917), Debs et la plupart des dirigeants socialistes, opposés à la guerre, sont emprisonnés. La révolution russe provoque d'importantes divergences au sein d’un parti déjà affaibli par la répression, car un grand nombre de socialistes américains s'inspirent bien davantage du christianisme social que du marxisme[334].
Sur le reste du continent, que ce soit en Amérique du Sud ou en Amérique du Nord, le socialisme continue d'être présent en tant que force politique, mais en adoptant des visages très différents. Au Canada, la Fédération du commonwealth coopératif (Co-operative Commonwealth Federation, CCF), fondée en 1932, est la principale force politique socialiste. Le parti remporte dès 1944 l'élection générale de la Saskatchewan : Tommy Douglas devient le chef du gouvernement de la province, formant le premier gouvernement socialiste d'Amérique du Nord. La CCF — qui, en 1955, change son nom français en Parti social démocratique du Canada — gagne en importance grâce à son alliance avec la principale confédération syndicale du pays, le Congrès du travail du Canada : en 1961, le parti devient le Nouveau Parti démocratique (NPD), qui bénéficie d'un poids important à l'échelle fédérale. Le NPD, dirigé par Tommy Douglas jusqu'en 1971, continue de remporter des majorités dans certaines provinces, pesant sur certains gouvernements minoritaires sans jamais cependant parvenir à conquérir lui-même le pouvoir fédéral[335].
En Amérique latine, les partis socialistes se heurtent dans les années 1950 à diverses difficultés d'ordre social et politique : l'industrialisation du continent entraîne une urbanisation et l'accroissement d'un prolétariat souvent peu qualifié, que les partis socialistes peinent à toucher. Les partis  du continent tendent en outre à monopoliser l'espace politique. Les consignes pro-américaines de l'Internationale socialiste créent des difficultés supplémentaires aux partis socialistes, qui concilient difficilement cette ligne avec celle de la pleine libération du sous-continent. Les partis socialistes ont, quantitativement, souvent moins d'influence que certains groupuscules d'extrême-gauche. L'arrivée au pouvoir de Fidel Castro à Cuba permet en outre à un communisme populiste et tiers-mondiste d'acquérir une aura importante dans les milieux de gauche latino-américains. Au Chili, le Parti socialiste parvient à dépasser l'audience traditionnelle des partis modérés en renouant son alliance avec le Parti communiste et en présentant, dès 1956, Salvador Allende à l'élection présidentielle. Allende remporte l'élection de 1970 sous la bannière de la coalition de l'Unité populaire, mais sa politique est bientôt gênée tant par l'ingérence de Cuba et par la radicalisation de l'extrême-gauche chilienne qui prône l'instauration d'un , que par l'opposition de la droite et de l'extrême-droite ; le coup d'État du 11 septembre 1973, mené par l'armée chilienne, met brutalement un terme au gouvernement socialiste d'Allende[336]. Au Nicaragua, un régime d'inspiration socialiste se met en place en 1979 quand le Front sandiniste de libération nationale réussit à renverser le dictateur Somoza. Le chef des sandinistes, Daniel Ortega, bénéficie du soutien de Cuba et d'autres régimes communistes mais son gouvernement, s'il connaît une dérive autoritaire, ne va pas jusqu'à interdire l'opposition ni à collectiviser totalement l'économie. Les sandinistes, qui restent au pouvoir jusqu'à leur défaite électorale de 1990, font preuve d'une inspiration politique éclectique qui mêle le castrisme, le maoïsme, le communisme soviétique, la social-démocratie européenne, la théologie de la libération et une touche d'anarcho-syndicalisme[337],[338].
Les pays anglophones des Caraïbes connaissent, eux aussi, des gouvernements d'inspiration socialiste : Michael Manley, chef du Parti national du peuple, devient Premier ministre après avoir remporté les élections de 1972 sur un programme de réformes sociales. Si le PNP affiche sa fidélité aux valeurs du socialisme démocratique, son aile gauche et ses alliés d'extrême-gauche tendent à le faire évoluer vers des positions plus radicales. L'échec de la politique économique du gouvernement Manley lui vaut de perdre les élections de 1980. Le Parti national du peuple revient cependant au pouvoir lors des élections de 1989[339]. À la Grenade, le New Jewel Movement, parti d'inspiration socialiste ayant rapidement évolué vers le communisme, réalise un coup d'État en 1979 et instaure un , dirigé par Maurice Bishop : le nouveau pouvoir entreprend une transition vers le socialisme et se rapproche très vite de Cuba, puis des autres pays communistes. En 1983, un putsch interne au parti plonge le pays dans le chaos, provoquant l'invasion de l'île par les États-Unis et la fin du régime[340].
En Océanie, les partis de centre-gauche héritiers des traditions occidentales comme le Parti travailliste australien et le Parti travailliste néo-zélandais existent de longue date ; les partis australien et néo-zélandais occupent dans les échiquiers politiques de leurs pays respectifs une place comparable à celle de leur homologue de l'ancienne métropole britannique. Par ailleurs, une idéologie spécifique à la région apparaît dans les années 1980 avec le socialisme mélanésien théorisé par Walter Lini, premier ministre du Vanuatu : ambitionnant pour son pays un rôle de leader dans la région, Lini envisage le socialisme de la Mélanésie comme une idéologie  où la coutume deviendrait le moteur d'une évolution  de la société vanuataise. Le socialisme mélanésien est alors présenté comme devant permettre la coopération entre États mélanésiens et la définition d'une politique étrangère commune : dans cette optique, Walter Lini soutient les revendications indépendantistes du Front de libération nationale kanak et socialiste en Nouvelle-Calédonie[341].
Avec la fin de la guerre froide et la chute du bloc communiste, les partis socialistes européens acquièrent de manière durable une position dominante à gauche. La social-démocratie, au sens de socialisme démocratique, apparaît après la chute de l'URSS et de ses alliés européens comme l' à la gauche de l'échiquier politique[342]. Avec la fin des régimes communistes, une partie des anciens PC au pouvoir, auto-dissous, donnent naissance à de nouveaux partis à l'identité socialiste et réformiste : le Parti communiste bulgare devient ainsi le Parti socialiste bulgare, le Parti socialiste ouvrier hongrois le Parti socialiste hongrois, le Parti ouvrier unifié polonais la Social-démocratie de la république de Pologne ; l'ancien Parti socialiste unifié d'Allemagne, au pouvoir en RDA, donne naissance au Parti du socialisme démocratique qui devient ensuite Die Linke. Les régimes communistes restants continuent par ailleurs de se réclamer du socialisme[343] : la république populaire de Chine, largement convertie aux mérites de la mondialisation économique, met en pratique depuis les années 1990 une . Le fait d'être libérés de la pression sur leur gauche permet à certains partis ouest-européens d'afficher de manière plus ouverte leur identité réformiste. Pierre Mauroy, alors premier secrétaire du Parti socialiste français, déclare ainsi en 1991 : [342].
Les partis socialistes européens poursuivent dans les années 1980-1990 la mue réformiste entamée depuis le début de l'après-guerre. La crise identitaire des partis socialistes démocratiques, déjà entamée avant la chute du communisme, se poursuit alors que s'accentuent les conversions au libéralisme économique. En 1989, l'Internationale socialiste adopte une nouvelle déclaration de principe qui ne mentionne plus l'abolition du capitalisme comme but suprême du socialisme. L'ensemble des partis, à des degrés divers en fonction, se convertit ouvertement à l'économie de marché. Wim Kok engage dès 1986 le Parti travailliste néerlandais sur la voie du recentrage. Paavo Lipponen, chef du Parti social-démocrate de Finlande, multiplie les professions de foi libérales. Le Parti socialiste d'Autriche, d'abord réticent face aux idées libérales, est poussé à les adopter par le défi de la construction européenne[344] : cette conversion conduit l'ancien chancelier Bruno Kreisky, président d'honneur à vie du parti, à démissionner de son poste[345]. Le parti autrichien se rebaptise ensuite Parti social-démocrate d'Autriche. Le Parti social-démocrate suédois des travailleurs se convertit également à un certain degré de libéralisme tout en restant à gauche de la social-démocratie européenne. Le Parti socialiste ouvrier espagnol adopte, rapidement après son arrivée au pouvoir en 1982, une orientation néo-libérale. Le Parti travailliste norvégien, dans l'opposition entre 1981 et 1986, abandonne dans l'intervalle toute référence à une  au profit de  comme la liberté, l'égalité et la démocratie. Au sein du Parti socialiste belge, la tendance réformiste est menée par le courant dit des . Le Parti social-démocrate d'Allemagne réalise sa mue centriste sous les leaderships de Rudolf Scharping, puis de Gerhard Schröder, qui prône l'orientation vers un . En Grèce, le PASOK adopte des positions plus modérées qu'auparavant lors de son retour au pouvoir en 1993 : le recentrage du parti est poursuivi par Kóstas Simítis, successeur d'Andréas Papandréou. La situation de l'Italie est particulière : au début des années 1990, l'Opération Mains propres entraîne la chute de Bettino Craxi et la fin du PSI ; l'espace au centre gauche est alors occupé par l'ancien Parti communiste italien, auto-dissous et refondé sous le nom de Démocrates de gauche (parti affilié à l'Internationale socialiste et, en 2006, fusionné au sein du Parti démocrate). Les fidèles de Craxi se réunissent au sein du Nouveau PSI, allié au leader de la droite Silvio Berlusconi, puis reforment en 2007 un Parti socialiste italien[344].
Au Royaume-Uni, l'évolution du Parti travailliste (Labour Party) est particulièrement spectaculaire. Le Labour, dans l'opposition entre 1979 et 1997, s'éloigne de son ancienne identité strictement ouvriériste pour s'orienter, en s'appuyant sur une étude de l'état concret de la société britannique, vers une , théorisée notamment par le sociologue Anthony Giddens, et qui se situerait entre la social-démocratie classique, étatique et redistributive, et le néolibéralisme dérégulateur. Tony Blair, chef du Parti travailliste à partir de 1994, vise à unifier les deux grands courants de la pensée de centre gauche, le socialisme démocratique et le libéralisme, dont il juge que le divorce a affaibli la pensée progressiste en Occident durant le XXe siècle. Sa réflexion, qui s'appuie sur un ensemble d'intellectuels et de think tanks, s'inspire également de l'exemple des Nouveaux démocrates américains liés à Bill Clinton : elle aboutit à une métamorphose en profondeur du travaillisme britannique, sur le plan du discours comme sur celui des fondements idéologiques[346]. La clause IV du programme travailliste est révisée, supprimant la notion de propriété collective des moyens de production[347]. La période dite du New Labour a des répercussions bien au-delà du seul milieu politique britannique : le courant d'idées de Tony Blair, désigné du nom de blairisme et qui revendique sa parenté avec le social-libéralisme, s'affirme bientôt comme l'un des principaux axes du centre-gauche européen. Ayant gagné les élections de 1997, Tony Blair devient premier ministre et occupe ce poste jusqu'en 2007, devenant le premier dirigeant travailliste britannique à effectuer plusieurs mandats consécutifs en tant que chef du gouvernement[346].
Le Parti socialiste français, où la  de Michel Rocard n'a pas réussi à s'imposer durablement, suit la même voie de recentrage que ses homologues européens, mais cette conversion ne s'accompagne pas d'un débat public de fond, ni même d'échanges d'idées conséquents. Revenu au pouvoir après les élections législatives de 1997 qui amènent Lionel Jospin à la tête du gouvernement, le Parti socialiste poursuit dans cette voie mais n'adopte pas ouvertement de thèses comparables à celles de Tony Blair. Plusieurs de ses dirigeants, comme Laurent Fabius ou Dominique Strauss-Kahn - ce dernier prônant le passage d'un  à un  qui ferait figure de troisième voie entre le socialisme redistributif traditionnel et le socialisme libéral - font cependant figure de tenants du libéralisme économique au sein du PS. Le gouvernement de Lionel Jospin, s'il ressuscite sous le nom de gauche plurielle l'ancienne union de la gauche avec le PCF en y incluant les Verts et mène plusieurs mesures comme la loi sur les 35 heures, est marqué par un décalage entre un discours de principe antilibéral et la pratique effective du pouvoir. Bien que Lionel Jospin ait signé en avril 1997 une déclaration de principe avec le PCF s'engageant à stopper les privatisations, celles-ci sont, dans les faits, plus nombreuses que sous toutes les majorités précédentes. En l'absence de réflexion de fond menée sur les orientations du Parti, la période 1997-2002 entraîne une nouvelle forme de déperdition de sens quant à l'identité socialiste en France[348]. Candidat à l'élection présidentielle de 2002, Lionel Jospin déclare , phrase destinée à rassembler au centre mais analysée par la suite par François Hollande comme ayant fait  dans l'opinion[349],[350].
Entre la fin des années 1990 et le début des années 2000, le continent européen connaît une forte proportion de gouvernements socialistes et de centre-gauche. En sus des gouvernements de Lionel Jospin en France et de Tony Blair au Royaume-Uni, Romano Prodi remporte en 1996 les élections générales en Italie au nom de la coalition de L'Olivier qui comprend les Démocrates de gauche (Prodi doit néanmoins céder sa place dès 1998 à Massimo D'Alema, lui-même remplacé en 2000 par Giuliano Amato à la suite de la déroute de la gauche italienne aux élections régionales). Gerhard Schröder devient chancelier fédéral en Allemagne après la victoire du centre-gauche aux élections fédérales de 1998 et mène une politique nettement libérale, se trouvant en porte-à-faux avec Oskar Lafontaine, président fédéral du SPD. Cette période dite d' est marquée par une certaine opposition sur le plan des idées entre Tony Blair et Gerhard Schröder d'une part et Lionel Jospin de l'autre. Blair et Schröder publient en 1999 un manifeste commun fortement empreint d'idées libérales et plaidant pour . Lionel Jospin, lui se positionne à l'encontre de cette  trop ouvertement libérale[351],[352]. L' disparaît dans le courant des années 2000, avec la défaite du centre-gauche italien aux élections générales de 2001, celle de Lionel Jospin à l'élection présidentielle de 2002 et celle de Gerhard Schröder aux élections fédérales de 2005[353].
Les partis socialistes démocratiques demeurent néanmoins, dans une grande partie du continent européen, les principales forces au centre-gauche de l'échiquier politique de leurs pays respectifs et les bénéficiaires mécaniques des alternances politiques. Ainsi, parmi d'autres exemples européens, José Luis Rodríguez Zapatero, chef du PSOE, est chef du gouvernement en Espagne de 2004 à 2011, José Sócrates, chef du Parti socialiste portugais, premier ministre du Portugal de 2005 à 2011, Giórgios Papandréou, chef du PASOK, dirige le gouvernement de la Grèce de 2009 à 2011, Helle Thorning-Schmidt, présidente des Socialdemokraterne, est premier ministre du Danemark de 2011 à 2015, Elio Di Rupo, président du Parti socialiste belge, est choisi comme premier ministre de la Belgique en 2011 pour mettre fin à la crise politique et François Hollande, candidat du Parti socialiste français, remporte l'élection présidentielle de 2012[354].
Les grands partis de centre-gauche ne sont cependant pas épargnés par l'usure du pouvoir face aux crises sociales et économiques et par la défiance croissante d'une partie de l'électorat vis-à-vis des forces politiques traditionnelles. Le Parti socialiste ouvrier espagnol perd les élections de 2011 puis est talonné aux élections suivantes par Podemos. En Grèce, le PASOK, confronté à une très grave crise économique, est battu aux élections de 2012 et distancé, puis marginalisé lors des scrutins suivants, par la coalition de gauche radicale SYRIZA. En France, le Parti socialiste sort profondément affaibli du quinquennat de François Hollande, qui renonce à se représenter à la présidentielle 2017 ; le mauvais score de son candidat Benoît Hamon, largement dépassé par le centriste Emmanuel Macron (qu'ont rejoint de nombreux cadres socialistes) et par le candidat de gauche radicale Jean-Luc Mélenchon, souligne ensuite l'ampleur de la crise du PS[355]. Lors des législatives 2017, le Parti socialiste français réalise le pire score de son histoire, au profit notamment de La République en marche ! mais aussi, sur sa gauche, de La France insoumise[356].
L'idée de socialisme continue de connaître en Occident des variations et de se marier à différents courants, comme l'écologie politique : ainsi, le concept d'écosocialisme, résumé en 1975 par Joël de Rosnay comme [357],[358], est ensuite défini comme une forme de socialisme qui prendrait en compte les impératifs écologiques du XXIe siècle[359]. L'écosocialisme fait partie du vocabulaire utilisé par différentes tendances de gauche[360],[361] et d'extrême-gauche[362], ainsi que par des auteurs altermondialistes[363].
L'orientation de centre-gauche des partis socialistes européens continue de créer en leur sein des divergences, qui vont parfois jusqu'à la scission pour exprimer les tendances plus radicales de la gauche : Oskar Lafontaine, ancien président du SPD, en désaccord avec la politique menée par Gerhard Schröder, quitte en 2005 le parti social-démocrate et, avec d'autres socialistes dissidents, fonde un nouveau parti, l'Alternative électorale travail et justice sociale. Le parti des déçus du SPD s'allie ensuite au Parti du socialisme démocratique, héritier de l'ancien parti communiste est-allemand, pour former le nouveau parti Die Linke. En France, Jean-Luc Mélenchon, ancien ministre du gouvernement Jospin, quitte le PS pour fonder en 2009 le Parti de gauche, qui s'allie au Parti communiste français et à sept autres petits partis de la gauche antilibérale pour former la coalition du Front de gauche ; Jacques Généreux, alors secrétaire à l'économie du Parti de gauche, théorise en 2009 un  fondé sur le développement humain, la justice sociale, l'écologie, la laïcité, la démocratie économique et l'internationalisme[364]. Jean-Luc Mélenchon fait concurrence à François Hollande lors de l'élection présidentielle de 2012[365],[366]. Lors de celle de 2017, Mélenchon, qui se présente cette fois sous l'étiquette La France insoumise, se classe quatrième au premier tour en distançant largement le candidat socialiste Benoît Hamon[367]. La France insoumise dépasse ensuite le PS à l'Assemblée nationale[368].
En Grèce, dans les années 2010, la coalition de gauche radicale SYRIZA surclasse largement le PASOK[369],[370]. En Espagne, Podemos, une formation altermondialiste se réclamant du socialisme démocratique, fait concurrence au PSOE. En septembre 2015, le Parti travailliste britannique élit à sa tête Jeremy Corbyn, un représentant de la gauche radicale, adversaire du blairisme et partisan du retour à des conceptions socialistes plus traditionnelles[371],[372].
Dans le monde arabe, durant les années 2000 et 2010, plusieurs gouvernements se réclamant à divers titres du socialisme sont renversés : en 2003, le régime de Saddam Hussein en Irak prend fin lors de l'invasion militaire du pays. Par la suite et dans un contexte différent, plusieurs chefs d'État arabes sont renversés durant la période dite du  : au cours de l'année 2011, Zine el-Abidine Ben Ali en Tunisie et Hosni Moubarak en Égypte sont contraints par des contestations populaires à quitter le pouvoir. Leurs partis respectifs sont exclus de l'Internationale socialiste. Cette même année, en Libye, Mouammar Kadhafi est tué à l'issue d'une révolte contre son régime. En Syrie, le régime de Bachar el-Assad, confronté lui aussi à une insurrection, tente de se réformer et promulgue en 2012 une nouvelle constitution qui élimine toute référence au socialisme[373].
En Amérique latine, la fin des années 1990 et l'ensemble des années 2000 voit l'élection d'une série de dirigeants se réclamant, à des degrés divers et sur des registres politiques variés, du socialisme. En décembre 1998, Hugo Chávez est élu président du Venezuela, inaugurant un nouveau cycle  en Amérique latine. Chávez se fait le promoteur d'une  appuyée sur des organisations de masse et nourrie de références non seulement bolivariennes et tiers-mondistes, mais également volontiers maoïstes et castristes[374]. Le président vénézuélien plaide pour l'instauration d'un , qui reposerait sur le , l'État prenant un rôle accru dans l'économie[375]. Chávez, qui reste à la tête du Venezuela jusqu'à son décès en mars 2013, s'emploie à diffuser son influence à l'échelle du continent. En 2005, il impulse la création de l'Alliance bolivarienne pour les Amériques, un organisme de coopération entre gouvernements latino-américains d'orientation socialiste : Cuba, très proche du Venezuela chaviste, participe également à cette organisation[376],[377]. En 2003, Luiz Inácio Lula da Silva, candidat du Parti des travailleurs d'orientation socialiste (les composantes de ce parti, situé dans son ensemble au centre gauche, vont de la social-démocratie au trotskisme) est élu président du Brésil[378]. Evo Morales, candidat du Mouvement vers le socialisme, est élu président de la Bolivie en 2005 ; Daniel Ortega, chef du Front sandiniste de libération nationale, redevient en 2007 président du Nicaragua ; Rafael Correa, candidat de l'Alianza País, parti socialiste et bolivarien, est élu la même année président de l'Équateur[379] ; Mauricio Funes, candidat du Front Farabundo Martí de libération nationale, ancienne guérilla marxiste reconvertie en parti social-démocrate, devient en 2009 président du Salvador[380] ; Ollanta Humala, candidat du Parti nationaliste péruvien, d'orientation socialiste et nationaliste, devient en 2011 président du Pérou. Au début des années 2010, une majorité des gouvernements d'Amérique latine se situent à gauche ou au centre-gauche, avec cependant une nette diversité en ce qui concerne l'idéologie des dirigeants et les contextes nationaux[377]. Les gouvernements socialistes sud-américains subissent cependant eux aussi l'usure du pouvoir et, après la mort d'Hugo Chávez, le Venezuela présidé par son successeur Nicolás Maduro s'enfonce dans une profonde crise économique et politique[381].
Du fait de la polysémie du terme socialisme et de l'évolution importante des formes politiques que recouvre cette dénomination, les critiques portées au socialisme sont elles aussi très diverses. Au XIXe siècle, pendant et après les révolutions de 1848, le socialisme a été considéré par ses opposants comme un facteur de désordre, de subversion et de haine sociale[382]. Au sein de la mouvance socialiste, les polémiques ont été particulièrement nombreuses : les  ont été ainsi dénommés par les marxistes qui leur reprochaient le peu de crédibilité de leurs théories[383]. Les différents courants socialistes ont continué, après 1848, de s'opposer vivement les uns aux autres : les partisans de Proudhon puis ceux de Bakounine ont affronté ceux de Marx, auxquels ils reprochaient leurs conceptions autoritaristes[384].
Le socialisme a été critiqué comme pouvant, dans ses formes extrêmes, s'opposer à la liberté et à la démocratie, parce qu'il suppose de contraintes[385]. Du fait de l'opposition entre socialisme et libéralisme, la critique du socialisme a notamment été une tendance lourde de la pensée économique libérale[386]. Friedrich Hayek, théoricien d'un libéralisme économique , dénonce dans son essai La Route de la servitude le socialisme comme , qui n'aurait été adopté comme  que par un extraordinaire paradoxe. Hayek considère, en remontant jusqu'à Saint-Simon, que les idées socialistes sont porteuses d'une conception autoritaire de la société qui ne peut conduire qu'à la dictature, socialisme et fascisme étant pour lui des régimes à la parenté  : selon cette vision, le projet socialiste de lutte contre le libéralisme et de contrôle de la société porte en lui les racines du national-socialisme, qui en est la continuation logique[387].
Du côté opposé de l'échiquier politique, les communistes ont dénoncé, en des termes parfois particulièrement virulents, les socialistes comme des  - voire, au moment de la ligne  du Komintern, des  - et ont revendiqué le monopole du véritable socialisme[249]. La référence commune au socialisme de ces deux familles politiques a par ailleurs encouragé la confusion et l'amalgame entre socialistes et communistes, le terme socialisme étant associé à la forme de gouvernement des régimes communistes, à leurs pratiques dictatoriales, voire totalitaires, et à l'échec de leurs économies planifiées[18],[5],[388].
Le terme de social-démocratie a, quant à lui, acquis dans certains discours une charge négative, en ce qu'il supposerait une forme  de réformisme, synonyme de l'abandon de [389]. Les méthodes de gouvernement inspirées des idées socialistes ont quant à elle fait l'objet d'oppositions, du fait de leur recherche de l'égalité sociale, qui aboutit parfois à un phénomène de nivellement. La Suède a ainsi, durant les longues années de gouvernement social-démocrate, pratiqué la redistribution des richesses par un impôt sur le revenu particulièrement fort et progressif : la social-démocratie suédoise a donc cherché à éliminer non seulement la pauvreté, mais également les grandes fortunes, et cette pression fiscale a contribué à motiver l'opposition, jusqu'à aboutir à la défaite électorale des sociaux-démocrates dans les années 1970, après plusieurs décennies au pouvoir[390].
S'agissant du socialisme contemporain, la forme moderne du socialisme démocratique, associé au concept sinon à la dénomination de social-démocratie, fait l'objet de critiques pour son manque de véritable substance idéologique : l'abandon de la doctrine marxiste, qui n'a pas forcément donné lieu à une réflexion théorique, aurait ainsi débouché, sur fond de conversion au libéralisme économique, sur une absence de réelle identité politique, au profit de la seule évocation de valeurs consensuelles comme la liberté ou la démocratie[348]. Le journaliste politique Éric Dupin note ainsi en 2002 que [391] Le constitutionnaliste et politologue Dmitri Georges Lavroff note pour sa part en 1999 que si les partis socialistes européens se sont imposés en tant que force de gouvernement, ils n'ont , le succès du socialisme démocratique s'étant fait [392].
D'un point de vue conceptuel, le socialisme reste obéré, selon l'ouvrage que lui consacre en 2015 le philosophe et sociologue Axel Honneth, par trois présupposés communs à ses premiers penseurs : la primauté donnée à l'organisation de l'économie, qui a conduit à négliger notamment la question des libertés politiques ; une identification postulée aux intérêts d'une force sociale déjà établie, le dispensant de vérification empirique ; la croyance en une forme de nécessité historique, en vertu de laquelle il s'est détourné de l'expérimentation[393]. L'ancrage de ces conceptions dans le contexte social et intellectuel de la révolution industrielle explique leur obsolescence après la Seconde Guerre mondiale, face à la complexification des sociétés modernes[394]. Revitaliser l'idée première, qui réside pour l'auteur dans la volonté de réaliser conjointement les trois principes de la Révolution française[395], passe selon lui par une révision dans le sens d'un expérimentalisme historique qui reviendrait notamment sur l'assimilation du marché au capitalisme[396], et d'une forme de vie démocratique qui intégrerait les différentes sphères de la vie sociale et leurs interactions[397].
Pour les articles homonymes, voir Commonwealth (homonymie).
Le Commonwealth of Nations (traduit en français par la ), communément appelé Commonwealth (qui signifie bien commun, richesse commune, prospérité partagée)[note 1] est une organisation intergouvernementale composée de 56 États membres qui sont presque tous d'anciens territoires de l'Empire britannique.
Le Commonwealth a émergé au milieu du XXe siècle pendant le processus de décolonisation. Il est formellement constitué par la Déclaration de Londres de 1949 qui fait des États membres des partenaires . Le symbole de cette libre association est le roi Charles III qui est chef du Commonwealth. Le roi est également le chef d'État monarchique des 15 royaumes du Commonwealth. Les autres États membres sont 36 républiques et 5 monarchies dont le monarque est différent[2].
Les États membres n'ont aucune obligation les uns envers les autres. Ils sont réunis par la langue, l'histoire, la culture et des valeurs décrites dans la Charte du Commonwealth telles que la démocratie, les droits de l’homme et l'État de droit.
Le Commonwealth a vu le jour le 19 novembre 1926, et comportait 6 membres : l'Empire britannique, l'Afrique du Sud, l'Australie, le Canada, ainsi que l'Irlande et Terre-Neuve.
Le Commonwealth était composé de 9 membres lors de l'arrivée d'Élisabeth II sur le trône, en 1952 : le Royaume-Uni, le Canada, l'Australie, la Nouvelle-Zélande, l'Irlande, l'Afrique du Sud, le Pakistan, l'Inde et le Sri Lanka.
Les 56 États membres actuels (2022) représentent 30 282 502 km2 de territoire sur les cinq continents. Leur population actuelle, est estimée à 2 558 903 391 habitants, soit environ le tiers de l'humanité.
Trois États regroupent 80% des habitants : l'Inde, le Bangladesh et le Pakistan, et le continent le plus représenté est l'Afrique, avec 21 pays membres[3].
Le Commonwealth tire ses origines des Conférences Impériales de la fin des années 1920, des conférences de Premiers ministres britanniques et coloniaux s'étant tenues périodiquement depuis 1887. Les dominions, États indépendants membres de l'Empire, ont vu leur pleine souveraineté garantie via la déclaration Balfour de 1926 et le Statut de Westminster de 1931, qui abolissaient les derniers droits d'ingérence qu'avait conservé le gouvernement britannique sur ces derniers. Le Commonwealth fut établi comme une association d'États indépendants et souverains, « librement associés » et égaux, dont l'adhésion reposait sur une allégeance commune à la couronne britannique[4]. Aujourd'hui, quatre pays membres n'ont jamais connu la domination britannique (Mozambique, Togo, Gabon et Rwanda).
L'Empire fut en partie démantelé après la Seconde Guerre mondiale, à la suite de l'émergence de mouvements indépendantistes dans les territoires assujettis (particulièrement en Inde, sous l'influence du pacifiste Gandhi) et à l'affaiblissement du gouvernement britannique face au coût de la guerre. La rétrocession de Hong Kong à la Chine, en 1997, marque officieusement la « fin de l'Empire »[5].
Les membres fondateurs du Commonwealth dans les années 1920 sont le Royaume-Uni et les dominions : Afrique du Sud, Australie, Canada, État libre d'Irlande, Nouvelle-Zélande, et Terre-Neuve. Après la décision du Canada, en 1947, d'établir une loi spécifique régissant la nationalité canadienne, les chefs d'État du Commonwealth se sont réunis pour préparer l'adoption de législation spécifique concernant la citoyenneté et la nationalité.
L'Irlande en fut membre, mais a adopté une constitution républicaine en 1937. En 1949, après que l'Irlande eut mis fin à sa pratique consistant à faire nommer des diplomates irlandais par le roi britannique, le Commonwealth cessa de considérer l'Irlande comme un membre. Le Commonwealth s'est considérablement agrandi dans les dernières décennies : composé de dix membres en 1956, il en regroupait 54 le 6 juin 2021[6].
En 1932, les accords d'Ottawa instaurent le système de préférence impériale : les membres du Commonwealth bénéficient d'une réduction des droits de douane lorsqu'ils commercent avec d'autres États membres. À l'inverse, les États non membres du Commonwealth doivent payer des droits de douane plus importants pour commercer avec les pays membres. L'objectif de ces accords est d'établir un système de libre-échange parmi les membres de l'Empire britannique.
Le British Nationality Act (en) de 1948 a accordé le statut de « citoyen du Royaume-Uni et des colonies » (CUKC) à toute personne née au Royaume-Uni ou dans l'une des colonies de l'empire. Ces personnes étaient simultanément « citoyens du Commonwealth ». Jusqu'au Commonwealth Immigrants Act (en) de 1962, tout citoyen du Royaume-Uni et des colonies pouvait librement entrer et résider sur le territoire du Royaume Uni. La loi de 1948 permettait l'obtention d'une double nationalité, sauf pour les citoyens du Commonwealth ou les sujets britanniques ayant adopté la nationalité de l'État en question : en d'autres termes, une personne demandant la citoyenneté canadienne, par exemple, perdait le statut de CUKC. Le British Nationality Act de 1964 permettait toutefois aux personnes ayant abandonné leur citoyenneté britannique au profit de la nationalité d'un État membre du Commonwealth de reprendre leur nationalité originelle. Il empêchait aussi toute personne menacée de devenir apatride de perdre son statut de CUKC. La même année, une autre loi permet aux citoyennes du Royaume-Uni et des colonies de transmettre leur statut à leur enfant si celui-ci risque sinon de devenir apatride. En règle générale, la transmission de nationalité ne pouvait se faire que par le père.
Le Commonwealth s'est considérablement agrandi depuis son début. Le début du Commonwealth moderne fut marqué en 1947 par l’indépendance de l’Inde, qui conserva néanmoins des liens avec le Commonwealth. Composé de dix membres outre les anciennes colonies en 1956, il en regroupe plus de cinquante en 2000. L’augmentation se fit surtout pendant les années 1940 et les années 1960, période durant laquelle plusieurs pays d'Asie et d'Afrique ont accédé à l'indépendance et ont décidé de se joindre à l'organisation à titre d'États souverains. Depuis lors, bon nombre de petits pays des Antilles, de l'océan Indien et des îles du Pacifique sont venus à leur tour grossir les rangs.
À l'opposé, lors de la conférence du Commonwealth de 1961 (en), l'Afrique du Sud devait renouveler son adhésion parce qu'elle était devenue une république. Mais les pays membres à population non blanche, ainsi que le Canada, s'y opposent à cause de l'apartheid en Afrique du Sud. Devant cette opposition l'Afrique du Sud se retire. C'est en 1994, après l'abrogation de l'apartheid et sous la présidence de Nelson Mandela, que l'Afrique du Sud revient dans le Commonwealth[7].
Deux événements significatifs pour l’histoire du Commonwealth : en 1971, la Singapore Declaration of Commonwealth Principles (en), qui propose de se rallier aux principes éthiques des droits de l'homme et à l'égalité raciale et économique. Ensuite, la création du Commonwealth Fund for Technical Cooperation (CFTC) qui permet une coopération technique pour les pays membres.
En 1949, les membres du Commonwealth sont convenus que la monarchie britannique devait être le symbole de la libre association des nations membres et, en tant que tel, « chef du Commonwealth », indépendamment du fait qu'un pays membre en fasse ou non son chef d'État. Charles III est le monarque de 15 « royaumes » parmi les 56 pays membres, et chef du Commonwealth pour tous. Il est présent à tous les sommets, mais il ne participe pas aux réunions.
En 1995, le Mozambique, devient le premier pays n'ayant pas de relation historique avec le Royaume-Uni à rejoindre le Commonwealth, suivi en 2009 par le Rwanda. Le Gabon et le Togo rejoignent le Commonwealth le 25 juin 2022[8].
Le 2 octobre 2013, la Gambie, alors sous la présidence autoritaire de Yahya Jammeh, annonce son retrait « avec effet immédiat » du Commonwealth, « une institution néo-coloniale ». Le Commonwealth avait demandé un meilleur respect des droits de l'homme dans le pays[9]. En février 2017, peu après son élection à la présidence de la Gambie, Adama Barrow annonce sa volonté de réintégrer le Commonwealth[10]. La Gambie est de nouveau membre du Commonwealth depuis le 8 février 2018[11].
Le 13 octobre 2016, en réponse là aussi aux exigences de l'organisation en matière de droits de l'homme, les Maldives du président autocrate Abdulla Yameen annoncent leur retrait du Commonwealth[12]. Vainqueur de l'élection présidentielle de septembre 2018, le démocrate Ibrahim Mohamed Solih annonce son intention de postuler à la réintégration des Maldives dans le Commonwealth[13].
L'appartenance au Commonwealth n'interdit pas l'adhésion à d'autres organismes. L'Organisation internationale de la francophonie regroupant principalement les anciens territoires de l'empire colonial français comprend onze membres communs avec le Commonwealth et d'autres nations francophones seraient intéressées, à la fin des années 2000, par cette association[14].
La position de chef du Commonwealth a été créé par la Déclaration de Londres (en) de 1949 pour George VI, qui était précédemment roi de tous les États membres du Commonwealth : il s'agissait alors de permettre à l'Inde, qui s'apprêtait à devenir une république, de rester au sein de l'organisation.
Il s'agit d'une fonction symbolique qui représente la libre association des États membres.
Charles III est l'actuel chef du Commonwealth. Le titre fait partie de son titre officiel dans chacun des quinze royaumes du Commonwealth[15] dont il est chef d'État. Le titre n'est toutefois pas héréditaire et à la mort de l'actuel monarque le successeur de la Couronne ne deviendra pas automatiquement chef du Commonwealth[16]. Cependant, lors du sommet de Windsor, le 20 avril 2018, les dirigeants des pays du Commonwealth ont décidé par consensus que le prince Charles succéderait à sa mère comme chef du Commonwealth[17].
La réunion des chefs de gouvernement du Commonwealth (Commonwealth Heads of Government Meeting ou CHOGM) est l'héritière des conférences impériales et coloniales puis des réunions des Premiers ministres du Commonwealth et est aujourd'hui le principal organe de décision du Commonwealth. Elle est organisée tous les deux ans et réunit les chefs de gouvernement (Premier ministre ou président) des pays membres.
Des réunions réunissant les ministres des Finances de la Justice ou de la Santé des États membres ont également lieu régulièrement.
Les membres suspendus ne participent pas aux réunions[15].
Le chef du gouvernement, hôte de la réunion des chefs de gouvernement, est appelé « président en exercice du Commonwealth » (Commonwealth Chairperson-in-Office) et garde ce titre jusqu'à l'ouverture de la réunion suivante[18].
Le secrétariat du Commonwealth, créé en 1965, est le principal organe permanent du Commonwealth et est chargé de faciliter les consultations et coopérations entre les gouvernements des États membres. Le secrétariat représente le Commonwealth à l'Assemblée générale des Nations unies en tant qu'observateur. Il est responsable de l'organisation des sommets du Commonwealth, rencontres des ministres, des rencontres consultatives et des discussions techniques et aide au développement des politiques. Il procure aussi une assistance technique aux gouvernements dans le domaine du développement social et économique et en soutien aux valeurs du Commonwealth.
Le secrétariat est dirigé par le secrétaire général du Commonwealth élu par les chefs de gouvernement du Commonwealth pour un mandat de quatre ans renouvelable une fois. L'actuelle secrétaire générale est Patricia Scotland de la Dominique depuis le 1er avril 2016.
En raison de leur histoire commune, les États membres du Commonwealth ne se considèrent généralement pas comme  les uns aux autres[19],[20],[21]. Ainsi, entre eux, les États membres ne s'échangent pas des ambassadeurs mais des hauts-commissaires. Ceux-ci sont considérés comme les représentants de leur gouvernement et pas du chef d'État comme le serait un ambassadeur.
Dans les pays tiers dans lequel leur propre pays n'est pas représenté, les citoyens du Commonwealth peuvent faire appel à l'assistance consulaire de l'ambassade britannique locale[22].
De plus, certains États accordent aux citoyens du Commonwealth une préférence par rapport aux citoyens d'autres pays qui vivent sur leur territoire. Par exemple, le Royaume-Uni et d'autres pays, principalement dans les Caraïbes, accordent le droit de vote aux citoyens du Commonwealth dans les mêmes conditions que les citoyens du pays.
La réunion des chefs de gouvernement du Commonwealth a lieu tous les deux ans (la prochaine se déroulera en 2024[23]) ; à cette occasion, les participants discutent des grandes questions politiques, économiques et sociales, de leur incidence sur leurs pays respectifs et des mesures à adopter, au sein du Commonwealth ou conjointement dans d'autres organes internationaux. Avant 1971, toutes les réunions se tenaient à Londres ; depuis, elles se déroulent chaque fois dans un pays différent, le chef de gouvernement du pays hôte en assume la présidence. Un communiqué est publié à l'issue de chaque réunion. Il arrive qu'une déclaration soit faite pour énoncer une série d'objectifs et de principes généraux communs.
Le Commonwealth fait la promotion d'une série de valeurs communes à ses membres telles que l'égalité, la non-discrimination, la démocratie et la primauté du droit. La déclaration de (en) Harare de 1991 a reconnu l'importance particulière qu'il accorde aux droits de la personne, à l'éthique démocratique, à l'égalité des Humains, au développement durable et à la protection de l'environnement; la Commonwealth Human Rights Initiative est chargée d'en promouvoir les objectifs. Au cours du temps, il s'est spécialisé dans certains domaines, ce qui lui a permis d'aider directement ses membres à faire face à des problèmes communs ou particuliers. En effet, ceux-ci bénéficient de l'appui d'un vaste réseau d'organismes privés, bénévoles et professionnels, comprenant entre autres des associations juridiques, médicales, d'universités et de parlementaires, des professionnels et des organisations médiatiques et sportives. Partageant une langue de travail commune et des systèmes juridiques, d’administration publiques et d’éducation semblables, le Commonwealth s’est érigé sur un passé commun pour devenir une association dynamique d’États en harmonie avec le monde moderne.
Par ailleurs, le Commonwealth joue un rôle important pour le progrès social et économique de ses membres. Le CFTC (Fonds du Commonwealth pour la coopération technique) a soutenu les efforts des pays membres en développement afin d'accélérer le rythme de leur croissance économique. Il fournit, à court terme, les compétences techniques manquantes ou insuffisantes afin de permettre aux gouvernements concernés dans des domaines aussi divers que les exportations et la promotion industrielle, les services juridiques et économiques de haut niveau, et, à plus long terme, il assure tout un éventail de programmes de formation dans les secteurs considérés comme les piliers du développement économique. L'association des pays du Commonwealth crée donc des programmes de solidarité et de coopération internationale pour aider les petits États membres. Il servait aussi d'aide aux autres pays en guerre.
Les Jeux du Commonwealth ont lieu tous les quatre ans. Seuls les 56 pays du Commonwealth peuvent participer.
En 1947, l'Inde fut le premier État à sortir du système établi en quittant l'Empire britannique pour devenir une république. Elle ne pouvait théoriquement  pas être membre du Commonwealth, puisque le monarque anglais ne représentait plus rien pour elle. Toutefois, le nouvel État souhaitant rester membre de l'organisation, les dirigeants politiques des États membres ont trouvé un accord allant dans ce sens. C’est la déclaration de Londres du 26 avril 1949, qui stipule que l’appartenance à l’organisation ne peut plus être basée sur une allégeance à la Couronne britannique. L’Inde a ainsi ouvert la porte au Pakistan, au Sri Lanka, puis à toutes les nouvelles républiques et monarchies avec un souverain local, qui composent l'actuel Commonwealth.
La déclaration de Londres confirme que le monarque britannique reste le personnage le plus important dans chacun des royaumes du Commonwealth et garde certaines prérogatives, comme le commandement des forces militaires ou la création de lois. Dans cette même déclaration, les dirigeants reconnaissent et s’accordent sur le fait que les membres du Commonwealth des Nations sont :
« Libres et égaux, coopérant librement dans le même but de paix, de liberté et de progrès »[3].
Depuis 1949, le chef du Commonwealth ne dispose d'aucun pouvoir effectif sur chacun des États membres, y compris les monarchies dont il est le chef d'État, et le Royaume-Uni n'a plus de position hiérarchique supérieure dans l'organisation[3].Sur les autres projets Wikimedia :
Ne doit pas être confondu avec racialisme.
Le racisme est une idéologie qui, partant du postulat[1] de l'existence de races au sein de l'espèce humaine[2], considère que certaines catégories de personnes sont intrinsèquement supérieures à d'autres[2]. Il se différencie ainsi du racialisme qui, partant du même postulat, ne considère pas les races comme inégales[3]. Cette idéologie peut amener à privilégier une catégorie de personne à une autre, qui se trouve reléguée à une classe sociale jugée inférieure et subit alors, de manière intersectionnelle, le mépris de classe en plus du racisme[4]. Le Petit Larousse a deux définitions du racisme, au sens strict du terme, comme « idéologie fondée sur la croyance qu'il existe une hiérarchie entre les groupes humains, les « races » ; comportement inspiré par cette idéologie », et au sens large du terme, comme « une attitude d’hostilité répétée voire systématique à l’égard d’une catégorie déterminée de personnes ».
Cette hostilité envers une autre appartenance sociale (que la différence soit culturelle, ethnique – ou tout simplement due à une couleur de peau) – se traduit aussi par des formes de xénophobie ou d’ethnocentrisme. Certaines formes d’expression du racisme, comme les injures racistes, la diffamation raciale, la discrimination, sont considérées comme des délits dans plusieurs pays.
Les idéologies racistes ont servi de fondement à des doctrines politiques conduisant à pratiquer des discriminations raciales, des ségrégations ethniques et à commettre des injustices et des violences pouvant aller, dans les cas extrêmes, jusqu'au génocide.
Selon certains sociologues, le racisme s’inscrit dans une dynamique de domination sociale à prétexte racial[5]. Le « racisme inversé » est pour sa part une expression qui use du terme « racisme », mais décrit un acte ou un propos venant non des membres d'un groupe social dominant, mais d'un groupe anciennement ou actuellement dominé ; la dénonciation d'un racisme inversé ne suppose pas l'adhésion aux idées racistes qui sous-tendent par exemple le suprémacisme blanc.
Selon le CNRTL, le mot racisme serait apparu en 1902[6] alors que le mot raciste daterait de 1892[7].
Selon Charles Maurras[8], Gaston Méry (1866-1909), pamphlétaire, journaliste collaborateur à La Libre Parole — le journal antisémite et polémiste d'Édouard Drumont — est la première personne connue à avoir utilisé le mot « raciste » en 1894[9],[10],[11].
Toutefois l'adjectif « raciste »[12] et le nom « racisme » ne s'installent dans le vocabulaire général en France qu'à partir des années 1930[13]. Léon Trotski l'emploie en 1930 dans son Histoire de la révolution russe, pour qualifier les tenants modernes des théories racistes[14], ce qu'il développera encore en 1933 vis-à-vis du nazisme[15].
Les deux mots font leur entrée pour la première fois dans le dictionnaire français Larousse en 1932[16].
La littérature met, au XIXe siècle, en avant le caractère pluridimensionnel du racisme. On peut distinguer :
Si la notion de « race humaine » et le concept du racisme sont partie liée, l’étude de leurs relations nécessite d’opérer une première distinction entre la race en tant que concept biologique et la race en tant que constructivisme social que l’on peut définir comme « un signe ou un ensemble de signes par lesquels un groupe, une collectivité, un ensemble humain est identifié, dans certains contextes historiques précis, cette apparence socialement construite variant suivant les sociétés et les époques »[20].
Au cours de l'histoire, les définitions sociales de la « race » se sont souvent appuyées sur de présupposés caractères de nature biologique. La race (en tant que construction sociale) est cependant devenue largement indépendante des travaux menés sur la classification biologique des êtres humains qui ont montré que la notion de race humaine n'est pas pertinente pour caractériser les différents sous-groupes géographiques de l'espèce humaine, car la variabilité génétique entre individus d'un même sous-groupe est plus importante que la variabilité génétique moyenne entre sous-groupes géographiques[21],[22]. Cette conclusion est cependant contestée par A. W. F. Edwards (en) qui critique, dans son article La diversité génétique humaine : l'erreur de Lewontin (en) (2003), l'argument, présenté en 1972 par Richard C. Lewontin The Apportionment of Human Diversity (La répartition de la diversité humaine)[23], soutenant que la division de l'humanité en races est taxonomiquement invalide[24].
Le consensus scientifique actuel rejette l’existence d'arguments biologiques qui pourraient légitimer la notion de race[25], reléguée à une représentation arbitraire selon des critères morphologiques, ethnico-sociaux, culturels ou politiques[26]. Cette autonomie se manifeste pleinement depuis la seconde moitié du XXe siècle[27] où les effets du système de perception raciste perdurent en dépit d'un usage moins fréquent, et malgré le rejet du concept de race par la communauté scientifique.
Essai sur l'inégalité des races humaines est un ouvrage du Français Joseph Arthur de Gobineau paru en 1853 et visant à établir l'existence de races et de différences les séparant. L’ouvrage sera l’un des fondements des idéologies racistes du XXe siècle[28].
Le mécanisme perceptif du racisme peut être décomposé en plusieurs opérations logiques.
Le racisme se fonde sur la focalisation du regard du raciste sur une différence, souvent anatomique. Elle peut être « visible » – la pigmentation de la peau – mais ne l’est pas nécessairement : le regard raciste peut exister sans s’appuyer sur des différences visuelles évidentes. La littérature antisémite a ainsi abondamment cherché, sans succès, à définir les critères qui pourraient permettre de reconnaître visuellement les Juifs et a finalement dû mettre en avant des différences invisibles, imperceptibles pour l'œil humain[réf. souhaitée].
Le racisme associe des caractères physiques à des caractères moraux et culturels. Il constitue un système de perception, une « vision syncrétique où tous ces traits sont organiquement liés et en tout cas indistinguables les uns des autres »[29]. L'identification des traits physiques ou la reconnaissance du signe distinctif (l'étoile juive par exemple) génère immédiatement chez le racisant une association avec un système d'idées préconçues. Dans le regard du racisant, « l'homme précède ses actes »[30]. Si la focalisation du regard raciste rend le corps visé plus visible que les autres, il a donc aussi pour effet de faire disparaître l’individualité derrière la catégorie générale de la race[31].
Le raciste considère les propriétés attachées à un groupe comme permanentes et transmissibles, le plus souvent biologiquement. Le regard raciste est une activité de catégorisation et de clôture du groupe sur lui-même.
Le racisme s’accompagne souvent d’une péjoration des caractéristiques du groupe visé. Le discours raciste n’est toutefois pas nécessairement péjoratif. Pour Colette Guillaumin, les « bonnes caractéristiques font, au même titre que les mauvaises caractéristiques, partie de l’organisation perceptive raciste »[32]. La phrase « Les Noirs courent vite » constitue ainsi un énoncé raciste malgré son apparence méliorative[réf. souhaitée].
Le discours raciste peut évoquer la supériorité physique des groupes visés (ainsi la vigueur ou la sensualité des Noirs) pour souligner par contraste leur infériorité intellectuelle. Les qualités qui leur sont attribuées (l’habileté financière des Juifs par exemple) sont la contrepartie de leur immoralité ou alimentent la crainte de leur pouvoir souterrain.
Mais plus encore, au-delà du contenu — positif ou négatif — des stéréotypes racistes, l’activité de catégorisation, de totalisation et de limitation de l’individu à des propriétés préconçues n’est en soi pas une activité neutre du point de vue des valeurs. Dans cette perspective, voir et penser le monde social dans les catégories de la race relève déjà d'une attitude raciste.
Historiens et ethnologues ne sont pas d'accord sur la question de l’origine du racisme ; deux conceptions principales s'opposent à ce propos. La première considère que le racisme est un sous-produit du capitalisme européen, en lien avec le colonialisme[33]. La seconde que différentes formes de racisme se sont succédé au cours de l’histoire en Europe, et ce depuis l'Antiquité[33].
Le terme « race », appliqué à des êtres humains, est écrit pour la première fois en 1684 par François Bernier, dans un article du Journal des Sçavans. Il y écrit « quatre ou cinq espèces ou races d’hommes dont la différence est si notable qu’elle peut servir de fondement à une nouvelle division de la Terre »[34],[35].
Il existait entre les historiens, depuis la seconde moitié du XXe siècle, un consensus relativement large pour considérer que l'utilisation de la notion de racisme dans l’Antiquité est un anachronisme. En effet, toutes les sociétés antiques et primitives sont, de notre point de vue contemporain, des sociétés racistes et xénophobes.
Les Anciens Grecs distinguent les peuples de l'Hellade, des autres peuples qu'ils appellent barbares. Presque tous les autres peuples antiques avaient la même représentation duale du Monde en deux races, les peuples apparentés, et les peuples étrangers ou ennemis; cette opposition entre deux collectifs est ce qui définit le domaine politique[36] et le droit des gens[37]. Parmi les peuples considérés comme étrangers, tous ne sont pourtant pas ennemis : les relations militaires, commerciales et diplomatiques instituaient des peuples amis, clients, alliés ou invités qui pouvaient alors être reconsidérés fictivement comme des peuples apparentés.
L'utilisation du terme « race » en tant que synonyme intégral de peuple/nationalité perdure jusqu'à la fin du XIXe siècle. Ainsi, les œuvres littéraires de Jules Verne abondent de formules stéréotypées comme « les Allemands, race industrieuse et organisée », « les Français, race romantique et galante » ou « les Américains, race entreprenante et dynamique », jusque dans les conversations entre bons amis d'origines différentes, sans la moindre intention négative dans l'usage du mot.
Les structures de parenté, donc les questions de race[38], sont toujours fondamentales et fondatrices dans la représentation que les peuples antiques ou primitifs ont d'eux-mêmes et des autres peuples[39]. Tout le système d'obligation et de solidarité sociale des sociétés antiques ou primitives est basé sur l'appartenance au groupe familial, et à la plus ou moins grande proximité de parenté: l'affiliation (phylai)[40]. On note que celle-ci n'est pas nécessairement biologique, mais peut être la fiction résultant d'une adhésion ou d'une adoption, et d'apparentements de convenance. À côté de la société grecque avec son genè et ses phratries, on trouve des structures politiques claniques chez d'autres peuples comme les Celtes avec les notions de peuples apparentés/alliés[41]. Cette conception dure pendant tout le Moyen Âge et une partie des Temps modernes[42].
La mythologie et les prescriptions religieuses fixent les règles d'exogamie qui favorisent les alliances hors du groupe consanguin, tout en interdisant celles avec les membres des peuples étrangers. De ce fait, depuis la plus haute Antiquité, jusqu'à ces derniers siècles, les peuples du Monde restent extrêmement endogames, qu'ils soient sédentaires et sans contacts avec des étrangers, ou qu'ils soient au contraire nomades au milieu des peuples étrangers. Dans ce dernier cas, l'identité du groupe est maintenue par des prescriptions sociales ou religieuses[43] interdisant une trop grande proximité de vie et des alliances étrangères qui finiraient par provoquer son assimilation[44]. C'est pourquoi, plus on s'éloigne dans l'histoire, plus on remarque que les peuples qui sont traditionnellement des migrants ou créent une colonie, continuent à se marier dans la moitié du génome dont ils se sont détachés[45], et non dans le peuple au milieu duquel ils vivent. Il faut remarquer qu'à ces époques, ces règles concernent l'immigration qui ne se fait pas individuellement, mais comme pour les colonies phéniciennes, grecques ou carthaginoises, par groupes complets[46] capables de recréer ailleurs une nouvelle société identique et fermée.
Les questions de guerre et de paix entre les tribus ou les peuples débutent par des refus ou des ruptures d'alliances matrimoniales[47], et se terminent par des alliances, ou des enchaînements d'alliances, entre les lignages des chefs[48], et à partir de là la possibilité de relation et d'alliance entre toutes les autres familles. Il importe de préciser que ces prescriptions s'imposent aux groupes, mais pas à des individus isolés ou à des familles désaffiliées.
Le récit biblique fait recommencer l'histoire de l'Humanité après le déluge, avec les trois fils de Noé, Sem, Cham, et Japhet, dont descendent les trois lignées qui peuplent les rives de la Méditerranée. La Table des peuples de la Genèse donne[49], avec la descendance de ces trois frères, l'origine généalogique de tous les peuples de la Terre qui sont présentés à la fois comme des peuples généalogiquement distincts, et en même temps apparentés. Ce dernier trait, qui rappelle l'unicité du règne humain, le monogénisme, est une originalité qu'on ne trouve pas chez beaucoup de peuples primitifs qui se réservent l'appellation d'homme, rejetant les autres dans le monde animal.
Une interprétation de la malédiction de Canaan dans le Livre de la Genèse[50] et de la « Table des peuples » qui en dérive, peut être à l'origine d'idéologies racistes dans cette région du monde ou pour les croyants s'inspirant de la Bible[51].
La destruction du temple de Jérusalem par Titus fils de l´empereur Vespasien s'accompagne d'une destruction des généalogies, qui sera pour le peuple Juif la cause de sa dispersion et d'un grand désarroi quant à son identité[52]. Ce genre de représentation généalogique totalisante des différents groupes ethniques connus se retrouve souvent dans les descriptions ethnologiques des peuples primitifs.
La conception selon laquelle l'utilisation de la notion de racisme dans l’Antiquité est un anachronisme, est remise en question par les travaux de l'historien Benjamin Isaac qui propose la notion de « proto-racisme » traversant l'Antiquité grecque puis romaine, notion qui relève déjà d'un « racisme conceptualisé, fondé sur une argumentation d’allure scientifique qui se veut démonstrative »[53]. La pensée proto-raciste, qui évoluera évidemment au fil des siècles et des déplacements de centres d'influence et de pouvoir, se fonde, selon l'historien, sur deux théories qui ne seront que peu remises en question : d'une part, suivant le traité Des airs, des eaux, des lieux datant du Ve siècle av. J.-C. et attribué à Hippocrate, un classement déterministe des groupes humains basé sur la géographique qui définirait « des traits de caractère collectifs immuables », dans une conception qui induit rapidement une hiérarchisation des peuples[réf. souhaitée].
Maurice Sartre nuance toutefois le propos, expliquant qu'il existe des conceptions divergentes, voire opposées, à cette représentation, citant notamment l'explorateur et historien antique Hérodote ou encore le géographe Strabon qui « montre avec une force tout aussi convaincante les limites de la théorie environnementaliste » dont il ne fait pas usage dans la description qu'il fait des peuples et de leurs mœurs[54].
Le philosophe Christian Delacampagne perçoit, quant à lui, dans l’attitude païenne – égyptienne, grecque puis romaine – face aux juifs et dans la partition entre hommes libres d’un côté, femmes, enfants et esclaves de l’autre, des « classifications biologiques », de « type raciste »[55].
Il convient néanmoins de noter que si les arguments de type raciste ont pu servir à justifier la domination des Grecs et des Romains, ils n'ont jamais débouché sur des politiques d'exclusion ni – a fortiori – d'extermination. Au contraire, la capacité d'intégration, d'assimilation voire promotion des étrangers dans l'Empire gréco-romain – dans un relatif respect de leur culture et de leurs traditions – est bien connue des historiens. Néanmoins, on peut voir un lien entre le proto-racisme antique et les théories racistes contemporaines dans une commune « négation des évidences au profit de théories préconçues dont peu importe le bien-fondé scientifique pourvu qu’elles justifient la situation dominante et le statut privilégié d’un groupe »[54].
C’est surtout le Moyen Âge qui donne des arguments aux partisans de l’existence d’un racisme antérieur à la modernité. Pour l’historien spécialiste de l'antisémitisme Gavin I. Langmuir, l'une de ses manifestations serait la cristallisation de l’antijudaïsme des premiers théologiens chrétiens en un antisémitisme chrétien dès le XIIIe siècle[56]. D’autres en voient les premières manifestations dès la fin du XIe siècle et les premiers pogroms qui jalonnent la première croisade populaire menée par Pierre l'Ermite. Au XIIIe siècle, la crise rencontrée par l’Église catholique, menacée par les hérésies cathares, albigeoises, vaudoises aboutit à une rigidification de sa doctrine qui se manifeste notamment par la création de l'Inquisition dans les années 1230 et par ce que Delacampagne désigne comme la « démonisation » des « infidèles »[57].
Selon Delacampagne, l’idée que la conversion absout le Juif s’efface alors devant la croyance que la judéité est une condition héréditaire et intangible. Ce mouvement n’épargne d’ailleurs pas d’autres catégories de la population. Sa manifestation la plus probante est la mise en place progressive à partir de 1449 d’un système de certificat de pureté de sang (limpieza de sangre) dans la péninsule Ibérique pour accéder à certaines corporations ou être admis dans les universités ou les ordres. Ce mouvement, qui se traduit par le décret de l'Alhambra de 1492, concerne quatre groupes précis : les Juifs, les musulmans convertis (morisques), les pénitenciés de l’Inquisition et les cagots, c’est-à-dire les descendants présumés de lépreux[58].
Delacampagne mentionne la ségrégation qui touche cette dernière catégorie de population comme une étape majeure dans la constitution du racisme moderne. Selon lui, c'est la première fois que la discrimination d’un groupe social reçoit au XIVe siècle une justification appuyée sur les conclusions de la science. Les chirurgiens, tel Ambroise Paré, apportent en effet leur caution à l’idée que les cagots, descendants présumés de lépreux, continuent de porter la lèpre bien qu’ils n’en manifestent pas les signes extérieurs[59].
Plusieurs études ont mis en avant l’existence d’attitudes que leurs auteurs considèrent comme racistes dans des sociétés extérieures à l’aire culturelle européenne. Au Japon, la transmission héréditaire de l’appartenance à la caste des burakumins jusqu’au début de l'ère Meiji a pu être analysée comme le produit d’une construction symbolique de type raciste.
Les travaux menés par l’historien Bernard Lewis sur les représentations développées par la civilisation musulmane à l’égard des autres êtres humains concluent sur l’existence d’un système perceptif qu’il qualifie de raciste, notamment à l’égard des populations noires[61].
Au Moyen Âge, le racisme des Arabes à l'égard des Noirs, en particulier des Noirs non musulmans, fondé sur le mythe[62] de la malédiction de Cham, le père de Canaan, prononcée par Noé[63], servit de prétexte à la traite négrière et à l'esclavage, qui, selon eux, s'appliquait aux Noirs, descendants de Cham qui avait vu Noé nu lors de son ivresse (une autre interprétation les rattache à Koush). (Histoire extraite de la Bible). Les Noirs étaient donc considérés comme « inférieurs » et « voués » à l'esclavage. Plusieurs auteurs arabes les comparaient à des animaux[64]. Le poète al-Mutanabbi méprisait le gouverneur égyptien Abu al-Misk Kafur au Xe siècle à cause de la couleur de sa peau[64]. Le mot arabe aabd عبد (pl. aabid عبيد) qui signifiait esclave est devenu à partir du VIIIe siècle plus ou moins synonyme de « Noir »[65], prenant une signification similaire au terme "nègre" dans la langue française du XXe siècle. Quant au mot arabe zanj, il désignait de façon péjorative les Noirs[66], avec une connotation raciale officielle que l'on retrouve dans les textes et discours racialistes. Ces jugements racistes étaient récurrents dans les œuvres des historiens et des géographes arabes : ainsi, Ibn Khaldoun a pu écrire au XIVe siècle : [67]. À la même période, le lettré égyptien Al-Abshibi écrivait : [68]. Les Arabes présents sur la côte orientale de l'Afrique utilisaient le mot « cafre » pour désigner les Noirs de l'intérieur et du Sud. Ce mot vient de kāfir qui signifie « infidèle » ou « mécréant »[69].
Les différents auteurs qui conçoivent le racisme comme une spécificité de la modernité européenne s’accordent pour mettre en avant la conjugaison de trois facteurs dans la genèse de cette nouvelle attitude :
Pour Colette Guillaumin[72] le racisme est contemporain de la naissance d’un nouveau regard porté sur l’altérité ; il est constitué par le développement de la science moderne et la substitution d’une causalité interne, typique de la modernité, à une définition externe de l’homme qui prévalait avant la période moderne.
Alors que l’unité de l’humanité trouvait auparavant son principe à l’extérieur de l’homme, dans son rapport à Dieu, l’homme ne se réfère désormais qu’à lui-même pour se déterminer. Comme l'attestent les débats théologiques sur l’âme des Indiens ou des femmes, le rejet de la différence et les hiérarchies sociales s’appuyaient sur une justification religieuse ou basée sur un ordre sacré (caste) ; ils se parent désormais des habits de la justification biologique, renvoyant à l’ordre de la nature[73]. La conception de cette Nature elle-même connaît une mutation profonde : elle devient mesurable, quantifiable, réductible à des lois accessibles à la raison humaine.
Ce changement de regard engendre un système perceptif essentialiste : l’hétérogénéité au sein de l’espèce humaine ne doit son existence qu’à une différence logée dans le corps de l’homme, que les scientifiques européens s’acharneront à mettre en évidence tout au long du XIXe siècle et au cours de la première moitié du XXe siècle. Pour Pierre-Henri Boulle, on peut percevoir en France dès la fin du XVIIe siècle les premières expressions de ce mode de perception. C’est au XVIIIe siècle qu’il se répand parmi les élites politiques, administratives et scientifiques, avant de se généraliser au plus grand nombre dans le courant du XIXe siècle[74].
Pour Colette Guillaumin, ce mode de perception se généralise au tournant des XVIIIe siècle et XIXe siècle[75]. Dans la première partie de son ouvrage Les origines du totalitarisme, Hannah Arendt date l’apparition de l’antisémitisme, qu’elle différencie de l’antijudaïsme, du début du XIXe siècle ; c’est aussi la date d’origine qu’assigne le philosophe Gilbert Varet aux « phénomènes racistes expressément dits »[76].
La propagation hors de l’Europe apparaît dans cette optique comme un produit de l’influence européenne : André Béteille développe ainsi la thèse d’une « racialisation » du système de castes en Inde après la colonisation britannique[77]. Au Japon, des travaux menés par John Price, Georges De Vos, Hiroshi Wagatsuma ou Ian Neary au sujet des Burakumin parviennent à des conclusions identiques[78].
La question de l’antériorité ou de la postérité du racisme au développement de l’esclavage dans les colonies européennes fait l’objet de nombreux débats. Le consensus s’établit néanmoins au sujet du rôle joué par le développement de l’esclavage sur le durcissement et la diffusion de l’attitude raciale. L'esclavage colonial se développe en effet, paradoxalement, à une époque où, en Europe, l'humanisme, la philosophie des Lumières (philosophie) et la théorie du droit naturel devraient logiquement mener à sa condamnation. Le racisme pourrait être le produit (conscient ou non) de cette contradiction, le seul artifice permettant de refuser à certaines populations le bénéfice de droits fondamentaux reconnus à l'Homme en général consistant à croire à l'existence d'une hiérarchie entre les races.
Selon l’historien américain Isaac Saney, « les documents historiques attestent de l'absence générale de préjugés raciaux universalisés et de notions de supériorité et d'infériorité raciales avant l'apparition du commerce transatlantique des esclaves. Si les notions d'altérité et de supériorité existaient, elles ne prenaient pas appui sur une vision du monde racialisée »[79].
Développement de l’esclavage et de la science moderne ont étroitement interagi dans la construction du racisme moderne. La catégorie de « nosopolitique » qualifie chez la philosophe Elsa Dorlin l’usage des catégories de « sain » et de « malsain » par le discours médical appliqué dans un premier temps aux femmes, puis aux esclaves. Alors que le Blanc, considéré comme « naturellement » supérieur par les médecins, est défini comme l’étalon de la santé, le tempérament des Noirs est par contraste déclaré « pathologique » ; il est porteur de maladies spécifiques, que seule la soumission au régime de travail imposé par les colons peut atténuer, mais difficilement guérir, tant elles paraissent intrinsèquement liées à sa nature[80].
Le « racisme scientifique », ou « racialisme » (ou « raciologie »), classifie les êtres humains d'après leurs différences morphologiques en application d'une méthode héritée de la zoologie.
Les théoriciens du racialisme comptent des personnes telles que l'anthropologue allemand Johann Friedrich Blumenbach, le français Georges Vacher de Lapouge, partisan de l'eugénisme, l'écrivain français Joseph Arthur de Gobineau, célèbre pour son Essai sur l'inégalité des races humaines, paru en 1853, le Britannique de langue allemande Houston Stewart Chamberlain, dont l'œuvre théorise le rôle historique de la race aryenne comme ferment des classes dirigeantes indo-européennes et le français d'origine suisse George Montandon, auteur d'une taxonomie des races dans son ouvrage La race, les races. Mise au point d'ethnologie somatique, paru en 1933.
En Europe et aux États-Unis, le paradigme racial s’est étroitement articulé à partir du XIXe siècle, à l’extérieur avec la politique impérialiste et, sur le plan intérieur, avec la gestion politique des populations minoritaires. Pour Hannah Arendt, « la pensée raciale » est ainsi devenue une idéologie avec l’ère de l’impérialisme débutant à la fin du XIXe siècle[81]. L’idéologie raciste devient alors un « projet politique » qui « engendre et reproduit des structures de domination fondées sur des catégories essentialistes de la race »[82]. Le racisme, explique-t-elle, est d'abord la transformation des peuples en races, la diversité humaine n'étant plus expliquée par les influences culturelles acquises par chacun après son arrivée dans le monde, mais au contraire par l'origine.
À l’image de la diversité des positions racistes dans le monde académique, les formes de racisme et donc les usages politiques de la race ont fortement varié selon les contextes nationaux et la position occupée par leurs promoteurs dans l’espace politique.
En 2006, théorisant le « mélange humain » (et le distinguant du « métissage », à fortes connotations racialistes), le philosophe Vincent Cespedes utilise le concept de « mixophobie » (mixo, « mélange », phobia, « peur ») pour rendre compte de « la peur du mélange », fondement psychologique du repli des racistes sur leur race, opposée aux autres « races » avec lesquelles ils ne veulent pas se mélanger[83]. Il oppose à ce concept un autre néologisme : la « mixophilie »[84] (« l'amour du mélange »).
L’un des points fondamentaux d’opposition des doctrinaires racistes est la question de la mixité raciale. La position « mixophobe » se caractérise par un rejet du « métissage », présenté comme un facteur de dégénérescence des groupes humains. Il existe toutefois un large spectre de positions mixophobes, depuis le rejet pur et simple de tout contact entre les « races » jusqu’à la promotion du métissage, sous réserve du respect des conditions de son efficacité[réf. nécessaire].
La position mixophobe radicale est le corollaire de la construction du mythe de la pureté de la race qui affirme la supériorité des races pures sur les races dites métissées. L’imaginaire médical de la souillure ou de la contamination du sang en constitue l’un des motifs récurrents. Au milieu du XIXe siècle, deux des chefs de file du racisme biologique, Joseph Arthur de Gobineau (1816-1882) et Robert Knox (1791-1862), contribueront largement à l’introduction de cette position en France et en Grande-Bretagne[85]. Les promoteurs du mythe de la race aryenne – Vacher de Lapouge, Houston Stewart Chamberlain, et plus tard Adolf Hitler – qui voient dans la « race germanique » la survivance à l’état pur de la « race indo-européenne » se caractérisent tous par une mixophobie radicale.
Le rejet de la mixité peut connaître des gradations. Nombreux sont les scientifiques qui réfutent la thèse du « choc des hérédités » de Vacher de Lapouge selon laquelle le métissage peut être tenu pour un facteur d’infécondité[86]. Pour les partisans du métissage, les bienfaits de celui-ci restent toutefois conditionnés au respect de certaines règles. Comme l’affirment la majorité des raciologues, pour que le métissage soit profitable, il convient notamment que « la distance entre les races ne soit pas trop grande ». Pour ces mixophobes modérés, comme les philosophes Gustave Le Bon, Ernest Renan, Théodule Ribot ou la grande majorité des polygénistes républicains, seul le métissage entre les races blanches ne présente aucun risque et devrait être préconisé[87].
Pour les rares mixophiles, le métissage peut répondre à deux préoccupations :
La hantise du métissage ne s’accompagne pas nécessairement d’une prescription politique : dans l’Essai sur l'inégalité des races humaines, qui énonce la première philosophie de l'histoire basée sur le concept de race, le pessimisme ne fait que ruminer la décadence de la civilisation occidentale dont l’essence aurait été altérée par la contamination du sang de la race blanche[90]. S’il voit dans la pénétration des idées républicaines l’une des manifestations de cette dégénérescence, il n’en tire pas de conséquences politiques : le processus en cours lui semble irréversible. Cette position est toutefois restée extrêmement marginale et la longue liste des suiveurs de Gobineau a tiré de ses postulats des conclusions nettement plus volontaristes.
La position mixophobe conduit à la défense d’une stricte séparation des groupes humains constitués en races. Sur le plan de la politique extérieure, les mixophobes se caractérisent souvent par des positions anti-colonialistes, conséquences de leur refus du modèle assimilationniste produit par la colonisation. Gobineau, Robert Knox, Gustave Le Bon, ou Hitler marquent tous leur réprobation devant les aventures coloniales de leurs pays respectifs[85]. Le philosophe Pierre-André Taguieff considère que l’ethno-différentialisme est l’actualisation sur des bases culturalistes de cette position mixophobe[91].
Sur le plan de la politique intérieure, la conséquence logique de ce racisme d’exclusion est l’instauration d’un système ségrégationniste : les lois de Nuremberg en Allemagne, les lois Jim Crow aux États-Unis ou l’apartheid sud-africain en sont autant de manifestations. La défense de la pureté de la race peut aussi aboutir à un racisme « purificateur » ou d’extermination ; c’est celui qui sera mis en œuvre par le régime nazi avec le génocide des Juifs et des Tziganes. La mixophobie est aussi, comme pour Vacher de Lapouge ou le régime nazi, l’une des positions idéologiques compatibles avec l’eugénisme.
À l’opposé, le racisme mixophile s’incarne au XIXe siècle dans une position colonialiste et assimilationniste dont l’objectif est la « réduction universelle des différences […] à un modèle unique », celui de l’impérialisme occidental[92].
La suprématie de la race blanche ou caucasienne est un postulat sur lequel s’accordent très largement les scientifiques, philosophes et hommes politiques du XIXe siècle[réf. nécessaire]. Combiné avec la mission civilisatrice, le suprémacisme blanc est un élément fondamental de l’idéologie coloniale. Une fois opérée la conquête, il constitue aussi le principe justificatif des législations opérant des distinctions de droit sur une base raciale, la forme paroxystique de cet ordre juridique inégalitaire étant la ségrégation raciale.
Dans le cadre de la colonisation britannique apparaît l’expression . La conception racialiste naît au croisement du développement des États coloniaux et des théories scientifiques contemporaines. À la fin de XIXe siècle, le racisme est pour l’historien Nicolas Lebourg  : c’est une impulsion à l’encontre de l’évolution du monde qui fait se côtoyer de nombreuses ethnies et une aspire à le [93].
Les idéologies coloniales des pays se réclamant d’un fonctionnement démocratique se sont trouvées confrontées au problème de leur légitimité, au regard des principes censés régir leur ordre politique et juridique. En France tout particulièrement, elle doit surmonter sous la Troisième République le paradoxe de l’affirmation d’une volonté de conquête et d’assujettissement d’une part, et de principes émancipateurs et égalitaires d’autre part. Le programme colonial français ne peut se réaliser que par l’affirmation d’une infériorité tenue pour évidente et incontestable des populations visées, laquelle justifie une mission civilisatrice dont le fardeau repose sur les seules épaules de la race blanche[94].
Dans la deuxième moitié du XIXe siècle, les rapports entre science et politique évoluent considérablement. Les personnalités politiques recourent non seulement à l’autorité des scientifiques, dont le prestige va croissant, pour légitimer leurs décisions. Mais plus encore, ils sont imprégnés d’une représentation du monde qui voit dans le mécanisme de la nature la loi organisatrice de la destinée humaine : la vogue du paradigme évolutionniste constitue la toile de fond scientifique de l’idéologie coloniale de la fin du XIXe siècle.
Le système évolutionniste d’Herbert Spencer, traditionnellement tenu pour le précurseur du « darwinisme social », marque un glissement de la théorie darwinienne du monde naturel au monde social. Postulant, avec Lamarck mais contre Darwin, l’hérédité des caractères acquis, Spencer considère que le libre jeu du marché, qui est selon lui le plus à même d’assurer efficacement « la sélection des plus aptes », doit être le moteur du progrès humain. Le libéralisme de Spencer, qui se traduit notamment par un refus des visées coloniales étatistes, ne prône pas d'interventions de l'État dans le processus civilisateur (les États y sont au contraire amenés à disparaître). Étendu aux collectifs, nationaux ou ethniques, conçus comme des entités homogènes, le mot d’ordre évolutionniste de Spencer connaîtra cependant une large fortune dans le camp colonialiste, au travers du concept de « lutte des races »[95].
Selon cette conception, la lutte que se livreraient depuis l’origine les différents groupes humains doit conduire à la domination des races les plus aptes et à la disparition inexorable des races inférieures. Après la conquête de l'Algérie par la France, les médecins français, constatant la baisse de la population « indigène », n'y verront que la confirmation d’une extinction prochaine et prévisible de la race arabe, qu’ils jugent inadaptée aux nouvelles conditions de leur temps[96]. La lutte des races n’implique ainsi pas nécessairement un processus violent d’extermination : les tenants du darwinisme social sont persuadés que les races inférieures disparaîtront silencieusement de la surface du globe, « sans que l’homme blanc et civilisé ait à se souiller les mains d’un sang innocent »[97].
Sur le continent européen lui-même, le succès énorme des zoos humains constitue pour Pascal Blanchard, Nicolas Bancel et Sandrine Lemaire l’une des modalités de transmission du « racisme scientifique » à une large partie de la population[98]. À partir des années 1870, ces zoos exposent dans les grandes capitales européennes et américaines, jusque dans les années 1930, des hommes et des femmes issus des peuples colonisés dans un environnement reconstitué, aux côtés des bêtes sauvages. Le Jardin d'acclimatation de Paris par exemple, lors d'expositions, a exhibé - à côté des animaux - des ressortissants d'ethnies diverses derrière des barreaux, et ceci jusqu'en 1931[99]. Le principe en sera repris pour les Expositions universelles, les Expositions coloniales et jusqu'aux foires régionales. Ces exhibitions humaines contribuent à fixer « un rapport à l’autre fondé sur son objectivation et sa domination »[100]. Elles s'insèrent dans le schéma évolutionniste en mettant en scène la frontière entre civilisés et sauvages et s’accompagnent du déploiement d'un racisme populaire dans la grande presse[101].
Une fois les territoires conquis, la question de l’administration des populations colonisées fut à l’origine de nombreux débats. Dans quelle mesure ces peuples inférieurs pouvaient être associés à la gestion de leurs territoires ? La France, initialement porteuse d'un modèle assimilationniste qui visait à l’exportation des institutions françaises sur le territoire colonial, se tourna progressivement vers une politique d’association pendant qu’elle appliquait à travers l’indigénat un régime d’exception aux populations conquises.
Cet ordre juridique exorbitant au droit commun trouvait sa justification dans deux principes qui peuvent être considérés comme complémentaires. D’un côté, un principe pragmatique considérait que le maintien de l’ordre colonial nécessitait des règles et des sanctions plus sévères à l’encontre des indigènes. Rien ne devait laisser paraître que la pression du colonisateur se desserrât un jour. De l’autre, un principe idéologique, qui prenait racine dans une perception raciste du colonisé, n’entendait pas laisser voix au chapitre à des peuples qui n’était pas dignes, pas aptes ou pas murs pour exercer un pouvoir à l’égal des colonisateurs.
L’étude des races, à travers l’anthropologie ou l’ethnologie, fut largement mobilisée : elle devait permettre de déterminer avec qui le pouvoir colonial pouvait s’associer, quelles étaient les races civilisables et celles qui étaient par nature rétives ou incapables d’accéder à un niveau supérieur de civilisation. En Algérie, ce travail aboutit à la construction de l'opposition entre Arabes et Kabyles. Considéré comme plus proche biologiquement et culturellement de la « race française », le Kabyle est présenté comme un allié potentiel contre l’Arabe, présenté comme fier, nomade, insoumis et fainéant.
La notion de « race » qui s’élabore dans la situation d’occupation coloniale n’est cependant pas uniforme. Des présupposés plus ou moins biologisants s’opposent dans des conceptions concurrentes de la race. Une grande partie des anthropologues conclut ainsi à l’origine biologique de l’inégale perfectibilité des races. Cependant, selon l’historienne Emmanuelle Saada, les représentations de la majorité des élites coloniales empruntent peu au modèle anthropologique des « raciologues » mais se fondent sur une conception « organique » des rapports entre le milieu et la culture[102]. L’imprégnation du milieu et les habitudes multi-séculaires sont considérées comme les déterminants de comportements sociaux largement réifiés et essentialisés : chaque « race » possède des caractéristiques psychologiques et des aptitudes qui lui sont propres. Seul un travail de longue haleine, basé sur l’éducation de plusieurs générations successives, peut conduire les indigènes à s’arracher à leur civilisation originelle pour embrasser les principes supérieurs qui gouvernent les « races européennes »[103].
Ces deux conceptions partagent toutefois le présupposé du différentialisme racial et se rejoignent dans leurs conclusions pratiques. Dans tous les cas, le retard biologique ou civilisationnel des races inférieures nécessite de prolonger leur mise sous tutelle et le maintien d’un ordre juridique et politique différencié entre métropole et colonies et, sur le territoire colonial, entre colons et colonisés. La mission civilisatrice imposa donc des mesures à double tranchant. Si elle fut un frein à la mise en œuvre d’une politique radicalement ségrégationniste, elle justifia le maintien d’une tutelle présentée comme indispensable à l’accomplissement du dessein civilisateur que s’octroyaient les colonisateurs.
Dans la deuxième moitié du XIXe siècle, la question de la hiérarchisation au sein de la race blanche est sur le continent européen au cœur de deux phénomènes appelés à jouer un rôle prépondérant dans les deux conflits mondiaux du XXe siècle : l’exacerbation des rivalités nationales et la montée de l’antisémitisme.
La distinction opérée au sein de la  entre Aryens et Sémites constitue l’un des vecteurs de la biologisation de l’antisémitisme. En France, Vacher de Lapouge est parmi les premiers à prétendre donner une caution scientifique à la doctrine aryaniste, en s’appuyant « sur des bases anthropométriques, et plus particulièrement craniométriques » (morphométrie)[104].
Si la méthode de Lapouge est rapidement discutée, la distinction entre Aryens et Sémites est d’usage courant au sein des milieux politiques ou savants européens. Le philosophe Ernest Renan distingue ainsi les Indo-européens des Sémites ; les seconds, novateurs quand ils ont introduit le monothéisme, doivent selon lui s’effacer devant les premiers qui sont désormais appelés à gouverner le genre humain[105].
En Allemagne, particulièrement à l'Université de Göttingen, autour de Karl Otfried Müller (1797-1840), se met en place la doctrine du miracle grec : les Grecs athéniens auraient été les plus purs de la race aryenne, ce qui permettait d'évacuer les hypothèses sémites, mésopotamiennes ou égyptiennes des origines dudit miracle grec.
Comme le note l’historien George L. Mosse, le racisme est à l’origine d’un système symbolique de mythes et de symboles qui, s’emparant de la question des origines, des difficultés et des triomphes de la race, dessine une trajectoire qui tend à se confondre avec le récit national en construction[106]. Le stéréotype national physique, qui s’élabore au XIXe siècle prend, en Allemagne par exemple, une apparence raciale (l’Allemand blond…).
L’usage du mythe aryen, rapidement récupéré en Allemagne par le nationalisme de droite, illustre bien les effets de cette concurrence nationale. Si pour le Français Vacher de Lapouge la race aryenne a une signification strictement zoologique, elle prend avec Houston Stewart Chamberlain un tournant nationaliste[107]. La « race germanique » devient, sous la plume de cet essayiste d’origine britannique évoluant dans les milieux wagnériens, la plus pure des branches de la race aryenne. Outre des Juifs, la doctrine aryaniste permet aux Allemands de se distinguer des Latins et en particulier des Français, considérés comme inférieurs car métissés.
Pour faire face à ce glissement de l’usage de l’aryanisme, défavorable à la nation française, Ernest Renan refuse, comme nombre de ses compatriotes, notamment républicains, le concept de « race pure » et défend la thèse du métissage historique des peuples européens[108]. Le refus de l’aryanisme se présente comme le refus du jeu de l’exacerbation des rivalités nationales. Le sentiment anti-allemand influencera néanmoins en France les études de psychologie des peuples et de leurs caractères nationaux. S’il place la race aryenne au sommet de la hiérarchie des races, Hippolyte Taine distingue en son sein les « races germaniques » des races latine et hellénique. Les premières, « inclinées vers l’ivrognerie et la grosse nourriture » par la fréquentation des forêts humides et froides, s’opposent aux secondes dont l’environnement favorable a permis le développement d’une culture raffinée[109].
Les enjeux diffèrent considérablement outre-Atlantique où la problématique raciale est essentiellement concentrée sur la distinction entre Blancs et Noirs. Toutefois, en réaction à l’immigration irlandaise massive des années 1840 due à la « crise de la pomme de terre », et dans le contexte de la guerre avec le Mexique, est forgé aux États-Unis le concept d’« anglo-saxonisme »[110], également nommé par l'acronyme WASP (White Anglo-Saxon protestant).
Il connaîtra une grande fortune lorsqu’à la fin du XIXe siècle une campagne visant à restreindre l’immigration en provenance du sud et de l’est de l’Europe, menée notamment par Madison Grant, cherchera à vanter la supériorité de la « race nordique » sur les autres « races blanches ».
Le racisme d'État est historiquement une ségrégation raciste institutionnalisée et, à l'ère moderne, une discrimination systémique qui implique l'État.
L’historien américain George M. Fredrickson recense trois régimes politiques « ouvertement racistes » au XXe siècle : le sud des États-Unis sous les lois Jim Crow (1865-1963), l’Afrique du Sud sous l’apartheid (1948-1991), l’Allemagne nazie (1933-1945)[111]. Ces régimes présentent la caractéristique commune d’afficher une idéologie officielle explicitement raciste et d’avoir institutionnalisé dans la loi une hiérarchie présentée comme naturelle et indépassable entre le groupe dominant et le groupe dominé. L’une des mesures les plus significatives de cet arsenal juridique ségrégationniste est la prohibition des mariages interraciaux ; elle transcrit dans l’ordre juridique l’idéologie mixophobe de la . Sur le plan économique, la restriction des opportunités du groupe ségrégué le maintient dans un état de pauvreté qui alimente le discours sur sa prétendue infériorité.
Après l'abolition de la ségrégation raciale aux États-Unis, en 1967, les militants Stokely Carmichael et Charles V. Hamilton (en) publient le livre Le Pouvoir Noir: pour une politique de libération aux États-Unis (en) où ils conceptualisent, sous les appellations de  et , l'idée d'un racisme voilé qui continuerait à structurer l'ordre social. Carmichael et Hamilton y écrivent que le racisme individuel est souvent identifiable, mais que le racisme institutionnel est moins perceptible en raison de sa nature [112].
Au début du XXIe siècle, le terme de « race » reste toujours d'usage courant dans certains milieux et le racisme se manifeste toujours sur les cinq continents sous des formes plus ou moins directes.
Le racisme à l'échelle des relations individuelles se traduit par des paroles ou des actes racistes envers d'autres individus. Le racisme individuel est étroitement lié d'une part à la xénophobie, la haine, le bellicisme, l'ethnisme, l'intolérance et l'idéologie de supériorité culturelle ou personnelle, d'autre part au déclassement social et au ressentiment. Généralement le racisme, comme position directrice, est déduit (de signes extérieurs) ; il peut aussi être induit (de comportements). Il est affirmation d'une logique identitaire ou réaction à une logique identitaire. C'est le passage de l'induction à la déduction qui est fondateur pour la politisation du racisme[réf. nécessaire].
En raison de la connotation très négative du mot en Occident, peu de partis politiques se revendiquent ouvertement comme racistes. De nombreux partis d'extrême droite ont cependant été accusés de véhiculer des discours de ce type à travers des positions xénophobes. L'apologie du racisme étant condamnée, ils peuvent promouvoir des doctrines dérivées comme l'ethno-différencialisme ou le racialisme.
Au Zimbabwe, le parti ZANU du président Robert Mugabe a mis en place une politique visant à exproprier les fermiers blancs, invoquant une redistribution corrigeant l'injustice passée où ceux-ci recevaient préférentiellement les terres[113],[114],[115].
Dans les pays occidentaux, des mouvements suprémacistes noirs prônent la supériorité de la race noire. Ce fut notamment le cas du New Black Panthers Party[116],[117], un temps représenté par Khalid Abdul Muhammad. En France, la Tribu Ka de Kémi Séba, qui prônait la supériorité de la race noire et la séparation des races, a été dissous pour provocation à la haine raciale[118].
Dans la période post-coloniale, est apparu ce que les auteurs appellent le néo-racisme, un « racisme sans races », différentialiste et culturel, qui se focalise sur les différences culturelles et non sur l’hérédité biologique comme le racisme classique. Dans ce néo-racisme, la catégorie « immigration » est devenue un substitut contemporain à la notion de « race ». Le racisme différentialiste consiste à dire que puisqu'il ne peut y avoir hiérarchie des races ni des cultures, celles-ci ne doivent cependant pas se mélanger mais rester séparées et cloisonnées[119],[120].
Le généticien suédois Svante Pääbo, qui a découvert que quelque 4 % du génome des Européens actuels est hérité de l'homme de Néanderthal, considère que la lutte antiraciste ne relève pas du champ scientifique[121].
La publication de la « déclaration sur la race » en 1950 par l'UNESCO encouragera nombre de biologistes à rappeler régulièrement l'absence de validité scientifique de la notion de « races humaines ». On peut citer notamment Albert Jacquard, auteur de L'Équation du nénuphar en 1998[122].
La revue Science a publié en février 2008 l'étude génomique la plus complète effectuée à cette date. Les chercheurs ont comparé des fragments d'ADN de 650 000 nucléotides chez 938 individus appartenant à 51 ethnies. La conclusion de ces travaux est qu'il existe sept groupes biologiques parmi les hommes : les Africains subsahariens, les Européens, les habitants du Moyen-Orient, les Asiatiques de l'Est, les Asiatiques de l'Ouest, les Océaniens et les Indiens d'Amérique. Howard Cann, chercheur de la Fondation Jean-Dausset, cosignataire, précise : « Tous les hommes descendent d'une même population d'Afrique noire, qui s'est scindée en sept branches au fur et à mesure du départ de petits groupes dits fondateurs. Leurs descendants se sont retrouvés isolés par des barrières géographiques (montagnes, océans…), favorisant ainsi une légère divergence génétique ». En approfondissant encore leur étude, les généticiens ont pu déterminer des sous-groupes : huit en Europe et quatre au Moyen-Orient, mais avec moins de certitude[123].
Selon une étude de l'expert Chao Tian, en 2009, ayant calculé les distances génétiques (Fst) entre plusieurs populations en se basant sur l’ADN autosomal, les Européens du Sud tels que les Grecs et Italiens du Sud apparaissent soit à peu près autant distants des Arabes du Levant (Druzes, Palestiniens) que des Scandinaves et Russes, soit plus proches des premiers. Un Italien du Sud est ainsi génétiquement deux fois et demie plus proche d'un Palestinien que d'un Finlandais[124],[125],[126] mais une telle distance avec les Finlandais n'est pas représentative des distances entre les Européens, elle s'explique parce que les Finlandais sont mélangés avec des Asiatiques sibériens, d'affinité proche des Sami, les Finlandais sont donc un peuple génétiquement assez isolé des autres européens (y compris des Scandinaves et des Russes), ce qui les éloigne du reste des Européens sur le plan des distances génétiques[127]. De même, les Italiens du Sud constituent un groupe plus distant[128]. Plus globalement, les principaux peuples européens montrent une grande proximité génétique entre eux, qui les différencie nettement des populations extra-européennes[129].
En outre, la portion du génome humain relative à la couleur de la peau humaine, en l'occurrence le gène codant la production de la mélanine, ne représente qu'une infime partie de l'ensemble de ce génome (trois gènes communs aux vertébrés sur les 36 000 gènes du génome[précision nécessaire]). Cf. à ce sujet, l'article Couleur de la peau.
Les pratiques racistes constituent une violation des droits de l'homme et sont réprimées par de nombreux pays (parfois sous l'appellation de hate speech, ou « discours de haine »: voir Législation internationale sur le discours de haine).
Pour la plupart des pays occidentaux, la discrimination et le racisme sont beaucoup plus que des délits, punis pénalement ; ils représentent également une atteinte aux valeurs qui fondent la démocratie. Celle-ci reconnaît l'égale dignité de chaque citoyen à participer à la chose publique, à poursuivre son bonheur et son épanouissement indépendamment de sa naissance.
En France, par exemple, le législateur n'a cessé au fil du temps, et particulièrement après la Seconde Guerre mondiale, de compléter le dispositif législatif afin de réprimer plus efficacement toutes les formes de racisme. Dès 1881, la loi sur la liberté de la presse punit la diffamation raciste « d'un emprisonnement de un mois à un an et d'une amende de 1 000 F à 1 000 000 de francs »[130].
Il a pour cela créé ou modifié en 1990 (loi Gayssot[131]) un certain nombre d'incriminations d'une part dans le code pénal, d'autre part dans la loi du 29 juillet 1881 sur la liberté de la presse et dans la loi relative à la communication audiovisuelle. La loi de 1881 avait déjà été modifiée par la loi du 1er juillet 1972 relative à la lutte contre le racisme[132], qui punit entre autres l'injure raciste, la discrimination raciale effectuée par un agent dépositaire de l'autorité publique.
La loi de 1972 introduit en outre à l'art. 24 de la loi de 1881 la disposition suivante :
« Ceux qui, par l'un des moyens énoncés à l'article 23, auront provoqué à la discrimination, à la haine ou à la violence l'égard d'une personne ou d'un groupe de personnes à raison de leur origine ou de leur appartenance à une ethnie, une nation, une race ou une religion déterminée, seront punis d'un emprisonnement d'un mois à un an et d'une amende de 2 000 F à 300 000 F ou de l'une de ces deux peines seulement[132]. »
La peine prévue est aujourd'hui « d'un an d'emprisonnement et de 45 000 euro d'amende ou de l'une de ces deux peines seulement »[133],[134]
Sur le plan international, c'est en premier lieu à l'Unesco qu'il incombe de promouvoir la lutte contre le racisme, comme le déclare ouvertement la charte constitutive de l'institution de 1945. En pratique, la visibilité de l'action de cette organisation onusienne dans ce domaine est aujourd'hui très réduite quand on la compare à la protection du patrimoine mondial[135].
D'après un sondage mené sur 1 011 personnes entre les 17 et 22 novembre 2005 par l'institut CSA, un tiers des Français se déclarait raciste, sans toutefois préciser dans quelle acception de ce terme[136]. Toujours selon la même enquête, 63 % de la population pensait que « certains comportements peuvent justifier des réactions racistes ». Un sondage similaire réalisé au Québec du 22 décembre 2006 au 3 janvier 2007 par l'institut Léger Marketing[137], prétendait donner comme analyse que 59 % des Québécois étaient faiblement, moyennement ou fortement racistes. Comme le précédent, ce sondage réalisé dans le contexte d'un débat parfois tendu sur la question des accommodements raisonnables a déclenché une polémique dans la province, en particulier sur la même absence de définition claire au concept de « racisme ». La question posée était « Vous, personnellement, à quel point vous considérez-vous raciste ? »[138].
Les études scientifiques sur le racisme ne sont jamais menées de manière aussi directe, mais par l'utilisation de différentes questions servant à définir des indicateurs de racisme[138].
« votre objectif est-il aussi de lutter contre le racisme? - […] je ne prétends pas combattre le racisme car il ne s’agit pour moi pas d’une question scientifique, mais plutôt d’un positionnement éthique et politique. »
« several distinct regions can be distinguished within Europe: 1) Finland, 2) the Baltic region (Estonia, Latvia and Lithuania), Eastern Russia and Poland, 3) Central and Western Europe, and 4) Italy, with the southern Italians being more “distant” »Sur les autres projets Wikimedia :Relatif au racisme :
Opposé au racisme :
Cet article possède un paronyme, voir Turk.
Cet article concerne une des langues parlées en Turquie. Pour la famille de langues apparentées, voir Langues turques. Pour les autres significations, voir Turcs.
Le turc (autonyme : Türkçe ou Türk dili) est une langue parlée principalement en Turquie et à Chypre. Il appartient à la famille des langues turques. Bien que les langues d'autres pays turcophones, principalement des républiques de l'ancienne URSS, soient proches du turc (surtout l'azéri et le turkmène), il existe d'importantes différences phonologiques, grammaticales ou lexicales entre ces langues.
Au-delà de la Turquie elle-même, le turc est utilisé dans l'ancien territoire de l'Empire ottoman par des populations d'origine ottomane, turcique ou des populations musulmanes qui ont adopté cette langue. Ces turcophones sont nombreux en Bulgarie, en Grèce (concentrés en Thrace occidentale), dans les Balkans (Bosnie-Herzégovine et Kosovo), dans la partie nord de l'île de Chypre (République turque de Chypre du Nord), dans le nord de l'Irak (surtout à Kirkouk), en Macédoine et en Roumanie (essentiellement en Dobroudja). C'est pourquoi le turc de Turquie est aussi nommé « turc osmanlı » (Osmanlı Türkçesi).
Le turc est, typologiquement, une langue agglutinante. Elle utilise principalement des suffixes et peu de préfixes. C'est une langue SOV (sujet-objet-verbe). Elle comporte un système d'harmonie vocalique.
Le turc fait partie du sous-groupe turc, ou sous-groupe de l'Ouest, des langues oghouzes, qui comprennent le gagaouze et l'azéri. Les langues oghouzes forment le sous-groupe du sud-ouest des langues turques, une famille de langues composée de 30 langues vivantes parlées en Europe de l'Est, Asie centrale et Sibérie. Certains linguistes pensent que les langues turciques font partie de la grande famille des langues altaïques[réf. souhaitée]. Environ 40 % des locuteurs natifs de langues turques sont des locuteurs natifs turcs[2]. Les caractéristiques du turc, tels que l'harmonie vocalique, l'agglutination et l'absence de genre grammatical, sont universelles au sein des langues turques et des langues altaïques[2]. Il existe un degré d'intercompréhension élevé entre le turc et les autres langues oghouzes, comme l'azéri, le turkmène, le kachkaï, le gagaouze et le turc gagaouze des Balkans[3].
Les plus anciennes inscriptions turciques connues sont les deux monumentales inscriptions de l'Orkhon, trouvées en Mongolie moderne. Érigées entre 732 et 735, en l'honneur du prince Kul Tigin et de son frère l'empereur Bilge Kaghan, elles constituent une autre importante découverte récente. Après la découverte et la mise au jour de ces monuments et d'autres dalles de pierre associées par des archéologues russes autour de la vallée de l'Orkhon entre 1889 et 1893, il a été établi que la langue sur ces inscriptions était le vieux-turc, utilisant l'alphabet de l'Orkhon. Elles ont également été appelées « runes turques » ou « runiforme » en raison de la ressemblance avec l'alphabet runique germanique[4].
Avec l'expansion turcique au cours du Haut Moyen Âge (VIe – XIe siècles), les peuples parlant des langues turciques répartis en Asie centrale s'étendent dans une vaste région, de la Sibérie à l'Europe et à la Méditerranée. Au XIe siècle, les Seldjoukides, des Oghouzes, en particulier, introduisent en Anatolie le turc oghouze, ancêtre direct du turc moderne[5]. Également pendant le XIe siècle, un linguiste des langues turques, Mahmoud de Kachgar, du khanat qarakhanide, a publié le Recueil des langues turques (en turc ottoman : Divânü Lügati't-Türk) ; c'est le premier dictionnaire complet des langues turques avec également une carte de la répartition géographique des peuples turcophones[6].
À la suite de l'adoption de l'Islam en 950 par les dynasties qarakhanides et seldjoukides, qui sont toutes deux considérées comme les ancêtres ethniques et culturels des Ottomans, la langue administrative de ces États a acquis un grand nombre de mots empruntés à l'arabe et au persan. La littérature turque pendant la période ottomane, en particulier la poésie ottomane Divan, a été fortement influencée par le persan, et notamment par l'emprunt de la métrique poétique et d'une grande quantité de mots importés. D’où les nombreux mots commun aux deux langues (persan et turc ). La langue littéraire et officielle au cours de la période de l'Empire ottoman (1299-1922) est appelée turc osmanlı, ou turc ottoman. C'était un langage considérablement différent ; un mélange de turc, de persan et d'arabe, largement inintelligible pour les classes moins instruites et pour les membres de la société rurale qui parlaient un « turc de tous les jours », connu sous le nom de kaba Türkçe (en français : « turc rugueux »), beaucoup plus pur et qui fut la base du turc moderne[7]. A part une poignée d'universitaires, personne ne parle plus turc ottoman. Enfin l'influence du grec médiéval n'est pas négligeable : 15 % du vocabulaire turc sont empruntés à cette langue. Le turc est largement implanté dans d'anciennes zones propres au grec, comme Smyrne et Trébizonde. Les mariages mixtes après la conquête ottomane ont accentué les apports et les emprunts au grec. Le turc a laissé aussi un certain héritage de mots au grec moderne.
L'idée d'un changement d'alphabet apparut à la fin du XIXe siècle chez le penseur persan Mirza Malkom Khan et l'écrivain azeri Mirza Fatali Akhundov.
Le turc est la langue maternelle des Turcs de Turquie et de la diaspora turque dans trente autres pays. En particulier, les minorités parlant turc existent dans les pays ayant appartenu (en partie ou entièrement) à l'Empire ottoman, comme la Bulgarie, Chypre, la Grèce (essentiellement en Thrace occidentale), la Macédoine du Nord, la Roumanie, et la Serbie[réf. souhaitée]. Plus de deux millions de personnes parlent turc en Allemagne, et il y a également une importante communauté de turcophones en France, aux Pays-Bas, en Autriche, en Belgique, en Suisse, et au Royaume-Uni[8]. En raison de l'assimilation culturelle des immigrants turcs dans les pays hôtes, les immigrés ethniques turcs ne parlent pas tous la langue couramment depuis la naissance.
Depuis la constitution turque de 1982[9], le turc est la langue officielle de la Turquie. C'est également la langue officielle  de la république turque de Chypre du Nord[10], pays reconnu uniquement par la Turquie. C'est une des deux langues officielles de la république de Chypre, avec le grec[11].
Le turc est une des langues officielles de plusieurs organisations : le Conseil turcique, l'Organisation internationale pour la culture turque et l'Avrasya Askerî Statülü Kolluk Kuvvetleri Teşkilatı.
Le turc a un statut reconnu de langue minoritaire dans plusieurs pays : en Bulgarie, où il comprend 606 000 locuteurs[réf. nécessaire]. Il est parlé dans les régions de Burgas, Khaskovo, Kurdzhali, Smolyan, Starta Zagora et Yambol. Il fut reconnu en tant que langue minoritaire en 1991 selon la loi nationale d'éducation. L'apprentissage du turc comme première langue est possible. En Grèce, il est langue provinciale en deux provinces, et il compte environ 40 000 locuteurs à partir de 2014. Au Kazakhstan, il y a environ 90 000 locuteurs turcs[réf. nécessaire], répartis dans tout le territoire. En Roumanie, le turc est une langue minoritaire reconnue qui compte 25 300 locuteurs à partir de 2014[réf. nécessaire]. Il est parlé dans les départements de Constanta et Tulcea.
Le turc comprend les variétés anatoliennes, chypriotes, karamali et syriennes turkmènes[12].
Dans les mots d'origine turque, les sons [c], [ɟ], et [l] sont en distribution complémentaire avec [k], [ɡ], et [ɫ]. Les premiers sont adjacents aux voyelles antérieures, et les derniers sont adjacents aux voyelles postérieures. Cependant la distribution de ces phonèmes est souvent aléatoire dans les emprunts étrangers et les noms propres, et [c], [ɟ], et [l] se présentent souvent avec des voyelles postérieures[13].
Le phonème /ɰ/, généralement appelé yumuşak g (« g faible »), ‹ ğ › dans l'orthographe turque, représente une consonne spirante vélaire-antérieure faible, ou une consonne spirante palatale entre voyelles antérieures. Il n'apparaît que devant une voyelle. En fin de mot ou précédant une autre consonne, il s'efface et allonge la voyelle précédente à la place[14].
Dans de nombreux noms qui se terminent par un ‹ k › post-vocalique, celui-ci devient ‹ ğ › quand une voyelle y est suffixée à cause d'une alternance consonantique. Une alternance similaire s'applique à certains mots d'emprunt qui se terminent par ‹ p › et ‹ t › ; ceux-ci deviennent respectivement ‹ b › et ‹ d › lors de l'ajout d'une voyelle[15]. Ces ‹ p ›, ‹ t ›, ‹ k › proviennent du dévoisement final de ‹ b ›, ‹ d › et ‹ ğ ›, et  lorsqu'une voyelle est ajoutée, ces consonnes ne sont plus en position finale et ne subissent donc plus ce dévoisement.
Les voyelles de la langue turque sont, par ordre alphabétique, ‹ a ›, ‹ e ›, ‹ ı ›, ‹ i ›, ‹ o ›, ‹ ö ›, ‹ u ›, ‹ ü ›[16]. Le système vocalique du turc peut être considéré ayant trois dimensions, les voyelles s'opposant les unes aux autres selon trois oppositions binaires :
Il n'y a pas de diphthongues en turc. Lorsque deux voyelles se rencontrent, ce qui se produit dans certains emprunts lexicaux à l'arabe, chaque voyelle conserve son individualité. Cependant, une légère diphtongue peut se produire lorsque deux voyelles entourent un yumuşak g. Par exemple, le mot soğuk (« froid ») peut être prononcé [sou̯k] par certains locuteurs[réf. souhaitée].
La langue turque distingue deux types d'harmonie, l'une obéissant à la profondeur des voyelles et l'autre à la profondeur des voyelles et à leur degré d'ouverture. Certains traits grammaticaux sont régis par la première, d'autres par la seconde.
Le principe d'harmonie veut qu'un mot se décline avec une voyelle du même type que la dernière voyelle du mot. Toutefois, les mots turcs venus du vieux turc n'ont des voyelles que d'un seul type ; c'est donc tout le mot qui obéit à l'harmonie.
Le premier type d'harmonie est connu comme l'harmonie simple ou l'harmonie en E. Elle obéit à un seul critère : la distinction de point d'articulation des voyelles (antérieures ou postérieures) débouchant sur deux déclinaisons possibles.
Elle détermine surtout le pluriel, -lar (/ɫar/, avec une voyelle postérieure) ou -ler (/ler/, avec une voyelle antérieure).
Exemples :
Le second type d'harmonie, appelé harmonie complexe ou harmonie en İ, obéit à deux critères : le point d'articulation des voyelles (antérieures ou postérieures, comme pour l'harmonie simple) et leur arrondissement (arrondies ou non arrondies), ce qui débouche sur quatre déclinaisons possibles.
On peut illustrer l'usage de l'harmonie complexe avec la particule interrogative mi / mı / mü / mu ; quoiqu'elle soit détachée du mot qui la précède, elle obéit à l'harmonie de ce dernier. Les courtes traductions du tableau suivant en donnent des exemples.
L'accent tonique turc porte le plus souvent sur la dernière syllabe, mais cette généralité obéit à plusieurs exceptions. Certaines suffixations et interjections ainsi que certains emprunts (en particulier depuis l'italien ou le grec), adverbes ou interrogatifs, sont le plus souvent accentués sur l'avant-dernière syllabe.
Le traitement de l'accent tonique des noms propres est particulier ; il est nommé « accent Sezer » (« Sezer stress » en anglais) du nom de son découvreur, Engin Sezer. Il décrit que cet accent tonique survient sur l'antépénultième syllabe (proparoxyton) si elle est « lourde » et que l'avant-dernière est « légère » (Ankara, Mercimek) et qu'il survient sur l'avant-dernière syllabe (paroxyton) dans tous les autres cas (Adana, Oregon, Vaşington). Cet accent tonique est contrastif.
Il n'y a ni articles ni genres en turc[17].
Le turc est une langue agglutinante et utilise fréquemment les affixes, en particulier les suffixes[18]. Un mot peut avoir de nombreux affixes, qui peuvent être également utilisés pour créer de nouveaux mots. Les relations entre des mots se créent à l’aide des suffixes ajoutés à la fin des mots. Il est ainsi possible de créer un verbe depuis un nom ou un nom depuis une base verbale (voir la section Formation des mots). 
La plupart des affixes indiquent la fonction grammaticale du mot[19]. Les seuls préfixes originaux sont allitératifs, qui accentuent les syllabes utilisées avec les adjectifs ou les adverbes : sımsıcak ("chaud bouillant" < sıcak) et masmavi (« bleu vif » < mavi)[20].
La grammaire turque n'offre pas un contraste de définitude (le/un) aussi rigoureux qu'en français par exemple, mais l'accusatif suggère un objet défini (c'est la raison pour laquelle on y réfère parfois comme à un accusatif défini). En outre, il existe un article indéfini : bir. L'absence des deux marquages indique une quantité indéfinie, mais dans ce cas, le nom reste singulier.
Le turc dispose de cinq cas en plus du nominatif : l'accusatif, le génitif, le datif, l'ablatif et le locatif. Les deux premières obéissent à l'harmonie vocalique complexe et les autres à l'harmonie vocalique simple (voir #Harmonie vocalique). C'est pour cette raison que les mots déclinés n'obéissent pas forcément à la terminaison donnée, mais plutôt à cette terminaison harmonisée.
Le tableau ci-dessous résume les généralités dans les déclinaisons. Le premier exemple (köy) est basique ; le deuxième (ağaç), montre d'une part un voisement de la consonne finale (ç>c à l'accusatif, au génitif et au datif) et d'autre part un dévoisement de la consonne de la déclinaison (d>t à l'ablatif et au locatif). 
Le turc n'utilise pas de pronoms possessifs pour exprimer la possession, mais des affixes. Les noms « possédés » peuvent se décliner également. Le tableau montre les formes possessives du mot « ev », « maison », au nominatif singulier.
Notez que « leurs maisons » se traduit « evleri ». La marque n'est pas doublée en « evlerleri » (celle du possesseur plus celle du possédé), ce qui en fait un homonyme grammatical avec « leur maison ».
En plus des traits grammaticaux évoqués ci-dessus, les noms peuvent hériter de l'aspect grammatical qu'on applique généralement aux verbes.
Le tableau suivant témoigne de l'usage extensif des affixes turcs sur un même mot. Les traits s'appliquent dans l'ordre suivant : pluriel, forme possessive, déclinaison, aspect. Ainsi « dans nos maisons » se traduit « evlerimizde ».
Les verbes turcs indiquent la personne. Ils peuvent être négatifs, potentiels (« peut »), ou impotentiels (« ne peut pas »). En outre, les verbes turcs montrent temps (présent, passé, futur, et aoriste), mode (conditionnel, impératif, inférentiel, nécessitatif, et optatif), et aspect. La négation est exprimée par l'infixe -me- immédiatement après le radical.
Tous les verbes turcs sont conjugués de la même manière sauf pour les verbes irréguliers et défectifs i-, la copule turque (correspondant au français être), qui peut être utilisée sous des formes composées (la forme abrégée est appelée un enclitique) : Gelememişti = Gelememiş idi = Gelememiş + i- + -di.
Le turc est une langue SOV : l'ordre des mots dans les phrases simples étant, contrairement au français, où le verbe est placé entre le sujet et l'objet, sujet-objet-verbe (comme en coréen et en japonais, qui, selon certains linguistes, seraient également des langues altaïques). Dans les phrases plus complexes, la règle de base est que le qualifiant précède le qualifié : ce principe inclut un important cas particulier, les modificateurs participiaux évoqués plus haut.
Le défini précède l'indéfini : on dira çocuğa hikâyeyi anlattı (« elle a raconté l'histoire à l'enfant ») mais hikâyeyi bir çocuğa anlattı (« elle a raconté l'histoire à un enfant »)[21].
Il est possible d'altérer le sens des mots pour renforcer l'importance de certains mots ou certaines phrases. La règle principale est que le mot avant le verbe est toujours accentué. 
Par exemple, pour dire « Hakan est allé à l'école » avec un accent sur le mot « école » (okul, l'objet indirect), on dira « Hakan okula gitti. Si l'accent est placé sur « Hakan » (le sujet), ce sera « Okula Hakan gitti »: « c'est Hakan qui est allé à l'école ». Dans ce dernier cas, le deuxième exemple est rarement employé même s'il est grammaticalement correct.
Bien que SOV soit commun, OSV peut également être utilisé. OSV est utilisé en turc pour souligner le sujet, par exemple: "Yemeği ben pişirdim" = Le repas / je / cuit (C'est moi, pas quelqu'un d'autre, qui ai cuisiné le repas).
La dernière édition de 2010 du Büyük Türkçe Sözlük (Grand Dictionnaire turc), le dictionnaire officiel turc, édité par la Turkish Language Association, contient 616 767 mots, expressions, termes et noms[22].
L'édition de 2005 du Güncel Türkçe Sözlük, contient 104 481 mots, parmi lesquels 86 % sont turcs et 14 % sont d'origine étrangère[23]. Les plus importants emprunts étrangers dans le vocabulaire turc sont d'origine arabe, française, perse, italienne, anglaise et grecque[24].
Le turc utilise très souvent l'agglutination pour former de nouveaux mots à partir des noms et des radicaux. Les mots turcs proviennent largement de l'apposition de suffixes dérivés à un ensemble relativement peu élevé de mots de base.
Voici un ensemble de mots qui est dérivé d'une racine :
Un autre exemple commence par la racine verbale :
Les nouveaux mots viennent également souvent de mots composés, deux mots existants dans un nouveau
Il existe, plus rarement, des préfixes. Outre ceux qui sont encore présents dans des mots d'origine étrangère (psi-, dans psikoloji, psikopat...), on utilise maintenant trois mécanismes de préfixation fondés sur la répétition ou le redoublement d'un mot. Cela concerne surtout les adjectifs et les adverbes[25].
Lorsqu'il s'agit d'exprimer une manière de faire, le redoublement complet de l'adjectif sans altération est la forme la plus appropriée[25].
Assez rare et utilisée dans le registre familier, cette forme attribue le sens de "similarité floue" ou "un mouvement loin de la qualité totale ou de sens". Elle est utilisée avec des substantifs[25].
Forme plus courante que la précédente, elle ne concerne que les adjectifs (essentiellement visuels)[25].
Note : Dans yapayaplnız, le a suivant le redoublement est une voyelle épenthétique.
Alors que le turc est une langue extrêmement régulière, il n'y a pas de règles précises régissant le choix de la consonne après la première voyelle (la coda). Les choix possibles sont /p/, /m/, /r/, /s/ ; le plus utilisé est /p/[25].
Le turc a connu différents systèmes d'écritures dont une adaptation de l'alphabet arabe. Ce dernier a été utilisé pour noter le turc d'Anatolie du XIIIe siècle au 1er novembre 1928, date à laquelle la romanisation (baptisée « Révolution des signes ») décidée par Mustafa Kemal (Kemal Atatürk) est devenue officielle, dans le cadre de sa politique à la fois nationaliste et modernisatrice de la société turque.
Depuis lors, c'est l'alphabet latin qui est utilisé, complété de diacritiques (la cédille, l'accent circonflexe, le tréma et la brève) ainsi que d'une lettre typographiquement étonnante, i sans point, ı, ce qui implique également à l'inverse un i majuscule avec point İ.
La lettre ı sert à noter le ou non arrondi (« i vélaire » ou « tendu », appellations traditionnelles mais trop dépendantes de la graphie pour pouvoir décrire correctement le son correspondant), représenté par [ɯ] dans l’alphabet phonétique international.
L’alphabet grec, l’alphabet arménien et l’alphabet hébreu ont aussi été utilisés respectivement par les communautés orthodoxe, arménienne et juive, l’alphabet arabe étant associé avec la religion musulmane.
L'alphabet de l'Orkhon (qualifié parfois de runiforme du fait de sa ressemblance avec les runes scandinaves) et l'alphabet ouïghour ont été utilisés pour transcrire le vieux-turc qui n'est pas l'ancêtre direct du turc de Turquie, le vieux-turc appartenant à la branche orientale des langues turques et le turc moderne à la branche méridionale.
L'alphabet turc est presque phonétique, ce qui signifie que l'on peut presque le prononcer simplement en le lisant (voir Prononciation du turc). Il existe cependant quelques irrégularités.
Il contient 29 lettres : A, B, C, Ç, D, E, F, G, Ğ, H, I, İ, J, K, L, M, N, O, Ö, P, R, S, Ş, T, U, Ü, V, Y, Z
Seuls les mots d'emprunt utilisent les lettres Q, W et X.
Dostlar Beni Hatırlasın d'Aşık Veysel Şatıroğlu (1894–1973), un ménestrel et poète très apprécié dans la littérature folklorique turque.
Les nombres en turc suivent toujours la même syntaxe.
modifier - modifier le code - modifier WikidataLe Dit du Genji (源氏物語, Genji monogatari?, ou Conte du Genji, ou Roman de Genji) est une œuvre considérée comme majeure de la littérature japonaise du XIe siècle, attribuée à Murasaki Shikibu. Comme avec la plupart des ouvrages de l'époque de Heian, le Dit du Genji a probablement été écrit principalement (ou peut-être entièrement) en caractères kana (écriture phonétique japonaise) et non en caractères chinois, car il a été écrit par une femme pour un public féminin. 
L'intrigue du livre se déroule pendant l'époque de Heian. Le Genji est un fils d'empereur qui ne peut prétendre au trône. Il est donc à l'origine (源, gen?) d'une nouvelle branche (氏, ji?) impériale.
Le Dit du Genji, qui se présente comme un récit véridique (物語, monogatari?), raconte la vie d'un de ces princes impériaux, d'une beauté extraordinaire, poète accompli et charmeur de femmes. Toutefois, bien que le roman soit présenté comme une histoire vraie, on pense généralement que Murasaki Shikibu s'est inspirée de Fujiwara no Michinaga (966-1028), un homme d'État réputé[réf. nécessaire].
Il s'agit pour beaucoup du premier roman psychologique du monde. Le caractère intemporel des relations humaines y est pour beaucoup et, si les us et coutumes de la cour peuvent nous être étrangers, les vicissitudes que rencontrent les personnages sont bien plus familières. Par bien des aspects, l'œuvre est une critique incisive et complète des mœurs décadentes de la cour de Heian, mais avec un regard intérieur, intime car, après tout, l'autrice est elle-même une membre de la cour. Si on prend en compte la date de l'œuvre, les sujets abordés sont très en avance sur leur temps. Il y a là la femme bafouée, le mari jaloux, la courtisane, le séducteur impénitent, la fascination du pouvoir, les différentes classes sociales, l'argent.
Une des difficultés majeures de lecture réside dans le fait que les personnages (plus de deux cents) sont presque tous nommés uniquement par leur titre dans la cour impériale. L'histoire durant plusieurs dizaines d'années, les protagonistes évoluent et donc changent de titre. Les lecteurs et les traducteurs contemporains utilisent divers sobriquets pour suivre les nombreux personnages du roman.
Le Dit du Genji a plus de 1000 ans, la date officielle de célébration du millénaire du Genji ayant été fixée à l'an 2008. On put voir cette année-là plusieurs adaptations scéniques de ce classique de la littérature, dont le fameux chapitre de « La Fleur de safran »[2].
Le Japon connaît deux principaux cultes : le shintoïsme et le bouddhisme, ce dernier étant à l'époque du Genji implanté sur l'archipel depuis quatre siècles environ. Ces cultes se sont souvent influencés et ne s'excluent pas mutuellement.
On retrouve évidemment ce mélange dans l'œuvre avec des références culturelles omniprésentes qui pourront parfois surprendre le lecteur moderne. Par exemple, les deux principaux tabous du culte shintoïste sont la naissance et la mort, ce qui expliquera que Genji se dise souillé lorsqu'une de ses amantes meurt chez lui. De même, les esprits sont des créatures courantes dans les croyances de l'époque et personne ne s'étonnera dès lors qu'il fasse procéder à des rites de purification dès que sa femme tombe malade. L'autrice elle-même explique la mort du Genji par la malveillance d'un personnage.
Les fréquents voyages que le Genji fait dans les monastères ou les sutras prononcés contre les mauvaises influences sont des utilisations magiques de traditions bouddhiques, bien implantées au Japon, en particulier grâce au prince Shōtoku.
À l'époque du roman, ce sont les Song qui sont les maîtres de la Chine. Toutefois, le rayonnement culturel de leurs prédécesseurs, les Tang a débordé sur les pays alentour, dont le Japon où leur littérature est bien connue et leur langue pratiquée à la cour. Les Chinois sont même nommés « Hommes de Kara » (唐人, karabito?, où 唐 est le caractère kara qui désigne les Tang) dans le roman. C'est la même chose pour le terme de Morokoshi (ja) qui désigne la Chine.
Bien sûr moins présente que la Chine, la Corée est néanmoins représentée. Elle est désignée par « pays de Koma » dans le livre.
Cette œuvre est l'occasion de plonger au cœur de la vie à la cour impériale, des us et coutumes qui ponctuent la vie très normée des aristocrates.
Le Dit du Genji est souvent qualifié de roman psychologique. Il est aussi un recueil précieux des différents arts qui ponctuaient la vie à la cour tant ces derniers faisaient partie intégrante du protocole et des réceptions.
Le récit est traversé d'environ 800 poèmes japonais, waka (和歌?), dont le sens pour un lecteur non averti peut être difficile à saisir. Ces poèmes sont très nombreux parce que les manières de l'ère Heian interdisaient qu'on s'exprimât d'une façon directe, car c'eût été trop familier et franc (omote et ura). Les poèmes offraient une manière (entre autres) de s'exprimer en termes indirects, tout en montrant la culture de l'interlocuteur. Par conséquent, les personnages s'expriment souvent en citant des poèmes qui étaient très connus à l'époque. Les dames du roman héritent bien souvent de surnoms tirés de ces poèmes.
La calligraphie et ses différents styles est également un élément culturel fondamental et le Genji semble même l'utiliser pour évaluer la qualité d'une personne féminine. Élevée au Japon en art majeur, la calligraphie était indispensable aux gens de la cour. On remarque que les caractères chinois n'étaient pas si prisés et qu'il était fort bien vu d'user des caractères féminins, les kanas. Une calligraphie leur est d'ailleurs dédiée.
La calligraphie est d'autant plus importante que les échanges entre les personnages, en particulier entre les hommes et les femmes, sont basés sur l'échange de lettres qui ponctuent les relations interpersonnelles. En effet, selon la coutume de la cour, les femmes de l'aristocratie devaient rester cachées à l'intérieur de leur résidence, ne jamais se montrer aux regards masculins sous peine de paraître des femmes légères. Elles restaient ainsi cachées toute leur vie et rares étaient les occasions où elles pouvaient sortir de leur demeure. Elles devaient rester invisibles y compris des jardins de leur demeure dans l'hypothèse où un homme en visite se serait trouvé dans leur jardin. Des stores les cachaient en permanence de la vue extérieure. Et lorsqu'un membre de la famille venait en visite, cela s'appliquait également entre frères et sœurs ; c'est cachés derrière une tenture que la discussion avait lieu.
L'échange de lettres était donc un des moyens principaux de communication. Ces lettres étaient souvent remises par l'intermédiaire des serviteurs, des dames d'atour. Ces lettres faisaient l'objet d'un choix précis quant au papier : texture, couleur — tout était soigneusement réfléchi en fonction du message que l'on avait à transmettre. Il était de bon ton de toujours répondre et très mal vu de ne pas le faire. Lorsque la destinataire ne souhaitait pas répondre directement, c'est une de ses dames de confiance qui rédigeait la réponse à sa place.
Une fois écrite, la lettre était très souvent attachée à un végétal, dont le choix était également lié au type de message que l'on souhaitait faire passer.
C'est le passe-temps de beaucoup de membres de la cour, tous ou presque jouent d'un instrument. Le koto (cithare japonaise) et le shamisen (luth japonais) sont omniprésents. Une bonne partie du roman décrit des concerts de musique et relate le plaisir qu'ils apportent.
La peinture est également un autre art majeur de la cour, et même si tous ne la pratiquent pas, chacun a une bonne connaissance des styles et un concours de peinture est même organisé où le Troisième Empereur, très versé dans cet art, joue le rôle d'arbitre. Et chacun de produire les tableaux qu'il conserve chez lui.
Se vêtir paraît être un passe-temps des plus éminents à la cour de Heian. Le Genji choisit toujours ses tenues avec soin et la concordance des couleurs ainsi que la qualité des tissus est primordiale.
La culture japonaise est très liée aux passages des saisons. La couleur des vêtements s'accorde ainsi à la nature : blanche pour l'hiver, très colorée pour l'été, brun pour l'automne.
Le statut social des personnages influe sur le choix des vêtements : vert pâle pour un jeune aristocrate devant faire ses preuves à la cour.
Les nobles d'alors se parfumaient beaucoup. Ils parfumaient les pièces et les vêtements bien sûr, mais également les objets comme les lettres. Les parfums étaient souvent des parfums à brûler, se rapprochant de l'encens.
Il était à l'époque impératif pour un ou une aristocrate de maîtriser la confection des parfums, chaque personne détenant les recettes nécessaires à la confection du parfum le plus adapté à la cérémonie visée. Chaque recette normée et transmise de génération en génération faisait partie de la culture impérative à détenir. Une éducation était incomplète sans cette capacité à confectionner un parfum.
Le Dit du Genji est l'occasion d'un témoignage de concours de parfum où chacun rivalise afin de produire le parfum le plus adapté.
On notera qu'aucun des personnages principaux du Dit du Genji n'est cité par son nom propre. En effet, à l'époque Heian, on se référait aux gens de la cour par leur titre, l'usage du nom étant considéré comme grossier. On s'amusera de voir quels sont les personnages cités par leur nom dans le roman, souvent des servants. Le nombre de personnages et leur parenté complexe font de ce roman une œuvre délicate à suivre où la moindre promotion va changer le nom d'un protagoniste. Il y aurait plus de deux cents protagonistes dans le roman.
Heian, l'ancienne capitale (aujourd'hui Kyōto), était organisée autour du palais impérial, depuis la Première Avenue, qui passait au nord du palais, situé entre la Première et la Deuxième Avenue, jusqu'à la Neuvième Avenue, au sud de la ville.
C'est la première résidence du Genji.
Lorsque le Genji commence à avoir un poste à la mesure de ses ambitions, il se fait construire une nouvelle résidence sur la Sixième Avenue. Il la crée suivant quatre directions, avec un jardin associé à chaque saison. Il y loge notamment ses dames les plus proches. Puisqu'une des ailes de la résidence est constituée de l'ancienne résidence de la Dame de la Sixième Avenue, cette aile revient naturellement à sa fille, l'Impératrice, qui y a grandi.
Le Dit du Genji se compose de 54 livres. Chaque livre s'appelle jō (帖) :
Partie 1
Partie 2
Partie 3, alias la partie Uji
La première traduction française, due à Kikou Yamata et publiée en 1928 sous le titre Le Roman de Genji, ne reprend que les neuf premiers chapitres du Genji Monogatari. Elle fut réalisée d'après la version anglaise de A. Waley, le texte original ancien et la traduction en japonais moderne d'Akiko Yosano.
La seule traduction française disponible à ce jour est celle de René Sieffert, en deux parties, « Magnificence » et « Impermanence ». Une nouvelle traduction est en cours, mais devrait prendre plusieurs années, en raison de l'ampleur du travail à accomplir.
Une édition illustrée de cette traduction est disponible, accompagnée de 500 illustrations de la peinture japonaise du XIIe au XVIIe siècle et d'un commentaire iconographique pour chaque illustration. L'édition intitulée Le Dit du Genji de Murasaki-shikibu illustré par la peinture traditionnelle japonaise[3] compte trois volumes, ainsi qu'un livret intitulé À la découverte du Dit du Genji, sous coffret illustré, 1 280 pages, valisette de transport, Éditions Diane de Selliers, 2008  (ISBN 978-2-903656-37-9) ; et une version en plus petite taille  (ISBN 978-2-903656-46-1). Le choix du corpus d'images (genji-e), ainsi que tous les commentaires de cette édition, sont l'œuvre d'Estelle Leggeri-Bauer, maître de conférence à l'INALCO.
Le Dit du Genji a été un thème favori de la peinture japonaise, notamment dans le mouvement du yamato-e à l'époque de Heian, la peinture sur panneaux de l'école Tosa ou des estampes ukiyo-e[4]. Minamoto no Morotoki mentionne déjà des peintures inspirées du Genji en 1119 ; la plus ancienne œuvre conservée reste les Rouleaux illustrés du Dit du Genji (env. 1120-1140), un emaki typique de l'art de la cour de Heian, caractérisé par ses pigments riches apposés sur toute la surface du papier (tsukuri-e) et une atmosphère nostalgique, intimiste, suspendue dans le temps[5]. Plusieurs autres adaptations sur rouleaux, panneaux ou paravents sont mentionnées à l'époque de Kamakura, mais le thème devient réellement classique à l'époque de Muromachi (XIVe-XVe siècles). Ainsi, les figures les plus influentes comme Oda Nobunaga commanditent des peintures du Genji ; de nombreux exemplaires des écoles Tosa et Kanō demeurent de nos jours[6]. Ces peintures sont généralement désignées sous le terme générique de Genji-e (littéralement « peinture de Genji »)[7].
À l'occasion de l'édition d'une version du Dit du Genji illustrée, Diane de Selliers a parcouru le monde pendant sept ans afin de retrouver les œuvres disséminées dans des collections privées, des musées, des temples. Son édition permet au lecteur de s'approprier visuellement ce monde de la cour impériale japonaise et les commentaires des œuvres présentées sont une aide non négligeable à la compréhension de cette œuvre fleuve[8]
Le Dit du Genji reste un thème populaire au XXe siècle. Plusieurs mangas s'en inspirent, partiellement ou intégralement, comme Asaki yume mishi de Waki Yamato, ou les œuvres homonymes de Miyako Maki, Hōsei Hasegawa ou Tatsuya Egawa.
Au cinéma, Le Roman de Genji (1951) est un film de Kōzaburō Yoshimura qui propose une réinterprétation du thème classique (bunga-eiga, genre cinématographique inspiré par la littérature), mélangeant clairement les cultures médiévales et contemporaines[9]. Dans le monde des animes figure aussi le film d'animation expérimental de Gisaburō Sugii : Le Roman de Genji (1987) ; une autre adaptation de 2009 par le studio TMS Entertainment-Tezuka Productions s'intitule Genji monogatari sennenki, série en onze épisodes[10],
Au théâtre, certains des chapitres les plus célèbres, comme Yugao ou Suhetsumuhana (« La Fleur de safran »), ont été adaptés par des metteurs en scène contemporains, japonais ou occidentaux.
Le personnage de jeu vidéo Genji d'Overwatch édité par Blizzard est inspiré par ce récit, en effet en plus de partager leur nom, le personnage de jeu vidéo se retrouve dans une lutte concernant la succession à la tête de son clan entre lui et son frère.
Le verso des billets de 2.000 yens représente une scène du Dit du Genji et un portrait de Murasaki Shikibu.
Le Dit du Genji est classé n°1 dans les romans les plus importants à avoir lu dans Les Chemins de L'Essentiel de Jacques Attali[11].
Vous lisez un « article de qualité » labellisé en 2020.
Pour les articles homonymes, voir Mercure.
Mercure est la planète la plus proche du Soleil et la moins massive du Système solaire[N 1]. Son éloignement au Soleil est compris entre 0,31 et 0,47 unité astronomique (soit 46 et 70 millions de kilomètres), ce qui correspond à une excentricité orbitale de 0,2 — plus de douze fois supérieure à celle de la Terre, et de loin la plus élevée pour une planète du Système solaire. Elle est visible à l'œil nu depuis la Terre avec un diamètre apparent de 4,5 à 13 secondes d'arc, et une magnitude apparente de 5,7 à −2,3 ; son observation est toutefois rendue difficile par son élongation toujours inférieure à 28,3° qui la noie le plus souvent dans l'éclat du soleil. En pratique, cette proximité avec le soleil implique qu'elle ne peut être vue que près de l'horizon occidental après le coucher du soleil ou près de l'horizon oriental avant le lever du soleil, en général au crépuscule.
Mercure a la particularité d'être en résonance spin-orbite 3:2, sa période de révolution (~88 jours) valant exactement 1,5 fois sa période de rotation (~59 jours), et donc la moitié d'un jour solaire (~176 jours). Ainsi, relativement aux étoiles fixes, elle tourne sur son axe exactement trois fois toutes les deux révolutions autour du Soleil.
Mercure est une planète tellurique, comme le sont également Vénus, la Terre et Mars. Elle est près de trois fois plus petite et presque vingt fois moins massive que la Terre mais presque aussi dense qu'elle. Sa densité remarquable — dépassée seulement par celle de la Terre, qui lui serait d'ailleurs inférieure sans l'effet de la compression gravitationnelle — est due à l'importance de son noyau métallique, qui représenterait 85 % de son rayon, contre environ 55 % pour la Terre.
Comme Vénus, Mercure est quasiment sphérique — son aplatissement pouvant être considéré comme nul — en raison de sa rotation très lente. Dépourvue de véritable atmosphère pouvant la protéger des météorites (il n'existe qu'une exosphère exerçant une pression au sol de moins de 1 nPa ou 10−14 atm), sa surface est très fortement cratérisée et globalement similaire à la face cachée de la Lune, indiquant qu'elle est géologiquement inactive depuis des milliards d'années. Cette absence d'atmosphère combinée à la proximité du Soleil engendre des températures en surface allant de 90 K (−183 °C) au fond des cratères polaires (là où les rayons du Soleil ne parviennent jamais) jusqu'à 700 K (427 °C) au point subsolaire au périhélie. La planète est par ailleurs dépourvue de satellites naturels.
Seules deux sondes spatiales ont étudié Mercure. Mariner 10, qui survole à trois reprises la planète en 1974–1975, cartographie 45 % de sa surface et découvre l'existence de son champ magnétique. La sonde MESSENGER, après trois survols en 2008-2009, se met en orbite autour de Mercure en mars 2011 et réalise une étude détaillée notamment de sa topographie, son histoire géologique, son champ magnétique et son exosphère. La sonde BepiColombo a pour objectif de se mettre en orbite autour de Mercure en décembre 2025.
La planète Mercure doit son nom au messager des dieux dans la mythologie romaine, Mercure. La planète est nommée ainsi par les Romains à cause de la vitesse avec laquelle elle se déplace dans le ciel. Le symbole astronomique de Mercure est un cercle posé sur une croix et portant un demi-cercle en forme de cornes (Unicode : ☿). Il s'agit d'une représentation du caducée du dieu Hermès, équivalent de Mercure dans la mythologie grecque. Mercure a également donné son nom au troisième jour de la semaine, mercredi (« Mercurii dies »).
Mercure a l'excentricité orbitale la plus élevée des planètes du Système solaire, avec pour valeur environ 0,21. Cela implique que sa distance au Soleil varie de 46 à 70 millions de kilomètres[5],[2] au cours de sa révolution. Le diagramme de gauche illustre les effets de l'excentricité, en montrant l'orbite de Mercure superposée à une orbite circulaire ayant le même demi-grand axe. Cette variation de distance par rapport au Soleil fait que la surface de Mercure est soumise à une force de marée exercée par le Soleil qui est environ 17 fois plus forte que celle de la Lune sur Terre[6]. Combiné avec sa résonance de 3:2 de la rotation de la planète autour de son axe, cela entraîne également des variations complexes de la température de surface[7],[8].
L'excentricité de l'orbite de Mercure varie de manière chaotique de 0 (orbite circulaire) à une valeur très importante de plus de 0,45 sur plusieurs millions d'années du fait de l'influence des autres planètes[9],[10]. En 1989, Jacques Laskar, du Bureau des longitudes, démontre que les planètes intérieures du Système solaire avaient toutes des courses chaotiques. Cependant, Mercure est celle dont le mouvement est le plus chaotique[10],[11].
L'orbite de Mercure est inclinée de 7 degrés par rapport au plan de l'orbite terrestre (écliptique), comme le montre le schéma de droite. Par conséquent, les transits de Mercure devant le Soleil ne peuvent avoir lieu que lorsque la planète traverse le plan de l'écliptique, au moment où elle se trouve entre la Terre et le Soleil, c'est-à-dire en mai ou en novembre. Cela se produit environ tous les sept ans en moyenne[12].
L'inclinaison de l'axe de rotation de Mercure sur son plan orbital est la plus faible du Système solaire, à peine 2 minutes d'arc, soit environ 0,03 degré[13]. Cela est significativement plus faible que celle de Jupiter, qui a la deuxième plus petite inclinaison axiale de toutes les planètes, à 3,1 degrés. Cela signifie que pour un observateur aux pôles de Mercure, le centre du soleil ne s'élève jamais à plus de 2 minutes d'arc au-dessus de l'horizon[13].
En certains points de la surface de Mercure, un observateur pourrait voir le soleil se lever à un peu plus des deux tiers de l'horizon, puis se coucher avant de se lever à nouveau, le tout au cours de la même journée mercurielle[N 2]. En effet, quatre jours terrestres avant le périhélie, la vitesse orbitale angulaire de Mercure est égale à sa vitesse de rotation angulaire, de sorte que le mouvement apparent du soleil cesse ; plus près du périhélie, la vitesse orbitale angulaire de Mercure dépasse alors la vitesse de rotation angulaire. Ainsi, pour un observateur hypothétique sur Mercure, le soleil semble se déplacer dans une direction rétrograde. Quatre jours terrestres après le périhélie, le mouvement apparent normal du soleil reprend et il se lève à nouveau à l'est pour se coucher à l'ouest[14].
Pour la même raison, il y a un couple de points sur l'équateur de Mercure (l'un d'entre eux étant situé dans le bassin Caloris[15]), distants de 180 degrés en longitude, où à chacun desquels, un an mercurien sur deux (ce qui équivaut à une fois par jour mercurien), le soleil passe au-dessus d'est en ouest, puis inverse son mouvement apparent et passe à nouveau au-dessus d'ouest en est (lors du mouvement rétrograde), puis inverse son mouvement une seconde fois et passe au-dessus une troisième fois d'est en ouest[16],[17]. Au cours de l'année mercurienne alternée, c'est à l'autre point de ce couple que ce phénomène se produit. L'amplitude du mouvement rétrograde étant faible en ces points, l'effet global est que, pendant deux ou trois semaines, le soleil est presque stationnaire au-dessus du point, et est à son plus haut niveau de brillance parce que Mercure est au périhélie[18]. Cette exposition prolongée au moment où la planète est au plus proche du Soleil fait de ces deux points les endroits les plus chauds sur Mercure (d'où le nom Caloris, signifiant « chaleur » en latin)[18],[19]. Un de ces points a servi de référence pour le méridien 0°[N 3].
Inversement, il y a deux autres points sur l'équateur, à 90 degrés de longitude de distance des premiers, où le soleil ne passe au-dessus que lorsque la planète est à l'aphélie, une année mercurienne sur deux, à un moment où le mouvement apparent du soleil dans le ciel de Mercure est relativement rapide. Ces points reçoivent ainsi beaucoup moins de chaleur solaire que ceux du couple décrits ci-dessus[19]. Il en résulte une journée mercurienne également « étrange » pour un observateur qui y serait situé. Celui-ci verra le soleil se lever puis se recoucher, puis se relever à l'horizon Est ; et à la fin de la journée à l'Ouest, le soleil se couchera puis se relèvera, pour se recoucher[20]. Ce phénomène s'explique aussi par la variation de la vitesse orbitale de Mercure : quatre jours avant le périhélie, la vitesse (angulaire) orbitale de Mercure étant exactement égale à sa vitesse (angulaire) de rotation, le mouvement du soleil semble s'arrêter[14],[19].
Mercure atteint sa conjonction inférieure (point où elle est au plus proche de la Terre) tous les 116 jours terrestres en moyenne (ce qu'on appelle la période synodique)[2], mais cet intervalle peut aller de 105 jours à 129 jours, en raison de l'orbite excentrique de la planète[21],[22]. Entre 1900 et 2100, Mercure s'est approchée au minimum, (et ne s'approchera donc pas plus), de la Terre d'environ 82,1 × 106 kilomètres (soit 0,55 unité astronomique), le 31 mai 2015[23]. Sa période de mouvement rétrograde peut varier de 8 à 15 jours terrestres de part et d'autre de la conjonction inférieure. Cette grande amplitude est aussi due à l'excentricité orbitale élevée de la planète[14].
De par sa proximité avec le Soleil, c'est Mercure, et non Vénus, qui est la planète la plus proche de la Terre en moyenne, même si l'orbite de Vénus est la plus proche de celle de la Terre[24],[25]. Ce raisonnement peut même être étendu, et Mercure est en réalité la planète la plus proche en moyenne pour chacune des autres planètes du Système solaire, y compris Uranus et Neptune (orbitant respectivement à 19 et 30 UA)[26],[27].
Alors qu'il étudiait Mercure afin d'en dresser une première carte, Schiaparelli remarque après plusieurs années d'observation que la planète présente toujours la même face au Soleil, comme la Lune le fait avec la Terre. Il en conclut alors en 1889 que Mercure est synchronisée par effet de marée avec le Soleil et que sa période de rotation équivaut à une année mercurienne, soit 88 jours terrestres[28]. Cette durée est cependant erronée et il fallut attendre les années 1960 avant que les astronomes ne la revoient à la baisse[29].
Ainsi, en 1962, des observations par radar à effet Doppler sont effectuées par le radiotélescope d'Arecibo sur Mercure afin d'en apprendre plus sur la planète et de vérifier si la période de rotation est bien égale à la période de révolution. Les températures relevées du côté de la planète censé être toujours exposé à l'ombre sont alors trop importantes, ce qui suggère que cette face sombre est en réalité parfois exposée au Soleil. En 1965, les résultats obtenus par Gordon H. Pettengill et Rolf B. Dyce révèlent que la période de rotation de Mercure est en fait de 59 jours terrestres[30], avec une incertitude de 5 jours. Cette période sera ajustée plus tard, en 1971, à 58,65 jours à ± 0,25 jours grâce à des mesures plus précises — toujours par radar — effectuées par R.M. Goldstein[31]. Trois ans plus tard, la sonde Mariner 10 apporte une meilleure précision, mesurant la période de rotation à 58,646 ± 0,005 jours[31]. Il se trouve que cette période est exactement égale aux 2/3 de la révolution de Mercure autour du Soleil ; c'est ce qu'on appelle une résonance spin-orbite 3:2[32],[33].
Cette résonance 3:2, une spécificité de Mercure, est stabilisée par la variance de la force de marée le long de l'orbite excentrique de Mercure, agissant sur une composante dipolaire permanente de la distribution de masse de Mercure[34] et par le mouvement chaotique de son orbite[35]. Dans une orbite circulaire, il n'y a pas de telle variance, donc la seule résonance stabilisée pour une telle orbite est 1:1 (par exemple, Terre-Lune). Au périhélie, là où la force de marée atteint son maximum, elle stabilise les résonances, comme 3:2, en obligeant la planète à pointer son axe de moindre inertie (là où le diamètre de la planète est le plus grand) approximativement vers le Soleil[34],[32].
La raison pour laquelle les astronomes pensaient que Mercure était verrouillée avec le Soleil est que, à chaque fois que Mercure était la mieux placée pour être observée, elle se trouvait toujours au même point sur son orbite (en résonance 3:2), présentant ainsi la même face à la Terre ; ce qui serait aussi le cas si elle était totalement synchronisée avec le Soleil. Cela est dû au fait que la période de rotation réelle de Mercure de 58,6 jours est presque exactement la moitié de la période synodique de Mercure valant 115,9 jours (c'est-à-dire le temps mis par Mercure pour revenir à la même configuration Terre–Mercure–Soleil) par rapport à la Terre[14]. L'erreur de Schiaparelli peut aussi être imputée à la difficulté d'observation de la planète avec les moyens de l'époque[28].
En raison de sa résonance 3:2, bien qu'un jour sidéral (la période de rotation) dure environ 58,7 jours terrestres, le jour solaire (durée entre deux retours successifs du Soleil au méridien local) dure 176 jours terrestres, c'est-à-dire deux années mercuriennes[36]. Cela implique qu'une journée et une nuit durent chacune exactement une année sur Mercure, soit 88 jours terrestres (presque un trimestre)[37].
Une modélisation précise basée sur un modèle des marées a démontré que Mercure a été capturé dans l'état de spin-orbite 3:2 à un stade très précoce de son histoire, entre 10 et 20 millions d'années après sa formation[38]. De plus, des simulations numériques ont montré qu'une future résonance séculaire avec Jupiter pourrait faire croître l'excentricité de Mercure jusqu'à un point où il y aurait 1 % de chance que la planète entre en collision avec Vénus d'ici à 5 milliards d'années[N 4],[39],[40]. La prédiction à long terme de l'orbite de Mercure s'inscrit dans la mécanique du chaos : certaines simulations démontrent même que la planète pourrait être éjectée du Système solaire[41].
Comme pour l'ensemble des planètes du Système solaire, l'orbite de Mercure connaît une très lente précession du périhélie autour du Soleil, c'est-à-dire que son orbite est elle-même en rotation autour du Soleil. Cependant, contrairement aux autres planètes, la période de précession du périhélie de Mercure ne concorde pas avec les prédictions faites à l'aide de la mécanique newtonienne[42].
En effet, Mercure connaît une précession légèrement plus rapide (5600 secondes par siècle) que celle à laquelle on peut s'attendre en appliquant les lois newtoniennes de la mécanique céleste (5557 secondes par siècle), et se trouve en conséquence en avance d'environ 43 secondes d'arc par siècle[43],[44].
Les astronomes ont donc, dans un premier temps, pensé à la présence d'un ou de plusieurs corps entre le Soleil et l'orbite de Mercure dont l'interaction gravitationnelle perturberait le mouvement de cette dernière. L'astronome français Urbain Le Verrier, qui avait découvert en 1846 la planète Neptune à partir d'anomalies dans l'orbite d'Uranus[45], se penche sur le problème et suggère la présence d'une planète inconnue ou d'une seconde ceinture d'astéroïdes entre le Soleil et Mercure[46]. Des calculs effectués, en prenant en compte l'influence gravitationnelle de ces corps, devaient alors concorder avec la précession observée.
Le 28 mars 1859, Le Verrier est contacté par le médecin français Edmond Lescarbault à propos d'une tache noire qu'il aurait vu passer devant le Soleil deux jours avant et qui était probablement, selon lui, une planète intramercurienne[47]. Le Verrier postule alors que cette planète — qu'il nomme Vulcain — est responsable des anomalies du mouvement de Mercure et se met en tête de la découvrir. À partir des informations de Lescarbault, il conclut que Vulcain tournerait autour du Soleil en 19 jours et 7 heures à une distance moyenne de 0,14 UA. Il déduit également un diamètre d'environ 2 000 km et une masse de 1/17e de celle de Mercure. Cette masse est cependant bien trop faible pour expliquer les anomalies, mais Vulcain reste une bonne candidate au corps le plus gros d'une hypothétique ceinture d'astéroïdes interne à l'orbite de Mercure[48].
Le Verrier profite alors de l'éclipse de Soleil de 1860 pour mobiliser tous les astronomes français afin de repérer Vulcain, mais personne ne put la trouver. La planète fut ensuite recherchée pendant des décennies, sans succès même si certains astronomes pensèrent l'avoir vue[49],[50], jusqu'à ce qu'une explication relativiste soit proposée.
En 1916, Albert Einstein avance la théorie de la relativité générale. En appliquant les paramètres dits post-képlériens de sa théorie au mouvement de Mercure, Einstein fournit l'explication de la précession observée en formalisant la gravitation comme étant affectée par la courbure de l'espace-temps[51],[52]. La formule de précession subie par l'orbite obtenue par Einstein est :
où  est le demi-grand axe de l'ellipse,  son excentricité,  la constante gravitationnelle,  la masse du Soleil, et  la période de révolution sur l'ellipse[53].
Avec pour valeurs numériques, , , ,  et , on retrouve 0,1035 secondes d'arc par révolution, ce qui correspond, avec les 415,2 révolutions de Mercure par siècle, à :
L'effet est faible : seulement ~43 secondes d'arc par siècle pour Mercure, il faut donc environ 2,8 millions d'années pour un tour complet en excès[N 5] (ou douze millions de révolutions), mais coïncide bien avec l'avance du périhélie précédemment mesurée. Cette prédiction validée constitue un des premiers grands succès de la relativité générale naissante[52]. Des effets similaires, mais plus faibles, existent aussi pour les autres planètes : 8,6247 secondes d'arc par siècle pour Vénus, 3,8387 pour la Terre et 1,351 pour Mars[54].
Mercure est l'une des quatre planètes telluriques du Système solaire, et possède un corps rocheux comme la Terre. C'est également la plus petite, avec un rayon équatorial de 2 439,7 km[2]. Mercure est également plus petite — bien que plus massive — que deux satellites naturels du Système solaire, Ganymède et Titan. Mercure est composée d'environ 70 % de métaux (principalement dans le noyau) et de 30 % de silicate (principalement dans son manteau)[55],[19]. La densité de Mercure est la deuxième plus élevée dans le Système solaire, avec 5,427 g/cm3, soit à peine moins que la densité de la Terre, qui est de 5,515 g/cm3 [2],[56]. Si l'effet de la compression gravitationnelle devait être ignoré, c'est Mercure qui serait plus dense avec 5,3 g/cm3 contre 4,4 g/cm3 pour la Terre[57], du fait d'une composition avec des matériaux plus denses.
La densité de Mercure peut être utilisée pour déduire des détails sur sa structure interne. Bien que la haute densité de la Terre résulte sensiblement de la compression gravitationnelle, en particulier au niveau du noyau terrestre, Mercure est beaucoup plus petite et ses régions internes ne sont pas aussi comprimées. Par conséquent, pour qu'elle ait une densité aussi élevée, son noyau doit être volumineux et riche en fer[58].
Les géologues estiment que le noyau de Mercure occupe environ 85 % de son rayon[59],[60], ce qui représenterait ainsi environ 61,4 % de son volume contre 17 % pour la Terre par exemple[N 6]. Des recherches publiées en 2007 ont un temps suggéré que le noyau de Mercure était totalement liquide (nickel et fer)[61],[62],[63]. Plus récemment, d'autres études utilisant des données de la mission MESSENGER, achevée en 2015, amènent cependant les astronomes à penser que le noyau interne de la planète est en réalité solide[60],[64],[65]. Autour du noyau se trouve une couche centrale externe solide de sulfure de fer et un manteau composé de silicates[66],[67]. D'après les données de la mission Mariner 10 et les observations terrestres, la croûte de Mercure aurait une épaisseur entre 35 et 54 km[68]. Une caractéristique distinctive de la surface de Mercure est la présence de nombreuses crêtes étroites, s'étendant jusqu'à plusieurs centaines de kilomètres de longueur. On pense qu'elles se sont formées lorsque le noyau et le manteau de Mercure ont refroidi et se sont contractés à un moment où la croûte s'était déjà solidifiée[65].
Le noyau de Mercure a une teneur en fer plus élevée que celle de tout autre objet du Système solaire[69]. Cette forte concentration en fer est la raison pour laquelle on la surnomme parfois « la planète métallique »[58] ou « la planète de fer »[70]. Comprendre l'origine de cette concentration permettrait d'en apprendre beaucoup sur la nébuleuse solaire primitive et les conditions dans lesquelles le Système solaire s'est formé. Trois hypothèses ont été proposées pour expliquer la haute métallicité de Mercure et son noyau gigantesque.
La théorie la plus largement acceptée à ce sujet est que Mercure avait à l'origine un rapport de métal sur silicate similaire à celui des météorites de chondrite communes, que l'on pense être typiques de la matière rocheuse du Système solaire, et avec une masse environ 2,25 fois supérieure à sa masse actuelle[71]. Ensuite, au début de l'histoire du Système solaire, Mercure aurait été frappée par un planétésimal d'environ 1/6e de cette masse et de plusieurs milliers de kilomètres de diamètre[71]. L'impact aurait enlevé une grande partie de la croûte et du manteau d'origine, laissant derrière lui le noyau métallique qui aurait fusionné avec celui du planétésimal, et un mince manteau. Un processus similaire, connu sous le nom d'hypothèse de l'impact géant, a été proposé pour expliquer la formation de la Lune[71] à la suite de la collision de la Terre avec l'impacteur Théia.
Alternativement, Mercure pourrait s'être formée à partir de la nébuleuse solaire avant que la production d'énergie du Soleil ne se soit stabilisée. Au départ, sa masse aurait été le double de celle actuelle mais lorsque la protoétoile s'est contractée, les températures à proximité de Mercure auraient pu se situer entre 2 500 et 3 500 K et peut-être même atteindre 10 000 K[72]. Une grande partie de la roche de surface de Mercure aurait ainsi pu être vaporisée à ces températures, formant une atmosphère de vapeur de roche qui aurait ensuite été emportée par le vent solaire[72].
Une troisième hypothèse suppose que la nébuleuse solaire aurait provoqué une traînée sur les particules à partir desquelles Mercure s'accrétait, ce qui signifie que des particules plus légères ont été perdues du matériau d'accrétion et n'ont pas été recueillies par Mercure[69]. Ainsi, le taux d'éléments lourds, comme le fer, présents dans la nébuleuse solaire était plus important au voisinage du Soleil, voire ces éléments lourds étaient distribués graduellement autour du Soleil (plus on s'en éloignait, moins il y avait d'éléments lourds). Mercure, proche du Soleil, aurait donc amassé plus de matériaux lourds que les autres planètes pour former son noyau[73].
Cependant, chaque hypothèse prévoit une composition de surface différente. La mission MESSENGER a trouvé des niveaux de potassium et de soufre plus élevés que prévu à la surface, ce qui suggère que l'hypothèse d'un impact géant et d'une vaporisation de la croûte et du manteau ne s'est pas produite car le potassium et le soufre auraient été chassés par la chaleur extrême de ces événements[73]. Ainsi, les résultats obtenus jusqu'à présent semblent favoriser la troisième hypothèse mais une analyse plus approfondie des données est nécessaire[74]. BepiColombo, qui arrivera en orbite autour de Mercure en 2025, fera des observations pour tenter d'apporter une réponse[75].
La surface de Mercure est couverte d'un tapis poussiéreux, de cassures et de cratères[30]. La surface de Mercure est similaire à celle de la Lune, montrant de vastes plaines de minéraux (silicates) ressemblant à des mers lunaires et de nombreux cratères, ce qui indique qu'elle est géologiquement inactive depuis des milliards d'années[76],[77]. Pour les astronomes, ces cratères sont très anciens et racontent l'histoire de la formation du Système solaire, lorsque les planétésimaux entraient en collision avec les jeunes planètes pour fusionner avec elles. Par opposition, certaines portions de la surface de Mercure semblent lisses, vierges de tout impact[78],[79]. Il s'agit probablement de coulées de lave recouvrant un sol plus ancien et plus marqué par les impacts. La lave, une fois refroidie, donnerait lieu à l'apparition d'une surface lisse, blanchâtre. Ces plaines datent d'une époque plus récente, postérieure à la période de bombardements intenses. La découverte des plaines volcaniques sur la surface permet de mettre en cause des chutes d'énormes astéroïdes atteignant le manteau, et pouvant créer en même temps des éruptions volcaniques à l'opposé de la planète.
La connaissance de la géologie de Mercure n'ayant été basée que sur le survol de la sonde Mariner 10 en 1975 et sur des observations terrestres, elle fut la moins bien connue des planètes telluriques jusqu'à 2011 et la mission MESSENGER[63]. Par exemple, un cratère inhabituel avec des creux rayonnants est découvert grâce à cette mission, que les scientifiques appellent un temps cratère de l'Araignée avant de le renommer Apollodorus[80],[81].
Mercure possède différents types de formations géologiques[82],[83],[84] :
Mercure a été lourdement bombardée par des comètes et des astéroïdes pendant et peu après sa formation, il y a 4,6 milliards d'années, ainsi que pendant un épisode ultérieur, peut-être distinct, appelé le Grand bombardement tardif, qui s'est terminé il y a 3,8 milliards d'années[85]. Pendant cette période de formation intense de cratères, Mercure subit des impacts sur toute sa surface, facilités par l'absence de toute atmosphère pour ralentir les impacteurs[86]. Aussi, Mercure est alors volcaniquement active ; des bassins tels que le bassin Caloris sont remplis de magma, produisant des plaines lisses semblables aux mers lunaires[78],[79]. Après le grand bombardement, l'activité volcanique de Mercure aurait cessé, soit environ 800 millions d'années après sa formation[87].
La surface de Mercure est plus hétérogène que celle de Mars ou de la Lune, qui contiennent toutes deux des étendues importantes de géologie similaire, comme les maria et les planitiae[88].
Le diamètre des cratères de Mercure varie de petites cavités en forme de bol à des bassins d'impact multi-annulaires de plusieurs centaines de kilomètres de diamètre. Ils apparaissent dans tous les états de dégradation, des cratères rayonnés relativement frais aux restes de cratères très dégradés. Les cratères de Mercure diffèrent subtilement des cratères lunaires en ce que la zone couverte par leurs éjections est beaucoup plus petite, conséquence de la plus forte gravité de Mercure à sa surface[89]. Selon les règles de l'UAI, chaque nouveau cratère doit porter le nom d'un artiste célèbre depuis plus de cinquante ans, et mort depuis plus de trois ans, avant la date à laquelle le cratère est nommé[90].
Le plus grand cratère connu est le bassin Caloris, avec un diamètre de 1 550 km (soit près du tiers du diamètre de la planète), qui fut formé à la suite de la chute d'un astéroïde d'une taille avoisinant les 150 km, il y a près de 3,85 milliards d'années[91]. Son nom (Caloris, « chaleur » en latin) vient du fait qu'il est situé sur l'un des deux « pôles chauds » de la surface de Mercure, pôles faisant directement face au Soleil lorsque la planète est au périhélie[15]. L'impact qui a créé le bassin Caloris a été si puissant qu'il a provoqué des éruptions de lave qui ont laissé un anneau concentrique de plus de 2 km de haut entourant le cratère d'impact. Il s'agit d'une grande dépression circulaire, avec des anneaux concentriques. Plus tard, de la lave a certainement coulé dans ce grand cratère, et en a lissé la surface.
À l'antipode du bassin Caloris se trouve une grande région de terrain très vallonnée et accidentée, de la taille de la France et de l'Allemagne réunies, connue sous le nom de « Terrain étrange » (en anglais Weird Terrain)[92]. Une hypothèse pour son origine est que les ondes de choc générées lors de l'impact de Caloris ont voyagé autour de Mercure, convergeant à l'antipode du bassin (à 180 degrés). Les fortes contraintes qui en ont résulté ont fracturé la surface, soulevant le sol à une hauteur de 800 à 1 000 m et produisant cette région chaotique[93],[94]. Une autre hypothèse est que ce terrain s'est formé à la suite de la convergence des éjecta volcaniques à l'antipode de ce bassin[95].
L'impact ayant créé le bassin Caloris a également contribué à la formation de l'unique chaîne de montagnes de Mercure : les Caloris Montes[96],[97].
Au total, environ 15 bassins d'impact sont identifiés sur Mercure. Un bassin notable est le bassin Tolstoï, de 400 km de large, avec de multiples anneaux et qui a une couverture d'éjectas s'étendant jusqu'à 500 km depuis son pourtour et dont l'apparition marque l'ère du Tolstoïen. Les bassins Rembrandt et Beethoven, ayant une couverture d'éjecta volcaniques de taille similaire, font également partie des plus gros cratères d'impact de la planète avec une largeur respective de 716 et 625 km[89].
Comme la Lune, la surface de Mercure a probablement subi les effets des processus d'érosion spatiale, notamment le vent solaire et les impacts de micrométéorites[89].
Mercure présente également des cratères fantômes partiellement d'origine tectonique formés de graben et de dorsum. Uniques dans le Système solaire, ils sont découverts en 2011[98].
Il existe deux régions de plaines géologiquement distinctes sur Mercure[89],[99],[100].
Premièrement, les plaines légèrement vallonnées dans les régions situées entre les cratères sont les plus anciennes surfaces visibles de Mercure[89], antérieures aux terrains fortement cratérisés. Ces plaines entre les cratères semblent avoir effacé de nombreux cratères plus anciens, et montrent une rareté générale de petits cratères de moins de 30 km de diamètre environ[99].
Deuxièmement, les plaines lisses sont de vastes zones plates qui remplissent des dépressions de tailles diverses et ressemblent beaucoup aux mers lunaires. Elles remplissent notamment un large anneau entourant le bassin Caloris. Contrairement aux mers lunaires, les plaines lisses de Mercure ont les mêmes albédos que les anciennes plaines entre les cratères. Malgré l'absence de caractéristiques volcaniques incontestables, la localisation et la forme arrondie et lobée de ces plaines soutiennent fortement des origines volcaniques[89]. Toutes les plaines lisses de Mercure se sont formées beaucoup plus tard que le bassin Caloris, comme indique leur densité de cratères sensiblement plus faible par rapport à celle de la couverture d'éjection de Caloris[89]. Le fond du bassin Caloris est rempli d'une plaine plate géologiquement distincte, fragmentée par des crêtes et des fractures selon un schéma à peu près polygonal. Il n'est pas clair s'il s'agit de laves volcaniques induites par l'impact ou des impactites[89].
Une caractéristique inhabituelle de la surface de Mercure est la présence de nombreux plis de compression appelés escarpements (ou Rupes) qui sillonnent les plaines. À la suite de la phase chaude de sa formation, c'est-à-dire après la fin du Grand bombardement tardif qui a un temps rendu toutes les planètes du système solaire des boules incandescentes[101], l'intérieur de Mercure s'est contracté et sa surface a commencé à se déformer, créant des crêtes[102]. Ces escarpements peuvent atteindre une longueur de 1 000 km et une hauteur de 3 km[103]. Ces caractéristiques de compression peuvent être observées simultanément avec d'autres caractéristiques, telles que des cratères et des plaines lisses, indiquant qu'elles sont plus récentes[104].
La cartographie des caractéristiques de Mercure grâce aux photographies prises par Mariner 10 a d'abord suggéré un rétrécissement total du rayon de Mercure de l'ordre de 1 à 2 km du fait de ces compressions[105], intervalle ayant plus tard été augmenté de 5 à 7 km, à la suite des données de MESSENGER[106],[107]. Aussi, des failles de poussée à petite échelle sont trouvées, d'une hauteur de plusieurs dizaines de mètres et d'une longueur de quelques kilomètres, qui semblent avoir moins de 50 millions d'années. Cela indique que la compression de l'intérieur et l'activité géologique de surface qui en résulte se poursuivent toujours à cette petite échelle[105],[108]. Après cette découverte, la supposée inactivité géologique de Mercure, et des petites planètes en général, pourrait être remise en cause[109].
Le Lunar Reconnaissance Orbiter découvre en 2019 l'existence de petites failles de poussée similaires sur la Lune[110].
Comme pour la Terre, la Lune ou Mars, l'évolution géologique de Mercure peut être divisée en grandes périodes ou époques[111]. Ces âges sont basés sur une datation relative uniquement, les dates avancées ne sont donc que des ordres de grandeur[89] .
Périodes géologiques de Mercure (en millions d'années) :
Il s'étend du tout début de l'histoire du système solaire à la période de bombardements intenses[112], soit de -4,5 à -3,9 milliards d'années. La nébuleuse solaire primitive s'est condensée et a commencé à former de la matière solide ; d'abord de petite masse qui à force de s'accumuler (processus d'accrétion) a produit des corps de plus en plus gros, ayant une force d'attraction de plus en plus importante, jusqu'à former la principale masse de Mercure. La nature homogène ou hétérogène de cette accumulation de matière reste encore inconnue : on ne sait pas si Mercure s'est formée à partir d'un mélange de fer et de silicate qui se sont ensuite dissociés pour former séparément un noyau métallique et un manteau de silicate, ou si le noyau s'est formé en premier, à partir de métaux, puis le manteau et la croûte ne sont venus qu'après, lorsque les éléments lourds comme le fer sont devenus moins abondants aux environs de Mercure. Il y a peu de chance pour que Mercure ait possédé une atmosphère initiale (juste après l'accumulation de matière), ou alors elle se serait évaporée très tôt avant l'apparition des plus anciens cratères. Si Mercure avait eu une atmosphère, on aurait pu remarquer une érosion des cratères par les vents, comme sur Mars[113]. Les escarpements présents majoritairement dans les régions « inter-cratères » (qui sont des surfaces plus anciennes que les cratères) et qui traversent parfois certains des plus vieux cratères, montrent que le refroidissement du noyau et la contraction de la planète se sont produits entre la fin de la première période et le début de la seconde[112].
La seconde période (de -3,9 à -3,85 milliards d'années) est caractérisée par un fort bombardement météoritique par des corps relativement gros (des résidus du processus d'accrétion), couvrant la surface de Mercure par des cratères et des bassins (cratères larges de plus de 200 km de diamètre), et se termine à la formation du bassin Caloris[114]. Il n'est pas certain que cette période soit la phase terminale de l'accrétion de Mercure ; il est possible qu'il ne s'agisse que d'un second épisode de bombardement indépendant de cette accumulation. D'autant plus que c'est l'époque du grand bombardement tardif[115]. Elle porte ce nom car elle a vu la formation du bassin Tolstoï.
La formation du bassin Caloris marque le commencement de cette période (de -3,85 à -3,80 milliards d'années). L'impact météoritique a donné lieu à de fortes transformations de la surface de Mercure : la création de l'anneau montagneux Caloris Montes autour du cratère produit par l'impact et les déformations chaotiques de l'autre côté de la planète[116]. L'asymétrie de la répartition interne des masses qu'il a occasionnée, à l'échelle de la planète, a été le pivot sur lequel se fonde la synchronisation des périodes rotation/révolution : le bassin Caloris est (avec son antipode) un des « pôles équatoriaux chauds ».
La quatrième époque géologique de Mercure s'étale de -3,80 à -3 milliards d'années et débute après la collision donnant lieu au bassin Caloris. Elle couvre la période de volcanisme qui s'ensuivit[114]. Des coulées de lave ont formé une partie des grandes plaines lisses, grossièrement similaires aux maria lunaires. Cependant, les plaines lisses recouvrant le bassin Caloris (Suisei, Odin, et Tir Planitia) auraient été formées par des éjectas lors de l'impact Caloris.
S'étendant respectivement de -3 milliards d'années à -1 milliard d'années puis depuis -1 milliard d'années à aujourd'hui, ces périodes sont marquées par de petits impacts météoritiques : peu d'événements majeurs se sont produits sur Mercure durant ces périodes[112]. Ces ères prennent également le nom de cratères : le Mansur et le Kuiper.
La présence de plaines plus jeunes (les plaines lisses) est la preuve que Mercure a connu dans son passé une activité volcanique[99]. L'origine de ces plaines est mise en évidence à la fin des années 1990 par Mark Robinson et Paul Lucey en étudiant les photographies de Mercure. Le principe est de comparer les surfaces lisses — formées à partir de coulées de laves — avec les autres, non lisses (et plus anciennes). S'il s'agissait bien d'éruptions volcaniques, ces régions devaient être d'une composition différente de celle qu'elles recouvraient, puisque composées de matériaux venant de l'intérieur de la planète[117].
Les images prises par Mariner 10 sont d'abord recalibrées à partir d'images prises en laboratoire avant le lancement de la sonde, et d'images prises durant la mission des nuages de Vénus (Vénus présente une texture plutôt uniforme) et de l'espace profond. Robinson et Lucey étudient ensuite divers échantillons de la Lune — qui aurait connu une activité volcanique similaire — et notamment la réflexion de la lumière afin de faire un parallèle entre la composition et la réflexion de ces matériaux[117].
À l'aide de techniques avancées de traitement d'images numériques (qui n'étaient pas possibles à l'époque de la mission Mariner 10), ils appliquent un code de couleurs aux images afin de différencier les matériaux minéraux sombres des matériaux métalliques. Trois couleurs sont utilisées : le rouge pour caractériser les minéraux opaques, sombres (plus le rouge est prononcé, moins il y a de minéraux sombres) ; le vert pour caractériser à la fois la concentration d'oxyde de fer (FeO) et l'intensité du bombardement de micrométéorites, également appelé « maturité » (la présence de FeO est moins importante, ou la région est moins mature, sur les portions plus vertes) ; le bleu pour caractériser le rapport UV/lumière visible (l'intensité de bleu augmente avec le rapport). La combinaison des trois images donne des couleurs intermédiaires. Par exemple, une zone en jaune peut représenter une combinaison d'une forte concentration en minéraux opaques (rouge) et d'une maturité intermédiaire (vert)[117].
Robinson et Lucey remarquent que les plaines sont marquées de couleurs différentes par rapport aux cratères et ils peuvent en déduire que ces plaines sont de composition différente par rapport aux surfaces plus anciennes (caractérisées par la présence de cratères). Ces plaines ont dû, à l'instar de la Lune, être formées par des coulées de lave. De nouvelles questions se posent alors quant à la nature de ces remontées de roche en fusion : celles-ci peuvent être de simples épanchements fluides, ou des éruptions explosives[118]. Cependant, toutes les plaines n'ont peut-être pas pour origine des coulées de lave. Il est possible que certaines se soient formées à partir de retombées de poussières et de fragments du sol, éjectés lors de gros impacts météoritiques[119].
Certaines éruptions volcaniques ont pu également se produire à la suite de grosses collisions. Dans le cas du bassin Caloris, le cratère généré par l'impact devait avoir à l'origine une profondeur de 130 km ; atteignant probablement le manteau et le faisant alors partiellement entrer en fusion lors du choc (du fait de pression et température très importantes). Le manteau est ensuite remonté lors du réajustement du sol, comblant le cratère. Ainsi, sachant qu'une partie de la surface de Mercure provient de son intérieur, les scientifiques peuvent en déduire des informations sur la composition interne de la planète[120].
Les images obtenues par MESSENGER, quant à elles, révèlent des preuves de nuées ardentes sur Mercure provenant de volcans boucliers de faible hauteur[121],[122],[123]. Ces données MESSENGER ont permis d'identifier 51 dépôts pyroclastiques à la surface, dont 90 % se trouvent dans des cratères d'impact[124]. Une étude de l'état de dégradation des cratères d'impact qui accueillent les dépôts pyroclastiques suggère que l'activité pyroclastique s'est produite sur Mercure pendant un intervalle prolongé[124].
Une « dépression sans rebord » à l'intérieur de la bordure sud-ouest du bassin Caloris se compose d'au moins neuf cheminées volcaniques qui se chevauchent, chacune pouvant atteindre individuellement jusqu'à 8 km de diamètre. Il s'agit donc d'un stratovolcan[125]. Les fonds des cheminées se trouvent à au moins 1 km sous leurs parois et ressemblent à des cratères volcaniques sculptés par des éruptions explosives ou modifiés par l'effondrement dans des espaces vides créés par le retrait du magma dans un conduit. L'âge du système complexe volcanique serait de l'ordre d'un milliard d'années[125].
Mercure est une planète très chaude. La température moyenne en surface est d'environ 440 K (167 °C)[2],[126]. C'est la température de stabilisation en dessous du régolite, où le sous-sol n'est plus soumis à l'alternance des « ondes » thermiques de la journée et de la nuit. Aussi, la température de surface de Mercure varie environ de 100 à 700 K (−173 à 427 °C)[127],[128]. Elle ne dépasse jamais 180 K aux pôles en raison de l'absence d'atmosphère et d'un fort gradient de température entre l'équateur et les pôles[129]. Le point subsolaire au périhélie, à savoir (0°N, 0°W) ou (0°N, 180°W)[N 7], atteint 700 K à ce moment mais seulement 550 K à l'aphélie (90° ou 270°W)[130]. Du côté non éclairé de la planète, la température moyenne est de 110 K[129],[131]. Depuis la surface de Mercure le soleil apparaît, en fonction de l'orbite elliptique, entre 2,1 et 3,3 plus gros que depuis la Terre, et l'intensité de la lumière solaire à la surface de Mercure varie entre 4,59 et 10,61 fois la constante solaire, c'est-à-dire que la quantité d'énergie reçue par une surface perpendiculaire au Soleil est en moyenne 7 fois plus élevée sur Mercure que sur Terre[130].
Bien que la température de la lumière du jour à la surface de Mercure soit généralement extrêmement élevée, il est possible que de la glace soit présente sur Mercure. En effet, du fait de l'inclinaison quasi nulle de son axe de rotation, les zones polaires de Mercure ne reçoivent des rayons solaires que rasants. Aussi, le fond des profonds cratères des pôles n'est alors jamais exposé à la lumière directe du soleil, et les températures y restent inférieures à 102 K grâce à cette obscurité permanente, soit bien moins que sur la température moyenne de la planète de 452 K[132]. À ces températures, la glace d'eau ne se sublime quasiment plus (la pression partielle de vapeur de la glace est très basse).
Des observations radar effectuées dans le début des années 1990 à partir du radiotélescope d'Arecibo et de l'antenne de Goldstone indiquent la présence de glace d'eau aux pôles Nord et Sud de Mercure[133]. En effet, la glace d'eau est caractérisée par des zones à réflexion radar élevée et une signature fortement dépolarisée, contrairement à la réflexion radar typique du silicate, constituant la majeure partie de la surface de Mercure. Aussi, il existe des zones de forte réflexion radar près des pôles[134]. Les résultats obtenus avec le radiotélescope d'Arecibo montrent que ces réflexions radar sont concentrées dans des taches circulaires de la taille d'un cratère. D'après les images prises par Mariner 10, la plus grosse d'entre elles, au pôle Sud, semble coïncider avec le cratère Chao Meng-Fu. D'autres, plus petites, correspondent également à des cratères bien identifiés.
On estime que les régions glacées contiennent environ 1014 à 1015 kg de glace[135],[136]. Celles-ci sont potentiellement recouvertes de régolite empêchant la sublimation[137]. En comparaison, la calotte glaciaire de l'Antarctique sur Terre a une masse d'environ 4 × 1018 kg et la calotte polaire sud de Mars contient environ 1016 kg d'eau[135]. Deux sources probables pour l'origine de cette glace sont envisagées : le bombardement météoritique ou le dégazage de l'eau de l'intérieur de la planète. Les météorites frappant la planète ont pu apporter de l'eau qui serait restée piégée (gelée par les basses températures des pôles) aux endroits où se sont produits les impacts. De même pour les dégazages, certaines molécules ont pu migrer vers les pôles et s'y retrouver piégées[135],[138].
Bien que la glace ne soit pas la seule cause possible de ces régions réfléchissantes, les astronomes pensent que c'est la plus probable[138]. La sonde BepiColombo, qui se mettra en orbite autour de la planète vers 2025, aura parmi ses tâches d'identifier la présence ou non de glace sur Mercure[139].
Mercure est trop petite et chaude pour que sa gravité ne puisse retenir une atmosphère significative sur de longues périodes[140]. Ainsi, elle est quasi inexistante à tel point que les molécules de gaz de l'« atmosphère » entrent plus souvent en collision avec la surface de la planète qu'avec d'autres molécules de gaz. Il est ainsi plus approprié de parler de son exosphère[141], commençant dès la surface de Mercure, directement « ouverte » sur l'espace. Celle-ci est ténue et limitée en surface[142], principalement composée de potassium, de sodium et d'oxygène (9,5 %). On y trouve aussi des traces d'argon, de néon, d'hydrogène et d'hélium[2],[143]. La pression de surface exercée est inférieure à 0,5 nPa (0,005 picobar)[2].
Cette exosphère n'est pas stable et est en réalité transitoire[144] : les atomes composant principalement l'exosphère de Mercure (potassium et sodium) ont une durée de vie (de présence) estimée à trois heures avant d'être libérés dans l'espace et d'une heure et demie lorsque la planète est au périhélie[145]. Ainsi, les atomes sont continuellement perdus et réapprovisionnés à partir de diverses sources.
Les atomes d'hydrogène et d'hélium proviennent probablement de la capture des ions du vent solaire, se diffusant dans la magnétosphère de Mercure avant de s'échapper à nouveau dans l'espace. La désintégration radioactive des éléments de la croûte de Mercure est une autre source d'hélium, ainsi que de sodium et de potassium[146]. De la vapeur d'eau est présente, libérée par une combinaison de processus tels que les comètes frappant sa surface, la pulvérisation cathodique (créant de l'eau à partir de l'hydrogène du vent solaire et de l'oxygène de la roche) et la sublimation à partir des réservoirs de glace d'eau dans les cratères polaires ombragés en permanence. La sonde MESSENGER a également détecté de grandes quantités d'ions liés à l'eau comme O+, OH-, et H3O+ [147],[148]. En raison des quantités de ces ions qui ont été détectées dans l'environnement spatial de Mercure, les astronomes supposent que ces molécules ont été soufflées de la surface ou de l'exosphère par le vent solaire[149],[150].
Le sodium, le potassium et le calcium sont découverts dans l'atmosphère au cours des années 1980-1990, le consensus étant qu'ils résultent principalement de la vaporisation de la roche de surface frappée par des impacts de micrométéorites[151], dont celle de la comète de Encke, qui créent un nuage zodiacal[152]. Toutefois, selon une autre hypothèse, la lumière solaire contribuerait aussi à libérer le sodium de la surface de la planète[153]. En 2008, du magnésium est découvert par MESSENGER[154],[155]. Des études indiquent que, parfois, les émissions de sodium sont localisées en des points qui correspondent aux pôles magnétiques de la planète. Cela indiquerait une interaction entre la magnétosphère et la surface de la planète[156].
Malgré sa petite taille et sa lente période de rotation de 59 jours, Mercure possède un champ magnétique notable. Révélé par les magnétomètres de Mariner 10, en mars 1974, il surprend les astronomes qui pensaient jusqu'à ce moment que Mercure était dépourvue de toute magnétosphère car sa vitesse de rotation lente diminue l'effet dynamo. De plus, il était supposé à l'époque que le noyau de la planète s'était déjà solidifié du fait de sa petite taille[67],[158]. L'intensité du champ magnétique à l'équateur de Mercure est d'environ 200 nT, soit 0,65 % du champ magnétique terrestre qui vaut 31 µT[159],[2]. Comme celui de la Terre, le champ magnétique de Mercure est dipolaire. Cependant, contrairement à la Terre, les pôles de Mercure sont alignés avec l'axe de rotation de la planète[160]. Les mesures des sondes spatiales Mariner 10 et MESSENGER indiquent que l'intensité et la forme du champ magnétique sont stables[160].
Il est probable que ce champ magnétique soit généré par un effet de dynamo, d'une manière similaire au champ magnétique de la Terre[62],[161]. Cet effet de dynamo résulterait de la circulation du noyau externe liquide riche en fer de la planète. Des effets de marée particulièrement forts, causés par la forte excentricité orbitale de la planète, permettraient de maintenir le noyau à l'état liquide nécessaire à cet effet de dynamo[66].
Le champ magnétique de Mercure est suffisamment puissant pour dévier le vent solaire autour de la planète, créant ainsi une magnétosphère située entre deux arcs de choc (ou « bow shock »)[157]. La magnétosphère de la planète, bien qu'assez petite pour être contenue dans le volume de la Terre[156], est assez forte pour piéger le plasma du vent solaire. Cela contribue à l'érosion spatiale de la surface de la planète[160]. Les observations effectuées par Mariner 10 ont permis de détecter ce plasma de faible énergie dans la magnétosphère du côté obscur de la planète. Les éclats de particules énergétiques dans la queue de la magnétosphère de la planète indiquent que celle-ci est dynamique[156]. De plus, des expériences menées par la sonde ont montré que, tout comme celle de la Terre, la magnétosphère de Mercure possède une queue séparée en deux par une couche neutre[162].
Lors de son deuxième survol de la planète le 6 octobre 2008, MESSENGER découvre que le champ magnétique de Mercure peut être extrêmement perméable. L'engin spatial rencontre en effet des « tornades » magnétiques[163] (des faisceaux tordus de champs magnétiques reliant le champ magnétique planétaire à l'espace interplanétaire) mesurant jusqu'à 800 km de large, soit un tiers du rayon de la planète. Ces tubes de flux magnétique torsadés forment des fenêtres ouvertes dans le bouclier magnétique de la planète à travers lesquelles le vent solaire peut entrer et impacter directement la surface de Mercure par reconnexion magnétique[164]. Cela se produit également dans le champ magnétique terrestre, cependant le taux de reconnexion est dix fois plus élevé sur Mercure[164].
La magnitude apparente de Mercure peut varier entre -2,48 (alors plus lumineuse que Sirius) lors de sa conjonction supérieure et +7,25 (dépassant alors la limite de visibilité à l’œil nu située à +6 et la rendant donc invisible) autour de la conjonction inférieure[154],[165]. La magnitude apparente moyenne est de 0,23 avec un écart-type de 1,78, c'est-à-dire le plus grand de toutes les planètes, du fait de la forte excentricité orbitale de la planète. La magnitude apparente moyenne à la conjonction supérieure est de -1,89 alors que celle à la conjonction inférieure est de +5,93[165]. L'observation de Mercure est compliquée du fait de sa proximité avec le soleil dans le ciel, car elle est alors perdue dans l'éblouissement de l'étoile. Mercure ne peut être observée que pendant une courte période de temps au moment de l'aube ou du crépuscule[166].
Comme plusieurs autres planètes et les étoiles les plus brillantes, Mercure peut être observée pendant une éclipse solaire totale[167]. De plus, comme la Lune et Vénus, Mercure présente des phases vues depuis la Terre. Elle est dite « nouvelle » à la conjonction inférieure et « pleine » à la conjonction supérieure. Cependant, la planète est rendue invisible depuis la Terre à ces deux occasions parce qu'elle est obscurcie par le Soleil (sauf durant un transit)[166]. Aussi, techniquement, Mercure est la plus brillante lorsqu'elle est pleine. Ainsi, bien que Mercure soit le plus éloigné de la Terre lorsqu'elle est pleine, elle présente une plus grande surface éclairée visible et l'effet d'opposition compense la distance[168]. L'inverse est vrai pour Vénus, qui apparaît plus brillante lorsqu'elle est en croissant parce qu'elle est beaucoup plus proche de la Terre[169].
Néanmoins, l'apparition la plus brillante (pleine phase) de Mercure est en réalité incompatible avec l'observation pratique, en raison de son extrême proximité de la planète avec le soleil à ce moment. Le meilleur moment pour observer Mercure est ainsi pendant son premier ou dernier quart, bien qu'il s'agisse de phases de moindre luminosité. Les premier et dernier quarts de phase se produisent lors de l'élongation la plus importante à l'est (vers septembre/octobre), et à l'ouest (vers mars/avril) du soleil, respectivement[170]. À ces deux moments, la séparation de Mercure du soleil varie entre 17,9° au périhélie et 27,8° à l'aphélie[170],[171]. À son élongation maximale à l'ouest, Mercure se lève avant le lever du soleil, et à son élongation maximale à l'est, elle se couche après le coucher du soleil, la rendant plus facilement observable[172],[173].
Mercure est plus facilement visible depuis les régions tropicales et subtropicales que depuis des latitudes plus élevées[174]. Vue des basses latitudes et aux bons moments de l'année, l'écliptique coupe l'horizon à un angle aigu. À ce moment, Mercure se trouve directement au-dessus du soleil (c'est-à-dire que son orbite semble verticale depuis la Terre) et elle est au maximum de son élongation par rapport au soleil (28°)[174]. Quand arrive le moment de la journée terrestre où le soleil est à 18° au-dessous de l'horizon de sorte que le ciel est complètement sombre (crépuscule astronomique), Mercure se trouve à un angle de 28-18=10° au-dessus de l'horizon dans un ciel complètement sombre : elle est alors à son maximum de visibilité pour un observateur terrestre.
De plus, les observateurs situés dans l'hémisphère sud sont avantagés par rapport à ceux du nord, avec une latitude de valeur absolue égale. En effet dans cet hémisphère, l'élongation maximale de Mercure à l'ouest (matin) ne se produit qu'au début de l'automne (mars/avril) et son élongation maximale à l'est (soir) ne se produit qu'à la fin de l'hiver (septembre/octobre)[173]. Dans ces deux cas, l'angle d'intersection de l'orbite de la planète avec l'écliptique (et donc l'horizon) est alors à son maximum pendant ces saisons[175], ce qui permet à Mercure de se lever plusieurs heures avant le lever du soleil dans le premier cas et de ne se coucher que plusieurs heures après le coucher du soleil dans le second, à partir des latitudes moyennes méridionales comme l'Argentine et l'Afrique du Sud[173]. À l'inverse, dans l'hémisphère nord, l'écliptique est bien moins incliné le matin en mars/avril et le soir en septembre/octobre, Mercure est donc très proche de l'horizon même lors de son élongation maximum[176] même s'il arrive qu'elle soit bien visible, près de Vénus, dans le ciel[177].
Une autre méthode pour observer Mercure consiste à observer la planète pendant les heures de jour lorsque les conditions sont claires, idéalement lorsqu'elle est à son plus grand allongement. Cela permet de trouver facilement la planète, même en utilisant des télescopes avec de faibles ouvertures. Il faut cependant prendre grand soin de veiller à ce que l'instrument ne soit pas pointé directement vers le Soleil en raison du risque de lésions oculaires[178]. Cette méthode permet de contourner la limitation de l'observation au crépuscule lorsque l'écliptique est située à faible altitude (par exemple les soirs d'automne).
D'une façon générale, les observations de Mercure grâce à un télescope au sol ne révèlent cependant qu'un disque partiel de couleur orange éclairé avec peu de détails[179]. La proximité de l'horizon rend son observation avec les télescopes difficile, car sa lumière doit parcourir une plus grande distance à travers l'atmosphère terrestre et est perturbée par des turbulences, comme la réfraction et l'absorption qui rendent l'image floue. La planète apparaît généralement dans le télescope sous la forme d'un disque en forme de croissant. Même avec des télescopes puissants, il n'y a pratiquement pas de caractéristiques distinctives à sa surface. D'autre part, Le télescope spatial Hubble ne peut pas du tout observer Mercure, en raison de procédures de sécurité qui empêchent son pointage trop près du Soleil[180],[181].
Un transit de Mercure se produit lorsque la planète se situe entre l'observateur et le Soleil. Elle est alors visible sous la forme d'un très petit point noir traversant le disque solaire. Il serait également possible pour un observateur situé sur une autre planète de voir un transit, tel que le transit de Mercure depuis Vénus. Les transits de Mercure vus depuis la Terre ont lieu avec une fréquence relativement régulière à l'échelle astronomique d'environ 13 ou 14 par siècle[182], en raison de la proximité de la planète au Soleil.
Le premier transit de Mercure est observé le 7 novembre 1631 par Pierre Gassendi, bien que son existence ait été prévue par Johannes Kepler avant sa mort en 1630[183]. En 1677, l'observation du transit de Mercure permet pour la première fois de mettre en avant le phénomène de la goutte noire[184], un effet de la diffraction des instruments optiques.
Le transit de Mercure a également permis de réaliser différentes mesures, dont celle de la taille de l'univers[185] ou des variations à long terme du rayon du Soleil[186],[187].
Les transits peuvent se produire en mai à des intervalles de 13 ou 33 ans, ou en novembre tous les 7, 13 ou 33 ans. Les quatre derniers transits de Mercure datent du 7 mai 2003, du 8 novembre 2006, du 9 mai 2016 et du 11 novembre 2019 ; les quatre prochains auront lieu le 13 novembre 2032, le 7 novembre 2039, le 7 mai 2049 et le 9 novembre 2052[188],[189].
Mercure est connue depuis que les hommes s'intéressent au ciel nocturne ; la première civilisation à en avoir laissé des traces écrites est la civilisation sumérienne[190] (IIIe millénaire av. J.-C.) qui la nommait « Ubu-idim-gud-ud »[191] (signifiant la « planète sautante »[192]).
Les premiers écrits d'observations détaillées de Mercure nous viennent des Babyloniens avec les tablettes de Mul Apin. Les Babyloniens appellent cet astre Nabû en référence au dieu du savoir dans la mythologie mésopotamienne. Ils sont également les premiers à avoir étudié le mouvement apparent de Mercure, qui est différent de celui des autres planètes[193],[191].
Plus tard, dans l'Antiquité, les Grecs, héritiers des conceptions indo-européennes (paléoastronomie) considèrent jusqu'au IVe siècle av. J.-C. que Mercure visible avant le lever du soleil d'une part et Mercure visible après son coucher d'autre part relevaient de deux astres distincts. Ceux-ci sont appelés respectivement Στίλβων / Stílbōn, signifiant « celui qui brille »[194],[195] et Ἑρμῆς / Hermês (Hermès est le messager des dieux) en raison de son mouvement rapide[196]. Ce dernier est d'ailleurs toujours le nom de la planète en grec moderne. L'étoile du matin aurait également été appelée Ἀπόλλων / Apóllōn (Apollon)[197]. Les Égyptiens procédèrent de même en donnant le nom de Seth à l'étoile du matin et Horus à celle du soir[198],[199].
Les Romains[200] nommèrent la planète du nom du messager des dieux Mercure (en latin Mercurius), équivalent d'Hermès pour la mythologie romaine, parce qu'elle se déplace dans le ciel plus vite que toutes les autres planètes[31],[201]. Aussi dieu protecteur des commerçants, des médecins et des voleurs, le symbole astronomique de Mercure est une version stylisée du caducée d'Hermès[202]. Il est également supposé que le symbole proviendrait d'une dérivation de la première lettre de son nom grec ancien Στίλβων (Stilbōn)[203].
Ferry, un contributeur du Dictionnaire de Wahlen, écrit à ce sujet :
« Pourquoi donc une planète aussi peu importante dans le système dont elle fait partie porte-t-elle le nom du messager des dieux dans l'Olympe mythologique ? C'est qu'elle se trouve assez fréquemment en conjonction avec les autres planètes entre lesquelles ces rapprochements sont beaucoup plus rares. Comme la durée de sa révolution autour du Soleil ou son année n'est que le quart de l'année terrestre, dans ce court espace de temps on la voit se diriger vers une planète et après s'en être approchée s'éloigner pour faire une autre visite aussi promptement terminée. La fréquente répétition de cette sorte de voyages a pu faire concevoir l'idée d'un autre messager. »[204]
L'astronome gréco-égyptien Ptolémée évoque la possibilité de transits planétaires devant le Soleil dans son ouvrage Hypothèses planétaires. Il suggère que si aucun passage n'avait jamais été observé, cela était soit parce que des planètes telles que Mercure étaient trop petites pour être vues, soit parce que les passages étaient trop peu fréquents[205].
Dans la Chine ancienne, Mercure est connue sous le nom de  (Chen-xing 辰星)[206]. Elle est associée à la direction du nord et à la phase de l'eau dans le système de cosmologie des Cinq Phases (Wuxing)[207],[208]. Les cultures modernes chinoise, coréenne, japonaise et vietnamienne désignent la planète littéralement comme  (水星), basée sur les Cinq éléments[209]. La mythologie hindoue utilise le nom de Bouddha pour Mercure, et l'on pense que ce dieu présidait le mercredi[210],[211]. Le dieu Odin de la mythologie nordique est aussi associé avec la planète Mercure et au mercredi[212]. Ce lien avec le troisième jour de la semaine se retrouve également chez les Romains, ce qui a ensuite donné en français le nom Mercredi (pour « Mercurii dies », le jour de Mercure)[213],[214].
La civilisation maya aurait représenté Mercure comme un hibou (ou potentiellement quatre, deux représentant son apparition du matin et deux celle du soir) servant de messager vers le monde souterrain[215].
En astronomie arabe, l'astronome Al-Zarqali décrit au XIe siècle l'orbite géocentrique de Mercure comme étant une ellipse, bien que cette intuition n'ait pas influencé sa théorie astronomique ou ses calculs astronomiques[216],[217]. Au XIIe siècle, Ibn Bajja observe , ce qui a été plus tard suggéré comme le transit de Mercure et/ou de Vénus par l'astronome de Maragha Qotb al-Din Chirazi au XIIIe siècle[218],[219]. Cependant, des doutes sont soulevés par les astronomes plus récents sur l'observation des transits par les astronomes médiévaux arabes, ceux-ci ayant été potentiellement confondus avec des taches solaires[220],[221]. Ainsi, toute observation d'un transit de Mercure avant les télescopes reste spéculative[220].
En Inde, l'astronome Nilakantha Somayaji de l'école du Kerala développa au XVe siècle un modèle partiellement héliocentrique dans lequel Mercure orbite autour du Soleil, qui à son tour orbite autour de la Terre, similairement au système tychonique de Tycho Brahe proposé ensuite au XVIe siècle[222].
Les premières observations télescopiques de Mercure sont faites par Galilée au début du XVIe siècle[223]. Bien qu'il ait observé des phases lorsqu'il a regardé Vénus, son télescope n'est pas assez puissant pour voir les phases de Mercure. En 1631, Pierre Gassendi fait les premières observations télescopiques du transit d'une planète à travers le Soleil lorsqu'il voit un transit de Mercure prédit par Johannes Kepler[224]. En 1639, Giovanni Zupi utilise un télescope pour découvrir que la planète présente des phases similaires à celles de Vénus et de la Lune. L'observation démontre de façon concluante que Mercure orbite autour du Soleil[14],[225].
Un événement rare en astronomie est le passage d'une planète devant une autre vu de la Terre (occultation). Mercure et Vénus s'occultent l'une l'autre tous les quelques siècles et l'événement du 28 mai 1737 est le seul à avoir été observé historiquement, ayant été vu par John Bevis à l'Observatoire royal de Greenwich[226]. La prochaine occultation de Mercure par Vénus aura lieu le 3 décembre 2133[227].
Les difficultés inhérentes à l'observation de Mercure font qu'elle a été beaucoup moins étudiée que les autres planètes. En 1800, Johann Schröter fait des observations de sa surface, affirmant avoir observé des montagnes de 20 kilomètres de haut[228]. Friedrich Bessel utilise les dessins de Schröter pour estimer à tort la période de rotation comme étant de 24 heures et une inclinaison axiale de 70°. Dans les années 1880, Giovanni Schiaparelli cartographie la planète avec plus de précision et suggère que la période de rotation de Mercure est de 88 jours, la même que sa période orbitale en raison d'une rotation synchrone. L'effort de cartographie de la surface de Mercure est poursuivi par Eugène Antoniadi, qui publie en 1934 un livre comprenant à la fois des cartes et ses propres observations. De nombreuses caractéristiques de la surface de la planète, en particulier les formations d'albédo, tirent leur nom de la carte d'Antoniadi[229].
En juin 1962, les scientifiques soviétiques de l'Institut de radio-ingénierie et d'électronique de l'Académie des sciences de l'URSS, dirigé par Vladimir Kotelnikov, sont les premiers à faire rebondir un signal radar sur Mercure et à le recevoir, ce qui permit de commencer les observations radar de la planète[230],[231],[232]. Trois ans plus tard, les observations radar des Américains Gordon H. Pettengill et Rolf B. Dyce, à l'aide du radiotélescope de 300 mètres de l'observatoire d'Arecibo à Porto Rico, montrent de façon concluante que la période de rotation de la planète est d'environ 59 jours[233],[234]. La théorie selon laquelle la rotation de Mercure est synchrone était à cette époque répandue et ce fut donc une surprise pour les astronomes lorsque ces observations radio furent annoncées. Si Mercure était réellement verrouillée comme on le pensait auparavant, sa face obscure aurait été extrêmement froide, mais les mesures des émissions radio révèlent qu'elle est beaucoup plus chaude que prévu. Les astronomes hésitent un temps à abandonner la théorie de la rotation synchrone et proposent des mécanismes alternatifs tels que des vents puissants de distribution de la chaleur pour expliquer les observations[235].
L'astronome italien Giuseppe Colombo note que la période de rotation est d'environ deux tiers de la période orbitale de Mercure, et il est le premier à proposer que les périodes orbitales et de rotation de la planète soient verrouillées dans une résonance de 3:2 plutôt que de 1:1[34], comme c'est le cas entre la Terre et la Lune par exemple. Les données de Mariner 10 ont par la suite confirmé ceci[236].
Les observations optiques au sol n'ont pas permis d'en savoir beaucoup plus sur Mercure, mais les radioastronomes utilisant l'interférométrie micro-ondes, une technique qui permet d'éliminer le rayonnement solaire, ont pu discerner les caractéristiques physiques et chimiques des couches souterraines à une profondeur de plusieurs mètres[237],[238]. En 2000, des observations à haute résolution dites de lucky imaging sont effectuées par un télescope de l'observatoire du mont Wilson. Elles fournissent les premières vues permettant de connaître les caractéristiques de surface des parties de Mercure qui n'avaient pas été imagées lors de la mission Mariner 10[239]. La majeure partie de la planète est cartographiée par le télescope radar d'Arecibo, y compris les dépôts de ce qui pourrait être de la glace d'eau dans les cratères polaires ombragés[240].
Le premier astronome à avoir discerné des caractéristiques géologiques de Mercure est Johann Hieronymus Schröter qui, vers la fin du XVIIIe siècle, dessine en détail ce qu'il avait pu observer, dont des très hautes montagnes. Ses observations sont cependant infirmées par William Herschel qui ne put voir aucune de ces caractéristiques[228].
Par la suite, d'autres astronomes dressent des cartes de Mercure, dont l'italien Giovanni Schiaparelli et l'américain Percival Lowell (en 1896). Ils y voient des zones sombres en formes de lignes, similaires aux canaux de Mars qu'ils avaient également dessiné et qu'ils pensaient être artificiels[228],[241].
Carte de Giovanni Schiaparelli.
Carte de Percival Lowell (1896).
Carte d'Eugène Antoniadi (1934).
La meilleure carte d'avant Mariner 10 provient du Franco-grec Eugène Antoniadi, au début des années 1930[242]. Elle est utilisée pendant près de 50 ans jusqu'à ce que Mariner 10 renvoie les premières photos de la planète[243]. Antoniadi montre que les canaux n'étaient qu'une illusion d'optique[244]. Il reconnaît que l'élaboration d'une carte précise de Mercure est impossible à partir d'observations effectuées à l'aube ou au crépuscule à cause des perturbations atmosphériques (l'épaisseur d'atmosphère terrestre que la lumière doit traverser lorsque Mercure se trouve à l'horizon est importante et crée des distorsions de l'image). Il entreprend alors de faire des observations — dangereuses — en plein jour lorsque le soleil est bien au-dessus de l'horizon. Il gagne ainsi en netteté, mais perd en contraste à cause de la lumière du soleil. Antoniadi parvient tout de même à achever sa carte en 1934, composée de plaines et de montagnes[245].
Les coordonnées utilisées sur ces cartes ont peu d'importance dans la mesure où elles ont été établies alors qu'on pensait, comme Schiaparelli l'avait affirmé, que la période de rotation de Mercure sur elle-même était la même que la période de révolution autour du Soleil. Il s'agit donc de la face supposée toujours illuminée qui a été cartographiée[246]. Seuls Lowell et Antoniadi avaient annoté leurs cartes[245].
En 1974–75, Mariner 10 rapporte des photographies en haute résolution permettant la cartographie d'environ 45 % de sa surface[247],[30], révélant les détails topographiques jamais vus auparavant : une surface recouverte de cratères avec des montagnes et des plaines, très ressemblante à celle de la Lune[30]. Il est assez difficile de faire une corrélation entre les caractéristiques photographiées par la sonde et les cartes établies par télescope. Certaines des manifestations géologiques de la carte d'Antoniadi se sont révélées inexistantes[247]. Aussi, ces photographies permettent la publication en 1976 du premier atlas de la planète par la NASA (Atlas of Mercury), révélant pour la première fois les formations géologiques de la planète dont, par exemple, son unique chaîne de montagnes : Caloris Montes[97].
L'Union astronomique internationale définit en 1970 le méridien 0° comme étant le méridien solaire au premier périhélie après le 1er janvier 1950, c'est-à-dire à l'un des deux points chauds. Cependant, le système de coordonnées utilisé par Mariner 10 se fonde sur le méridien 20° qui coupe le cratère Hun Kal (signifiant « 20 » en maya)[248] — ce qui donne une légère erreur de moins de 0,5° par rapport au méridien 0° défini par l'UAI — car le méridien 0 était dans l'obscurité lors de ses survols[249],[250]. Le cratère Hun Kal est en quelque sorte le Greenwich de Mercure. L'équateur se trouve dans le plan de l'orbite de Mercure et les longitudes sont mesurées de 0° à 360° en allant vers l'ouest[251]. Ainsi, les deux points les plus chauds de l'équateur se trouvent aux longitudes 0° O et 180° O, et les points les plus froids de l'équateur se trouvent aux longitudes 90° O et 270° O. À l'inverse, le projet MESSENGER utilise une convention positive vers l'est[252].
Mercure est découpée en 15 quadrangles. Plusieurs méthodes de projection sont utilisées pour cartographier la surface de Mercure, suivant la position du quadrangle sur le globe. Cinq projections Mercator (projection cylindrique tangente à l'équateur) entourent la planète au niveau de l'équateur, entre les latitudes 25° nord et 25° sud ; quatre projections Lambert (projection conique) entre 20° et 70° de latitude pour chaque hémisphère ; et deux projections stéréographiques pour cartographier les pôles (jusqu'à 65° de latitude)[253].
Chaque quadrangle commence par la lettre H (pour « Hermès »), suivie de son numéro (de 1, pôle Nord, à 15, pôle Sud). Leur nom provient d'une caractéristique importante présente sur leur région (bassin, cratère, etc.) et un nom d’albédo (entre parenthèses) leur est attribué[253]. Les noms d’albédos assignés pour cette nouvelle carte proviennent de celle d'Antoniadi, puisque c'était celle utilisée jusque là par tous les observateurs depuis plusieurs décennies[229]. Ils servent pour repérer les quadrangles lors des observations au télescope depuis la Terre, où l'on ne distingue que les variations d'intensité de lumière.
En 2016, grâce à plus de 100 000 images prises par la sonde MESSENGER, la NASA fournit le premier modèle topographique de Mercure[254]. Celui-ci donne les points d'élévation maximales et minimales de la planète, respectivement à 4,48 km au-dessus de l'élévation moyenne situé sur un des terrains les plus anciens de la planète près de l'équateur et à 5,38 km sous l'élévation moyenne de la planète, au fond du bassin Rachmaninoff[254].
Atteindre Mercure depuis la Terre pose des défis techniques importants, car elle orbite beaucoup plus près du Soleil que la Terre[255]. Cela implique qu'une sonde se rendant sur Mercure doit dépenser plus d'énergie que pour se rendre sur Pluton[255].
Mercure possède une vitesse orbitale de 48 km/s, alors que la vitesse orbitale de la Terre est de 30 km/s. Par conséquent, l'engin spatial doit effectuer un grand changement de vitesse Delta-v pour entrer dans une orbite de transfert de Hohmann qui passe près de Mercure, par rapport au Delta-v requis pour d'autres missions planétaires[256]. De plus, il est nécessaire de se placer dans le plan orbital de Mercure, qui est incliné de 7° par rapport à l'écliptique, ce qui nécessite aussi de l'énergie[257].
L'énergie potentielle libérée en descendant le puits de potentiel du Soleil devient de l'énergie cinétique : une grande variation négative de vitesse devient alors nécessaire pour ralentir et se mettre en orbite stable[258]. Du fait de l'atmosphère négligeable de Mercure, un véhicule spatial dépend entièrement de ses moteurs à réaction, l'aérofreinage étant exclu[259]. Pour ces raisons, une mission impliquant un atterrissage sur Mercure est très difficile, raison pour laquelle cela n'a encore jamais été fait[260].
Cependant, les progrès dans le domaine de la mécanique spatiale rendent ce type de mission réalisable à un coût raisonnable grâce à un enchaînement de manœuvres d’assistance gravitationnelle[261],[262].
Aussi, la proximité de Mercure avec le Soleil implique qu'une sonde orbitant autour de la planète reçoit environ dix fois plus d’énergie du Soleil que lorsqu'elle se situe sur une orbite terrestre[263] et le sol de Mercure sur sa face éclairée réfléchit une grande partie de la chaleur qu’il reçoit du Soleil[259], accroissant les contraintes thermiques subies par un engin à basse altitude (les températures pouvant dépasser 400 °C à la surface de la sonde)[263].
Ces difficultés impliquent qu'un voyage vers Mercure nécessite plus de carburant que ce qui est nécessaire pour s'échapper complètement du Système solaire. Par conséquent, son exploration a été plus tardive que des planètes telles que Vénus ou Mars et seules deux sondes spatiales l'ont visité avant l'arrivée de BepiColombo prévue pour 2025[264].
Mariner 10 est la première sonde à étudier Mercure de près[265]. Développée par l'agence spatiale américaine, la NASA, et lancée le 3 novembre 1973, elle survole la planète à trois reprises, en mars et septembre 1974 et en mars 1975[30],[266]. À l'origine, elle est destinée à survoler et étudier Vénus, mais les astronomes pensent qu'ils pourraient en faire usage également pour étudier Mercure, dont on connaissait peu de choses. Mariner 10 est ainsi la première sonde à avoir utilisé l'assistance gravitationnelle d'une planète — Vénus — pour en atteindre une autre[267].
Équipée d’une caméra, d’un magnétomètre et de plusieurs spectromètres, Mariner 10 permet notamment la découverte d’un champ magnétique significatif et de la forte densité de la planète, révélatrice d’un noyau ferreux de grande taille. Les télescopes terrestres les plus puissants n’avaient pas permis d’obtenir des images de qualité de la surface, du fait de la proximité de l’alignement avec le Soleil. La sonde prend, durant ces trois passages, plus de 2 000 photographies[268] de Mercure. Les photos prises par Mariner 10 permettent cependant seulement de cartographier près de 45 % de la surface de la planète, car lors des trois passages Mercure présentait la même face au Soleil ; les régions à l'ombre étaient donc impossibles à cartographier. Ces images révèlent une surface couverte de cratères, à l’apparence très proche de celle de la Lune[269].
Mariner 10 permet de découvrir la présence d'une très mince atmosphère, ainsi qu'une magnétosphère. Cette dernière fut une surprise pour les astronomes. Elle apporte également des précisions sur sa vitesse de rotation. La mission arrive à terme le 24 mars 1975, lorsque la sonde se trouva à court de carburant. Comme son orbite ne peut plus être contrôlée avec précision, les contrôleurs de mission ordonnent à la sonde de s'éteindre[270]. Mariner 10 serait ainsi toujours en orbite autour du Soleil, passant près de Mercure tous les quelques mois[266].
MESSENGER (pour MErcury Surface, Space ENvironment, GEochemistry, and Ranging[271]) est la septième mission du programme Discovery, qui rassemble des projets d’exploration du Système solaire à coût modéré et durée de développement courte. La sonde, dont la masse, ergols compris, est de 1,1 tonne, emporte sept instruments scientifiques, dont plusieurs spectromètres, un altimètre laser, un magnétomètre et des caméras[257]. Elle est lancée le 3 août 2004 de Cap Canaveral, à bord d'un lanceur Delta II, le lancement ayant été décalé d'un jour pour cause de mauvais temps[272].
Il faut environ six ans et demi à la sonde avant qu'elle n'entre en orbite autour de Mercure[257]. Pour y parvenir, elle effectue durant son transit six survols rapprochés des planètes intérieures (la Terre en février 2005, Vénus à deux reprises en octobre 2006 et 2007 et Mercure à trois reprises, en janvier et octobre 2008 et en septembre 2009), avec quelques corrections de trajectoire intermédiaires. Lors de ces survols de Mercure, suffisamment de données sont recueillies pour produire des images de plus de 95 % de sa surface. MESSENGER observe par ailleurs le maximum solaire de 2012[257].
L’objectif de la mission est d’effectuer une cartographie complète de la planète[273], d’étudier la composition chimique de sa surface et de son exosphère, son histoire géologique, sa magnétosphère, la taille et les caractéristiques de son noyau ainsi que l’origine de son champ magnétique[274].
La fin de la mission, fixée initialement à mars 2011, est repoussée par deux fois jusqu'en avril 2015, et dans la phase finale, la sonde spatiale est placée sur une orbite plus rapprochée, permettant d'allonger le temps d'observation de ses instruments et d’accroître la résolution des données[275]. MESSENGER, après avoir épuisé les ergols utilisés pour maintenir son orbite, s'écrase sur le sol de Mercure le 30 avril 2015[276],[277].
Durant sa mission, MESSENGER prend plus de 277 000 photos[278], dont certaines possédant une résolution de 250 mètres par pixel, et permet de produire des cartes de sa composition globale, un modèle en trois dimensions de la magnétosphère, la topographie de l'hémisphère nord et caractériser les éléments volatils présents dans les cratères constamment ombragés des pôles[279].
À partir des années 2000, l'Agence spatiale européenne planifie en collaboration avec l'Agence spatiale japonaise une mission baptisée BepiColombo[280]. Celle-ci prévoit de placer deux sondes en orbite autour de Mercure : l'une pour l'étude de l'intérieur et de la surface de la planète (Mercury Planetary Orbiter), développé par l'ESA, et l'autre pour étudier sa magnétosphère (Mercury Magnetospheric Orbiter), développé par la JAXA[281]. Un projet d'envoi d'un atterrisseur embarqué avec la mission est prévu puis abandonné, pour des raisons budgétaires. Ces deux sondes sont envoyées par un lanceur Ariane 5 le 20 octobre 2018[282],[283]. Elles devraient rejoindre Mercure environ huit ans plus tard, fin 2025, en utilisant, comme les sondes précédentes, l'assistance gravitationnelle[284]. Sa mission principale durera jusqu'en mai 2027, avec une prolongation possible jusqu'en mai 2028[282].
Le programme BepiColombo a pour objectif de répondre à une douzaine de questions que se posent les astronomes[285], notamment au sujet de la magnétosphère et de la nature du noyau de Mercure (liquide ou solide), de la possible présence de glace au fond des cratères constamment à l'ombre, de la formation du Système solaire et de l'évolution en général d'une planète au voisinage de son étoile[281]. Des mesures très précises du mouvement de Mercure vont également être effectuées, afin de vérifier la théorie de la relativité générale, explication actuelle de la précession du périhélie observée dans son orbite[286].
La planète Mercure est un lieu récurrent dans les œuvres de science-fiction[287],[288]. Des thèmes courants liés à cette planète incluent les dangers d'être exposé au rayonnement solaire et la possibilité d'échapper à un rayonnement excessif en restant dans le terminateur lent de la planète (la frontière entre le jour et la nuit), notamment pour les œuvres écrites avant 1965, alors que l'on pensait encore que Mercure possédait une rotation synchrone 1:1 avec le Soleil (et avait donc une face en permanence vers le Soleil), comme dans Cercle vicieux d'Isaac Asimov, ou dans les nouvelles de Leigh Brackett[287]. Un autre thème abordé est celui des gouvernements autocratiques ou violents, avec par exemple Rendez-vous avec Rama d'Arthur C. Clarke[289]. Bien que ces récits soient fictifs, d'après des études publiées en mars 2020, il est possible de considérer que des parties de la planète peuvent avoir été habitables. Ainsi, des formes de vie réelles, bien que probablement des micro-organismes primitifs, ont peut-être existé sur la planète[290],[291].
De plus, un cratère, au pôle nord ou au pôle sud de Mercure, serait peut-être l'un des meilleurs endroits extraterrestres pour l'établissement d'une colonie humaine, là où la température resterait constante à environ −200 °C[289]. Ceci est dû à une inclinaison axiale quasi nulle de la planète, et au vide quasi parfait à sa surface, empêchant l'apport de chaleur depuis les portions éclairées par le Soleil. De plus, de la glace se trouve dans ces cratères, permettant un accès à l'eau pour la colonie[292].
Une base n'importe où ailleurs serait exposée, en journée mercurienne (durant environ trois mois terrestres), à la chaleur intense du Soleil, puis durant une période nocturne identique, serait privée de la moindre source de chaleur extérieure : elle connaîtrait alors des températures diurnes de 430 °C et des températures nocturnes de −180 °C[289],[293]. Cependant, pour éviter ces variations thermiques, les installations pourraient être enterrées sous plusieurs mètres de régolithe qui, dans le vide, servirait aussi bien d'isolant thermique que de bouclier antiradiations. Des approches similaires ont été proposées pour l'installation de bases sur la Lune[294], dont le jour dure deux semaines, suivi d'une nuit de deux semaines également. D'une façon générale, la colonisation de Mercure revêt certaines similarités avec celle de la Lune, du fait de leur relativement grande période autour du Soleil, de leur inclinaison quasi nulle et de leur absence d'atmosphère : la colonisation de Mercure pourrait se faire avec presque les mêmes technologies[295]. Mercure aurait même un avantage par rapport à la Lune : la gravité étant sur la planète 38 % de celle de la Terre, cela est suffisant pour éviter aux astronautes la réduction de masse osseuse se produisant dans un environnement à très faible gravité[289].
Par ailleurs, la planète étant proche du Soleil, il serait possible de capter de grandes quantités d'énergie le jour, et de s'en servir ensuite la nuit[289]. En revanche, la protection des robots et des véhicules contre la chaleur de l'étoile pourrait poser beaucoup plus de difficultés, entraînant une limitation des activités en surface durant le jour ou une très importante protection thermique[263].
Une autre solution est évoquée dans les romans et les nouvelles de Kim Stanley Robinson, en particulier dans La Trilogie de Mars (1996) et 2312 (2012), où Mercure est le foyer d'une vaste ville appelée Terminator, peuplée d'un grand nombre d'artistes et de musiciens. Pour éviter le dangereux rayonnement solaire, la ville fait le tour de l'équateur de la planète sur des rails à une vitesse suivant la rotation de la planète, afin que le soleil ne se lève jamais complètement au-dessus de l'horizon. Une ville située du côté obscur de la planète, et suivant la lente rotation de la planète sur rails pour précéder le soleil est ainsi une solution réellement envisagée[293].
Finalement, une colonisation de Mercure revêtirait un intérêt économique, car il y réside des concentrations de minerais bien plus élevées que sur toutes les autres planètes du Système solaire[293].
« Mercury's crust is more analogous to a marbled cake than a layered cake »« Sean C. Solomon, the principal investigator for MESSENGER, said there was enough ice there to encase Washington, D.C., in a frozen block two and a half miles deep. »« The symbol for Mercury represents the Caduceus, a wand with two serpents twined around it, which was carried by the messenger of the gods. »
Cet article concerne le sens biologique du mot « espèce ». Pour les autres significations, voir Espèce (homonymie).
Dans les sciences du vivant, l’espèce (du latin species, « type » ou « apparence ») est le taxon de base de la systématique. La définition la plus communément admise est celle du concept biologique 
[1],[2],[3] : une espèce est un ensemble d'individus qui peuvent effectivement ou potentiellement se reproduire entre eux et engendrer une descendance viable et féconde, dans des conditions naturelles, mais une publication de 1997 indique qu'il existe 22 concepts d'espèces (espèce biologique, morphologique, écologique, comportementale…) dans la littérature scientifique[4]. Ainsi, l'espèce est la plus grande unité de population au sein de laquelle le flux génétique est possible et les individus d'une même espèce sont donc génétiquement isolés d'autres ensembles équivalents du point de vue reproductif.
Pourtant, le critère d’interfécondité ne peut pas toujours être vérifié : c'est le cas pour les fossiles, les organismes asexués ou pour des espèces rares ou difficiles à observer. D’autres définitions peuvent donc être utilisées[5] :
L'espèce est un concept flou dont il existe une multitude de définitions dans la littérature scientifique. Dans son sens le plus simple, le concept de l'espèce permet de distinguer les différents types d'organismes vivants. Différentes définitions permettent d'identifier plus précisément les critères distinctifs de l'espèce. L’évolution est la différence morphologique et génétique que l’on observe d’une génération à l’autre entre ascendants et descendants, qui ne sont jamais identiques sauf en cas de clonage, et ce sont aussi les changements dans l’effectif, l'aire de répartition et les comportements d’un groupe d'individus vivants[6]. En outre, ce nom a pu changer en raison de nouvelles découvertes, descriptions ou analyses : ainsi, un même taxon peut avoir plusieurs dénominations successives et il arrive aussi que plusieurs espèces soient identifiées là où auparavant on n'en voyait qu'une, ou inversement, que l'on regroupe au sein d'une même espèce plusieurs noms (et types) différents (par exemple larves et adultes, ou bien mâles et femelles).
Avec le temps, les conditions et indications à réunir pour définir une espèce sont devenues plus nombreuses et strictes. Même si les citoyens et les pouvoirs publics n'en sont pas toujours conscients, la formation des spécialistes en classification (taxonomie) est essentielle pour la précision et la rigueur des travaux scientifiques concernant la biodiversité (mais aussi la minéralogie, la géologie et la paléontologie).
La définition la plus communément citée est celle du concept biologique de l'espèce énoncé par Ernst Mayr (1942)[7] : « Les espèces sont des groupes de populations naturelles, effectivement ou potentiellement interfécondes, qui sont génétiquement isolées d’autres groupes similaires »[8]. À cette définition, il a ensuite été rajouté que cette espèce doit pouvoir engendrer une progéniture viable et féconde[9]. Ainsi, l'espèce est la plus grande unité de population au sein de laquelle le flux génétique est possible dans des conditions naturelles, les individus d'une même espèce étant génétiquement isolés d’autres ensembles équivalents du point de vue reproductif[5]. Mais c'est probablement Georges Buffon qui fut le premier en 1749 à construire une définition biologique de l'espèce en écrivant : « On doit regarder comme la même espèce celle qui, au moyen de la copulation, se perpétue et conserve la similitude de cette espèce, et comme des espèces différentes celles qui, par les mêmes moyens, ne peuvent rien produire ensemble »[10].
Le concept biologique de l'espèce s'appuie donc entièrement sur l'isolement reproductif (ou isolement génétique), c'est-à-dire l'ensemble des facteurs biologiques (barrières) qui empêchent les membres de deux espèces distinctes d'engendrer une progéniture viable et féconde. D'après Theodosius Dobzhansky, il est possible de distinguer les barrières intervenant avant l'accouplement ou la fécondation (barrières précopulatoires ou prézygotiques), et les barrières intervenant après (barrières postcopulatoires ou postzygotiques)[11]. Les barrières prézygotiques vont empêcher la copulation entre deux individus d'espèces différentes, ou la fécondation des ovules dans le cas où l'accouplement a bien lieu. Si la fécondation a lieu malgré tout, les barrières postzygotiques vont empêcher le zygote hybride de devenir un adulte viable et fécond. C'est cet isolement reproductif qui va empêcher le pool génétique de chaque espèce de s'échanger librement avec les autres et ainsi d'induire la conservation de caractères propres à chaque espèce[12].
Pour certaines espèces, l'isolement reproductif apparait de manière évidente (entre un animal et un végétal par exemple) mais dans le cas d'espèces étroitement apparentées, les barrières sont beaucoup moins claires. Il est donc important de préciser que la reproduction entre individus d'une même espèce doit être possible en conditions naturelles et que la progéniture doit être viable et féconde. Par exemple, le cheval et l'âne sont deux espèces interfécondes mais leurs hybrides (mulet, bardot) le sont rarement ; la progéniture n'est pas féconde, il s'agit bien de deux espèces différentes[13]. De même, certaines espèces peuvent être croisées artificiellement mais ne se reproduisent pas ensemble dans le milieu naturel.
Néanmoins, le concept biologique de l'espèce possède certaines limites. L'isolement reproductif ne peut pas être déterminé dans le cas des fossiles et des organismes asexués (par exemple, les bactéries). De plus, il est difficile d'établir avec certitude la capacité d'un individu à s'accoupler avec d'autres types d'individus. Dans de nombreux groupes de végétaux (bouleau, chêne, saule…), il existe beaucoup d'espèces qui se croisent librement dans la nature sans que les taxonomistes les considèrent comme une seule et même espèce pour autant[12]. De nombreuses autres définitions ont donc également cours pour passer outre les limites du concept biologique de l'espèce.
Le concept morphologique de l'espèce est le concept le plus généralement utilisé en pratique. Il consiste à identifier une espèce d'après ses caractéristiques structurales ou morphologiques distinctives[12]. L'avantage de ce concept est qu'il est applicable aussi bien chez les organismes sexués qu'asexués et ne nécessite pas de connaître l'ampleur du flux génétique. Néanmoins, l'inconvénient majeur de ce concept réside dans la subjectivité de sa définition de l'espèce, qui peut aboutir à des désaccords quant aux critères retenus pour définir une espèce[5].
Une autre définition repose sur la notion de ressemblance (ou au contraire de degré de différence), concept encore très utilisé en paléontologie, où il n’y a pas d’autre option. Certains auteurs utilisent même ces deux principes pour définir les espèces.
L’étude de l’ADN permet de rechercher des ressemblances non visibles directement sur le plan physique (phénotype). Mais le critère quantitatif (nombre de gènes identiques) masque le critère qualitatif, par définition non mesurable. Ainsi, la classification des Orchidées de type Ophrys fait ressortir un grand nombre d’espèces, visiblement différentes (donc du point de vue phénotype) alors que leurs génotypes se sont révélés très proches. Le critère de ressemblance génétique est utilisé chez les bactéries (en plus des ressemblances phénotypiques). On sépare les espèces de manière que la variation génétique intraspécifique soit très inférieure à la variation interspécifique.
L’espèce biologique est aujourd’hui le plus souvent définie comme une communauté reproductive (interfécondité) de populations. Si cette définition se prête assez bien au règne animal, il est moins évident dans le règne végétal, où se produisent fréquemment des hybridations. On associe souvent le double critère de réunion par interfécondité et séparation par non-interfécondité, pour assurer la perpétuation de l’espèce.
Il existe aussi le concept d'espèce écologique, à relier à la notion de niche écologique. Une espèce est censée occuper une niche écologique propre. Cela revient à associer une espèce à des conditions de vie particulière. Cette définition proposée par Hutchinson[14] et par Van Valen[15] souffre des problèmes de recouvrement de niche (plusieurs espèces dont les niches sont très proches voire indiscernables).
Les espèces déterminantes sont des espèces retenues par certaines méthodes parce qu'elles sont remarquables pour la biodiversité ou menacées et jugées importantes dans l'écosystème (ou représentatives d'un habitat ou de l'état de l'écosystème) aux niveaux régional, national ou supranational pour élaborer certains zonages (habitats déterminants, trame verte et bleue, ZNIEFF modernisées, Natura 2000, etc.).
Définir l'espèce de manière absolue semble très difficile, voire impossible selon Darwin[16]. Plusieurs historiens affirment d'ailleurs que si Darwin s’était arrêté au problème de la définition de l’espèce, il n’aurait jamais publié son livre majeur De l'origine des espèces[17].
De manière simplificatrice, on peut ramener les diverses définitions qui ont été proposées sous trois rubriques différentes : concept typologique ou essentialiste de l'espèce (ressemblance morphologique par rapport à des individus de référence ou type) qui a prévalu pendant des siècles ; concept nominaliste (ressemblance phénoménologique des espèces qui n'ont pas d'existence) ; concept biologique ou populationnel (descendance d'ancêtres communs, liée au critère d'interfécondité) qui s’est imposé après l’avènement de la génétique mais suscite de nombreux problèmes au niveau de la classification scientifique des espèces[17]. Ce qui a conduit des chercheurs à proposer d'abandonner la nomenclature linnéenne, de ne plus donner de noms aux différents rangs taxinomiques et d'éliminer, entre autres, le mot espèce du vocabulaire de la taxinomie. Ils veulent introduire à la place le concept de LITU (Least-Inclusive Taxonomic Unit, unité taxonomique la moins inclusive (de)) qui représenterait le plus petit taxon que l’on puisse identifier[18].
Une question mérite d’être posée : la notion d’espèce constitue-t-elle une simple commodité de travail, ou possède-t-elle au contraire une réalité indépendante de notre système de classification ? Possède-t-elle une véritable signification dans l’absolu ? L’espèce est-elle une classe logique à laquelle des lois sont universellement applicables, ou a-t-elle la même réalité qu’un individu (par le lignage) ? Les réponses à ces considérations relèvent de l’épistémologie et de la sémantique opérationnelle autant que de la biologie.
Le problème se complique du fait que le critère d’interfécondité présente ou absente, n'est pas toujours applicable de façon tranchée : des populations A1 et A2, A2 et A3… An-1 et An peuvent être interfécondes, alors que les populations A1 et An ne le sont pas. C'est le cas, par exemple, des populations de goélands réparties autour du globe (rapporté par Konrad Lorenz). On parle alors d’espèce en anneau (cf. variation clinale). La notion d’espèce se dissout alors dans une sorte de flou.
L’interfécondité ne permet donc pas de dire qu’il s’agit de mêmes espèces tandis que la non-interfécondité suffit à dire qu’il s’agit d’espèces différentes. Cette non-interfécondité doit être recherchée aussi et surtout dans les descendants : chevaux et ânes sont interféconds mais leurs hybrides (mulet, bardot) le sont rarement. Les deux populations forment donc des espèces différentes.
De même, certaines races de chiens (anciennement Canis familiaris) s’hybrident sans problème — et ont une descendance féconde — avec des loups communs (Canis lupus), tandis que leur hybridation avec d’autres races de leur propre espèce Canis familiaris reste bien problématique - dans le cas par exemple d’une femelle Chihuahua et d’un mâle Saint-Bernard !
Cela s’explique par deux faits : le chien domestique est très polymorphe et c’est une sélection artificielle à partir de loups, ce dont il y a maintenant des preuves génétiques. On le nomme donc désormais Canis lupus familiaris, c’est-à-dire comme sous-espèce du Loup, donc parfaitement interfécond avec lui… dans la limite de ce que permet physiquement l’utérus récepteur.
Stricto sensu, le concept d'espèce suppose une hypothèse forte qui est la transitivité des interfécondations possibles ; en d'autres termes, on suppose que si X1 est interfécond avec X2, X2 avec X3, etc., X1 sera interfécond avec Xn quelle que soit la longueur de la chaîne. Konrad Lorenz signale que cette supposition n'est pas toujours vraie, en particulier chez des oiseaux marins entre continents. Il faut d'ailleurs bien que ce genre de discontinuité existe pour qu'un phénomène de spéciation commence à apparaître lui aussi.
Les éleveurs en avaient vraisemblablement une notion non formalisée depuis l’origine même de l’élevage. Platon spéculera que puisque l’on voit des chevaux et des vaches, mais jamais d’hybride des deux, il doit exister quelque part une « forme idéale » qui contraint un animal à être l’un ou l’autre. Aristote préfèrera pour sa part éviter ces spéculations et se contenter de répertorier dans l’Organon ce qu’il observe. Albert le Grand s’y essaiera à son tour plus tard.
Concept empirique, la notion d’espèce a évolué avec le temps et son histoire a été marquée par la pensée de grands naturalistes comme Linné, Buffon, Lamarck et Darwin. Au XVIIIe siècle, les espèces étaient considérées comme le résultat de la création divine et, à ce titre, étaient considérées comme des réalités objectives et immuables. Depuis l’avènement de la théorie de l’évolution, la notion d’espèce biologique a sensiblement évolué, mais aucun consensus n’a pu être obtenu sur sa définition.
La spéciation est le processus évolutif par lequel de nouvelles espèces apparaissent. La spéciation est à l'origine de la diversité biologique et constitue donc le point essentiel de la théorie de l'évolution. La spéciation peut suivre deux voies : l'anagénèse et la cladogénèse. L’anagénèse est une accumulation de changements graduels au cours du temps qui transforment une espèce ancestrale en une nouvelle espèce, cette voie modifie les caractéristiques d'une espèce mais ne permet pas d'augmenter le nombre d'espèces. La cladogénèse est la scission d'un patrimoine génétique en au moins deux patrimoines distincts, ce processus est à l'origine de la diversité biologique car il permet d'augmenter le nombre d'espèces.
En se basant sur les intervalles couverts par les espèces fossiles que l'on répertorie dans les sédiments bien datés, la durée de vie moyenne d'une espèce est de 4  à   5 millions d'années environ. Certaines évoluent plus vite, tels les mammifères et les oiseaux qui ont une durée de vie moyenne de l'ordre d'un million d'années, d'autres moins vite tels les bivalves qui atteignent environ 10 millions d'années par espèce[19]. L'extinction d'un genre se produit quant à elle en moyenne après 20 millions d'années d'existence[20].
En classification classique ou phylogénétique, l’espèce est le taxon de base de la systématique, dont le rang se trouve juste en dessous du genre.
Dans la classification scientifique, une espèce vivante ou ayant vécu est désignée suivant les règles de la nomenclature binominale, établie par Carl von Linné au cours du XVIIIe siècle. Suivant cette classification, le nom d'une espèce est constituée d'un binom latin (on dit habituellement binôme par erreur de traduction du terme anglais binomen et pas binomial) qui combine le nom du genre avec une épithète spécifique. Autant que possible, le nom est suivi de la citation du nom de l'auteur, abrégé (en botanique) ou complet (en zoologie), qui a le premier décrit l'espèce sous ce nom. Le nom de l’espèce est l’ensemble du binom, et pas seulement l’épithète spécifique, suivi du nom d'auteur et de la date.
Par exemple, les êtres humains appartiennent au genre Homo et à l’espèce Homo sapiens Linné, 1758.
Les noms scientifiques des espèces (en latin scientifique) s’écrivent en italique[a]. Le genre prend une majuscule initiale tandis que l'épithète spécifique reste entièrement en minuscule.
Quand le genre est connu mais que l'espèce n'est pas déterminée, il est d’usage d’utiliser comme épithète provisoire l’abréviation du latin species : « sp. », à la suite du nom du genre. Quand on veut désigner plusieurs espèces ou toutes les espèces[réf. nécessaire] d'un même genre, c'est l'abréviation « spp. » (pour species pluralis) qui est ajoutée. De même, « sous-espèce » est abrégé en « ssp. » (pour sub-species) et « sspp. » au pluriel (pour sub-species pluralis). Ces abréviations sont toujours écrites en caractères romains.
La nomenclature binominale, ainsi que d’autres aspects formels de la nomenclature biologique, constitue le « système linnéen ». Ce système de nomenclature permet de définir un nom unique pour chaque espèce, valable dans le monde entier, contrairement à la nomenclature vernaculaire.
Au sein d’une espèce donnée, une sous-espèce consiste en un groupe d’individus qui se trouvent isolés (pour des raisons géographiques, écologiques, anatomiques ou organoleptiques) et qui évoluent en dehors du courant génétique de la sous-espèce nominative, de référence.
Au bout d’un certain temps, ces groupes d’individus prennent des caractéristiques spécifiques qui les différencient l'une de l'autre. Ces caractères peuvent être nouveaux (apparition à la suite d'une mutation par exemple), mais dépendent de la fixation de caractéristiques variables chez l’espèce de base.
Ces deux bergeronnettes mâles ont été décrites comme deux sous-espèces différentes d’une même espèce, Bergeronnette grise :
Bergeronnette grise,  Motacilla alba alba
Bergeronnette de Yarrell,  Motacilla alba yarrellii.
Des sous-espèces différentes ont souvent la possibilité de se reproduire entre elles, car leurs différences ne sont pas (encore) suffisamment marquées pour constituer une barrière reproductive.
On peut s’interroger sur la validité de la définition d’une sous-espèce sachant que la définition du terme espèce reste fluctuante et controversée. Il en est ici de même et toutes les limites de la définition d’une espèce s’appliquent également pour celle d’une sous-espèce.
Carl von Linné recensait au XVIIIe siècle environ 6 000 espèces végétales et 4 400 espèces animales différentes dans la dixième édition (1758) du Systema Naturae[21]. Depuis cette époque et jusqu'en 2014, près de 1,9 million d'espèces ont été décrites[22] mais aujourd’hui, personne ne peut dire avec précision le nombre d’espèces existant sur la planète[23],[24]. Différentes estimations donnent un nombre total d'espèces variant entre à 3 à 100 millions. Un consensus récent a proposé un nombre précis minimum de 8,7 millions d’espèces (à l’exception des bactéries, trop difficiles à estimer)[25]. On décrit actuellement entre 16 000 et 18 000 nouvelles espèces par an, dont 10 % sont issues du milieu marin[26].
Les eucaryotes sont les animaux, les champignons, les plantes, les protozoaires… Alors qu’on estime qu'entre 5 ± 3 millions d’espèces vivantes sur la planète Terre ont été découvertes[27] (avec des extrapolations jusqu'à plus de 100 millions d'espèces à découvrir[28]), seulement 1,5 à 1,8 million d'espèces ont été décrites scientifiquement (témoin des difficultés liées à la notion d’espèce, ce nombre lui-même reste flou). Les espèces marines ne représentent que 13 % de l'ensemble des espèces décrites, soit environ 275 000, dont 93 000 pour les seuls écosystèmes coralliens[29].
La grande majorité des espèces non décrites sont des insectes (4 à 100 millions d'espèces suivant les estimations, qui vivraient principalement sur la canopée des forêts tropicales[30]), des némathelminthes (ou vers ronds : 500 000 à 1 000 000 d'espèces), et des eucaryotes unicellulaires : protozoaires ou protophytes, certains oomycètes, anciennement considérés comme des champignons, aujourd’hui classés dans les straménopiles ou les myxomycètes (moisissures visqueuses maintenant classées dans plusieurs groupes de protistes…).
Selon la liste rouge de l'UICN de 2006[31] et les données les plus récentes, les espèces vivantes décrites peuvent être réparties comme suit :
Environ 16 000 nouvelles espèces sont décrites chaque année, dont 1 600 espèces marines[29] et près de 2 000 espèces de plantes à fleur (369 000 espèces répertoriées en 2015)[54].
On estime qu’environ dix espèces disparaissent naturellement (c’est-à-dire hors de l’intervention de l’espèce humaine) chaque année[55], ou une sur 50 000 par siècle[56]. Mais il en est qui disparaissent aussi du fait de l’homme (voir dodo, diversité génétique…) : Edward Osborne Wilson en évalue le nombre à plusieurs milliers par an[57]. D’après l’Évaluation des écosystèmes pour le millénaire de 2005, le taux de disparition des espèces depuis deux siècles est dix à cent fois supérieur au rythme naturel[58] (hors grandes crises d'extinction), et sera encore multiplié par dix d'ici 2050, soit 1 000 à 10 000 fois le rythme d'extinction naturel.
Dans les deux autres grands groupes du vivant (les archées et les bactéries), la notion d'espèce est sensiblement différente[59],[60]. Le nombre total est encore moins bien connu que chez les eucaryotes, avec des estimations qui varient entre 600 000 et 6 milliards d'espèces… contre seulement 7,300 espèces de bactéries connues à l'heure actuelle[56],[61].
Suivi ou précédé d'un adjectif, on écrit une espèce bovine, une espèce protégée, etc. Suivi d'un substantif, on écrit l'espèce Mulot sylvestre ou l'espèce Apodemus sylvaticus[62].
« Une espèce de » est suivi d'un singulier ou d'un pluriel, selon que cette expression est prise dans le sens d'une approximation (sorte de) ou d'une population (groupe de). En français usuel, on écrit « Le bonobo est une espèce de singe » (une sorte de singe) mais un biologiste écrira de préférence « Le Bonobo est une espèce de primates » (un groupe de primates). En effet, en biologie, suivi d'un déterminant introduit par « de », on écrit une espèce (ou une sous-espèce) de mammifères, d'oiseaux, de reptiles ou bien des espèces d'insectes[62]. Sous entendu, une « population à caractères stables » de mammifères, oiseaux, etc.[pas clair]
Exemple : « Solanum juzepczukii est une espèce de plantes herbacées et tubéreuses de la famille Solanaceae » ou « la floraison de chaque espèce de plantes vivaces[63] ».
On utilise les abréviations « sp. » au singulier et « spp. » au pluriel, qui correspondent au mot latin species. Cette abréviation s'emploie souvent après le nom d'un genre, pour indiquer « espèce non précisée », par exemple Russula sp. signifie « espèce du genre Russule ».
modifier - modifier le code - modifier WikidataLe muscle est un organe composé de tissu mou retrouvé chez les animaux. Il est composé de tissus musculaires et de tissus conjonctifs (+ vaisseaux sanguins + nerfs).
Les cellules musculaires (composant le tissu musculaire) contiennent des filaments protéiques d'actine et de myosine qui glissent les uns sur les autres, produisant une contraction qui modifie à la fois la longueur et la forme de la cellule. Les muscles fonctionnent pour produire de la force et du mouvement. Ils sont principalement responsables du maintien et de l'évolution de la posture, de la locomotion, ainsi que du mouvement des organes internes, tels que la contraction du cœur et la circulation des aliments dans le système digestif par péristaltisme.
Les tissus musculaires dérivent du mésoderme (couche de cellules germinales embryologique) grâce à un processus connu sous le nom de la myogenèse. Il existe trois types de muscles : strié squelettique, strié cardiaque et lisse. Le cœur, les muscles de l'ouïe[1] et les muscles lisses se contractent sans intervention de la pensée et sont qualifiés d'involontaires ; tandis que les muscles striés squelettiques se contractent eux sous la commande volontaire[2]. Les fibres musculaires striés squelettiques sont divisés en deux catégories, celles à contraction rapide et celles à contraction lente.
Les muscles utilisent de l'énergie obtenue principalement par l'oxydation des graisses (lipides) et des hydrates de carbone (glucides) en condition aérobie, mais aussi par des réactions chimiques en condition anaérobie (notamment pour la contraction des fibres rapides). Ces réactions chimiques produisent de l'adénosine triphosphate (ATP), monnaie énergétique utilisée pour le mouvement des têtes de myosine[3].
Le terme muscle dérive du latin musculus, signifiant « petite souris », dont l'origine provient soit de la forme de certains muscles, soit de leur contraction ressemblant à des souris se déplaçant sous la peau[4],[5].
Le tissu musculaire est un tissu mou, et est l'un des quatre tissus fondamentaux présents chez les animaux (avec le tissu conjonctif, le tissu nerveux et le tissu épithélial). Il existe trois types de tissu musculaire reconnus chez les vertébrés :
Les muscles cardiaques et squelettiques sont dits « striés » car ils contiennent des unités structurelles particulières, les sarcomères, qui sont arrangés en faisceaux de façon très régulière ; les myofibrilles des cellules musculaires lisses ne sont pas disposées sous forme de sarcomères et n'apparaissent donc pas striées en microscopie optique.
Alors que les sarcomères dans les muscles squelettiques s'organisent en faisceaux parallèles, ceux du muscle cardiaque se connectent par des ramification en X. Les muscles striés se contractent et se relâchent sur une courte distance mais de façon intense et rapide, tandis que les muscles lisses soutiennent des contractions plus ou moins fortes qui se mettent en place beaucoup plus lentement et de manière plus ou moins permanente.
La masse volumique des muscles squelettiques pour les mammifères est d'environ 1,06 kg/litre (densité du tissu adipeux (graisse) est 0,9196 kg/litre). Le tissu musculaire est 15 % plus dense que le tissu adipeux.
Tous les muscles dérivent du mésoderme paraxial. Le mésoderme paraxial est divisé le long de l'embryon en somites, correspondant à un phénomène de segmentation du corps (retrouvé de façon plus évidente avec la colonne vertébrale). Chaque somite possède 3 sous-divisions, le sclérotome (qui forme les vertèbres), le dermatome (qui forme le derme cutané), et le myotome (qui forme les muscles). Le myotome est divisé en deux sections, l'épimère et hypomère, qui forment respectivement les domaines épaxiaux (ou paraxiaux) et hypaxiaux . Les domaines épaxiaux chez l'homme permettent la formation des muscles érecteurs du rachis et des petits muscles intervertébraux, et sont innervés par la ramification dorsale des nerfs spinaux. Tous les autres muscles proviennent des domaines hypaxiaux et sont inervés par la ramification ventrale des nerfs rachidiens (=nerfs spinaux).
Au cours du développement les myoblastes (cellules progénitrices musculaires) peuvent rester dans les somites pour former les muscles associés à la colonne vertébrale (épaxial), ou bien migrer dans le corps pour former tous les autres muscles (hypaxial). La migration des myoblastes est précédée par la formation du tissu conjonctif, généralement issu du mésoderme latéral. Les myoblastes suivent des signaux chimiques pour rejoindre leur emplacement approprié, puis fusionnent ensemble pour former les cellules des musculaires squelettiques (formation par syncytium).
Les muscles striés squelettiques (MSS) sont revêtus d'un tissu conjonctif (TC) dense appelé l'épimysium. L'épimysium ancre le tissu musculaire aux tendons, à chaque extrémité du muscle. Il protège également les muscles du frottement (contre d'autres muscles ou l'os). L'épimysium englobe de multiples faisceaux, contenant eux-mêmes 10 à 100 fibres musculaires. Les faisceaux sont eux recouverts de périmysium qui permet le passage des nerfs et de la circulation sanguine. Chaque fibre musculaire (correspondant aux cellules musculaires, le myocyte) est enfermée dans son propre TC, l'endomysium (TC lâche). 
En résumé, le muscle est composé de fibres (cellules) qui sont groupées en faisceaux, qui sont eux-mêmes regroupés pour former le muscle. À chaque niveau de regroupement une membrane de collagène (tissu conjonctif) entoure le paquet. Notons que ces membranes sont liées au tissu musculaire par des complexes protéiques (dystrophine, costamères) et sont résistantes à l'étirement. 
Pour finir, dispersés à travers le muscle on retrouve les fuseaux neuromusculaires (ou fibres intrafusales) qui fournissent de la rétroaction sensorielle pour système nerveux central (sensible au niveau d'étirement du muscle, rôle dans le réflexe myotatique).
Dans les cellules du muscle (ou fibres musculaires ou myofibre) on retrouve des myofibrilles, qui eux-mêmes sont des faisceaux de protéines filamenteuses (actine). Le terme « myofibrille » ne doit pas être confondu avec le terme « myofibre », qui est simplement un autre nom pour la cellule musculaire. Les myofibrilles sont une association complexe de filaments protéiques organisés en unités répétitives appelées sarcomères. L'aspect strié des muscles squelettiques et cardiaques résulte de la présence de ces sarcomères à l'intérieur des cellules. Bien que ces deux types de muscles contiennent des sarcomères, les fibres du muscle cardiaque sont généralement ramifiées pour former un réseau et interconnectées par des disques intercalés, donnant au tissu l'apparence d'un syncytium (ce n'en est pas un à proprement parler).
Les deux filaments caractéristiques du sarcomère sont l'actine et la myosine.
Un muscle peut se diviser en plusieurs parties dites chef musculaire (pars musculi ou caput musculi) ou chef de muscle. Un chef musculaire est individualisé par ses insertions, avec parfois une innervation et une fonction spécifique[6]. Les différents chefs d'un même muscle sont indépendants à leur insertion proximale (insérés en autant de tendons) pour se réunir et se fixer par un tendon commun à leur insertion distale[7].
Il existe des muscles de un à quatre chefs[8]. Par exemple, le biceps brachial a deux chefs, le triceps sural a trois chefs, le quadriceps fémoral a quatre chefs.
Les trois types de muscle (squelettiques, cardiaques et lisses) comportent d'importantes différences. Toutefois, tous trois utilisent le mouvement des fibres d'actine associées à de la myosine afin de créer une contraction. Dans le muscle squelettique, la contraction est stimulée par des potentiels d'actions transmis par des nerfs particuliers, les motoneurones (nerfs moteurs). Les muscles cardiaques et lisses ont leur contraction stimulée par des cellules stimulatrices internes à l'organe (se contractant spontanément de façon régulière), et avec une propagation de l'ordre de contraction de proche en proche (canaux ionique entre cellules). Tous les muscles squelettiques et beaucoup de muscles lisses ont leur contraction régulée par un neurotransmetteur : l'acétylcholine.
L'action qu'un muscle génère est déterminée par sa localisation et celle de ses insertions. La section transversale d'un muscle (plus que sa longueur) détermine la quantité de force qu'il peut générer en définissant le nombre de sarcomères qui peuvent fonctionner en parallèle. Chaque muscle squelettique contient de longues unités appelées myofibrilles, et chaque myofibrille est une chaîne de sarcomères. Puisque la contraction se produit en même temps pour tous les sarcomères connectés, ces chaînes de sarcomères raccourcissent ensemble, et ce raccourcissement de la fibre musculaire entraîne un changement de longueur de la myofibrille[9].
L'activité musculaire consomme la majeure partie de l'énergie (sans oublier que le cerveau compte lui pour 1/3). Toutes les cellules musculaires produisent de l'adénosine triphosphate (ATP), ces molécules énergétiques sont utilisées pour le mouvement des têtes de myosine. Les muscles peuvent stocker de l'énergie pour une utilisation rapide sous la forme de phosphocréatine (qui est générée à partir d'ATP et qui peut régénérer cet ATP si nécessaire grâce à la créatine kinase). Les muscles peuvent aussi stocker du glucose sous forme de glycogène (comme le foie). Ce glycogène peut être rapidement converti en glucose pour poursuivre les contractions musculaires. Au sein du muscle à contraction volontaire (muscles squelettiques), la molécule de glucose peut être métabolisée par voie anaérobie dans un processus appelé la glycolyse qui produit 2 ATP et 2 acides lactiques (à noter que dans des conditions aérobies, le lactate n'est pas formé; au lieu on produit de l’acétyl-CoA servant de cofacteur pour le cycle de Krebs). Chez les sportifs de haut niveau, les cellules musculaires contiennent également des globules de graisse à proximité, utilisés pendant l'exercice aérobie. La production d'énergie dans des conditions aérobie prend plus de temps et nécessite beaucoup d'étapes biochimiques, mais en contrepartie produit beaucoup plus d'ATP que la glycolyse anaérobie. Le muscle cardiaque peut facilement utiliser l'un des trois macronutriments (protéine, glucose et lipide) en aérobie rapidement et avec un rendement d'ATP maximal. Le cœur, le foie et les globules rouges peuvent réutiliser l'acide lactique (produit par les muscles squelettiques pendant l'exercice physique intense) dans leur propre métabolisme.
Au repos, le muscle squelettique consomme 54,4 kJ/kg (13,0 kcal/kg) par jour. Ces valeurs sont bien supérieures au tissu adipeux 18,8 kJ/kg (de 4,5 kcal/kg) et à l'os 9,6 kJ/kg (2,3 kcal/kg).
Le processus exact de la croissance des muscles n'est pas entièrement compris.
Cependant les théories dominantes prétendent qu'une activité musculaire trop importante déchire les fibres musculaires qui lors de leur réparation se prévalent d'un dommage par une forme de croissance[10].
Les maladies neuromusculaires (regroupant toutes les maladies) sont celles qui affectent les muscles et/ou leur contrôle nerveux. En général, les problèmes nerveux peuvent causer des spasmes ou un paralysie (mortelle si elle touche un muscle respiratoire). Une grande proportion de troubles neurologiques, allant de l'accident vasculaire cérébral (AVC) à la maladie de Parkinson en passant par celle de Creutzfeldt–Jakob, peuvent conduire à des problèmes du mouvement ou de coordination motrice.
Les symptômes de maladie musculaire peuvent inclure une faiblesse musculaire, la spasticité, des myoclonies et des myalgies. Les procédures pour diagnostiquer ces maladies sont les tests de niveau de créatine kinase dans le sang et l'électromyographie (mesure de l'activité électrique dans les muscles). Dans certains cas, une biopsie musculaire peut être faite pour identifier la myopathie, ainsi que les tests génétiques pour identifier les anomalies de l'ADN associées à ces myopathies et ces dystrophies.
Une élastographie non invasive permet de mesurer le « bruit » du muscle pour surveiller une maladie neuromusculaire. Le son produit par le muscle provient du raccourcissement des myofibrilles le long de l'axe du muscle. Au cours de la contraction le muscle se raccourcit, produisant des vibrations à la surface de ce dernier.
En France le Téléthon permet de recueillir des fonds sur la base des dons pour la recherche sur les myopathies.
Muscles faciaux (anglais)
Muscles des yeux.
Muscle temporal droit.
Muscles du cou (vue latérale).
Muscle du cou (vue ventrale).
Muscles du dos (Voir description de l'image pour la légende).
Muscles de l'abdomen (antérieur).
Muscles profonds bras (anglais).
Muscles pectoraux et du bras superficiels.
Muscles pectoraux et du bras profonds.
Muscles du bras superficiels (antérieur).
Muscles du bras profonds (antérieur).
Muscles du bras superficiels (postérieur).
Muscles du bras profonds (postérieur).
Muscles de la main palmaire (anglais).
Muscles de la cuisse (antérieur).
Muscles de la jambe et du genou (postérieur).
Muscles dans la région du périnée pour un homme (anglais).
Muscles dans la région du périnée pour une femme (anglais).
Muscles et ligaments chez le cheval
Muscles chez le chien (voir la description de l'image pour la légende)
Muscles chez le chat
Cet article  doit être recyclé (mars 2021).
Une réorganisation et une clarification du contenu paraissent nécessaires. Améliorez-le, discutez des points à améliorer ou précisez les sections à recycler en utilisant {{section à recycler}}.
Pour les articles homonymes, voir Sourd.
Ne doit pas être confondu avec Sourds (communauté).
 Mise en garde médicalemodifier - modifier le code - voir Wikidata (aide) La surdité est un état pathologique de l'audition caractérisé par une perte partielle ou totale de la perception des sons. Lorsque la perte est totale, il s'agit d'une cophose, parfois appelée anacousie. Le terme hypoacousie est quasi-synonyme de surdité, étant parfois réservé aux cas où la perte de l'audition est partielle.
La surdité peut être classée selon le degré de perte de l'ouïe et selon la localisation de l'atteinte. Son traitement médical est possible, par des appareillages externes ou internes (implants), quoique la récupération de l'audition des phonèmes de la langue puisse continuer à être altérée.
La surdité qui apparaît à l'âge adulte peut avoir des origines médicales ou traumatiques. Elle est un handicap contre lequel certains moyens sont mis en œuvre afin d'aider les personnes dites sourdes ou malentendantes.
Lorsqu'elle survient à un âge précoce et avant l'apprentissage du langage, la surdité peut compromettre l'apprentissage du langage et par conséquent l'apprentissage de la lecture, et entraîner de multiples problèmes associés à l'illettrisme et aux bas niveaux d'éducation. La surdité précoce nécessite une prise en charge éducative précoce offrant à l'enfant sourd des moyens de communication qui vont lui permettre de bien développer ses relations sociales et d'optimiser son développement émotionnel, cognitif et intellectuel. L'apprentissage de la langue des signes et l'aide à l'apprentissage de la langue orale par diverses méthodes de rééducation et de complément à la lecture labiales sont des méthodes validées ayant chacune des avantages démontrés, malgré leurs limites.
Donc, en conclusion, la surdité est un handicap auditif. C’est plus précisément une perte ou une grande diminution de l’ouïe. Il est important de distinguer les personnes sourdes de vieillesse des autres sourds car à partir de 65 ans, la surdité est considérée comme « normale ». Le terme « sourd » regroupe de nombreuses personnes atteintes de déficiences auditives différentes[1].
Le décibel est une unité qui permet de rendre compte de la force d’un son[1].
Chaque son est formé de vibrations. Le hertz correspond au nombre de vibrations par seconde. Plus il y a de vibrations, plus le son est aigu, moins il y a de vibrations, plus le son est grave. L’oreille humaine ne capte en moyenne que les sons compris entre 20 et 20 000 hertz[1].
Il existe différents degrés de surdité. Le Bureau International de l’AudioPhonologie les définit en calculant la perte moyenne de décibels aux fréquences 500 hertz, 1 000 hertz, 2 000 hertz et 4 000 hertz, fréquences que l’oreille saine capte habituellement.
On parle de surdité légère quand la perte moyenne est comprise entre 20 et 40 décibels. Les personnes atteintes de surdité légère ne perçoivent pas tous les éléments de la parole.
On parle de surdité moyenne quand il y a une perte d’audition entre 41 et 70 décibels. La personne atteinte de ce type de surdité perçoit la parole seulement à voix forte et près de l’oreille. Seuls les bruits forts sont perçus.
On parle de surdité sévère quand la perte auditive moyenne est entre 71 et 90 décibels. Une personne atteinte de surdité sévère ne perçoit plus du tout la parole et ne perçoit que des bruits très puissants.
On parle de surdité profonde pour une perte de 90 à 120 décibels[1].
On parle de surdité totale, ou cophose, à 120 décibels[1].
En conclusion, on parle d'audition normale ou subnormale lorsque la surdité est sous le seuil des 20 dB car malgré la perte de certains sons faibles, la compréhension du langage n'est pas affectée et il n'y a donc pas de handicap dans les situations sociales[2]. On parle de déficience auditive totale si la perte auditive moyenne est de 120 décibels. Les personnes atteintes de ce type de surdité ne perçoivent plus aucun bruit[1].
On classifie également la surdité selon la localisation de l'atteinte. Si l'oreille externe ou l'oreille moyenne est en cause, la surdité est dite « de transmission » ; si l'oreille interne ou le nerf auditif est en cause, la surdité est dite « de perception ». Si la surdité est d'origine neurologique, on parle de  « surdité centrale » (le terme « central » dans ce cas, fait référence au système nerveux central, pour la différencier des autres surdités qui touchent les cellules sensorielles de l'oreille, qui font partie du système nerveux périphérique). La surdité centrale peut être due à des lésions au niveau du tronc cérébral. Elle peut aussi être due à des lésions bilatérales des aires auditives, on parle alors de « surdité corticale »[3],[4].
Les deux oreilles peuvent être touchées (surdité bilatérale) et l'atteinte peut se limiter à une oreille (surdité unilatérale).
On parle de surdité congénitale chez les enfants sourds qui sont affectés de surdité dès la naissance[5].
Les termes utilisés pour parler de la population des sourds dans son entier varient d'une source à l'autre et ne sont pas toujours clairement définis.
Selon le « Petit Larousse » 2002, la culture est l’« ensemble des usages, des coutumes, des manifestations artistiques, religieuses, intellectuelles qui définissent et distinguent un groupe, une société ». Elle est également définie par l’ « ensemble de convictions partagées, de manières de voir et de faire qui orientent plus ou moins consciemment le comportement d’un individu, d’un groupe ». Le fait de parler d’une « culture sourde » n’est donc pas si surprenant. En effet, le handicap de la surdité implique un mode de communication différent ainsi qu’un mode de vie différent (utilisation d’un système lumineux ou de vibrations à la place de sonneries de téléphone, réveil, portes d’entrées, utilisation de messages textes ou du système « Procom16 » à la place des téléphones, etc.) C’est à partir de ces différences que le groupe des sourds se distancie du groupe entendant en ayant une culture propre. C’est à la période ou les écoles de sourds se sont créées que la communauté sourde a pris de l’ampleur. Cette communauté a pour particularité de communiquer en langue des signes. Les sourds ont besoin de se retrouver entre eux afin de communiquer à leur manière. Ils possèdent une langue, une culture, une histoire qui leur sont propres. Certains entendants qui signent (interprètes, enfants entendants de parents sourds) participent également à la vie de la communauté sourde. Lorsqu’on parle de culture sourde, il faut comprendre l’histoire commune, mais également toutes les expériences communes des sourds face à une société entendante. La culture sourde comprend aussi de nombreuses organisations dans tous les domaines (art, sport, éducation, information, etc.).
En France métropolitaine, environ une personne sur dix est atteinte d'une déficience de l'audition ; une personne de moins de vingt ans sur 25, mais une personne sur trois parmi les plus de 75 ans. Parmi ceux-ci, 300 000 sont sourds de type [9].
Parmi les enfants nés sourds, la proportion de gauchers est supérieure à la moyenne. Ainsi, d'après une étude, la proportion d'enfants sourds gauchers dépasse 20 % alors qu'elle est de 10 % chez les enfants ne présentant pas de surdité[10].
Le risque de trouble de l'audition est augmenté avec le tabagisme, l'obésité ou la présence d'un diabète[11].
La surdité est dans de nombreux pays (dont en France) reconnue comme maladie professionnelle (en France au tableau no 42 du régime général de la sécurité sociale et dans le tableau no 46 de la mutualité sociale agricole) dans les deux cas assortie d'une liste limitative des travaux susceptibles de provoquer la surdité reconnue et d'un temps minimum d'exposition d'un an (réduite à trente jours pour l'exposition aux bruits violents dans la mise au point des propulseurs, réacteurs et moteurs thermiques). La déclaration et les mesures doivent être faites après trois semaines de cessation de l'exposition au bruit professionnel et avant un an. La perte auditive doit être supérieure ou égale à 35 dB sur la meilleure oreille, déficit confirmé par une audiométrie tonale et vocale réalisée trois semaines à un an après la cessation de l'exposition aux bruits lésionnels (ce déficit audiométrique moyen de 35 dB est calculé en divisant par 10 la somme des déficits mesurés sur les fréquences 500, 1 000, 2 000 et 4 000 Hz, pondérés respectivement par les coefficients 2, 4, 3 et 1).
Alors que seulement environ 750 surdités professionnelles sont déclarées et reconnues chaque année en France, les études épidémiologiques pratiquées par la médecine du travail en Europe comme au Québec montrent une atteinte beaucoup plus importante : en France, l'enquête Sumer[12] donnait 27 % de salariés soumis à un bruit excessif et une autre enquête situe à 21,3 % soit 13,5 millions de salariés qui présentent un déficit auditif dû au bruit.
Le traumatisme sonore peut agir comme agent aggravant en cas de prise de médicaments potentiellement ototoxiques[13] et peut être une cause d'accidents du travail, domestiques et de la route.
De nombreuses technologies peuvent actuellement permettre une prise en charge rapide et efficace.
L'épreuve de Rinne permet de reconnaître le siège d’une lésion auditive. Afin de l’effectuer, le médecin se munit d’un diapason et le fait sonner près de l’oreille, puis fait sonner la queue de l’instrument entre les dents du patient.
Si l'audition par voie aérienne persiste et celle par voie osseuse a cessé, le test de Rinne est positif, et la lésion a eu lieu dans l’oreille interne ou dans l’un des centres cérébraux de l’audition. Si, au contraire, l’audition par voie aérienne a cessé et que celle par voie osseuse persiste, le résultat est négatif, et la lésion se situe dans l’oreille moyenne[14].
La perception auditive est mesurée en décibels HL, et correspond au rapport entre le niveau sonore minimal perçu par le sujet à un niveau correspondant à une audition normale. Elle se mesure avec des sons purs écoutés au casque dans un milieu insonore. L'audiogramme indique le résultat pour chacune des fréquences.
Pour déterminer le degré de surdité d’une personne, on se base sur les résultats de la meilleure oreille (celle qui a le moins de perte d'audition). Pour cette oreille, on fait alors la moyenne des pertes pour les fréquences de 500, 1000 et 2 000 Hz. En dessous de 20 dB de perte, l’audition est considérée comme normale. Pour le reste, on se reporte à la classification établie par le Bureau international d’audio-phonologie (BIAP), détaillée ci-après.
Une perte de 20 dB à 40 dB correspond à une surdité légère : 30 dB représente le volume sonore d'une conversation à voix basse[15], ce qui implique que la parole normale est perçue mais certains éléments phonétiques échappent au patient. La voix faible n'est pas correctement perçue. Un enfant atteint de surdité légère peut présenter des signes de fatigue, d'inattention, un certain flou de compréhension, des difficultés articulatoires. Au-dessus de 30 dB de perte, si l'enfant est gêné à l'école, l'appareillage est possible.
Pour une perte de 40 dB à 70 dB, la surdité est moyenne. 60 dB représentant le niveau sonore d'une conversation normale[15], cela implique qu’à ce niveau, la parole n’est perçue que si elle est forte. Chez l’enfant, les troubles du langage et de l'articulation sont importants, la compréhension est lacunaire. Entre 55 et 70 dB de perte, les enfants perçoivent la voix forte sans comprendre les paroles : l’appareillage et la rééducation sont alors nécessaires.
Pour une atténuation de 70 à 80 dB, la surdité est dite sévère, 80 dB représentant le niveau sonore d’une rue bruyante. Certains enfants atteints de surdité sévère entendent la voix à forte intensité mais ne comprennent pas les paroles. L’amplification des sons est insuffisante pour qu’il y ait élaboration spontanée de langage intelligible. Ces enfants procèdent par désignation de l’objet désiré : un appareillage, une rééducation et l’utilisation de la lecture labiale sont nécessaires.
Enfin, le bruit d'un marteau-piqueur monte à 100 dB et  celui d’un réacteur d'avion à 10 mètres à 120 dB. On parle donc de surdité profonde à partir de pertes supérieures à 90 dB. À cet état de surdité, l’enfant n’a aucune perception de la voix et aucune idée de la parole. Pour une surdité profonde, on recalcule une moyenne des seuils des fréquences 250, 500, 1000 et 2 000 Hz, ce qui permet de distinguer trois sous-catégories :
Pour des enfants atteints de surdité profonde, l’apprentissage de la langue des signes est prioritaire. Un appareillage auditif, une rééducation et l'utilisation de la lecture labiale sont nécessaires quand l’âge de communiquer est atteint, ainsi qu'un suivi orthophonique.
Pour les pertes supérieures à 120 dB, on parle de surdité totale ou cophose. Il s’agit généralement d’une surdité de perception due à un dysfonctionnement de la cochlée, aucun son ne peut être perçu. L’appareillage classique (audioprothèse) permet d’entendre les sons, mais pas de comprendre la parole. Seul l’implant cochléaire est efficace pour récupérer le maximum d'informations auditives.
À noter qu’en règle générale, plus la perte d’audition est forte, plus la récupération auditive par le biais de l’appareillage et de la rééducation est difficile, sauf pour les surdités post-linguales (survenues après la constitution d’une zone auditive et linguistique dans le cerveau).
La surdité brusque se caractérise par une baisse rapide de l'audition sans parfois même de signe annonciateur. Elle est en général unilatérale et peut être due à plusieurs facteurs: origine virale, auto-immune, une perforation de la membrane tympanique, des antécédents familiaux ou un traumatisme crânien. Mis à part ces facteurs, les origines de cette forme de surdité sont généralement peu connues. Le devenir de l'audition peut être spontanée totale, partielle ou malheureusement irrécupérable. Sa prévalence est comprise entre 5 et 20 cas pour 100 000 personnes[16].
La prise en charge d'une surdité brusque a fait l'objet de la publication de recommandation (médecine)s par l'« American Academy of Otolaryngology–Head and Neck Surgery » en 2012[17]. La surdité brutale est une vraie urgence nécessitant une prise en charge immédiate, dans les premières heures : un traitement parentéral immédiat (corticoïdes, vasodilatateurs), éventuellement oxygénothérapie hyperbare ; son efficacité est discutée, mais elle serait nulle après une semaine.
La recherche nécessaire d'une cause la retrouve rarement.
Le pronostic fonctionnel est péjoratif (50 % à 75 % ne récupèrent pas), surtout si la surdité est sévère ou profonde et si le traitement est retardé ou nul[18].
Une malformation appelée aplasie peut être majeure: le pavillon, le conduit auditif, la chaîne ossiculaire ou l'oreille interne sont touchés. Elle peut aussi être mineure: sténose du conduit auditif externe ou malformation d'un osselet (enclume, étrier ou marteau). Une malformation mineure des osselets peut alors entraîner des otites moyennes chroniques avec tympanosclérose, c'est-à-dire des séquelles dues aux inflammations chroniques de l'oreille moyenne. Cependant la progression de la surdité est assez lente.
La surdité d'origine génétique atteint environ 1 à 3 enfants sur 1000[19]. Beaucoup de ces surdités n'apparaitront qu'au bout de plusieurs années voire plusieurs dizaines d'années.
Les causes génétiques peuvent donner des surdités isolées (un peu moins de la moitié des cas étant dues à une mutation sur le gène GJB2[20] ou dans le cadre de syndromes malformatifs (syndrome de Waardenburg ou de Pendred).
Les infections virales ou parasitaires au cours de la grossesse comme la toxoplasmose, la rubéole et la maladie des inclusions cytomégaliques en sont souvent responsables. Cette dernière maladie est l'infection la plus fréquente chez la femme enceinte en Europe. Elle atteindrait jusqu'à 2 % des femmes enceintes. La moitié des fœtus de ces femmes seront atteints par le virus et 10 % des fœtus développeront au bout de quelques années une surdité. Ce virus serait responsable d'un peu plus du cinquième des surdités congénitales[21]. La perte d'audition est alors dans ce cas tardive et fluctuante. L'audition étant initialement normale, l'avenir auditif de ces enfants ne peut être prédit.
Parmi les principaux problèmes qui entraînent fréquemment des surdités de transmission, notons :
L'otospongiose est une maladie génétique, héréditaire mais à expression variable (la maladie peut sauter des générations avant de s'exprimer par une surdité). Plus souvent bilatérale, elle touche les os de l'oreille  (ostéodystrophie de la capsule otique). Causée par un trouble du métabolisme osseux, c'est-à-dire que le renouvellement des os est anormal. Des foyers de déminéralisation et d'ossification anormale de la capsule otique provoquent alors un blocage de la platine de l'étrier (Ankylose stapédo-vestibulaire) ou une ostéogenèse imparfaite (maladie de Lobstein ou des os de verre). L'otospongiose est plus fréquente chez les femmes, le rapport est de deux femmes pour un homme et de 0,1 à 2 % de la population. Elle intervient chez le jeune adulte (entre 20 et 40 ans). Les apparitions juvéniles sont plus rares.
Les séquelles d'otites (l'otite moyenne aiguë (OMA) est une infection de l'oreille moyenne concernant le tympan ou la caisse du tympan, petite cavité osseuse située entre le tympan et l'oreille interne et contenant les osselets) [22]donnent une altération du fonctionnement du système tympano-ossiculaire. Elles peuvent entraîner une perforation tympanique, une imperméabilité de la trompe d'Eustache, lyse ossiculaire avec interruption de la chaîne.
Le cholestéatome est une forme d'otite chronique avec une présence d'épiderme dans les cavités de l'oreille moyenne (épithélium pavimenteux). Ce surplus d'épiderme se présente comme un kyste ou comme une poche remplie de squames de peau. Elle grossit petit à petit pour provoquer une infection chronique de l'oreille moyenne mais aussi la destruction des structures osseuses contenues dans et autour de l'oreille.
La maladie osseuse de Paget (infection chronique du squelette adulte, au cours de laquelle le turnover osseux est accéléré dans certaines régions. La maladie peut être asymptomatique ou entraîner des douleurs osseuses ou des déformations d'apparitions progressives.) [23]cause une surdité due aux lésions qui touchent l'os temporal et le crâne qui provoquent un surplus de remodelage osseux anormal qui est fabriqué aboutissant à de nombreuses anomalies osseuses dont l'épaississement des osselets.
La presbyacousie ou surdité due à l'âge est due au vieillissement des cellules de l'oreille et à l'usure. Elle se manifeste par une perte d'audition bilatérale (touchant les deux oreilles). Elle apparaît progressivement à l'avancée de l'âge et intervient souvent à partir de soixante ans. Elle débute premièrement par la perte des sons aigus. Cependant l'âge de la presbyacousie peut dépendre de différents facteurs.
Le neurinome de l'acoustique est une tumeur nerveuse bénigne de l’angle ponto-cérébelleux. Elle se développe par les cellules de Schwann (à l'origine de la gaine de myéline entourant les axones du nerf). Le neurinome de l'acoustique est habituellement unilatéral, isolé et non héréditaire, sauf lorsqu'il s’agit d'une maladie génétique rare appelée maladie de Recklinghausen ou neurofibromatose.
L'otospongiose est une maladie héréditaire de la capsule labyrinthe est une cause fréquente de surdité de transmission de l'adulte.Le traitement est chirurgical. L'intervention chirurgical peut amener une bonne restitution de l'acuité auditive, sauf lorsque l'oreille interne est également atteinte[24].Ce type d'otospongiose évolue en fonction d'événements hormonaux (grossesses) et surtout en présence d'antécédents familiaux connus de surdité.
La maladie de Ménière est un trouble de l'oreille interne qui entraîne des vertiges, une perte de l'audition neurosensorielle fluctuante et des acouphènes. Il n'y a pas de test diagnostic fiable. Les vertiges et les nausées sont traités de façon symptomatique par des anticholinergiques ou des  benzodiazépines, lors de crises aiguës. Les diurétiques et un régime alimentaire pauvre en sel, la première ligne du traitement, diminuent souvent la fréquence et la gravité des épisodes. Dans les cas graves ou réfractaires, une aréflexie vestibulaire peut-être obtenue avec de la gentamicine en administration locale ou une intervention chirurgicale[25]. Les causes de cette maladie sont à l'heure actuelle encore très peu connues.
La surdité de perception endochocléaire est d’origine auto-immune. Ce type de surdité peut être intégré dans une maladie de système ou paraître isolé. L’organisme fabrique des auto-anticorps qui vont détruire les antigènes de la cochlée. La surdité de perception est bilatérale, asymétrique et s’installe sur quelques semaines ou mois.
Les traumatismes sonores peuvent se manifester à cause d'un claquage de l'oreille, au-dessus d'un certain volume au-dessus de 125 dB. Une longue exposition à un son aigu, à 4 000 Hz peut aussi causer une perte auditive partielle de 30 à 40 dB.
Le bruit entraîne une surdité par destruction de l'oreille interne qui survient sous forme d'accident à la suite d'un son de très forte intensité, ou progressivement par exposition prolongée à des bruits trop intenses (avec une corrélation entre le temps d'exposition et le niveau sonore). Le mécanisme est la destruction progressive irréversible des cellules ciliées de l'organe de Corti dont les premières cellules touchées sont celles de la perception des sons de fréquence 4 000 Hz ce qui explique l'évolution clinique et la nécessité, prévue par la législation d'une surveillance régulière par audiogrammes des salariés exposés au bruit.
L'évolution passe généralement par 4 phases : 
De très nombreux médicaments peuvent provoquer des lésions souvent irréversibles au niveau des structures nerveuses de l'oreille entraînant une baisse, parfois sévère, des capacités auditives. Cette ototoxicité dépend de la dose et de la durée des traitements et elle est variable d'un sujet à l'autre ; elle est aggravée par une mauvaise élimination du produit incriminé (insuffisance rénale par exemple).
Si le traitement commence par le dépistage, c'est le plus souvent le triangle ORL/audioprothésiste/orthophoniste et audiologiste qui prennent en charge le patient, en dehors des traitements chirurgicaux.
Grâce à l'évolution de la médecine, de nombreux progrès sont apparus afin de permettre l'amélioration de la qualité de l'audition pour personnes atteintes de surdité. De nombreux appareillages et techniques chirurgicales sont désormais disponibles :
Pour pallier les difficultés de communication lié à la surdité, les personnes sourdes ont plusieurs méthodes. Ces méthodes sont utilisées en fonction des interlocuteurs et des capacités de la personne sourde[1].
Cette méthode consiste à apprendre progressivement à lire sur les lèvres d’autrui (c’est ce que l’on appelle la lecture labiale) et à prononcer les syllabes. L’objectif étant qu’il puisse parler comme un entendant. Cette approche est plus facile pour les sourds qui possèdent un reste auditif, pour les malentendants ou pour des enfants devenu sourds mais ayant déjà appris la langue orale. Elle est difficile et demande davantage de concentration et de travail pour les autres car la lecture labiale permet de saisir uniquement 30% du message émis[1].
Le langage parlé complété (LPC) est issu du Cued Speech américain, soit littéralement « parole codée»). Cette méthode permet d’ajouter à la parole des indices visuels. Elle associe à chaque syllabe un signe élaboré par configuration manuelle. Ce système permet de supprimer les ambiguïtés de la lecture labiale et de différencier des syllabes qui ont la même image labiale. Le LPC est plus utile que la lecture labiale seule. Le LPC permet au sourd de saisir la totalité du message car il offre un soutien à la lecture labiale[1]. Il permet ainsi l'accès à la langue française dans des conditions comparables à celles d'un enfant entendant. C'est un outil très efficace pour l'apprentissage de la lecture et de l'écriture, car il permet une totale autonomie du sourd face au support écrit.
Le langage parlé complété est facile à apprendre (une vingtaine d'heures pour acquérir l'ensemble des clefs) ; il demande ensuite une pratique régulière. Il est souhaitable de l'utiliser en famille (parents, fratrie, grands-parents, cousins…).
Les choix familiaux peuvent être relayés dans le cadre scolaire, puisque des codeurs et codeuses professionnels en LPC sont autorisés — dans le cadre de la loi du 11 février 2005 pour l'égalité des droits et des chances, la participation et la citoyenneté des personnes handicapées — à intervenir en classe. Leur présence permet aux élèves sourds de bénéficier de l'intégralité du cours dispensé par le professeur, des interventions des élèves et de l'ambiance de classe : bruits divers, blagues, chahut… La vie de la classe est restituée dans son ensemble et permet donc l'intégration et la participation de l'élève sourd au sein du groupe.
Le LPC existe en France depuis une trentaine d'années. L'association pour la promotion de la langue française parlée complétée (ALPC) dispense informations et formations pour les parents et les professionnels, notamment les orthophonistes (ou logopèdes) et les éducateurs spécialisés.
Les langues des signes (LS) sont des langues visuelles et gestuelles, et non sonores comme les autres langues. Cette méthode est un mode de communication gestuel élaboré par les sourds eux-mêmes. Il peut exprimer toutes les particularités d’une langue. La LSF ou la LSQ détient un vocabulaire, une grammaire et une syntaxe qui lui sont propres tout en conservant la syntaxe de la langue française. Elle peut se pratiquer simultanément au langage oral. La LSF propose un signe pour chaque mot. Elle utilise aussi la dactylologie (un signe pour chaque lettre alphabétique) et la lecture labiale. L’expression du visage est importante car elle permet de renseigner l’interlocuteur sur la vitesse, la taille ou la quantité. On peut considérer la langue des signes comme la langue naturelle des personnes sourdes car elle est visuelle et leur permet une compréhension immédiate. La LSF est fortement liée à l’identité des personnes de la communauté sourde, comme nous le verrons dans la partie historique. Actuellement, un système d’écriture de la langue des signes a été mis sur pied (« SignEcriture » ou « SignWriting »). Il permet de représenter chaque signe par le biais d’un dessin qui indique la configuration de la main, les mouvements, les expressions faciales etc. Ainsi, chaque langue des signes peut être écrite[1]. Contrairement à une idée très répandue, celle-ci n’est pas universelle. Toutefois, certains signes sont communs à plusieurs pays, et cela peut permettre à des sourds, pourtant originaires de pays différents, de communiquer rapidement entre eux grâce des signes très iconisés. La langue des signes existe en France depuis plus de deux siècles. Il existe des interprètes en langue des signes.
On considère que la langue des signes est la langue naturelle des sourds car elle est acquise par l'enfant de façon naturelle lorsque ses parents communiquent en langue des signes. Ceci n'est pas une appellation exclusive. De nos jours, l'adoption ou non de la langue des signes comme langue maternelle de l'enfant est fonction du choix d'éducation des parents et des professionnels de la surdité. Un sourd peut apprendre n'importe quelle langue à partir du moment où il en maîtrise au moins une, au même titre qu'un entendant.
La lecture labiale, quant à elle, permet au sourd de comprendre un interlocuteur oralisant, mais ne lui permet pas de percevoir l’intégralité du message. On estime que 30 % seulement du message est « lu » sur les lèvres, le reste étant interprété par la personne sourde suivant le contexte (suppléance mentale), ce qui donne souvent lieu à des malentendus. Par exemple, certains sons se ressemblent énormément sur les lèvres comme baba, papa et mama. Des phonèmes sont invisibles sur les lèvres comme le /r/ et le /k/ et sont donc difficiles à percevoir. Il existe même des blagues sourdes tirant parti de ces confusions comme meilleurs veaux pour « meilleurs vœux »…
L’éducation bilingue propose d’apprendre aux enfants la LSF et le français écrit[28]. Elle permet à la personne sourde d’avoir une communication avec d’autres sourds qui répond à ses besoins ainsi que de communiquer avec les entendants. Cela lui donne l’accès à la langue nationale de son pays tout en lui laissant une langue maternelle qui lui permettra de développer sa propre identité liée à la communauté et à la culture sourde. Les parents entendants ayant des enfants sourds sont sensibilisés à répondre aux besoins de leur enfant et ainsi lui permettre de s’exprimer avec son langage naturel (gestes). Il faut savoir que la plupart des enfants sourds naissent de parents entendants (90%)[1].
L’appareillage (audioprothèse) permet aux malentendants de mieux entendre et aux sourds profonds d’avoir des repères sonores. Il est plus utilisé par les personnes qui deviennent sourdes en vieillissant. Beaucoup de parents entendants d’enfants sourds choisissent aussi cette option. Tous les sourds ne portent pas d’appareils, soit parce qu’ils ont une surdité trop profonde pour s’en servir efficacement, soit par choix personnel : on sait surtout que, pour les sourds et certains malentendants (sourds de naissance), l'appareillage n'est pas un miracle ; les repères sonores sont perçus d'une façon très différente de celle des entendants.
L’implant cochléaire est un appareil électronique composé d’un implant interne (une plaque métallique placée derrière l’oreille et des électrodes insérées dans la cochlée lors d’une opération chirurgicale) et d’un implant externe (un aimant qui est collé derrière l’oreille et un boîtier externe ou un contour d’oreille qui captent le son et le transmettent à l’implant interne). Il est utilisé pour les enfants sourds profonds et les devenus-sourds adultes sous certaines conditions (ancienneté de la surdité, état de la cochlée, appareils classiques non efficaces, etc.). L’implant cochléaire permet ainsi aux sourds profonds de retrouver une perception auditive, mais il ne remplace pas l’ouïe et nécessite une rééducation auditive importante.
On le désigne parfois sous le sigle de CC (close captioning)
Cinq fournisseurs en France proposent des solutions d'accessibilité téléphonique : Sourdline, Deafi, Acceo, Elioz et Rogervoice. Ces fournisseurs proposent de services d'insertion professionnelle, d'accessibilité aux conférences et/ou d'accessibilité de l'accueil téléphonique ou du service client.
A la suite de l'article 105 de la loi pour une République numérique adoptée en octobre 2016, depuis le 8 octobre 2018 les opérateurs proposent l'accessibilité téléphonique inclus avec un abonnement téléphonique. Le service est offert pour 3 heures d'appels par mois, du lundi au vendredi 8h30 à 21h et le samedi de 8h30 à 13h, hors jours fériés. Ce service est disponible via l'application Rogervoice (pour les abonnés des opérateurs membres de la Fédération française des télécoms notamment Bouygues, Orange et SFR) ou via Free Relais.
Un rapport du gouvernement[33] prévoit de revoir le dispositif global pour améliorer le parcours utilisateur et les conditions d'accès en France d'ici 2024.
Dans l’ensemble, on distingue deux grandes méthodes dans l’éducation des sourds :
Toutefois, ces deux méthodes d’éducation ne sont pas forcément contradictoires, même si elles sont l’objet de conflits et de discussions interminables entre les partisans de chaque méthode pour savoir laquelle est la meilleure.
Normalement, les parents confrontés à ce choix peuvent opter pour l’une ou l’autre des éducations dispensées aux enfants sourds bien que les moyens ne soient pas également répartis sur l’ensemble du territoire : on voit des familles déménager ou bien effectuer des transports assez longs pour les enfants sourds. Le choix se fait donc entre l’enseignement de la langue des signes et l’oral avec appareillage, ou la langue française parlée complétée (LFPC) et l’oral avec appareillage. Il est également possible d’utiliser les trois au départ, puis de ne retenir ensuite que la formule qui réussit le mieux à l’enfant sourd.
La problématique développée ici s’applique aux enfants sourds et malentendants qui éprouvent des difficultés lors de l’apprentissage de la lecture[35].
Certaines compétences orales sont utiles pour l’apprentissage de la lecture, c’est-à-dire : la référenciation d’illustration au texte, la distinction auditive, la représentation des phonèmes en mémoire à court terme de même que la conscience phonologique qui justement fait défaut chez les enfants sourds[36].
Ceux-ci n’y accèdent pas par la modalité auditive comme tous les autres enfants entendants, mais par la modalité visuelle. Ce qui engendre un déficit lexical majeur chez les enfants atteints de déficience auditive.
Lors des prémisses de la lecture, l’enfant sourd ou malentendant traite l’information en privilégiant la modalité visuelle et adopte une stratégie de lecture appelée “Visuo-graphique”. Cette méthode permet de comprendre le sens d’un mot dont on a mémorisé l’orthographe.
Les enfants sourds qui acquièrent un bon lexique orthographique (mémorisation visuelle) vont assimiler de nombreux mots écrits et ainsi réduire leurs difficultés lors de l’apprentissage de la lecture[37].
La réussite de l’apprentissage de la lecture est lié à la facilité d’associer l’écrit au signe[35]. Un autre avantage est mis en avant pour développer un bon apprentissage en lecture-écriture, il faut développer préalablement de bonnes capacités langagières par la langue orale ou la langue des signes.
Le développement dans l’éducation des technologies de l’information et de la communication a eu un réel impact sur la communauté sourde. Des applications, ainsi que la création de logiciels, leur ont permis de communiquer et apprendre. Quelques formations par la technologie associe le « visuel », l’écriture et la langue des signes. Malgré tout, il ne faut pas oublier qu’il existe une fracture numérique concernant l’accès à ces nouveaux outils[36].
Ce type d’enseignement par le numérique permet de prendre en considération l’élève sourd ou malentendant sans gêner son développement cognitif, émotionnel, intellectuel et culturel.
La première étape a été la reconnaissance de la Langue des Signes par le Parlement de la Communauté française de Belgique. En effet, il reconnaît officiellement la Langue des Signes belge francophone, le 21 octobre 2003, tandis que le Parlement flamand la reconnaît plus tardivement, soit le 26 avril 2006 (Décret relatif à la reconnaissance de la langue des signes, 2003).
Par la suite, la Belgique a signé et approuvé la Convention ONU, adoptée le 13/12/2006. Cette convention énonce dans son article 24 que les Etats signataires veillent à ce que le enfants handicapés ne soient pas exclus, sur le fondement de leur handicap, de l’enseignement primaire gratuit et obligatoire ou de l'enseignement secondaire. Et que chacun d’entre eux dispose d’aménagements raisonnables en fonction de ses besoins.
Selon le décret relatif à la reconnaissance de la langue des signes, nous pouvons définir les aménagements raisonnables comme des outils d’aide à la personne handicapée, des outils qui permettent de réguler l’équité entre les élèves et permettre une progression du parcours scolaire.
Toujours selon le décret, un besoin spécifique est issu d’une spécificité, d’une difficulté, d’une condition permanente ou semi-permanente qui bloque l’apprentissage et demande une aide spécifique pour l’apprenant afin de l’aider à surmonter cet obstacle.
Les missions du service d’aide à  l’inclusion scolaire et extrascolaire sont également cadrées dans un autre décret décret. Il s’agit notamment de collaborer avec la personne handicapée et sa famille en mettant en valeur ses ressources et son potentiel, de soutenir cette personne, de permettre qu’elle soit accueillie correctement et qu’elle bénéficie de prestations diverses complétant l’action scolaire, et finalement de soutenir son autonomie .
Les téléphones portables adopteront eux aussi très rapidement le mode 3G : un téléphone léger, incluant la visiophonie, non dédié aux sourds, donc susceptible d’évolutions, qui permet de communiquer directement en langue des signes. Cet échange direct en langue des signes est également rendu possible, dans le cadre privé interpersonnel par l’apparition de la web cam et, dans le domaine institutionnel, par le développement de la visioconférence[38].
Communiquer est un besoin essentiel pour l’être humain. La communication permet d’échanger des impressions, des messages et de se comprendre. Elle est indispensable à la participation à la vie sociale. C’est au niveau de la communication que se situent les difficultés de la rencontre entre sourds et entendants. En effet, ils n’ont pas le même mode de communication et cela implique une compréhension parfois partielle entre l’un et l’autre. C’est dans la communication que nous pouvons voir à quel point la surdité est un handicap qui touche les deux personnes qui souhaitent être en relation[1].
Gilles Ferrol définit la sociabilité comme le caractère sociable des individus et comme une aptitude psychologique à nouer contact avec autrui. Elle peut se voir dans les rapports avec la parentèle, le voisinage, l’engagement associatif, les sorties et réceptions. La sociabilité comprend l’ensemble des relations objectives entretenues par un individu avec les autres[1].
Selon Mercklé, la sociabilité peut se définir par le recensement de ses manifestations extérieures les plus facilement mesurables comme des réceptions à domicile, des sorties, la fréquentation des bals, des cafés, la pratique du sport, les promenades, les jeux, le téléphone, les sms, les mails, le courrier, les visites, etc. [1]
Le réseau est l’ensemble des liens ou des relations d’un individu donné avec d’autres personnes. Le capital social peut être vu comme le réseau durable de relations d’une personne. Il constitue ses ressources actuelles ou potentielles. Tout réseau se base sur des relations entre les personnes, que ce soit en échanges monétaires, transferts de biens, échanges de services, transmissions d’informations, ordres, contacts physiques, interactions verbales ou gestuelles, ou encore une participation commune à un même événement. Un réseau fonctionne sur le principe que chacun donne quelque chose et reçoit en échange. Selon Mercklé, les relations sociales peuvent être électives (relations amicales), semi électives (relations de parenté), non électives (relations de travail ou de voisinage), selon que la personne les choisisse ou non[1].
La relation entre le sourd et l’entendant est intéressante car elle réunit deux personnes différentes de par leur langage, leur culture et leurs représentations[1].
Le sourd ou l’entendant dans sa communauté connaît les plaisirs d’une communication immédiate et naturelle.
Les langues parlées sont faites pour être écoutées. Elles n’utilisent qu’un seul type de signes : la voix. Lorsqu’une personne parle, elle utilise sa voix, elle l’entend et peut donc la moduler (c’est ce que l’on appelle le feed-back auditif). Pour pouvoir l’écouter, elle utilise le canal auditif.
Les langues gestuelles, utilisées par les sourds, sont faites pour être vues : pour « parler », des gestes sont utilisés (c’est ce que l’on appelle le feed-back visuel).
Ainsi, selon Mottez (1981), dans ces deux types de communication, tout ce qui est émis est perçu. Ils sont directs et n’utilisent qu’un seul type de signe. Chaque signe émis est perçu à la fois par l’émetteur et le récepteur. C’est la meilleure solution pour que chaque personne puisse s’exprimer totalement et ne ressente aucune difficulté à comprendre.
Lors de la rencontre entre sourd et entendant, la communication sur le mode oral ne peut pas être direct. En effet, lorsque le sourd est récepteur du message oral émis par l’entendant, il reçoit uniquement une image visuelle (donnée par une lecture labiale). Lorsqu’il est émetteur, c'est-à-dire lorsqu’un sourd participe à un échange verbal, il utilise plusieurs signes : visuel et kinesthésique. Il a donc besoin de temps pour percevoir le sujet de la conversation, ne comprend pas tout et ne peut s’exprimer aisément, surtout dans une conversation à plusieurs. Seule la langue des signes lui permet de participer pleinement à une conversation.
Les difficultés que perçoivent les sourds dans la communication avec les entendants peuvent entraîner un sentiment d’isolement et de rejet. Au sein de la communauté sourde, par contre, ils ont en commun une langue, des valeurs, des règles de comportement, des traditions et une identité[1].
Les relations entre sourds et entendants sont marquées par les difficultés de communication, au même titre que deux personnes appartenant à des communautés linguistiques différentes.
Comme démontré dans les nouvelles approches du handicap, les sourds ont une déficience auditive.
Le handicap n'apparaît que lorsqu'ils sont confrontés au monde des entendants qui n'est pas adapté à leur déficience. Leur handicap n'en est plus un lorsqu'ils se regroupent entre eux et usent du même mode de communication[1].
Principales difficultés pour la personne sourde :
• Lecture labiale
• Compréhension du vocabulaire, de la grammaire et de la syntaxe (difficultés rencontrées lorsqu’on apprend une langue étrangère)
• Manque de précision de la part de l’entendant
• Manque de connaissances de l’entendant à propos de la surdité : il ne remarque pas que la personne est sourde ou pense que l’appareil auditif lui permet de bien entendre. Il lui parle donc comme à un autre entendant.
• L’entendant oublie parfois des réflexes qui pourraient aider à la compréhension (écrire ou faire des gestes)[1].
Ces difficultés à se comprendre peuvent altérer la relation et les interactions entre sourd et entendant. Il arrive parfois que l’un ou l’autre fasse semblant d’avoir compris. Il est possible également que les deux personnes parlent de sujets différents car dans les deux cultures les références sont différentes. Il se peut que l’un parle d’un thème peu connu de l’autre ou que les sous-entendus soient différents d’une culture à l’autre.
Ces difficultés de compréhension, les efforts qui s’ensuivent et les malentendus qui peuvent survenir créent une barrière à la communication : les deux personnes ont peur de mal se comprendre et elles retirent beaucoup moins de plaisir que dans une conversation habituelle. C’est ce qui peut les rendre réticentes à engager la conversation, à créer des relations sociales et à partager de l’amitié avec l’autre population. Si les difficultés se font trop persistantes, chacun préférera retourner dans sa communauté afin de pouvoir avoir des conversations approfondies.
Dans la communication avec l’entendant, un sourd peut se sentir assisté et diminué car il ne peut ni tout comprendre, ni montrer toutes ses capacités cognitives. Il ne peut pas toujours participer aux conversations de groupe et peut ressentir la sensation d’être étranger. Un sourd qui ne participe pas à la vie de sa communauté vit souvent des expériences négatives, il peut se sentir différent et anormal. Il peut plus rarement et difficilement avoir une communication en profondeur et n’a pas le plaisir de la communication sans efforts et sans stress. Il n’aura peut-être pas accès à certaines informations du fait de ses problèmes de compréhension.
Au sein de sa communauté, par contre, le sourd peut expérimenter réellement la participation à la vie de groupe car tous ont la même langue et ils partagent des expériences de vie semblables liées à la surdité. L’appartenance des sourds à leur communauté est très grande.
La participation à la communauté des sourds permet d’échanger des expériences, des savoir-être et savoir-faire spécifiques aux sourds et qui leur permettent l’intégration dans le monde entendant. Ils y partagent également des informations générales sur le monde extérieur. Un sourd qui a fait l’expérience de sa communauté n’attribue généralement pas les problèmes de communication avec les entendants à leur mauvaise volonté ou à sa déficience. Il est conscient que c’est la différence de langue, de culture et de mode de vie qui en est responsable.
Officiellement, le terme utilisé est "déficient auditif". Les termes "sourd" et "malentendant" sont souvent l'objet d'imprécisions. Une personne sourde peut l'être de naissance ou le devenir, tout comme pour un malentendant. On peut faire la différence entre un sourd et un malentendant en fonction de la perte auditive de la personne: un sourd total, profond ou sévère, préféra se désigner comme sourd. Le terme malentendant est trop souvent employé comme euphémisme pour désigner de manière politiquement correcte des personnes qui n'entendent pas du tout. De plus, ce terme peut avoir pour certains une connotation négative qui met en valeur un "manque" par rapport à des personnes "normales". Le mieux reste de demander à la personne concernée par quel terme celle-ci préfère être désignée.
Pour un sourd de naissance ayant appris à parler (dit « oraliste ») ou un devenu-sourd qui utilise la langue orale, on a tendance à directement les considérer comme malentendant. De la même manière, si un sourd sait et veut parler oralement, cela ne signifie pas qu'il ne connaît pas la langue des signes. Certains sourds savent à la fois parler et signer : c'est le bilinguisme. D'autres sourds (généralement de naissance) ont appris à parler dans leur enfance, mais ne souhaitent pas utiliser leur voix, pour de multiples raisons (respect de leur identité sourde, accent sourd trop prononcé etc.).
Le terme « sourd-muet » est quant à lui désuet. Il continue toutefois à être utilisé dans les médias et dans les actes officiels, preuve du manque d'information vis-à-vis de ce handicap. L'APEDAF (Association des parents d'enfants déficients auditifs francophones) milite contre l’utilisation de ce terme[39]. Ils donnent ainsi 5 raisons de ne plus dire « sourd-muet » :
Dans les temps anciens, on définissait une psychologie propre aux personnes sourdes : Itard leur attribuait un caractère méfiant et crédule. D'autres voyaient les sourds comme des personnes agressives et colériques.
Chaque individu sourd utilise ce handicap à sa façon : le sublimer, le nier, en faire un point d'identification ou un point de rejet, le haïr ou l'aimer, s'y reconnaître ou le renier. Tout est possible et c'est l'invention propre de chaque individu.
Des études ont prouvé que les personnes sourdes non atteintes d'un déficience visuelle remarquaient plus de détails tactiles et visuels que les entendants.
D'autres études ont pu montrer que la motricité des sourds était différentes de celle des entendants. Ils ont remarqué certaines infériorités chez les sourd mais ils les ont attribuées uniquement au « handicap communico-linguistique ».
D'autres encore ont recherché quel hémisphère du cerveau était prédominant chez les sourds (l'hémisphère gauche en général chez les entendants). Les chercheurs en sont venus à la conclusion qu'il pourrait exister une dominance du mode visuel sur le mode verbal chez les personnes portant un handicap de l'ouïe.
D'autres particularités de personnalité dues à l'influence de l'entourage du sourd :
– À l'influence de la famille : Il existe plus de problèmes chez les sourds ayant des parents entendants que chez les sourds ayant des parents sourds également. Un enfant avec des parents sourds aura plus facilement accès à la communication grâce aux signes que dans une famille entendante.
– À l'influence de l'étiologie : La maladie infantile de surdité peut avoir des séquelles comme des problèmes d'inadaptation.
– À l'influence du milieu : L'entourage influence le développement de la personnalité. L'attitude des entendants envers un sourd peut donc influencer sa capacité à créer des liens avec eux ainsi que sa propre acceptation de son handicap.
– Au mode de communication : Du fait de la différence de mode de communication, les capacités d'adaptation d'un groupe social à l'intérieur d'un autre peuvent être altérées[40].
Les sourds étaient perçu différemment selon les cultures et les religions.
Les Égyptiens et les Perses pensaient que cette infirmité était un signe de la faveur céleste.
Au contraire, dans d'autres religions, les parents cachaient l'existence d'un enfant sourd.
En Grèce, les sourds ont surtout été considérés comme des personnes non douées de raison ou d'intelligence[40].
Le peu d'écrits retrouvés sur les sourds laisse à penser qu'ils participaient à la vie des villages en se faisant comprendre par des gestes et des mimes tout en étant considérés comme les « débiles » ou « idiots » du village[40].
Les prêtres chargés de l'instruction des enfants des familles nobles commencèrent à éduquer les enfants sourds. Ils leur apprenaient à parler, à lire et à écrire. Certains d'entre eux utilisaient les gestes des sourds pour leur faire comprendre la langue orale, mais considéraient ces signes comme trop pauvres pour pouvoir exprimer une pensée. Le but poursuivi était la « démutisation » des sourds.
Il existait tout de même quelques exemples de sourds qui enseignaient à d'autres sourds en langue gestuelle[40].
L'abbé Charles Michel de l’Épée remarqua que les sourds utilisaient un langage complexe pour communiquer entre eux. Il supposa alors que les gestes pouvaient exprimer les pensées aussi clairement que le langage oral et décida d'apprendre ce langage gestuelle.
Le religieux fonda une école pour apprendre le français aux enfants sourds et développa un système de langage gestuel appelé « signes méthodiques ». Ce système était un mélange de gestes naturels qu'utilisaient ses élèves entre eux et de signes de son invention.
Ce système était efficace pour dicter aux élèves ce qu'ils devaient écrire en français. Cependant, le langage méthodique était tellement compliqué pour les sourds qu'ils comprenaient rarement ce qu'ils écrivaient. Mais puisqu'ils étaient réunis entre eux, ils pouvaient communiquer et perfectionner de cette manière leur langue naturelle.
Après la mort de l'abbé, son successeur inventa tellement de nouveaux signes que les élèves ne comprirent plus rien. De nombreux professeurs choisirent de revenir à un enseignement exclusivement oral[40].
En 1817, un éducateur de sourds, Bébian, prôna pour la première fois une éducation bilingue français, langue naturelle signée. Cette avancée permit aux sourds adultes de devenir professeurs.
La communauté sourde se développa et de nombreuses associations furent créées. De plus, un plus grand nombre de sourds atteignirent un haut niveau de formation.
Vers la fin des années 1800, les premiers appareils auditifs furent inventés et les gens pensaient de plus en plus que l'homme pourrait grâce à la science guérir les sourds.
À la suite de cela, au congrès international de Milan, en 1880, les oralistes interdirent la langue des signes. Cela leur fut simple puisque sur 164 participant, seulement 2 éducateurs étaient sourds.
Tous les livres concernant l'éducation gestuelle furent brûlés ou cachés, les éducateurs sourds furent licenciés. On obligea les enfants sourds à apprendre à parler sans utiliser les gestes. En Suisse comme en France et dans de nombreux pays d'Europe, les écoles optèrent pour l'éducation oraliste.
À la suite de cette interdiction, la majorité des sourds de 1950 furent sous-éduqués car ils avaient quitté l'école sans avoir un niveau suffisant. Ils ne trouvaient que des emplois sans responsabilités et ne pouvaient pas avoir de contact avec leurs collègues entendants puisqu'ils parlaient à peine français.
À cette époque la situation des sourds était bien meilleure aux États-Unis puisque la langue des signes n'était pas interdite et qu'elle continuait de se développer.
Grâce aux révolutions des années 1960, la population porta plus de respect aux langues minoritaires.
Après le Sixième Congrès de la Fédération Mondiale des Sourds en 1971, la langue des signes fut enfin reconnue par des enseignants interprètes qui pouvaient traduire de manière très efficace.
En 1979, en France, les écoles commencèrent à donner des classes d'enseignement bilingue pour les enfants sourds. Des cours de LSF furent mis en place pour la parentèle des enfants sourds et pour les enseignants.
Cependant, c'est seulement en 1991 que la loi Fabius en France reconnut la langue des signes pour l'éducation des jeunes sourds[40].
  Surdité dans l'art et la culture 
Sur les autres projets Wikimedia :Pédagogues : 
Médecins et chercheurs : 
 Mise en garde médicalemodifier - modifier le code - voir Wikidata (aide) La tuberculose est une maladie infectieuse causée par la bactérie Mycobacterium tuberculosis, qui se transmet par voie aérienne, avec des signes cliniques variables. Elle touche le plus souvent les poumons et peut parfois atteindre d’autres organes.
Elle arrive en tête des causes de mortalité d'origine infectieuse à l’échelle mondiale, devant le sida[1]. L'Organisation mondiale de la santé (OMS) rapporte à travers son rapport annuel consacré à la tuberculose et sorti en 2015 que 1,5 million de personnes sont mortes de la tuberculose l’année précédente[2]. Parmi les nouveaux cas de tuberculose enregistrés en 2019, 87 % sont survenus dans les trente pays présentant la plus forte charge de la maladie. Deux tiers des cas sont concentrés dans huit pays, avec l’Inde en tête, suivie de l’Indonésie, de la Chine, des Philippines, du Pakistan, du Nigéria, du Bangladesh et de l’Afrique du Sud[3].
Le rapport 2015 de l’OMS rapporte également que la mortalité a baissé de 47 % depuis 1990 grâce en grande partie au développement des traitements ainsi que les modalités de dépistage et de prévention[2]. Cela représente un bon indicateur du progrès réalisé par les systèmes de soins, diagnostiques et thérapeutiques.
Bien que la maladie soit curable, la forte prévalence dans les pays les plus pauvres s’explique par la conjonction d’un ensemble de facteurs : précarité, promiscuité, dénutrition, analphabétisme, infrastructure médicale insuffisante, et surtout épidémie d’infection par le VIH. La prévalence s’accroît en cas de guerre ou de famine[4]. On constate également que l’accès au diagnostic et au traitement pose un problème dans les pays concernés.
La tuberculose est connue au XVIIe siècle sous le nom de « peste blanche », en écho à la peste noire qui ravage l'Europe à cette époque[5].
Le terme « tuberculose » est utilisé pour la première fois par Johann Lukas Schönlein en 1839[5]. Il est issu du nom de la lésion unitaire de la maladie, le « tubercule », utilisé depuis le XVIIe siècle et formé depuis le latin tuber signifiant « excroissance »[5]. La tuberculose « miliaire » (caractérisée par la dissémination de très nombreux nodules de petite taille dans les deux poumons) tire son nom de la ressemblance de ces nodules avec des grains de millet[5].
Dans les textes consacrés à la tuberculose, ce terme est souvent abrégé en TB[6],[7] ou TBC[8].
Maladie au long cours aux manifestations très diverses et affectant aussi bien humains et animaux, la tuberculose est une maladie très ancienne, mais dont l'unité nosologique et l'étiologie ne furent établies qu'au XXe siècle[9].
Une ancienne théorie du XXe siècle postulait que la tuberculose humaine à M. tuberculosis dérivait de la tuberculose bovine à M. bovis, en étant une conséquence de la domestication animale[10].
Les études génomiques indiquent que M. tuberculosis et bovis ont évolué à partir d'un ancêtre commun présent chez les mammifères et qui aurait infecté les hominidés d'Afrique de l'Est, il y a trois millions d'années. Cet ancêtre commun aurait coévolué avec ses hôtes pour aboutir aux mycobactéries humaines et animales actuelles. Les souches modernes pathogènes M. tuberculosis seraient issues d'un clone apparu il y a 15 000 à 20 000 ans, ou 11 000 ans, à partir d'une souche ancestrale de M. tuberculosis[11],[12].
Dès l’Antiquité gréco-romaine, plusieurs auteurs ont décrit une maladie amaigrissante au long cours, dénommée suivant les uns « phtisie » (pour dépérissement), suivant les autres « tabès ». Hippocrate (Ve – IVe siècle av. J.-C.) puis Galien (IIe siècle apr. J.-C.) et Caelius Aurelianus (Ve siècle) en ont dressé les symptômes, notamment pulmonaires[13]. Arétée de Cappadoce (fin du IIe siècle) en a cependant dressé la description la plus détaillée. Ces descriptions initiales n’ont guère subi de modifications notables jusqu’au début du XIXe siècle.
L'origine de la maladie a été débattue durant l'Antiquité, entre héréditaire ou contagieuse[14]. Plus tard, Avicenne (début du XIe siècle) décrit la tuberculose comme uniquement contagieuse[15].
En 1733, Pierre Desault (1675-1737)[16] publie un Essai sur la phtisie[17], de même que François-Emmanuel Fodéré en 1795[18], mais la présentation la plus claire est celle de Gaspard Laurent Bayle avec ses Recherches sur la phtisie pulmonaire publiées en 1810. Bayle la définit moins par son expression clinique que par ses lésions anatomiques dont le tubercule[19],[20].
L'invention du stéthoscope par René Laennec en 1817 facilite le diagnostic de la maladie.
En 1839, le médecin allemand Johann Lukas Schönlein rassemble en une description unifiée les manifestations cliniques disparates de la maladie. Jusqu'alors, « phtisie » et « tuberculose » étaient souvent considérées comme deux entités, voisines mais distinctes[21]. Si Schönlein forge en 1834 le terme de tuberculose (en allemand: Tuberkulose), composé d'un nom latin et d'une terminaison grecque[22], la littérature médicale, tout comme le langage commun, continuera d'utiliser indistinctement, jusqu'au début du XXe siècle, les termes de « phtisie », « consomption » et « tuberculose »[23].
De 1865 à 1868, le médecin Jean-Antoine Villemin reproduit chez les animaux (lapins, cobayes) les lésions de la tuberculose humaine, par inoculation de tissu altéré humain[24]. Il peut ainsi affirmer que cette maladie, de nature jusqu'alors inconnue, est due à un microbe invisible par les moyens techniques de l'époque. Il démontre en 1869 que la transmission se fait par voie aérienne. Ses conclusions se heurtent à une forte opposition, en France notamment. Elles inspirent cependant des travaux comme ceux d'Edwin Klebs[25], Julius Cohnheim, Carl Salomonsen et Tappeiner qui aboutissent à établir de façon indubitable la contagiosité de la maladie[26].
En 1882 enfin, à la suite des travaux de Louis Pasteur, Robert Koch met en évidence le bacille tuberculeux à partir de lésions humaines[24] : le 24 mars 1882, il communique d'abord à la Société de physiologie de Berlin une note sur la recherche et la culture du bacille de la tuberculose ; le 10 avril, il publie dans le Berliner klinische Wochenschrift un mémoire sur l'étiologie de la tuberculose qu'il rapporte à un bacille décelé dans les crachats et les lésions tuberculeuses humaines.
En 1894, Carlo Forlanini met au point la première méthode thérapeutique invasive avec le pneumothorax artificiel intrapleural : par une injection d'air dans la cavité thoracique, entraînant la rétraction du poumon infecté, il obtient une amélioration de la maladie.
En 1940, Selman Waksman découvre l'action antituberculeuse de l'actinomycine puis, en 1942, de la streptothricine. Ces antibiotiques ne peuvent toutefois être utilisés en thérapeutique humaine ou vétérinaire du fait de leur trop grande toxicité.
En 1943, Waksman découvre enfin la streptomycine qui permet, un an plus tard, la première guérison par antibiotique d'un malade gravement atteint de tuberculose[27].
En 1948, a lieu le premier essai clinique randomisé de l'histoire de la médecine : l'épidémiologiste Austin Bradford Hill montre que la streptomycine[Note 1] est plus efficace que la collapsothérapie[28].
Le nombre annuel de nouveaux cas dans le monde, incluant les cas de rechute, était en 2006 d'environ 5,4 millions[29]. Il était en 2018 estimé à dix millions par l'OMS[30]. Environ 58 % des nouveaux cas se trouvent dans la région sud-est de l’Asie et les régions du Pacifique ouest. L'OMS estime par ailleurs qu'environ un quart de la population mondiale est porteuse d’une tuberculose latente, c’est-à-dire est porteuse de la bactérie sans toutefois développer de symptômes et sans être contagieuse. L'organisme estime entre 5 et 15 % le risque pour les porteurs sains de développer la maladie à un moment de leur existence[30].
La prévalence de la tuberculose en 2015 a chuté de 42 % depuis 1990. Elle varie d'un pays à un autre en fonction de plusieurs facteurs dont le niveau socio-économique ; le rapport annuel de l'OMS nous apprend que les pays en développement sont les plus touchés (95 % des cas) et en particulier la région de l’Asie du Sud-Est, avec 44 % des nouveaux nouveaux et l'Afrique (28 % des nouveaux cas mondiaux en 2018). La tuberculose est une cause majeure de mortalité chez les personnes infectées par le VIH. Elle serait responsable de 13 % environ des décès par sida dans le monde[31].
Environ 1,5 million de personnes sont mortes de la tuberculose en 2018, dont une personne sur six était porteuse du VIH.
La tuberculose, sans bénéficier de programmes de prévention et de cure aussi importants, tue ainsi à peu près deux fois plus[32] que le sida, soit environ 4 000 personnes par jour[33].
Paradoxalement cette augmentation du nombre de morts, rapportée à l'explosion démographique mondiale, représente un progrès dans la prévention. Par rapport à 1990, c'est-à-dire sur une période plus longue, la baisse est en effet de 47 %[33]. Autrement dit, une politique de santé efficace montre au terme de quinze années ses limites face à un certain nombre de freins à la prévention. Face aux formes résistantes et multirésistantes de la maladie, la bataille s'achemine en 2015 vers une défaite[33]. Il manque au programme mondial de recherche coopérative les deux tiers de son budget annuel, lequel reste limité à 700 millions de dollars, alors que la dépense mondiale totale consacrée à la prévention, assumée principalement par les États, atteint presque 6 milliards d'euros[33].
En 2014, on estime à 190 000 le nombre de morts par tuberculose résistante[33]. Ils ont été recensés dans 105 des 205 pays transmettant leurs informations à l'OMS[33].
La maladie comprend deux étapes : la tuberculose-infection qui peut rester latente et silencieuse, puis la tuberculose-maladie où les troubles se manifestent. Le risque de contracter une tuberculose dépend d'abord du risque d'exposition au bacille (infection), puis du risque de développer la maladie après infection.
Cette infection touche l'adulte jeune, et les hommes sont près de deux fois plus atteints que les femmes. La malnutrition et les intoxications médicamenteuses sont des causes reconnues de l'augmentation du nombre de cas.
Après l’âge de 60 ans, des personnes ayant été contaminées dans leur enfance ou leur adolescence (par M. tuberculosis, ou M. bovis), peuvent dans certains cas déclarer une tuberculose évolutive.
La tuberculose peut revêtir différentes formes selon la localisation du foyer infectieux. La tuberculose pulmonaire est la forme la plus fréquente et la source essentielle de la contagion. À partir du poumon, le bacille peut diffuser dans l'organisme et causer d'autres atteintes, ganglionnaires, ostéoarticulaires et urogénitales notamment. Les formes les plus létales sont les formes diffuses (miliaires) et méningées.
La primo-infection regroupe les manifestations cliniques, radiologiques et bactériologiques, survenant après un premier contact infectant avec le bacille de Koch (BK). Elle est souvent asymptomatique chez l’adulte[47] mais la littérature rapporte qu’elle peut être symptomatique chez les enfants à 90 % associant les signes généraux ; les symptômes sont dominés par la toux chez plus de la moitié des cas, les douleurs thoraciques chez 20 % des cas, et la dyspnée[48]. L’examen clinique met en évidence des adénopathies périphériques chez 37,5 % et la confirmation du diagnostic se fait à travers l’intradermoréaction à tuberculine (IDR)[48].
La tuberculose pulmonaire est la forme la plus fréquente et présente plus de 85 % des cas[49],[50]. Le tableau le plus classique et le plus fréquent chez les tuberculeux pulmonaires laisse définir le syndrome d’imprégnation tuberculeuse : fièvre, sueur nocturne, amaigrissement, anorexie[50].
Le dépistage de la tuberculose pulmonaire se fait généralement au cours de consultation habituelle par un interrogatoire auprès des malades qui présentent principalement des signes d’imprégnation ainsi que des symptômes respiratoires persistant pendant plus de deux semaines[51]. Cependant, le tableau symptomatique peut être polymorphe, représentatif dans son ensemble de toute la séméiologie bronchopulmonaire.
Clinique de la tuberculose :
+++ = courant (> 50 %), ++ occasionnel, + rare
Le diagnostic repose sur les différents éléments allant de l’interrogatoire à l’examen physique et clinique. L’interrogatoire doit enquêter sur la notion de contagion en tenant compte de la physiopathologie de la maladie et les modalités du développement de la lésion. L’installation de la maladie peut se faire progressivement en s’étalant sur plusieurs semaines et l’examen clinique se révèle utile à la recherche des éléments d’orientation vers l’affirmation du diagnostic. Quant à l’examen clinique, les éléments qui peuvent faire suspecter la TBC sont nombreux[47] :
En revanche quand il s’agit d’une installation brutale, l’hémoptysie et crachats sanguins sont plus observés chez les personnes atteintes ainsi que l’épanchement pleural est observé que ce soit aérien ou liquidien[52].
Le clinicien devant un contexte de symptomatologie pulmonaire qui persiste au-delà de 15 jours, et en tenant également compte de la prévalence dans la région en cause, est en mesure de suspecter la tuberculose et procède à la procédure de l’affirmation de diagnostic. Cela doit conduire vers des examens complémentaires : radiographie du thorax et l’examen cytobactériologique des crachats[53]. Le diagnostic définitif est fondé sur l’isolement ou la culture du bacille, habituellement à partir des crachats.
Les cavernes sont une complication fréquente de la tuberculose pulmonaire[54]. On appelle caverne une cavité creusée au sein du parenchyme pulmonaire[55]. Les bacilles tuberculeux se développent initialement dans le poumon sous forme de nodules, appelés granulomes, qui sont peu à peu entourés de lymphocytes et de macrophages destinés à contenir l'infection[54]. Un granulome peut évoluer soit vers la disparition sans cicatrice, soit vers la caverne, sans que les mécanismes sous-jacents soient compris en totalité[55]. Les vestiges de macrophages détruits occupent le centre du granulome et forment la majeure partie de la nécrose caséeuse[55]. Chez certains patients, cette nécrose caséeuse se liquéfie et devient un milieu de culture adapté aux bacilles, qui prolifèrent. Des enzymes protéolytiques érodent alors la capsule fibreuse située en périphérie du granulome, et son centre liquide peut alors se vider peu à peu[55]. Lorsqu'une caverne tuberculeuse arrive en communication avec l'arbre bronchique, la dissémination des bacilles dans l'air expiré augmente la contagiosité[54],[56]. Par ailleurs, la quantité importante de bacilles contenus dans les cavernes favorise le développement de résistances aux anti-tuberculeux[56].
Elle est très fréquente au même titre que la tuberculose pulmonaire. Les adénopathies sont souvent médiastinales et hilaires[53]. Elles sont en général de taille modérée, mais peuvent parfois obstruer une bronche. En périphérie les adénopathies sont surtout cervicales, puis axillaires et inguinales. Initialement fermes et mobiles, les ganglions sont ensuite fixés par une péri-adénite avec peu de signes inflammatoires[57]. Les fistules externes à bord irrégulier laissent ensuite sourdre (suinter) un pus caséeux qui se recouvre de croûtes. C'est cette maladie, nommée scrofule ou écrouelles que les rois de France et d'Angleterre étaient censés guérir par simple toucher. La croyance et les cérémonies qui y étaient attachées ont perduré jusqu'au XIXe siècle.
Le diagnostic repose sur l’examen bactériologique du liquide de ponction et l’examen anatomopathologique à la suite de la biopsie ganglionnaire[57]. L’apparition des signes généraux doivent encourager un traitement sans attendre le diagnostic microbiologique dans ces pays.
Selon la même étude[57] faite en Tunisie (une région endémique de la tuberculose) qui a porté sur cinquante patients atteints de la tuberculose ganglionnaire, il a été remarqué que les adénopathies étaient principalement cervicales (75 %), puis médiastinales (21 %), sus-claviculaires (9,4 %) et axillaires (6,3 %). Les tailles ont été retrouvées surtout entre 3 et 5 cm. Les adénopathies étaient inflammatoires ou fermes. Dans 21,9 % des cas, la tuberculose ganglionnaire était associée à d’autres types de tuberculose. La bactériologie a permis le diagnostic chez 65,6 % des cas.
La tuberculose urogénitale est responsable de 14 à 41 % des atteintes extrapulmonaires[58]. Le rein est très souvent infecté lors d’une tuberculose miliaire le plus souvent au niveau du cortex rénal. Cliniquement, les lésions sont très souvent unilatérales. Au niveau du rein, elles sont préférentiellement situées dans la médullaire où elles vont produire des granulomes épithélioïdes avec une nécrose caséeuse (lésion spécifique à la tuberculose) aboutissant à une destruction tissulaire.
L’atteinte se manifeste par des symptômes liés à la distension rénale en cas d’atteinte urétérale. Elle peut se manifester cliniquement par une cystite banale. Or, le diagnostic est évoqué devant une pyurie sans germe. En cas de lésion génitale, les lésions touchent fréquemment l’épididyme se traduisant par une épididymite.
Le clinicien oriente vers l’examen d'urine (recherche de bacille de Koch) des urines afin de confirmer le diagnostic. L'examen cytobactériologique des urines permet d'évaluer une partie des conséquences. Environ 50 à 75 % des hommes avec une atteinte génitale ont des anomalies radiologiques au niveau de l’appareil urinaire. Un bilan radiologique de tout l’appareil urinaire (uroscanner, à défaut une urographie intraveineuse) est toujours indiqué ainsi qu’une radiographie du thorax à la recherche d’une localisation pulmonaire[59].
La forme la plus fréquente et la plus redoutable est la tuberculose rachidienne dite mal de Pott. La localisation au niveau de l’espace intervertébral (mal de Pott) est la plus fréquente des localisations ostéoarticulaires et la plus grave car elle siège au voisinage des structures nerveuses importantes. Elle représente 10 % des cas de tuberculose[60].
Selon une étude tunisienne[61] qui a porté sur 180 cas de tuberculose rachidienne, il a été remarqué que l'état général a été altéré chez 80 patients (44 %). Une fièvre modérée allant de 37,8 °C à 38,5 °C a été notée chez 55 patients (30 %) et une adénopathie satellite dans 40 cas (27 %). Selon la même étude, l'intradermoréaction à la tuberculine pratiquée 120 fois a été positive dans 85 % des cas.
La phase initiale de l’infection est marquée par l’atteinte des espaces intervertébraux rachidiens, et l’atteinte de la synoviale, où se développe la lésion tuberculeuse laissant apparaître une symptomatologie plus ou moins spécifique marquée par : des douleurs au niveau du rachis de type mécanique ainsi qu’un syndrome infectieux modéré et des signes de déficit neurologique s’installant progressivement[62]. Si elle est dépistée et traitée à ce stade d’invasion, l’évolution se fait vers la guérison et sans destruction.
En revanche et en absence de traitement, l’évolution se fait lentement à partir d’un état stable vers la phase d’état où la destruction est irréversible. Sur le plan tissulaire, on remarque une lésion spécifique à la tuberculose : la nécrose caséeuse développée dans l’espace intravertébral et laissant des séquelles majeures et irréversibles. La symptomatologie est variée et marquée par des douleurs intenses avec une tuméfaction de la région affectée et adénopathie du site drainant la région ainsi que les troubles neurologiques.
Le clinicien oriente vers les examens complémentaires afin d’affirmer son diagnostic et cela à travers la radiologie qui fournit des images plus ou moins spécifiques et la mise en évidence du BK via l’examen bactériologique et anatomopathologique à la suite d'une ponction/biopsie réalisée sur le site de l’infection.
Une forme extrêmement rare, mais très redoutable. Le tableau clinique et radiologique sont atypiques[63]. Elle touche les enfants et les adultes jeunes. Dans sa forme typique la méningite tuberculeuse associe un syndrome méningé et un syndrome infectieux progressif et peu intense : fièvre au long cours, otalgie, vomissement. Le syndrome méningé est caractérisé par les trois symptômes : la raideur de la nuque, les nausées et les vomissements.
À la suite de la pratique de la PL, le LCR est classiquement clair, avec une lymphocytose, une hyperalbuminorachie et une hypoglucorachie. Sa gravité nécessite la mise en route rapide du traitement.
Des séquelles fonctionnelles s’observent dans plus d’un tiers des cas : hémiplégie, paralysie des paires crâniennes, troubles sensoriels, calcifications intracrâniennes, etc.
Rare en Occident où elle ne représente que 1,5 à 2,1 % de l'ensemble des formes que peut prendre la tuberculose, la forme cutanée de la tuberculose est cependant endémique en région tropicale et au Maghreb[64]. Elle adopte alors des formes cliniques variées : chancre tuberculeux, tuberculose cutanée miliaire, lupus tuberculeux, scrofulodermes ou gommes tuberculeuses, ou encore tuberculose péri-orificielle[64].
La classification de Beyt de 1980[65], basée sur des critères physiopathologiques, est aujourd'hui la référence[66] pour distinguer les diverses formes :
La réaction cutanée tuberculinique met en évidence la présence d’une hypersensibilité retardée induite par les antigènes mycobactériens (Mycobacterium tuberculosis, BCG, certaines mycobactéries atypiques). La réaction cutanée à la tuberculine est explorée par IDR. Cette IDR est réalisée par une injection dans le derme à la face antérieure de l'avant-bras d'un volume exact de 0,1 ml de la solution liquide de tuberculine. La validité d'interprétation du test tuberculinique nécessite une technique parfaite[67].
La tuberculine provoque des indurations au niveau du site de l’injection. Une réaction est jugée positive lorsque le diamètre d'induration est >= à 5 mm[68]. En absence de vaccination, une induration supérieure à 8 mm doit témoigner et attester une primo-infection tuberculeuse. Si l'induration est supérieure à 25 mm, la tuberculose maladie doit être évoquée[68]. Cette pratique se révèle outil dans le diagnostic des TBC ganglionnaires (positive chez 100 %), et la phase de la primo-infection de la tuberculose[48].
Après avoir extrait des expectorations ou du liquide de ponction auprès du malade, l’échantillon du prélèvement fait l’objet d’un examen cytobactériologique à travers les différentes techniques[50] :
La spécificité des modalités de diagnostic a fait l’objet de plusieurs études d’évaluation d’efficacité. Une étude rapporte que la méthode de coloration Zde N et la MF bénéficient d’une spécificité élevée allant jusqu’à 98 % dans le cas de la MF. Une étude montre également que la concordance diagnostique entre la MF et la culture (souvent prise comme référence) est largement supérieure à celle qui existe entre la méthode de ZN et la culture (95,1 % contre 69,6 %)[70].
Les méthodes de ZN et l’examen de culture apportent une grande valeur diagnostique vu leur caractère économique en termes de cout. Ils sont des examens peu couteux et accessibles et fortement recommandés dans le diagnostic de la TBC. Ils se révèlent très pratiques et efficaces dans le diagnostic des tuberculoses ganglionnaire, pulmonaire, méningée et ostéoarticulaire[69].
L’examen anatomopathologique se pratique sur l’échantillon résultant de la biopsie. Il met en évidence une lésion spécifique à la tuberculose dite : granulome épitheloide gigantocellulaire à centre caséeux nécrosé. Le terme de granulome désigne l’ensemble des éléments cellulaires présents dans un foyer inflammatoire. Le foyer tuberculeux est entouré d’une grande cellule polynucléaire résultant de la fusion des macrophages tel que mentionné dans la physiopathologie de la maladie[53].
Met en évidence la présence de l’ADN du germe dans l’échantillon issu de la ponction ou des crachats en cas de TBC pulmonaire[53]. La technique de l’art et telle que recommandée par l’OMS est réalisée à l’aide d’un dispositif nommé GeneXpert et l’utilisation du test rapide Xpert MTB/RIF[72]. La technique s’appuie sur l’amplification de l’ADN germique[73] via la cartouche Xpert MTB/RIF riche en ADN polymérase (enzyme intervenant dans la réplication de l’ADN au cours du cycle cellulaire) et les ressources énergétiques et protéiques nécessaires à cette procédure. Son utilisation a largement augmenté depuis 2010, date à laquelle l’OMS a recommandé pour la première fois son utilisation. Cela comporte un avantage majeur portant sur son efficacité et son faible coût[74].
De nombreuses études[75] ont évalué l’efficacité de la cartouche dans la mise en évidence des BK et BK résistants à la Rifampicine ; il est rapporté par une méta-analyse compilant plusieurs études[75] que le test MTB/RIF est spécifique à plus de 90 % et présente un outil important facilitant l’accès à un diagnostic précis à faible cout. La mise en évidence des BK est établie dans 2 heures après le test[76].
La modalité de diagnostic radiologique se révèle très utile et indiquée comme étant un examen de première intention en cas de tuberculose pulmonaire et ostéoarticulaire[77]. Cela est justifié par les images radiologiques plus ou moins spécifiques à la tuberculose. La littérature rapporte que la spécificité varie de 27 à 81 % selon l’étude. Les imageries idéales et révélatrices de la tuberculose répondent à certains attributs dont le premier est lié au siège. En raison de l’affinité que les BK ont envers les régions aérées, la lésion radiologique est souvent observée dans les parties hautes des poumons. Néanmoins, l’image radiographique peut contenir différentes formes :
Un premier vaccin fut expérimenté en 1886 par Vittorio Cavagnis[78] tandis qu'à cette même époque Robert Koch tenta vainement de développer un sérum curatif basé sur la tuberculine[79]. En 1902, à partir d'un bacille d'origine humaine atténué, Behring essaya un vaccin contre la tuberculose bovine : le bovovaccin. Behring proposa également, sans succès, la tuberculase[80]. Toujours dans le domaine vétérinaire, Koch essaya le tauruman. Pour mémoire, il faut aussi citer le sérum de Marmorek (1904)[81], le sérum de Maragliano, les sérums de Richet et Héricourt, ainsi que les tentatives peu honnêtes de Friedmann (en) et de Spahlinger[82]. C'est en 1921, qu'Albert Calmette et Camille Guérin de l'Institut Pasteur de Lille essayent avec succès le premier vaccin contre la tuberculose sur lequel ils travaillaient depuis 1908 — qui était conçu pour être un vaccin vétérinaire. Baptisé BCG (pour « Bacille de Calmette et Guérin » ou « Bilié de Calmette et Guérin ») ce vaccin issu d'une souche vivante atténuée de Mycobacterium bovis deviendra obligatoire en France en 1950.
L’efficacité de la vaccination par BCG se limite à la protection contre l’évolution mortelle de la tuberculose, particulièrement la méningite tuberculeuse et la maladie disséminée (miliaire). Le vaccin est plus efficace chez le nouveau-né et l'enfant que chez l'adulte (protection estimée entre 75 et 85 % des formes graves du nourrisson et du jeune enfant et entre 50 et 75 % des formes de l'adulte[83]).
Il ne permet donc pas d'empêcher la transmission de la maladie et d'enrayer l'épidémie mondiale. L'avenir est dans la recherche des gènes de virulence du bacille.
Sur base d'études faites à grande échelle et organisées par l'Organisation mondiale de la santé (OMS), certains pensent que l'efficacité du BCG est faible : dans une étude faite sur 260 000 personnes dans un pays d'endémie tuberculeuse (en Inde), les auteurs n'ont pas trouvé de différence significative entre le groupe qui avait reçu le BCG et celui qui ne l'avait pas reçu[84]. Une autre étude faite également en Inde sur 366 625 personnes a montré que le BCG n'avait aucune action préventive sur les formes de tuberculose pulmonaire parmi les adultes[85].
En juillet 2007, la ministre française de la santé, Roselyne Bachelot, a annoncé la suspension de l'obligation de vacciner tous les enfants et les adolescents contre la tuberculose par le BCG, à l'occasion de la présentation du nouveau programme de lutte contre cette maladie[86].
En France (depuis 1964), en Belgique et en Suisse, cette maladie est sur la liste des maladies infectieuses à déclaration obligatoire.
En France, en particulier, c'est la mise sous traitement antituberculeux qui fait partie de la déclaration. Cela permet d'inclure les cas confirmés bactériologiquement et les cas probables reposant sur un faisceau d'arguments épidémiologiques, cliniques et d'imagerie en l'absence de preuve bactériologique formelle. En effet ces cas probables nécessitent les mêmes investigations d'enquête épidémiologique pour rechercher d'éventuels cas contact ou contaminant autour d'eux.
Toutes les espèces de vertébrés peuvent être atteintes spontanément par différents types de bacilles tuberculeux. Ces tuberculoses animales peuvent être cause de zoonoses.
Les symptômes de la tuberculose animale n’ont été décrits et rapprochés de la forme humaine que très tardivement. Si Aristote décrivit déjà les scrofules chez les animaux, les lésions de la tuberculose bovine restèrent longtemps confondues avec celles de la péripneumonie contagieuse et de l’hydatidose bovine ou de la morve des chevaux. Le premier à rapprocher les tubercules humains de ceux du bœuf fut Ernst Friedrich Gurlt, en 1831. Auparavant la tuberculose bovine, surtout dans les cas de localisation pleurale, a pu être assimilée plutôt à la syphilis humaine[87].
Afin de prévenir la transmission du bacille chez l'humain, soit par voie aérienne directement par contact, soit par voie digestive après ingestion de viande ou de lait insuffisamment cuit ou pasteurisé, la plupart des pays développés ont entrepris d'assainir leur cheptel bovin[réf. souhaitée].
La détection des animaux porteurs se fait par voie clinique, allergique et par recherche des lésions évocatrices sur les carcasses à l'abattoir. En France, cette prophylaxie est obligatoire depuis 1963 sur tout le territoire national pour tous les bovins âgés de plus de six semaines. Pour que les animaux puissent se déplacer sans contrainte, le cheptel doit obtenir le statut « officiellement indemne de tuberculose ». Pour cela, tous les animaux sont testés régulièrement par intradermotuberculination. La fréquence est annuelle mais peut être allégée quand la prévalence de la maladie dans le département est faible. Les animaux réagissant peuvent être soit testés comparativement par une tuberculine aviaire (pour détecter les faux positifs), soit envoyés à l'abattoir (l'abattage est alors subventionné) où les lésions évocatrices seront recherchées par un vétérinaire inspecteur, et éventuellement confirmées par diagnostic de laboratoire. Les troupeaux où l'infection est confirmée peuvent faire l'objet d'un abattage total, également subventionné[réf. souhaitée].
Parallèlement, en France, un réseau de cinq mille vétérinaires sanitaires surveille l'apparition de signes cliniques évocateurs. Des visites sanitaires biennales de tous les troupeaux de bovins sont obligatoires. La vaccination au BCG positivant le test intradermique, elle est interdite sur le territoire français.
Cette politique a permis de faire considérablement baisser la prévalence de la tuberculose bovine. En 2000, la commission européenne a reconnu à la France le statut de pays officiellement indemne. En 2006, le taux d'incidence était de 0,032 % de cheptels infectés alors qu'il était de près de 25 % en 1955. Il convient de distinguer la tuberculose bovine de la paratuberculose bovine, due également à une mycobactérie, mais qui n'est pas une zoonose[réf. nécessaire].
Avant l’obligation de pasteurisation du lait, la proportion des cas de tuberculose humaine d’origine bovine était estimée à 1,3 % des cas de tuberculose humaine. En France aujourd'hui, les rares cas de tuberculose humaine d'origine animale (0,5 % des cas) sont constatés dans leur majorité chez des sujets de plus de soixante ans, ce qui est le signe d’une infection ancienne.
La fréquence de la tuberculose chez les carnivores domestiques, essentiellement due à M. bovis ou M. tuberculosis, a baissé, en même temps que celles des tuberculoses humaine et bovine. Le vétérinaire doit cependant toujours veiller à ce que les carnivores ne servent pas de relais épidémiologique secondaire dans un foyer de tuberculose, qu’il soit animal ou humain. Le diagnostic de la tuberculose des carnivores est extrêmement difficile à poser.
Afin de dépister la tuberculose auprès des populations géorgiennes, le Centre National pour le Contrôle des Maladies et de la Santé Publique de Géorgie utilise un camion autonome qui permet à l’équipe médicale de se stationner à n'importe quel endroit, pour procéder aux examens.
Le traitement est d'une durée de six mois pour une tuberculose pulmonaire à bacille de Koch sensible chez un patient immunocompétent, comprenant 2 mois de quadrithérapie antibiotique (isoniazide, rifampicine, pyrazinamide et éthambutol), puis 4 mois de bithérapie (isoniazide et rifampicine)[88]. Le traitement prolongé est indispensable afin de guérir de la maladie et éviter l'émergence de souches résistantes dont l'évolution est souvent beaucoup plus grave. Malheureusement, ce traitement étant long et complexe, il est difficile à faire respecter aux patients en particulier dans les pays dont le système de santé est lacunaire, où la thérapie sous observation directe est difficile à mettre en œuvre, ce qui favorise l'apparition d'antibiorésistance.
En plus et afin	de prévenir	ce risque de résistances, une association d’antibiotiques est utilisée. En effet, les mécanismes de résistance étant spécifiques, chaque antituberculeux de l’association va tuer les bacilles mutants résistants à l’autre antituberculeux. En suivant ce raisonnement, il parait logique de proposer une association de	deux antibiotiques.	L’isoniazide et	la rifampicine sont cette association de base. Il faut toutefois prendre en compte l’historique de mise sur le marché des antituberculeux. L’isoniazide était, dans les années 50, un des rares antituberculeux disponible. Par conséquent des échecs de traitement ont déjà eu lieu avec sélection de mutants résistants (on parle de « résistance secondaire »). Ces patients ont pu transmettre ces souches résistantes à d’autres patients qui ont développé une tuberculose avec une résistance d’emblée (on parle alors de « résistance primaire »). Du fait de la	circulation de ces souches résistantes il faut ajouter un troisième antituberculeux à la bithérapie associant rifampicine et isoniazide, l’éthambutol. Celui-ci permet d’être assuré d’avoir toujours une bithérapie en cas de résistance primaire à l’isoniazide (5% des cas en France). Cependant, cette trithérapie nécessite une durée de traitement d’au moins 9 mois pour éradiquer les bacilles persistants qui présentent un métabolisme ralenti et qui sont à l’origine des rechutes. Le pyrazinamide ajouté pendant les 2 premiers mois permet de réduire la durée du traitement de 9 à 6 mois.
La rifadine est un traitement qui peut être utilisé pour traiter la tuberculose osseuse.
L'isoniazide est utilisé généralement à la dose de 5 mg, en association avec trois autres antibiotiques. L’isoniazide inhibe la multiplication des bactéries responsables de la tuberculose. Ce médicament doit être administré à jeun. Antibiorésistance : une large épidémie de cas de tuberculose résistante à ce médicament s'est déclarée à Londres de 1995 à 2006[89].
La rifampicine est utilisée habituellement à la dose de 10 mg/kg et par jour, pendant une durée de six mois, pour le traitement de la tuberculose. Cet antibiotique est un fort inducteur enzymatique : il accélère la dégradation des autres médicaments, notamment les contraceptifs oraux. Les femmes sous contraceptifs sont donc invitées à revoir leur traitement à la hausse (après consultation du gynécologue), voire à passer à une contraception mécanique (préservatif…) pendant la durée du traitement. La rifampicine provoque une coloration orangée des urines. C'est un bon moyen d'objectiver l'observance du traitement.
La streptomycine (découverte par Selman Waksman vers 1946) fut le premier antibiotique actif contre le bacille de Koch. Il est contre-indiqué chez la femme enceinte et doit impérativement être associé à d'autres antituberculeux (INH et PAS). Par voie intramusculaire chez l'adulte : 15 à 25 mg/kg et par jour. Par voie intrarachidienne : pour l'adulte, 25 à 100 mg/j, pour un enfant, 20 à 40 mg/kg et par jour en deux ou quatre injections. Surveillance du traitement : les fonctions auditives et rénales devront être surveillées régulièrement.
L'éthambutol est utilisable chez la femme enceinte. Elle doit être utilisée le matin à jeun en une seule prise, quinze à vingt milligrammes par kilogramme. Ne pas dépasser vingt-cinq milligrammes par kilogramme par 24 heures sans dépasser soixante jours, puis réduire à quinze milligrammes par kilogramme et par jour. Surveillance par un fond d'œil et un examen de la vision des couleurs mensuels.
Le bédaquiline (R207910), une molécule de la famille des diarylquinolines, pourrait se révéler prometteuse contre Mycobacterium tuberculosis. Elle fait naître trois espoirs : raccourcissement de la durée du traitement ; envisager des prises une seule fois par semaine en association avec un autre antituberculeux ; être active sur des souches multi résistantes, avec une efficacité bactéricide bien supérieure à celle de l'isoniazide et de la rifampicine. Ce médicament est actuellement en phase très précoce de son développement. Seules des études approfondies chez l'humain permettront de vérifier que ces espoirs sont fondés.
Des corticoïdes sont ajoutés au traitement antituberculeux en cas de méningite tuberculeuse, de résistances ou de rechute de traitement. La corticothérapie doit être commencée après instauration de l'antibiothérapie, à la dose de 0,5 à 1 mg/kg et par jour pour une durée de un à deux mois.
Le traitement des infections latentes repose soit sur l’isoniazide en monothérapie pour une durée de 6 ou 9 mois, soit sur l’association isoniazide-rifampicine pendant 3 mois.
La résistance aux traitements est due à des traitements insuffisants en doses ou en durée[90]. Elle pose des problèmes importants car la tuberculose est beaucoup plus délicate à soigner, surtout en cas de résistances à plusieurs anti-tuberculeux (multi résistance). Dans le pire des cas, elle est dite étendue lorsqu'elle concerne des antibiotiques de première intention (isoniazide, rifampicine) et un ou plusieurs antibiotiques de seconde intention.
Le dépistage de ces formes est difficile, la mise en culture du germe en présence des différents antibiotiques (antibiogramme) requérant plusieurs semaines pour avoir un résultat du fait de la lenteur de multiplication du mycobactérium. La recherche directe de mutations responsable de la résistance est faisable et donne de bons résultats[91]. Du fait de son coût, ces techniques sont difficilement applicables dans les pays pauvres. Une autre méthode consiste à observer la croissance de la souche de mycobacterium au microscope, en présence de différents antibiotiques. Elle donne des résultats fiables et assez rapide (une semaine)[92].
Le traitement des formes résistantes consiste en l'utilisation d'antituberculeux testés comme efficace sur la souche en question, complété par une fluoroquinolone et par des antibiotiques, dits de seconde ligne, comme la moxifloxacine, la bédaquiline, le delamanid[93]. La prise en charge a fait l'objet d'un document de recommandations publié par l'OMS en 2006[94]. (Voir aussi DOTS, DOTS-Plus). D'autres traitements comme le télacébec[95] sont également développés.
Un cocktail de médicaments fait de bédaquiline, de linézolide et de prétomanide[96], réparti en 5 comprimés par jour, permet de réduire la durée de traitement de la tuberculose résistante à 6 mois (plus 6 mois de suivi thérapeutique) avec un taux de guérison de 90 %[96] — contre des traitements de 18 à 24 mois avec la prise de 30 à 40 gélules quotidiennes et de nombreuses piqures jusque-là[96]. L'ajout de mixifloxacine à la combinaison bédaquiline-linézolide-prétomanide permet d'être, au moins, aussi efficace que le traitement habituel dans les formes résistantes à la rifampicine[97].
Avant la découverte d'antibiotiques efficaces, on pratiquait la collapsothérapie. La collapsothérapie est un affaissement de la partie atteinte du poumon et d'une partie du thorax par insufflation d'air, le pneumothorax, ou par chirurgie mutilante.
L'écrivain François Abgrall (1906-1930), décédé très jeune à 23 ans de cette maladie à une époque où elle était très mal soignée a décrit dans son livre Et moi aussi, j'ai eu vingt ans ![98] qui est un témoignage précieux sur le statut du malade tuberculeux vers 1925, les symptômes de cette maladie et la manière dont elle était soignée en France dans les années 1920. À l'époque, la seule thérapeutique proposée est la collapsothérapie. La technique consiste à mettre le poumon  en laissant entrer l'air ou en injectant un produit huileux entre les feuillets de la plèvre, détachant ainsi le poumon des côtes. Cette manœuvre soulage provisoirement le patient mais s'accompagne de complications multiples parmi lesquelles la perte du poumon n'est pas exclue.
Ces méthodes ont disparu des pays occidentaux dans les années 1950.
Selon une étude d'Olivier Neyrolles de l'Institut Pasteur, le bacille de Koch serait stocké dans les cellules adipeuses[99]. C'est ainsi qu'il résisterait aux antibiotiques les plus puissants et qu'il serait capable de réapparaître après de nombreuses années chez des personnes guéries.
Le traitement bactériophagique pourrait représenter à terme une solution de traitement dans les cas de résistance aux antibiotiques. En effet le bacille de Koch appartient au genre Mycobacterium dont les membres présentent des caractéristiques similaires. En mai 2019, une équipe internationale de l'Université de Pittsburgh a réussi à traiter avec succès par phagothérapie un patient hospitalisé à Londres victime d'une infection généralisée par Mycobacterium abscessus, en utilisant un cocktail de phages modifiés génétiquement administré par voie intraveineuse[100].
MammaliaPour le cladogramme, voir Mammalia (classification phylogénétique).
ClasseSous-classes de rang inférieurLes Mammifères (Mammalia) sont une classe d'animaux vertébrés caractérisés par la présence de fourrures (excepté pour certains mammifères marins), d'une oreille moyenne comportant trois os, d'un néocortex et de glandes mammaires, dont les représentants femelles nourrissent leurs juvéniles à partir d'une sécrétion cutanéo-glandulaire spécialisée appelée lait (on dit alors qu'elles allaitent). Les mammifères comportent 6 495 espèces connues en 2018 qui, selon les classifications scientifiques, sont distribuées en 29 ordres, 153 familles et en près de 1 200 genres.
En termes de cladistique, qui reflète l'histoire de l'évolution, les mammifères sont les uniques représentants actuels des synapsides, un groupe qui comprend notamment de célèbres représentants non-mammaliens comme Dimetrodon. Avec les Sauropsides (« reptiles » et oiseaux), ils constituent le clade des Amniotes, au sein de la super-classe des Tétrapodes. Les synapsides se sont divisés en plusieurs groupes (traditionnellement et incorrectement appelés « reptiles mammaliens » ou par le terme pélycosaures, et maintenant connus sous le nom de « mammifères souches » ou « proto-mammifères »), avant de donner naissance aux Thérapsides, un groupe majeur issu des Sphénacodontes, durant le début du Permien moyen. Les plus vieux mammifères connus sont des fossiles du Jurassique inférieur et sont issus des cynodontes, un groupe avancé de thérapsides. Leur aire de répartition est planétaire, ils ont conquis une grande partie des niches écologiques de la macrofaune et demeurent un des taxons terrestres dominants depuis l'Éocène, après l'extinction Crétacé-Paléogène.
Du point de vue de l'évolution et de l'écologie, les premiers mammifères étaient  insectivores et avaient un mode de vie terrestre et généralement nocturne. Ce taxon s'est grandement diversifié au fil de son histoire évolutive, au point qu'un de ses principaux ordres (les chauves-souris) a acquis le vol battu. Un certain nombre de lignées ont évolué vers un mode de vie aquatique partiel (Phoques, Ours blanc, Castor, Hippopotame, Loutre, Campagnol amphibie, Ornithorynque…) ou total (Cétacés, Siréniens…), tout en conservant de leur ancêtre tétrapode la respiration pulmonaire. De même, l'écholocalisation est bien présente dans certains ordres (chiroptera, cétacés) alors qu'elle se fait rare dans le reste du règne animal.
De nombreux mammifères sauvages, en dépit d'un statut d'espèce protégée, figurent sur la liste rouge de l'UICN (notamment les superprédateurs) — certains font l'objet de plans de restauration ou de réintroduction. De même, certaines races de certaines espèces élevées par l'homme jusqu'au XIXe siècle (pour la traction animale, la viande, le lait, la laine ou comme animal de bât) ont disparu ou ont fortement régressé au profit de quelques races sélectionnées pour leur productivité. Quelques espèces sont devenues invasives, notamment après introduction délibérée ou accidentelle dans de nouveaux biotopes en relation avec les activités humaines, alors qu'aucun prédateur n'endigue la croissance de ces nouvelles colonies.
Les mammifères forment une classe d'animaux vertébrés descendant des Thérapsides. 
Ils possèdent tous des glandes mammaires, lesquelles pourraient être issues des glandes sébacées ou des glandes sudoripares[1]. Tous nourrissent leurs jeunes avec du lait produit via ces glandes par les femelles. Chez certaines espèces comme Dyacopterus spadiceus et Pteropus capistratus (en), la lactation peut se faire chez les mâles[2],[3]. Parmi les mammifères actuels, les monotrèmes sont les seuls à ne pas posséder de mamelles.
Hormis l'allaitement, plusieurs autres aspects physiologiques et morphologiques permettent de distinguer les mammifères d'autres clades.
Les modes de locomotion varient en fonction de la niche écologique occupée : vol battu chez les chiroptères et vol plané par homoplasie chez plusieurs lignées (Petaurus, Dermoptera, etc.), quadrupédie chez la plupart des mammifères terrestres (qu'il s'agisse d'une quadrupédie de marche, de course, arboricole, etc.), bipédie occasionnelle ou constante chez une minorité de taxons (Homo, Pangolins terrestres[4], Pan, probablement certains des plus lourds Sthenurus[5], les Macropodidés, etc.)
Le mammifère terrestre le plus massif connu ayant jamais existé est Paraceratherium transouralicum (environ 10 à 20 t), le plus petit est Batodonoides vanhouteni (1,3 g). Aujourd'hui le plus massif est l'Éléphant de savane d'Afrique (6 t) et le plus léger le Pachyure étrusque (1,8 g), le plus petit (par la taille) est la Kitti à nez de porc (2,9–3,3 cm).
Concernant les mammifères aquatiques, la plus volumineuse espèce est la Baleine bleue ou rorqual bleu, avec certains spécimens de plus de 30 mètres de longueur et d'une masse supérieure à 170 tonnes[6].
En 2018, le groupe contient 6 495 espèces[7] qui, selon les classifications scientifiques, sont distribuées en près de 29 ordres, 153 familles et 1 200 genres[8].
Les poils sont un trait plésiomorphe au sein des mammifères, peut-être même un héritage des Reptiles mammaliens ayant conduit aux Mammifères. C'est une formation dermique caractéristique, utile à la régulation thermique (ex. : fourrure) et dans certains cas à la mécanoception (vibrisses), voire à la sélection sexuelle (crinière)[9]. Chez certaines lignées, la fourrure a évolué pour laisser place à des piquants ou des écailles (ex. : Pangolin, Échidné), ou quasiment disparu comme chez les Cétacés.
La quasi-totalité des espèces présentent aussi des griffes ou des sabots, sauf chez les espèces strictement aquatiques qui les ont perdus au cours de leur évolution.
Les mammifères sont homéothermes à de très rares exceptions près (exemple : certains rats-taupes, les paresseux, Myotragus balearicus qui est une espèce caprine désormais éteinte). Pour maintenir une température corporelle constante (comprise dans une fourchette entre 32 °C et 42 °C (hors hibernation ou estivation) selon les espèces[10]), les mammifères consomment beaucoup de dioxygène et d'énergie — ce qui est rendu possible par la présence d'un tissu pulmonaire alvéolé ainsi que d'un muscle propre aux mammifères séparant cavités abdominales et cavité thoracique, appelé diaphragme[11], qui amplifie les mouvements respiratoires effectués avec la respiration costale. Ces animaux sont aussi dotés d'un type de tissu entièrement destiné à la production de chaleur qu'on appelle graisse brune.
Chez certaines espèces, la progéniture n'est pas apte à autoréguler sa température à la naissance, conférant un rôle parental supplémentaire de thermorégulation à la mère en plus de l'allaitement. Ces espèces ne sont cependant pas considérées comme poïkilothermes.
Certaines espèces sont capables de survivre à des variations environnementales en altérant leur taux métabolique le temps de passer la mauvaise période. Ces altérations prennent diverses appellations selon la saisonnalité et l'altération métabolique observée :
Le cœur est constitué de deux demi-cœurs complets, chacun composé d'un ventricule et d'une oreillette ; d'où complète séparation circulatoire des sangs de différentes provenances (poumons, autres organes) et destinations (autres organes, poumons).
À noter qu'une communication temporaire entre les deux oreillettes existe pendant la période embryofœtale. Cette communication se ferme peu après la naissance. Il existe des malformations cardiaques dues à un problème de fermeture de cette communication ; elles peuvent être mortelles à terme (ex. : cœur univentriculaire) ou quelquefois passer inaperçues durant la majeure partie de la vie de l'individu (ex. : foramen ovale perméable).
Le cerveau est pourvu d'une couche supplémentaire de tissus nerveux appelé néocortex.
Étant des tétrapodes, les mammifères possèdent tous une ceinture scapulaire (dite aussi ceinture pectorale) et une ceinture pelvienne — que cette dernière soit développée (ex. : pattes des macropodidés) ou vestigiale (comme chez les cétacés ou les siréniens). Les membres antérieurs sont, comme chez les reptiles mammaliens, à autopode dirigé vers l’avant. Ancestralement, les pattes sont pentadactyles avec un carpe constitué d'une dizaine d'os évoluant différemment selon les mammifères[12].
La colonne vertébrale est différenciée, il y a présence de côtes et d’un diaphragme — certaines caractéristiques physiologiques comme la ventilation pulmonaire à diaphragme expliquent potentiellement la disparition des côtes ventrales qu'on retrouve chez les non-mammifères (par exemple chez les pélycosauriens). La plupart des mammifères ont sept vertèbres cervicales, exception faite des lamantins, des paresseux didactyles qui en ont six et les paresseux tridactyles qui en ont neuf[13].
Le crâne des mammifères est synapside. Il possède deux condyles occipitaux permettant l’articulation à l'os atlas, la première vertèbre cervicale.
Le volume de la boîte crânienne est important, en comparaison avec les reptiles par exemple, pour loger un encéphale et surtout un cervelet plus important.
La cavité buccale est partagée entre un étage olfactivo-respiratoire et un étage masticateur par une structure osseuse : le palais. Certains paléontologues proposent que cela permettrait la respiration et mastication simultanées. Plus probablement cette surface dure permet la manipulation des aliments et donc une meilleure mastication[14] puisque beaucoup d'espèces non-mammifères possèdent un palais charnu qui leur permet déjà de manger et respirer en même temps[15].
La mâchoire est puissante et richement innervée. Elle est constituée d'un seul os dentaire appelé mandibule, et s'articule avec l'os squamosal pour se mouvoir.
L'oreille moyenne est également singulière, avec des particularités souvent utilisées par les paléontologues pour déterminer si un fossile est bien un mammifère. Elle comporte notamment la chaine ossiculaire (marteau, enclume, étrier), considérée par les paléontologues comme la « signature » des mammifères vrais parmi les mammaliformes. De fait, l'os carré a évolué pour devenir l'enclume[16] et avec le marteau et l'étrier, compose l'oreille moyenne.
Les dents sont la partie la plus dure du squelette, c'est pourquoi de nombreux mammifères fossiles ne sont connus que par leurs dents, complétées parfois d'un fragment de mâchoire ou mieux encore leur crâne. Les dents sont typiques de chaque espèce et permettent notamment d'évaluer le régime alimentaire des espèces auxquelles elles appartenaient. Comme chez les thérapsides, le groupe à partir duquel il est admis qu'ils se soient différenciés, les mammifères ont une denture ayant la particularité d'être :
Certains mammifères ont des dents à croissance continue (ex. : castor).
Ils apportent des soins aux jeunes qui ne peuvent vivre sans l'aide de leurs parents durant les premiers temps de leur existence.
Certaines espèces sont sociales et on a découvert deux espèces eusociales (rat-taupe nu et rat-taupe de Damara).
La plupart des mammifères communiquent par divers moyens tels que :
La lignée des mammifères est aussi ancienne que celle des Dinosaures et s'est tout autant diversifiée qu'eux jusqu'à l'extinction Crétacé-Paléogène (K-Pg), qui les a également pratiquement éradiqués il y a 66 millions d'années (Ma). Comme ceux des oiseaux, des ancêtres des Monotrèmes, des Marsupiaux et des Placentaires ont survécu et se sont à nouveau diversifiés, particulièrement les placentaires — jusque-là très minoritaires — qui ont rapidement occupé de nombreuses niches écologiques laissées vacantes par l'extinction[21].
Les mammifères sont issus des Mammaliaformes qui apparaissent il y a environ 225 Ma, au Trias, alors que les terres émergées sont rassemblées en un unique supercontinent, la Pangée. Leurs os de l'oreille moyenne sont clairement séparés de ceux de la mandibule, un trait hérité des Probainognathiens via les Cynodontes non encore mammaliaformes. Les plus anciens fossiles connus attribués à des mammifères datent du Jurassique, après donc l'extinction marquant la fin du Trias. Au Jurassique et au Crétacé, de 201 à 66 Ma, les mammifères se spécialisent en rongeurs, grimpeurs, fouisseurs, nageurs et planeurs mais restent tous de petite taille (au plus celle d'un Blaireau), au contraire des dinosaures qui ne le sont que rarement. Ils se caractérisent par un métabolisme à sang chaud, la présence de poils, des dents spécialisées (incisives, canines, incisives, prémolaires et molaires) et l'allaitement des petits. Les monotrèmes, les marsupiaux et les placentaires sont présents dès 125 Ma mais parmi de nombreux autres groupes (dont celui des multituberculés, le mieux représenté) qui disparaissent lors de l'extinction K-Pg[21].
Sur l'ensemble du Mésozoïque, on n'avait recensé jusqu'à la fin du XXe siècle que 150 à 300 espèces de mammifères, regroupées dans 27 familles, dont une dizaine de familles de marsupiaux, et une dizaine de placentaires[22]. Après la disparition des dinosaures lors de l'extinction Crétacé-Paléogène, les mammifères placentaires et marsupiaux ont connu une radiation évolutive majeure. Parmi les placentaires, les chiroptères ont développé leur propre voie évolutive en adoptant ailes et système d'écholocalisation.
De nombreuses recherches, relancées par la génétique[23], permettent de comprendre comment s'est déroulée cette explosion radiative. Une des théories les plus intéressantes propose que plusieurs groupes se soient développés séparément sur des continents alors isolés. Les afrothériens, qui seraient issus du Gondwana, à l'époque où il était séparé de la Laurasie, regroupent les proboscidiens, les hyracoïdes, les siréniens, les tubulidentés, les macroscélides, ainsi que les rats-taupes, les Tenrecidae et potamogales. Certains ordres auraient subi une sévère compétition lors de la reconnexion de l'Afrique avec l'Asie. Les xénarthres viendraient d'Amérique du Sud. Les euarchontoglires (regroupant primates, dermoptères, scandentiens et glires) et les laurasiathériens (chiroptères, artiodactyles, périssodactyles,...), se seraient développés en Laurasie.
Selon une étude de Roi Maor de l'université de Tel Aviv publiée en 2017, les mammifères auraient tous été nocturnes à l'origine et n'ont commencé à avoir une activité diurne qu'après l'extinction des dinosaures il y a 66 millions d'années. Parmi les mammifères, les primates ont conquis le milieu arboricole[24].
Comme le nom l'indique (mammifère signifie « qui porte des mamelles », du latin mamma « mamelle »[25]), les femelles de cette classe peuvent allaiter leur progéniture. Les glandes mammaires sont une évolution des glandes sudoripares qui donnent des champs mammaires chez les protothériens et de vraies mamelles chez les autres mammifères.
Le choix de Linné de définir cette classe par la présence de glandes mammaires et non, par exemple, de poils, autre caractéristique de la classe, répond à la classification d’Aristote, qui avait repéré un ensemble de Vertébrés quadrupèdes, vivipares et porteurs de poils. Mais cette classification d’Aristote avait l’inconvénient d’exclure les Cétacés et les Chiroptères, qui étaient alors classés respectivement parmi les Poissons et les Oiseaux. La découverte des monotrèmes (par exemple l'ornithorynque) est postérieure (1798) à la définition de Linné (1758), mais elle a confirmé la pertinence de la classification opérée par le savant.
D'après ITIS et Mammal Species of the World, dans son édition de 2005, révisée en 2007[26] :
Tenrec ecaudatus, un Afrosoricida
Panthera tigris, un Carnivora
Divers Cetartiodactyla
Pteropus alecto, un Chiroptera
Dasypus novemcinctus, un Cingulata
Galeopterus variegatus, un Dermoptera
Erinaceus europaeus, un Erinaceomorpha
Procavia capensis, un Hyracoidea
Lepus californicus, un Lagomorpha
Rhynchocyon petersi, un Macroscelidea
Divers Perissodactyla
Manis temminckii, un Pholidota
Myrmecophaga tridactyla, un Pilosa
Divers Primates
Loxodonta africana, un Proboscidea
Divers Rodentia
Anathana ellioti, un Scandentia
Trichechus manatus, un Sirenia
Blarina brevicauda, un Soricomorpha
Orycteropus afer, un Tubulidentata
Sarcophilus harrisii, un Dasyuromorphia
Macropus giganteus, un Diprotodontia
Dromiciops gliroides, un Microbiotheria
Didelphis albiventris, un Didelphimorphia
Macrotis lagotis, un Peramelemorphia
Divers Monotremata
Le traditionnel ordre des Insectivores (Insectivora) est scindé en trois ordres : Afrosoricida (taupes dorées et tenrecidés), Erinaceomorpha (hérissons et gymnures) et Soricomorpha (musaraignes, taupes, etc.).
Le traditionnel super-ordre des Ongulés (Ungulata) se subdivise en Artiodactyla, Perissodactyla, Cetacea, Proboscidea, Sirenia, Hyracoidea et Tubulidentata. Les ordres Artiodactyla et Cetacea sont en fait un seul et même ordre aussi appelé Cetartiodactyla.
L'infra-classe des Marsupiaux (Marsupialia) inclut les sept ordres suivants : Didelphimorphia, Paucituberculata, Microbiotheria, Notoryctemorphia, Dasyuromorphia, Peramelemorphia, Diprotodontia, auxquels il faut ajouter deux ordres désormais éteints (Yalkaparidontia et Sparassodonta).
Le tableau indique une division correspondant plus ou moins aux ordres. Comme dans toute phylogénie, celle-ci reflète le savoir courant. Dans les zones d'incertitudes, citons la position des taupes dorées (ou rats-taupes, chrysochloridés) et des tenrecs (tenrécidés) qui pourraient devoir être séparés des Insectivora.
Le cladogramme ci-dessous présente la phylogénie des thérapsides selon T. S. Kemp (2011)[27] basée selon la proposition d'Hopson et Barghausen (1986)[28] :
La classification des mammifères est complexe. D'une manière simplifiée, on reconnaît trois grands groupes de mammifères, dont le regroupement correspond au type de placentation (en) possédé par leurs représentants :
La discipline qui étudie les mammifères se nomme la mammalogie.
Parmi les Mammifères, les Placentaires sont les plus nombreux avec plus de 6 000 espèces regroupées dans 114 familles. Viennent en second les Marsupiaux qui comptent 270 espèces regroupées en seize familles, et seulement cinq espèces en deux familles pour les Monotrèmes. Ils sont présents sur l'ensemble de la Terre, dans tous les types de milieux terrestres. Chaque année, pour environ 10 000 nouvelles espèces animales découvertes, cinq à dix seulement sont des mammifères. Ce chiffre a considérablement augmenté, puisqu'on estime que durant la première décennie du XXIe siècle, ce sont plus de 300 nouvelles espèces qui ont été décrites[30]. Il faut voir là l'impact de l'outil génétique, qui permet de distinguer des espèces à l'apparence identique. Certains spécialistes pensent que 7 000 espèces sont encore inconnues, une partie d'entre elles étant menacées d'extinction[31].
Il y a 10 000 ans, les humains et les animaux domestiqués représentaient 0,1 % de la biomasse des mammifères sur Terre, c’est-à-dire du poids total estimé des mammifères ; ils en représentaient 90 % au début du XXIe siècle[32]. Selon deux publications de la fin des années 2010, ce taux atteint alors 96 %[33],[34]. À eux seuls, les mammifères d’élevage représentent 60 % de la biomasse des mammifères[35]. Selon ces données, les humains et les animaux domestiqués représentent 18 % du total des vertébrés[34]. La biomasse de l’espèce humaine est dix fois supérieure à celle de l’ensemble des mammifères sauvages (5 500 espèces connues)[36]. Les bovins, ovins et porcins représentent une biomasse 14 fois plus importante que celle des mammifères sauvages ; les oiseaux d’élevage représentent une biomasse presque trois fois plus importante que les oiseaux sauvages[36].
Hormis l'espèce humaine et quelques races de bétail ou d'animaux de compagnie, ou espèces commensales des humains (rat, souris) ou espèces introduites (rat musqué, ragondin), la plupart des mammifères semblent en situation de vulnérabilité ou en voie de régression (en nombre d'individus, de populations, et en diversité génétique), et sont en train de subir une importante perte de diversité génétique, à cause de la réduction et fragmentation de leurs populations et de leurs habitats comme c'est le cas de l'orang-outan en Indonésie, ou à cause du braconnage comme c'est le cas par exemple de l'éléphant d'Afrique. Certaines espèces subissent des épidémies (zoonoses qui les déciment) et les modifications climatiques en menacent d'autres (l'ours blanc en particulier).
L'évaluation faite par l'UICN en 2008 révélait que, sur 5 487 espèces de mammifères, 1 181 étaient en danger d'extinction, soit environ 25 %, dont 188 « en danger critique d'extinction » et près de 450 « en danger ». Mais la situation réelle pourrait être bien pire, car 836 espèces de mammifères étaient classées dans la catégorie « données insuffisantes »[37]. 
Les espèces carnivores, ou piscivores dans le cas des mammifères marins, sont par leur situation haute de la chaîne alimentaire exposées aux effets encore mal évalués de cocktails de polluants dont perturbateurs hormonaux, toxiques, reprotoxiques, mutagènes, cancérogènes, aux captures accessoires de la pêche…
Les stratégies de conservation sont aujourd'hui fondées sur l'étude des niveaux critiques de pression et sur une prolongation des tendances historiques en matière d'état, pression et réponse sur les mammifères[38]. Les gestionnaires et responsables de la biodiversité (dont mammalienne) doivent rapidement comprendre ce qui change, où et quand, comment et pourquoi, ce qu'on peut encore faire, et quelles sont les options politiques possibles et leurs enjeux. Or, la pression sur les écosystèmes et sur les mammifères évolue de façon plus rapide et différemment de ce que l'humanité passée a connu[39].
Les mesures de protection passent par la lutte contre le braconnage et le trafic d'animaux, ainsi que par la sensibilisation de la société civile sur les risques que de grandes multinationales font courir à de nombreuses espèces en encourageant la déforestation, responsable de la destruction des habitats naturels dans les forêts tropicales, comme c'est le cas du palmier à huile en Indonésie.
Les outils et logiciels destinés à la prospective (ex. : GLOBIO + modèle IMAGE) appliqués à quatre scénarios prospectifs concluent que, sans efforts importants et sans réorientation des priorités, la situation des mammifères dans le monde va continuer à se dégrader[38]. En effet, pour les quatre scénarios, les endroits où les mammifères devraient être le plus menacés en 2050 ou 2100 ne sont pas ceux où les politiques de protection sont aujourd'hui les plus actives, et [38]. Les prospectivistes de la biodiversité invitent à établir de nouvelles priorités de conservation, sans abandonner celles qui sont en cours, en tenant mieux compte des futurs probables, tout en développant [38].
Pour les articles homonymes, voir Arbre (homonymie).
Un arbre (du latin arbor[1],[2],[3]) est une plante ligneuse terrestre comportant un tronc sur lequel s'insèrent des branches ramifiées portant le feuillage dont l'ensemble forme le houppier, appelé aussi couronne.
Les arbres sont des plantes pérennes qui vivent plusieurs années, plusieurs décennies, voire plusieurs siècles, et, dans de rares cas, plusieurs millénaires.
Selon les estimations, la planète compte entre 60 000 et 100 000 espèces d'arbres. Près de 40 % d'entre elles seraient menacées de disparition[4]. Une estimation récente (PNAS, 2022) répertorie environ 73 000 espèces d’arbres, dont 9 000 encore à décrire, la plupart vivant en zones tropicales[5].
Quand les arbres poussent les uns à côté des autres on parle de forêt, mais les arbres peuvent aussi croître de manière plus ou moins isolée hors des forêts (dans la savane notamment)[6].
Ils jouent un rôle majeur dans le fonctionnement écologique terrestre, en raison de leur capacité à stocker le carbone (leur production de matière sèche annuelle correspond à deux tiers de la production mondiale des plantes terrestres[7]), à prendre une part active dans le cycle de l'eau et de manière générale à constituer les écosystèmes complexes que sont les forêts, sources et refuges de biodiversité.
Ils constituent aussi pour les sociétés humaines une ressource considérable de matériaux (principalement du bois), de denrées (notamment des fruits) et de multiples services. Ils occupent dans presque toutes les cultures du monde une place pratique et symbolique importante.
La dendrologie (du grec dendron, « arbre », et logos, « discours, science ») est la science de reconnaissance (et classification) des arbres, et plus généralement la science des végétaux ligneux.
Il n'existe pas de définition universelle de l'arbre, tant ce concept recouvre une grande variété de formations et d'espèces aux agencements divers et localisés, si bien que les botanistes, arboriculteurs et forestiers continuent encore de débattre à ce sujet[8].
Dans l’optique de constituer des normes permettant de réaliser des comparaisons des ressources forestières à l’échelle mondiale, l'Organisation des Nations unies pour l'alimentation et l'agriculture (FAO) propose une définition, essentiellement basée sur la hauteur. La FAO considère qu'un arbre est une espèce végétale capable dans de bonnes conditions de croissance[9] de pousser au moins à 5 m de hauteur (pour les pays en développement) ou 7 m (pour les pays développés) à l'état adulte, ce qui le distingue de l'arbuste dont la hauteur à maturité est comprise entre 0,5 et 5 ou 7 m, et qui n'a pas de couronne définie[10]. La FAO inclut ainsi dans les arbres les espèces non ligneuses (bambous, palmiers) ayant les critères ci-dessus[11].
L'IFN définit l'arbre comme un végétal ligneux ayant une tige nue et non ramifiée dès la base, d'une hauteur supérieure ou égale à 5 m ou susceptible d'atteindre cette dimension à maturité in situ[12].
Les écologues distinguent parfois les espèces arborescentes dont la taille est au maximum de 15 m et les espèces arborées (arbres stricto sensu) qui dépassent cette hauteur[13].
Les botanistes donnent une définition plus restrictive, caractérisant les arbres par la croissance secondaire en épaisseur de leur tronc, et des couches de tissus similaires, ce qui favorise leur développement en hauteur et la ramification des branches leur permettant d'augmenter la capacité d'exploitation de l'espace aérien[14]. Cette caractéristique, associée à la position des feuilles sur plusieurs niveaux leur permettant de multiplier la surface d'échange pour la photosynthèse, les distingue des palmiers et des bambous qui n'ont pas de tronc. Cette anatomie rend leur tronc autoportant, ce qui exclut les macroalgues, comme Macrocystis, qui atteignent 50 m de hauteur mais ne sont verticales que grâce à la poussée d'Archimède, ou les lianes, plantes grimpantes qui s'élèvent verticalement en s'appuyant, en s'accrochant ou en s'enroulant sur ou autour d'un support vertical[15].
Au sens botanique, les arbres sont ainsi des plantes à bois véritable. Celui-ci, également appelé xylème secondaire, est produit par une rangée cellulaire (l'assise libéro-ligneuse) appelée cambium, située sous l'écorce. La genèse du bois est un processus répétitif qui dépose une couche nouvelle sur les précédentes. Le résultat est souvent visible sous la forme de cernes d'accroissement. Ce résultat est une croissance en épaisseur issue du fonctionnement du cambium qui est le méristème secondaire du bois (le phellogène étant le méristème secondaire de l'écorce). On ne trouve de plantes à bois véritable, et donc d'arbres au sens strict, que chez les Gymnospermes et les Angiospermes Dicotylédones. Preuves de leur origine commune, on trouve des homologues des gènes responsables de la croissance secondaire des arbres, chez les plantes herbacées et les gymnospermes[16].
Il existe d'autres types de plantes à bois véritable, mais les arbres s'en distinguent par des dimensions plus importantes (comparées à celles des arbustes) ou par la formation d'un tronc nettement individualisé (que ne possèdent pas les arbrisseaux) et porteur (alors que les lianes ligneuses doivent trouver un support pour s'élever).
Les spécialistes de l’architecture végétale distinguent quatre stades de développement : arbre jeune, stade adulte, stade mature, phase de sénescence[17] irréversible[18].
Dans la classification classique, les arbres font partie de la division des spermaphytes (Spermatophyta) : les plantes produisant des graines.
Le sous-embranchement des gymnospermes correspond aux plantes se reproduisant par des graines dites nues (embryon entouré de l'albumen et d'un tissu de protection), alors que celui des angiospermes correspond aux plantes se reproduisant avec des graines protégées (par un mécanisme de double fécondation, elles produisent des tissus nourriciers supplémentaires) à l'intérieur de l'ovaire qui donne le fruit.
Ce sont les plus anciens. Ce groupe (majoritairement monoïque) développe des ovules nus simplement protégés par des écailles. La pollinisation se fait grâce au vent ou à la simple gravité, leur dissémination pouvant être favorisée par certains primates (macaques), rongeurs (écureuils) et certaines espèces d'oiseaux spécialisées dans l'extraction de ces graines (becs croisés).
Communément, ces arbres sont appelés « conifères », car la plupart produisent des fruits en forme de cône, aussi appelés pommes de pin ou pives. Ils sont également qualifiés de « résineux » car la plupart produisent de la résine, substance chimique complexe qui permet à l'arbre de lutter contre le froid et contre certaines attaques de parasites ; cette résine est ainsi à son tour utilisée par des insectes : abeilles, fourmis, comme agent désinfectant dans leurs colonies. Ils possèdent plusieurs cotylédons.
Ce groupe fut en voie de régression au sens de l'évolution puisqu'il dut céder nombre de niche écologiques au groupe des angiospermes.
La maturation des graines gymnospermes est longue, allant de quelques mois à plusieurs années (pins : 2 à 3 ans).
Apparu plus récemment (plus de 100 millions d'années), ce groupe d'arbres communément appelé feuillus, est considéré comme plus évolué. On a donc vu se succéder d'abord les conifères comme le pin, puis les premiers feuillus colonisateurs comme le bouleau suivi du noisetier, de l'aulne, du frêne et du chêne qui devient l'espèce dominante en plaine alors que l'épicéa triomphe en montagne. À partir de 2000 av. J.-C., la température baisse à nouveau. Le hêtre qui a besoin de moins de chaleur que le chêne devient alors le feuillu dominant. En montagne, le sapin, le mélèze et le pin cembro rejoignent l'épicéa.
Dans leur mode de sexualité, les angiospermes ont développé une stratégie différente et plus économique en pollen, donc en énergie. Une coévolution avec les insectes permet une pollinisation plus raisonnée.
Les plantes protègent les ovules par des membranes, l'ensemble formant le fruit. Celui-ci peut être chez les arbres une baie, une drupe, un akène…
La production semencière de nombreux arbres forestiers varie d'une année à l'autre. Une année de production abondante (une « année à semences », appelée aussi « année semencière » ou « année à glands ») peut être suivie d'une ou plusieurs années de production médiocre ou nulle[26]. Les proverbes associés à cette production semencière (« année de glands, année d'enfants » pour symboliser la fécondité, « année de glands , année de cher temps » ou « année de glands, année de pommes ensuivant », « année à glands, année à sang », « année de glands, année de peste ») sont parfois sans fondement mais peuvent corréler l'année à l'importance de la pluie qui augmente la production de semences mais provoque aussi des nuisances climatiques[27].
Les feuilles de l'arbre, plus particulièrement les cellules du parenchyme palissadique, permettent de produire, par photosynthèse, cellulose, hémicellulose et lignine.
Certaines cellules permettent à un arbre de se redresser au cours de sa croissance grâce à des agrégats de glycogène qui jouent le rôle de niveau à bulle[29].
Plusieurs mécanismes permettent aux arbres de se défendre contre des parasites :
Le professeur A. Shigo découvre et explique ce principe, et le nomme CODIT (Compartimentation Of Decay In Trees). Cette découverte met en lumière l'incapacité des ligneux de cicatriser, et le modèle propre à la dendro biologie.
Un arbre est généralement composé de racines, d'un ou plusieurs troncs principaux et de ramifications appelées branches.
La partie basale du tronc qui est dégarnie de branches forme le fût. Sa zone circulaire inférieure faisant jonction avec les racines s'appelle le collet.
L'ensemble des branches forme le houppier. La silhouette d'un arbre est caractérisée par son ou ses fûts, l'angle des rameaux entre eux, la disposition des branches au départ du tronc ainsi que la forme générale de son houppier : on parle du port de l'arbre. Par exemple, un houppier triangulaire large à la base et en pointe au sommet caractérise de nombreux résineux.
Le tronc et les branches comportent sur leurs périphéries des cellules mortes appelées rhytidome ou écorce, celle-ci protège la partie vivante des branches et du tronc. Cette écorce peut être une simple petite pellicule ou être très épaisse chez certaines variétés : elle approche les 30 cm chez les séquoias.
La plupart des arbres possèdent des feuilles chargées d'assurer la photosynthèse et l'essentiel des échanges gazeux. Quelques espèces ont cependant, à la place des feuilles, d'autres organes qui peuvent leur ressembler et qui assurent les mêmes fonctions : certains acacias portent des phyllodes qui sont des pétioles transformés, certains euphorbes arborescents ont des rameaux nus chlorophylliens, les aiguilles des pins sont des pseudophylles (des fausses feuilles de formation secondaire) et les filaos possèdent des extrémités ressemblant à des tiges de prêles. En revanche, les aiguilles des sapins sont de vraies feuilles en forme d'aiguilles.
À la surface des troncs apparaissent quelquefois aussi des « épicormiques » : bourgeons, amas, pousses épicormiques (poils, gourmands et branches gourmandes), picots, sphéroblastes et broussins ; ceux-ci apparaissent à partir de stimuli (lumière, blessures, infections, tensions, etc.) et évoluent avec l'âge de l'arbre et selon l'essence considérée[31].
La morphologie du tronc, des branches et des racines, correspond à une structure fractale : chaque branche peut être considérée comme un tronc plus petit pourvu lui aussi de branches et ainsi de suite jusqu'aux plus petits rameaux. Racines et radicelles se structurent également de manière auto-similaire. Il en ressort que l'arbre a une forme de dimension fractale de l’ordre de 2,5[32]. Cette forme résulte du programme génétique de l'arbre, mais aussi d'interactions avec le sol, le climat, d'autres arbres, ou des animaux. La morphologie générale de l'arbre résulte ainsi de plusieurs facteurs, essentiellement : la maximisation de la performance hydraulique dans la conduction de la sève des racines vers les feuilles ; la portée mécanique maximale évitant aux arbres de s’effondrer sous leur propre poids ; la compétition pour l'accès à la lumière ; la réponse au vent, la thigmomorphogenèse, qui contrôle l’évolution du diamètre des branches[33].
Un accès différencié à la lumière où à une lumière plus vive (réverbérée par l'eau près des berges par exemple), ainsi que des contraintes et efforts internes modifiés par le vent, des accidents de vie sont sources de déformations de structures, dues à des maturations exprimées de manière différentielle lors de la formation du bois, des racines et de l'écorce[34]. De même quand un arbre se met à pencher à la suite d'un mouvement du terrain (les branches cherchent à se redresser)[34].
Ces déformations externes se traduisent par des modifications anatomique du bois, autrefois mis à profit, par exemple pour des bois de marine naturellement courbes (moins de risques de casse et de fentes)[34].
Le tronc est naturellement unique mais il arrive parfois, à la suite d'un accident de croissance, ou d'une section due à un herbivore ou à un castor, qu'il se dédouble ou qu'il soit fourchu. La sylviculture en taillis, qui coupe les arbres et laisse les souches bourgeonner, donne notamment des troncs multiples appelés « cépées ».
Le premier arbre connu date du Dévonien. Il s'agit d'Archaeopteris, qui aurait vécu il y a 370 millions d'années. Pendant le Carbonifère, une période au climat chaud et humide, de grandes forêts s'étendent sur la surface du globe. L'un des arbres les plus communs de cette époque est Lépidodendron : il atteint une hauteur de 30 m et a un tronc de 3 m de diamètre.
Les premiers conifères apparaissent à la fin de cette période ; les taxons les plus proches de ces gymnospermes primitifs seraient Araucaria, Podocarpus et Taxus[35].
En Europe, les trois dernières glaciations voient les essences des zones tempérées disparaître des zones septentrionales pour reculer vers le Sud ou survivre dans quelques « zones refuges » de l’Europe septentrionale (nord de la Méditerranée) pour ensuite reconquérir, « assez » rapidement (à une vitesse de 0,42 à 1 km/an), le continent lors du réchauffement holocène, 11 000 ans avant nos jours[36].
Il existe plusieurs manières de catégoriser les arbres :
Selon les estimations, la planète compte entre 60 000 et 100 000 espèces d'arbres. Les forêts ont une densité d’environ 500 arbres par hectare. Les forêts tropicales humides présentent une large variété floristique associée à une abondance de grands arbres (200 à 300 espèces en moyenne par hectare, jusqu'à 500 parfois), mais bien moindre que la variété microbienne dans le sol. Les forêts tempérées n'hébergent qu'une dizaine à une quinzaine d'espèces d'arbres par hectare[39]. L'UICN évalue dans les années 1990 à quelque 100 000 espèces arborées ou arborescentes connues dans l'ensemble de la biosphère[40]. Une modélisation réalisée en 2008 confirme cette estimation de 100 000 espèces[41].
En 1998, un rapport[42] sur l'état de conservation des quelque 100 000 espèces d'arbres recense à la fin du XXe siècle 95 espèces éteintes (y compris 18 à l'état sauvage), 976 dans un état de danger critique, 1 319 menacées et 3 609 vulnérables[43].
Une étude publiée en 2015 dans la revue Nature[44] revoit à la hausse d'un facteur 10 l'estimation du nombre d'arbres sur Terre[45] par rapport aux précédentes. « Les résultats indiquent qu'il y a environ 3 000 milliards d'arbres sur Terre, dont près de la moitié dans les forêts tropicales et subtropicales. Les régions boréales en abritent 740 milliards, et les régions tempérées, 610 milliards ». L'étude estime également que la surface occupée par les forêts diminue chaque année d'environ 192 000 km2 (un peu moins d'un tiers de la superficie de la France), ce qui représente environ 15,3 milliards d'arbres. Les chercheurs estiment que la superficie des forêts aurait diminué de 45,8 % depuis les grandes phases de défrichement (défrichages afin de disposer de surfaces cultivables) lors des débuts de l'agriculture il y a environ 11 000 ans[46], ces bouleversements étant jugés suffisamment profond par certains chercheurs que ces derniers, inspirés par le marxisme, ont évoqué une « révolution néolithique »[47].
Selon une étude publiée en 2017 par l'association Botanic Gardens Conservation International (BGCI), il existerait 60 065 espèces d'arbres différentes dans le monde. Le Brésil compte la plus grande variété d'arbres sur son territoire avec 8 715 espèces, suivie par la Colombie avec 5 776 espèces, et l'Indonésie avec 5 142 espèces. Excepté l'Arctique et l'Antarctique, où aucun arbre n'est recensé, l'Amérique du Nord présente la plus faible diversité avec 1 400 espèces. D'autre part, 58 % des espèces sont présentes dans un seul pays. Ainsi, 4 333 espèces se trouvent uniquement au Brésil, contre 2 991 espèces à Madagascar et 2 584 espèces en Australie. Le BGCI précise également que 9 600 espèces sont menacées d'extinction[48].
En 2022, un projet collaboratif mondial reprend les bases de données disponibles en les pondérant en fonction de la plus ou moins grande exhaustivité des études région par région, et aboutit à une estimation de 73 000 espèces, dont 9 200 à découvrir (40 % en Amérique du Sud). La plupart des espèces encore inconnues seraient rares, endémiques et tropicales ou subtropicales[49].
Depuis l'invention de la filière bois, la mécanique a fait son entrée dans la botanique. Il y a eu la simulation numérique par de Reffye de la croissance des arbres, puis la Mécanique de l'arbre sur pied par Guitard et le modèle mécanique de la croissance d'une branche par Schaeffer[50],[51].
L'arbre joue un rôle majeur dans le cycle du carbone. On le présente souvent comme étant en constante compétition pour les ressources que sont l’eau, les nutriments du sol, la lumière et le gaz carbonique. Cependant il est aussi capable de symbioses (microbiennes et fongiques, car il offre de nombreux habitats et micro-habitats associés à un microbiote spécifique, formant un arbre-monde ou un holobionte constitué d'un hôte et de son cortège de communautés microbiennes[52]) et d’échanges ou de partage de ressources, comparé par une étude récente à une sorte de marché souterrain horizontal du carbone[53]. Ainsi, il prend dans l’air le CO2 nécessaire à ses besoins immédiats et futurs (pour sa reproduction et croissance) ; il le stocke principalement sous forme de sucres, de lignine et de cellulose, il produit aussi des protéines complexes et des lipides. Ces molécules sont utilisées pour construire son tronc, ses branches, feuilles et racines. Mais au fur et à mesure de sa croissance, l'arbre échange aussi - et de plus en plus - de grandes quantités de carbone avec ses voisins, via le gigantesque [54],[55] de mycéliums des champignons symbiotiques du sol. Ces derniers acquièrent du sol et de la biomasse en décomposition des nutriments qu'ils transfèrent à leurs plantes-hôtes, en échange de carbone et d'autres nutriments (apport non négligeable car atteignant 80 % pour l'azote et le phosphore chez certains végétaux[54]). Ils permettent aussi de recycler et valoriser une grande partie du carbone qu'on aurait pu croire « perdu » par les feuilles mortes, les fleurs tombées, les pollens et le bois mort d’autres arbres ou de l’individu-arbre lui-même) et il est maintenant confirmé qu’ils permettent des échanges importants de carbone d’un arbre à l’autre ; et même entre arbres d’espèces différentes[53].
On avait déjà montré que les jeunes plants d'arbres bénéficiaient d'apports importants de carbone via le réseau mycorhizien souterrain[54], mais des chercheurs de l’Université de Bâle et de l'Institut Paul Scherrer (PSI) ont récemment (2016) montré que les arbres adultes de forêts tempérées exportent également de grandes quantités de sucres et bien plus loin qu'on ne le pensait. Pour cela, à partir d’une grue installée dans une forêt située près de Bâle, ils ont utilisé un réseau de longs tubes de plastique pour diffuser sur les couronnes d’arbres (épinettes) de 40 m, âgés de 120 ans environ un flux de dioxyde de carbone radiomarqué[56] afin de pouvoir tracer la cinétique environnementale de ce carbone dans l’arbre (des feuilles aux racines) et l’écosystème[53]. Lors de cette expérience, une surprise a été de rapidement aussi trouver ce carbone radiomarqué dans les racines des arbres voisins bien qu’ils n’en aient pas directement reçu, y compris chez des arbres provenant d'autres espèces (pin, mélèze)[53].
Pour les auteurs de l'étude : ceci confirme l'importance encore sous-estimée du rôle des champignons, et que la "Forêt est plus que la somme de ses arbres" ; Ainsi, dans leur rapport au CO2 (et donc au climat) les arbres ne doivent pas uniquement être considérés comme des individus, mais aussi comme éléments interagissants entre eux et avec les autres arbres, dans l’écosystème forestier et la biosphère[57].
Les plus vieux arbres connus au monde furent jusqu'en 2008 des pins de Bristlecone (Pinus longaeva) comme ici dans les Inyo Mountains, en Californie. Ils peuvent vivre plus de 4 000 ans, certains individus ont presque 5 000 ans.
Les arbres les plus volumineux du monde sont des séquoias géants (Sequoiadendron gigantea), comme ici le Grizzly Giant dans le Parc national de Yosemite. Le plus imposant spécimen, baptisé General Sherman, a un tronc de 1 487 m3 pour une hauteur de 83,8 m.
Les arbres aux troncs les plus gros sont des baobabs d'Afrique (Adansonia digitata) Leur tronc peut atteindre jusqu'à 7 m de diamètre.
Les arbres fournissent de nombreux services écosystémiques autres que la conservation de la biodiversité.
L'arbre procure des matières premières pour un grand nombre d'industries (papetière, seconde transformation du bois, chimique…) ; il joue un rôle économique important.
Voici quelques exemples de son exploitation :
Les humains ont autrefois récolté, conservé et consommé des écorces, aubiers (parfois transformés en farines) ainsi que des feuilles comestibles, de la sève, des résines... pour des usages alimentaires directs (consommation crue ou cuite) ou indirects (ex : pour le fumage de viandes et poissons, emballer ou cuire des aliments, etc.[70],[71], certains de ces usages étant encore pratiqués, notamment en zone sahélienne[72]. Certains arbres nourrissent des chenilles comestibles[73],[74], et le bois mort ou certains arbres affaiblis sont aussi des sources de champignons comestibles lignivores responsables de la pourriture du bois[75] et de larves comestibles d'insectes xylophages. Une grande partie des champignons forestiers mycorhiziens sont également dépendant des arbres (truffes notamment)[76].
En outre, de nombreuses espèces sont utilisées en phytothérapie ou en sylvothérapie (système immunitaire humain bénéficiant des effets attribués aux phytoncides)[77].
Certaines espèces d'arbres comme le moringa, le margousier ou le moabi cumulent de nombreux avantages alimentaires, pharmaceutiques, économiques et culturels ; ils sont actuellement étudiés pour être produits à grande échelle.
L'arbre urbain est maintenant considéré comme un bien commun et une source de services écosystémiques et d'intérêt public et général.
Il joue un rôle essentiel dans l'écologie urbaine — on parle parfois d′urbanisme végétal — comme élément de décor, d'aménagement, et participe à l'atténuation légère de la pollution sonore, de la pollution de l'air et des pics thermo-hygrométriques propres aux microclimats urbains, fonctionnant comme un véritable dispositif d'épuration atmosphérique et constituant un écrans anti-bruit[78]. David J. Nowak parle même de biotechnologie, pour décrire le boisement urbain (Urban forestry) et ses capacités à améliorer, dépolluer l'eau, l'air (un arbre adulte retient en moyenne 100 kg de poussières par an[79]), le sol, à tamponner les chocs climatiques et à constituer un puits de carbone[80].
Néanmoins les gestionnaires d'arbres urbains doivent relever de nombreux défis, car dans l'espace public et hors de quelques grands parcs urbains publics ou privés, ces arbres sont soumis à de nombreux stress qui abrègent fortement leur espérance de vie (ne dépassant généralement pas 30 ans[81]).
Les racines manquent de place et tendent à se diriger vers les égouts pour trouver de l'eau, au risque de les pénétrer et parfois les obstruer. Elles doivent se développer dans un sol souvent de piètre qualité, imperméabilisé et alors exposé à des alternances de manque et d'excès d'eau.
Le tronc, les branches et les racines subissent des agressions, notamment des dommages mécaniques et chimiques, le vandalisme, des tailles dures et la pollution urbaine. Les distances de plantation, le jalonnement et les protections demeurent parfois inadaptés. L'environnement évolue autour de l'arbre sans tenir compte de son intégrité, comme dans le cas de la coupure du système racinaire. Les propriétaires inexpérimentés ou certaines entreprises insuffisamment formées traitent les arbres de manière inappropriée. Les canopées manquent souvent de lumière le jour et subissent la pollution lumineuse la nuit. En raison de la bulle de chaleur urbaine et de la pollution lumineuse, le débourrement se montre souvent plus précoce, et la chute des feuilles beaucoup plus tardive ; parfois de plusieurs mois sous les lampadaires. Selon la NASA, ils produisent 20 % de moins d'oxygène que le même arbre dans la nature. Néanmoins une étude récente laisse penser qu'on a pu sous-estimer la capacité des arbres à épurer l'air de certains polluants, en particulier des composés organiques volatils.
Planter des arbres déjà adultes se montre très coûteux et la plantation et protection des jeunes arbres, qui installent mieux leur système racinaire mais restent vulnérables durant de longues années, reste difficile. Beaucoup de cultivars plantés en alignement mono-spécifiques s'avèrent à terme vulnérables aux épidémies et à divers pathogènes, en plus de contribuer à une perte de diversité génétique chez les espèces-mères. Une bonne gestion nécessite des inventaires souvent mis à jour et une surveillance sanitaire plus étroite. La taille reste obligatoire dans de nombreux cas, avec une accessibilité parfois difficile ; elle constitue une porte d'entrée pour de nombreux pathogènes et doit alors se poursuivre dans le temps chez certaines espèces.
Une gestion attentive et une communication adaptée sont nécessaires pour trouver le soutien ou l'appui actif du public, et pour que l'arbre urbain soit considéré par le plus grand nombre comme un avantage plus qu'un inconvénient.
Rappelons toutefois un principe :
« Situé dans un milieu qui lui convient, auquel il s'est peu à peu adapté, ne subissant pas de contraintes particulières dans son expansion aérienne ou souterraines et ne présentant pas de signes de dépérissement ou d'attaque parasitaires, un arbre n'a pas besoin d'être taillé. » (E. Michau)[82]
L'arbre est un schème qui semble quasiment universel. Même dans les contextes très artificialisés, il reste très souvent associé, notamment par les adultes des conurbations, au sentiment positif d'une présence agréable et relaxante. Une enquête faite aux États-Unis laisse penser que ce sentiment n'est pas uniquement lié aux expériences de l'enfance (présence d'arbres dans l'environnement proche, activité dans la nature…), mais aussi aux sentiments exprimés par les parents sur la nature, avec des variations selon le sexe, l'âge et l'origine ethnique des personnes interrogées[83].
Les gens montrent des préférences pour la taille, forme et couleur des arbres[84]. De manière générale, les couleurs vertes et rouges semblent préférées au jaunâtre et pourpre, peut-être parce que celles-là sont associées à des arbres en meilleure santé, et donc à un environnement plus propice au développement humain[84]. La naturalité d'un paysage, sa richesse et son harmonie, et la présence de l'arbre dans ce cadre jouent aussi un rôle important dans le sentiment de paix ou de bien-être qu'il procure[85]. Ainsi au Japon, une étude sur l'effet curatif de la végétation existant dans le paysage a montré que 94 % des interrogés décrivent spontanément préférer un paysage très naturel, contre 1 % préférant un paysage artificiel, avec des variations selon l'âge.
Dans la littérature, l'arbre provoque des émotions selon qu'il est tour à tour objet de crainte (arbre des sorcières, arbre au gibet, arbre terrifiant dans les cauchemars d'enfants comme chez George Sand) ou d'amour (lieu de la rêverie romantique, initiales ou cœur gravé sur l'écorce — pratique déjà décrite dans le temple d'Astrée où les noms des dieux sont inscrits dans l'écorce d'un chêne — ou encore lieu de la liberté comme dans Le Baron perché), arbre chtonien ou ouranien[86].
L'arbre symbolise tantôt les forces de la Vie comme l'arbre de vie, tantôt l'homme, tantôt une famille : arbre généalogique.
Dans la Bible, plus particulièrement dans le second récit de la Création du Livre de la Genèse, le tronc de l'arbre fait fonction de lien entre la terre où il a ses racines et le ciel où il est dirigé. L'arbre est donc un symbole de la communion entre les deux mondes : celui d'en haut où habite la divinité et celui d'en bas où habitent les humains[87].
Dans la Mythologie Nordique, Yggdrasil, l'Arbre Monde sur lequel reposent les neuf royaumes.
L’Arbre est la reprise de l'arbre de vie sumérien, puis mésopotamien avant de passer dans la Bible. Le vol des pommes d'or dans les jardins des Hespérides devient le fruit de l'arbre du Paradis[88].
Dans le jardin d'Éden, il y a des arbres, dont deux particuliers : l'arbre de vie, qui symbolise l'immortalité, et l'arbre de la connaissance du bien et du mal, qui symbolise le savoir illimité, deux caractéristiques réservées à Dieu[89].
En Islam, il est également fait référence, dans certains écrits spirituels, à l'Arbre du Monde[90].
Lors des cycles saisonniers, la « mort » présumée et la « renaissance » annuelle de l'arbre au printemps l'ont fait adopter comme symbole de la fécondité, de retour à la vie. Témoins les traditions d'arbre de mai et d'arbre de Noël.
Certains arbres ont une symbolique propre : l'olivier (Olea europea) représente la paix, la sérénité (c'est aussi un symbole du Christ), le chêne (Quercus sp.) représente la robustesse, la longévité.
On retrouve cette représentation dans certains tests psychologiques (Test de l'arbre) : les racines représentent l'ancrage de la personne dans sa propre vie, dans la réalité, le tronc sa posture, les branches et les feuilles son épanouissement.
L'olivier est un des symboles de l'Athènes antique : il aurait été offert à la cité par la déesse Athéna à l'occasion d'un concours avec le dieu de la mer Poséidon. L'olivier est aussi symbole de paix.
Arbres emblèmes : la feuille d'érable à sucre (Acer saccharum) est l'emblème du Canada, le cèdre (Cedrus libani) celui du Liban. Le pernambouc (Paubrasilia echinata) est l'arbre national du Brésil (voir aussi la liste des plantes-emblèmes).
Au Japon, Hanami, la période de floraison des cerisiers, les Sakura, (Prunus sp.) à la fin de l'hiver et Momijigari, la période de passage aux couleurs d'automne de l'érable japonais (Acer japonicum) sont des événements célébrés dans tout le pays.
En Afrique, l'arbre à palabres est un lieu traditionnel de rassemblement.
Les pièces de 1 et de 2 euros, œuvres de Joaquin Jimenez, sont frappées depuis 1999 ; [91].
Si la forêt et les bois sont plus ou moins protégés par le droit coutumier ou codifié de par le monde, l'arbre en tant qu'individu a rarement de statut juridique clair, même quand une valeur patrimoniale importante lui est unanimement reconnue (arbre remarquable).
Dans certaines cultures, des arbres peuvent être sacrés, de même qu'existent des bois sacrés.
Dans certains pays un arbre ou des arbres peuvent être protégés par une servitude environnementale qui peut interdire aux générations successives d'acheteurs d'un terrain de les couper et/ou exploiter.
L'arbre en ville a pu faire l'objet de protections spécifique car présentant une valeur particulière en raison de sa rareté et vulnérabilité dans le contexte urbain[92], d'abord grâce au droit du patrimoine « remarquable » (Loi du 31 décembre 1913 sur les monuments historiques, loi du 2 mai 1930 sur les sites et monuments naturels, Zones de Protection du Patrimoine Architectural Urbain et Paysager instaurées par la loi de décentralisation du 7 janvier 1983 et secteurs sauvegardés de la loi Malraux de 1962 qui toutes peuvent protéger des arbres remarquables), aujourd'hui sous l'égide du Service départemental de l'architecture et du patrimoine (SDAP).
Des réflexions existent pour donner aux arbres et en particulier aux arbres remarquables un statut plus clair[93],[94].
La loi Paysage a modifié les modalités d'enquête publique et introduit des outils permettant dans une certaine mesure la protection d'arbres dans le paysage (arbres et alignements remarquables)[95]. Parmi les outils mobilisables pour protéger des arbres figurent (principalement dans le cadre de la loi paysage et du droit de l'urbanisme) :
Des outils de contractualisation (chartes, plans, contrats...) sont susceptibles de prévenir ou limiter le risque de conflits juridiques souvent aléatoires et d'autres outils du droit de l'environnement peuvent parfois être utilisés : les Espaces naturels sensibles, la loi Littoral, les réserves naturelles régionales ou encore le droit civil quand il règle les relations de voisinage concernant les arbres en limites de propriété.
À la suite de la destruction de nombreuses haies et arbres isolés par les remembrements, il a été interdit de détruire des arbres durant la procédure de préparation d'un remembrement.
En Suisse, les arbres sont légiférés notamment dans la loi fédérale sur les forêts et l'Ordonnance sur les forêts. En 2011, le Conseil fédéral a fixé les lignes stratégiques de la politique forestière 2020, mises en œuvre notamment par l'Office fédéral de l'environnement[101]. Les cantons et certaines communes suisses ont également une législation concernant la protection des arbres.
Dans un souci de préservation de la biodiversité, le quart du budget de l'Arboretum national du vallon de l'Aubonne est financé à hauteur d'environ 250 000 francs suisses par les pouvoirs publics[102].
Aux Pays-Bas, en vertu des lois sur la protection environnementale, il est interdit de couper un arbre si un autre n'est pas planté à la place. Ainsi, le pays sécurise le nombre de ses arbres et peut uniquement le revoir à la hausse.
Cet article concerne l'élément chimique. Pour les autres significations, voir Étain (homonymie).
L'étain est l'élément chimique de numéro atomique 50, de symbole Sn (du latin stannum). C'est un métal pauvre du groupe 14 du tableau périodique. Il existe dix isotopes stables de l'étain, principalement ceux de masses 120, 118 et 116.
L'étain existe aux états d'oxydation 0, +II et +IV. À température ambiante le corps simple étain est un solide métallique.
L'étain est connu dès l'Antiquité, où il servait à protéger la vaisselle de l'oxydation et pour préparer le bronze. Il est toujours utilisé pour cet usage, et pour le brasage. Cet élément est peu toxique. Rare à l'état natif, l'étain est essentiellement extrait d'un minéral appelé cassitérite où il se trouve sous forme d'oxyde SnO2.
C'est un métal gris-argent, malléable, moyennement ductile à température ambiante. Il est hautement cristallisé et la déformation d'une lame d'étain produit du bruit ; on dit que l'étain « crie » ou « pleure » (phénomène de maclage).
Il résiste à la corrosion par l'eau de mer et l'eau douce, mais peut être attaqué par les acides forts. Cette résistance est de nature cinétique puisque le potentiel normal du couple Sn2+Sn = -0,136 V. Il est donc thermodynamiquement attaqué par l'eau, et bien sûr par l'oxygène.
À la pression atmosphérique, l'étain pur possède trois variétés allotropiques (il peut exister sous trois formes cristallines). Entre 13 °C et 162 °C, l'étain est de structure tétragonale (forme ), c'est l'étain blanc, de masse volumique 7,28 g cm−3. Au-dessus de 162 °C, on trouve la structure orthorhombique (forme ), cassante, que l'on peut pulvériser avec un mortier. En dessous de 13 °C, l'étain blanc se transforme lentement en étain gris, de structure diamant (forme ), de masse volumique 5,75 g cm−3.
Cette transformation et le changement de densité qui l'accompagne affectent la tenue mécanique du matériau. En dessous de −50 °C, la transformation est rapide et l'étain devient pulvérulent (tombe en poussière). C'est la « peste de l'étain » (« lèpre de l'étain » quand le phénomène reste superficiel). Ce phénomène .
Par déposition contrôlée de l'étain en phase gazeuse sur un substrat solide on peut former une monocouche d'atomes d'étain de structure hexagonale : le stanène, similaire au graphène. Le stanène est un isolant topologique bidimensionnel.
L'étain possède 40 isotopes connus, de nombre de masse variant de 99 à 138, et 32 isomères nucléaires. Parmi eux, 10 sont stables (trois sont potentiellement radioactifs, mais aucune désintégration n'a pour l'instant été observée), ce qui fait de l'étain l'élément comportant le plus d'isotopes stables, suivi par le xénon. Il y a de grandes chances que cette propriété ait un rapport avec le fait que l'étain possède 50 protons, un nombre magique.
L'étain est connu dès la Préhistoire sur toute la planète. C'est un des composants de la métallurgie du bronze. Le nom d'origine latine stannum ou stagnum fut d'abord utilisé pour un mélange d'argent et de plomb. L'étain utilisé par les Grecs et les Mésopotamiens des débuts de l'âge du bronze était extrait principalement en Afghanistan[10]. Les navires phéniciens franchirent les colonnes d'Hercule et allèrent jusqu'en Bretagne et en Cornouailles (les mythiques « îles Cassitérides ») à la recherche des mines d'étain (en grec ancien κασσίτερος / kasíteros). Plus tard, Jules César a signalé l'exploitation de minerais d'étain en Bretagne, qu'il situe à tort dans le centre du pays au lieu des Cornouailles[11].
La métallurgie de l'étain est une réduction de l'oxyde SnO2 par le carbone à haute température[12].
Il existe également un circuit de recyclage qui produit 30 % de l'étain. Ainsi, l'étain contenu dans un alliage appelé fer-blanc (acier recouvert de 0,3 % d'étain pour le protéger) est récupéré par traitement à la soude à 70 °C. L'acier reste à l'état métallique alors que l'étain est attaqué et produit des ions stannate SnO44−. Ceux-ci sont ensuite réduits en étain métallique par électrolyse[12].
La France ne possède plus de mine d'étain depuis 1975. Les dernières, en Bretagne (mine de Saint-Renan) en produisaient 500 t/an[12].
La Malaisie est le pays où se situent la plupart des réserves mondiales d'étain. La cassitérite y est notamment exploitée par dragage des fonds sous-marins, ce qui n'est pas sans poser de sérieux problèmes environnementaux.
Production minière mondiale 2014 [13]
Les bronzes sont de nos jours des mélanges de cuivre et d'étain. Le terme désignait autrefois tous les alliages du cuivre ; on l'appelait aussi airain, sans que la composition de l'alliage soit plus précise. Utilisé dès l'Antiquité, il a caractérisé l’âge du bronze.
Les autres alliages sont moins notoires, et les termes qui les décrivent sont plus précis.
L'étain peut s'utiliser au contact des aliments.
L'étain intervient sous forme pure ou alliée dans la fabrication de nombreux objets, notamment :
L'étain sert encore comme auxiliaire de fabrication. Le procédé le plus répandu pour celle du verre plat est le flottage sur lit d'étain en fusion (verre « float » ou verre flotté).
L'étamage consiste à recouvrir une pièce métallique d'une fine couche d'étain ou d'un alliage plomb-étain.
On étame le verre pour fabriquer des miroirs, les casseroles en cuivre pour éviter la formation d'oxyde de cuivre toxique (vert-de-gris), ou les conducteurs électriques pour améliorer les contacts et faciliter la brasure de composants.
Le fer-blanc est une tôle fine d'acier doux étamée, généralement par électro-déposition. Le fer-blanc était surtout utilisé pour fabriquer les emballages métalliques (boîtes de conserve).
Sur un circuit imprimé, l'étain pur (sans plomb) peut former des « whiskers », c'est-à-dire des fils micrométriques susceptibles de provoquer des courts-circuits. Le processus de formation des « whiskers », qui dure plusieurs mois, est mal compris (il semble que l'intensité du champ magnétique joue un rôle). Des remèdes existent (ajouts de traces d'autres métaux au moment du dépôt de la couche d'étain)[réf. souhaitée].
Ce métal n'est pas toxique, mais était et reste souvent associé à des traces de plomb, qui l'est.
Elle semble presque systématique dans les pays riches (dont France) mais en des proportions encore mal connue, et elle varie vraisemblablement selon de nombreux paramètres (environnementaux et alimentaires notamment).
En 2018 en France le « Volet périnatal » du programme national de biosurveillance a publié une évaluation de l'imprégnation des femmes enceintes notamment par le cobalt (et par 12 autres métaux ou métalloïdes et quelques polluants organiques).
Ce travail a été fait à l'occasion du suivi d'une cohorte de 4 145 femmes enceintes (« Cohorte Elfe »). Cette cohorte comprenait des femmes ayant accouché en France en 2011 hors Corse et TOM)[14]. Le dosage urinaire de 990 femmes enceintes arrivant à la maternité a confirmé une quasi-omniprésence de l'étain dans l'environnement[14] ; il a été retrouvé dans 91 % des échantillons d’urine analysées[14] (moyenne géométrique : 0,29 μg/L ; créatinine : 0,39 μ μg/g[14]). Ces taux sont relativement similaires à ceux cités hors de France par des études ayant porté sur des adultes, et (par quelques rares étude) concernant les femmes enceintes[14]. Dans le contexte périnatal français de 2011, le risque d'imprégnation par l’étain a semblé croître avec la consommation d'eau du robinet (peut-être en raison de la fréquence de l'étain dans les produits de soudures des canalisations ou dans certains matériaux entrant en contact avec l’eau destinée à la consommation humaine)[14].
L'étain utilisé en décoration a un brillant assez faible par rapport à celui d'autres métaux, mais suffisant pour être difficile à répliquer, sauf avec des peintures métallisées. La surface de l'étain oxydée (patinée) est plus sombre et moins brillante que celle de l'étain neuf.
La couleur étain est un nom de couleur en usage dans la mode, pour désigner une nuance de gris, qui ne peut avoir l'apparence du métal. Dans la décoration et le bâtiment, il s'utilise pour des surfaces grises ayant un certain brillant.
Dans les répertoires commerciaux, on trouve en fil à broder 169 étain[15] ; en matériaux de construction béton ciré étain[16].
L'étain est le composant principal d'un pigment jaune historique, l'or mussif, un bisulfure d'étain d'aspect doré, utilisé notamment dans l'art byzantin pour les icônes et les mosaïques. Il se compose de fines plaquettes donnant un éclat à la peinture. Il est référencé PY38 au Colour Index. C'est un produit vénéneux, abandonné aujourd'hui au profit de poudres de bronze[17].
Pour les articles homonymes, voir Temps (homonymie).
modifier - modifier le code - modifier WikidataLe temps ou la météo est l'ensemble des conditions physiques des basses couches de l'atmosphère à un moment précis et en un point précis. Sont communément associées au temps les conditions météorologiques dont les effets peuvent être directement ressentis : ennuagement ou rayonnement solaire, température, vent, précipitations et également visibilité. L'étude et la prévision du temps s'appellent la météorologie.
Quatre éléments interagissent pour produire le temps[1] : la pression atmosphérique, la température, l'humidité atmosphérique (hygrométrie) et le vent (généré par les variations de pression atmosphérique, l'altitude et le relief). Ces facteurs sont eux-mêmes influencés par 6 autres facteurs : la latitude, l'altitude, la végétation, les courants marins, la répartition terre-mer et la rotation de la Terre.
Les phénomènes météorologiques qui engendrent le temps peuvent se produire à différentes échelles temporelles et spatiales. Que l'on compare par exemple l'orage d'un après-midi d'été avec une vaste tempête des latitudes moyennes qui peut durer plusieurs jours et couvrir des milliers de kilomètres carrés. La plupart de ces phénomènes sont liés au soleil, comme source d'énergie, mais les éruptions volcaniques peuvent aussi avoir un effet catastrophique sur le climat, pendant plus ou moins longtemps selon l'altitude que les matériaux éjectés ont pu atteindre[a].
Sur terre, les phénomènes météorologiques incluent vent, nuage, pluie, neige, brouillard et tempêtes de sable. 
Les évènements moins communs incluent les catastrophes naturelles telles que les tornades, les cyclones tropicaux et tempêtes de neige. Pratiquement tous ces phénomènes familiers surviennent dans la troposphère (en dessous de l'atmosphère). La météo survient dans la stratosphère et peut affecter le temps dans la troposphère, mais les mécanismes exacts sont complexes et encore peu compris[2].
Le temps se produit principalement en raison des différences de pression atmosphérique, de température et d'humidité d'un endroit à l'autre. Ces différences peuvent se produire en raison de l'angle du soleil à n'importe quel endroit particulier, qui varie selon la latitude. En d'autres termes, plus on est loin des tropiques, plus l'angle du soleil est bas, ce qui rend ces endroits plus frais en raison de la propagation de la lumière du soleil sur une plus grande surface. Le fort contraste de température entre l'air polaire et l'air tropical donne naissance aux cellules de la circulation atmosphérique à grande échelle et au courant-jet.
Une fois les masses d'air mise en mouvement par ces différences, la rotation de la Terre initie une déviation sur les parcelles de fluide qui dépend de la direction du mouvement. Ainsi, les systèmes météorologiques aux latitudes moyennes, tels que les cyclones extratropicaux, se développent à cause des instabilités le long des zones baroclines et les nuages générés tournent autour d'un centre de basse pression. Sous les Tropiques, la mousson ou les cyclones tropicaux tirent leur énergie de l'instabilité thermique de la masse d'air et subissent aussi la rotation de la Terre. Finalement, des phénomènes à méso-échelle comme les orages dépendent de l'instabilité thermique mais sont peu étendus ce qui donne moins de temps à la rotation terrestre de les affecter.
L'atmosphère est un système chaotique. En conséquence, de petits changements dans une partie du système peuvent s'accumuler et s'amplifier pour provoquer des effets importants sur le système dans son ensemble. Cette instabilité est difficile à analyser et rend les prévisions météorologiques difficiles. Les météorologues s'efforcent continuellement d'étendre la limite de prévisibilité grâce au raffinement des modèles de prévision numérique du temps et de l'ingestion de données de plusieurs sources : stations météorologiques terrestres, maritimes, aériennes; radar et satellites météorologiques.
« Induction magnétique » redirige ici. Pour les autres significations, voir Induction.
Vous lisez un « bon article » labellisé en 2007.
modifier En physique, dans le domaine de l'électromagnétisme, le champ magnétique est une grandeur ayant le caractère d'un champ vectoriel[a], c'est-à-dire caractérisée par la donnée d'une norme, d’une direction et d’un sens, définie en tout point de l'espace et permettant de modéliser et quantifier les effets magnétiques du courant électrique ou des matériaux magnétiques comme les aimants permanents.
La présence du champ magnétique se traduit par l'existence d'une force agissant sur les charges électriques en mouvement (dite force de Lorentz) et par divers effets affectant certains matériaux (diamagnétisme, paramagnétisme, ferromagnétisme, etc.). La grandeur qui détermine l'interaction entre un matériau et un champ magnétique est la susceptibilité magnétique.
Les différentes sources de champ magnétique sont les aimants permanents, le courant électrique (c'est-à-dire le déplacement d'ensemble de charges électriques), ainsi que la variation temporelle d'un champ électrique (par induction électromagnétique).
Sauf exception, cet article traite du cas du régime statique ou indépendant du temps, pour lequel le champ magnétique existe indépendamment de tout champ électrique, soit en pratique celui créé par les aimants ou les courants électriques permanents. Toutefois, en régime variable, c'est-à-dire pour des courants électriques non permanents, ou des champs électriques variables, le champ magnétique créé, lui-même variable, est la source d'un champ électrique, et donc ne peut être considéré de façon indépendante (cf. champ électromagnétique).
Deux champs vectoriels apparentés[1] servent en physique à décrire les phénomènes magnétiques et peuvent de ce fait prétendre au nom générique de « champ magnétique » :
Lorsqu'il est nécessaire de faire la différence entre les deux, le champ  peut être qualifié de « champ d'induction magnétique » et le champ  de « champ d'aimantation » ou de « champ d'excitation magnétique ».
Bien que les normes internationales de terminologie[2] prescrivent de réserver l’appellation de « champ magnétique » au seul champ vectoriel , en physique fondamentale, le terme pris absolument désigne le plus souvent le champ vectoriel , en dehors du cas spécifique de l'étude des milieux continus. C'est bien de ce champ  qu'il est question dans le présent article.
La manifestation historique la plus élémentaire du champ magnétique est celle du champ magnétique terrestre, à travers sa tendance à faire tourner l'aiguille d'une boussole : laissée libre de tourner, l'aiguille s'aligne dans la direction du pôle nord, ce qui montre qu'elle subit un moment qui tend à l'aligner dans cette direction. Le couple  qui tend à ramener l'aiguille aimantée sur la direction du pôle magnétique est le produit vectoriel d'une grandeur vectorielle intensive caractéristique du lieu, le champ magnétique  (supposé localement uniforme), et d'une quantité vectorielle extensive, caractéristique de l'aiguille, son moment magnétique . Cette relation se traduit mathématiquement par :
Cette définition donne donc une méthode permettant, en pratique, de mesurer le champ magnétique en un point à partir d'un système comportant un moment magnétique déterminé. La même méthode permet symétriquement de mesurer le moment magnétique d'un échantillon inconnu placé dans un champ magnétique connu.
Mathématiquement, le champ magnétique est ainsi décrit par un champ pseudo vectoriel[c], qui se rapproche d'un champ de vecteurs par plusieurs aspects, mais présente quelques subtilités au niveau des symétries. Ceci étant dit, cette expérience primitive ne dit rien sur la nature du champ magnétique, ni sur celle du moment magnétique d'un objet qui s'y déplace.
Au XIXe siècle, l'étude de l'électromagnétisme a montré le lien entre électricité et magnétisme, à travers la force de Laplace : un conducteur parcouru par un courant électrique  est également soumis à une force linéique sur chaque élément de longueur , donnée par :
Cette équation, en faisant le lien entre le magnétisme et l'électricité, donne également la dimension du champ magnétique en fonction de ces grandeurs de base : si une force (exprimée en kg m s−2) est créée par une intensité (A) fois une longueur (m) fois un champ magnétique, c'est donc que ce dernier s'exprime normalement en kg s−2 A−1.
La discipline qui étudie les champs magnétiques statiques ou « quasi stationnaires » (ne dépendant pas du temps, ou faiblement) est la magnétostatique. Le champ magnétique n'apparaît cependant dans sa pleine dimension qu'en dynamique.
Dans un premier temps, les équations décrivant l'évolution du champ magnétique sont appelées équations de Maxwell, en l'honneur de James Clerk Maxwell qui les a publiées en 1873. Le champ magnétique et le champ électrique sont les deux composantes du champ électromagnétique décrit par l'électromagnétisme, pour un observateur au repos. Des ondes électromagnétiques peuvent se propager librement dans l'espace, et dans la plupart des matériaux. Ces ondes portent des noms différents (ondes radio, micro-onde, infrarouge, lumière, ultraviolet, rayons X et rayons gamma) selon leur longueur d'onde.
C'est cependant Albert Einstein qui dans un deuxième temps, en 1905, a proposé le premier la vision la plus cohérente du lien entre électrodynamique et champ magnétique, dans le cadre de la relativité restreinte qu'il venait de découvrir et qui en est indissociable. Lorsqu'une charge électrique se déplace, on doit employer les transformations de Lorentz pour calculer l'effet de cette charge sur l'observateur. Cette réécriture donne une composante du champ qui n'agit que sur les charges se déplaçant : ce que l'on appelle le « champ magnétique ».
Les applications de la maîtrise de ce champ sont nombreuses, même dans la vie courante : outre le fait que celui-ci est une composante de la lumière, il justifie l'attraction des aimants, l'orientation des boussoles et permet entre autres la construction d'alternateurs et de moteurs électriques. Le stockage d'informations sur bandes magnétiques ou disques durs se fait à l'aide de champs magnétiques. Des champs magnétiques de très forte intensité sont utilisés dans les accélérateurs de particules ou les tokamaks pour focaliser un faisceau de particules très énergétiques dans le but de les faire entrer en collision. Les champs magnétiques sont également omniprésents en astronomie, où ils sont à l'origine de nombreux phénomènes comme le rayonnement synchrotron et le rayonnement de courbure, ainsi que la formation de jets dans les régions où l'on observe un disque d'accrétion. Le rayonnement synchrotron est également abondamment utilisé dans de nombreuses applications industrielles.
Dès le VIe siècle av. J.-C., les philosophes grecs décrivaient — et tentaient d'expliquer — l'effet de minerais riches en magnétite. Ces roches étaient issues entre autres de la cité de Magnésie : elle donna son nom au phénomène.
L'aiguille « Montre-sud » est mentionnée pour la première fois au XIe siècle par Shen Kuo et, même s'il y a des attestations de la connaissance de l'aimant en Chine[3] dès le IIIe siècle av. J.-C., le problème du magnétisme terrestre apparaît beaucoup plus tard. L'utilisation de la boussole dans les techniques de navigation daterait du XIIe siècle et son usage exact reste à préciser du fait d'une navigation essentiellement côtière à cette époque[3]. Les boussoles faisaient usage du champ magnétique terrestre, qui se trouve être aujourd'hui à peu près aligné avec l'axe de rotation terrestre, raison pour laquelle une boussole, en indiquant le pôle magnétique, indique aussi (quoique approximativement) la direction du pôle géographique terrestre.
En Occident, Pierre de Maricourt fut l'un des premiers à travailler sur le magnétisme et publia, en 1269, son Epistola de Magnete à peu près à la même époque que les savants chinois. Au-delà du simple problème des priorités, il serait intéressant de savoir comment certaines techniques ont pu voyager et s'il n'est pas possible que des développements parallèles, et chronologiquement presque concomitants, se soient produits[3].
En décembre 1765, pour les encyclopédistes des Lumières, « le magnétisme est le nom général qu’on donne aux différentes propriétés de l’aimant »[4]. Ils attribuent ses effets à une « matière subtile[d], différente de l’air » (parce que ces phénomènes ont également lieu dans le vide) qu’ils appellent magnétique. Plus loin ils affirment que « c’est encore une question non moins difficile que de savoir s’il y a quelque rapport entre la cause du magnétisme & celle de l’électricité, car on ne connoît guère mieux l’une que l’autre. »
Jusqu'au début des années 1820 on ne connaissait que le magnétisme des aimants naturels à base de magnétite.
En 1820 Hans Christian Ørsted montre qu'un courant électrique parcourant un fil influence l'aiguille d'une boussole située près de celui-ci. Il fut cependant incapable d'expliquer ce phénomène à la lumière des connaissances de l'époque. En 1831 Michael Faraday énonce la loi de Faraday, qui trace un premier lien entre électricité et magnétisme.
En 1822 le premier moteur électrique est inventé : la roue de Barlow.
André-Marie Ampère proposa peu après une loi phénoménologique, aujourd'hui démontrée dans le cadre général de l'électromagnétisme, appelée théorème d'Ampère, qui relie le champ magnétique aux courants. Peu après, en 1825, l'électricien William Sturgeon crée le premier électroaimant.
En 1873 James Clerk Maxwell unifie le champ magnétique et le champ électrique au sein de la théorie de l'électromagnétisme. Ce faisant, il découvre une incompatibilité entre les lois de la mécanique classique et les lois de l'électromagnétisme. Ces dernières prédisent que la vitesse de la lumière est indépendante de la vitesse d'un observateur par rapport à la source qui émet la lumière, hypothèse incompatible avec les lois de la mécanique classique.
En 1873 l'ingénieur belge Zénobe Gramme invente le premier moteur électrique à courant continu utilisable à grande échelle.
En 1887 les Américains Albert A. Michelson et Edward Morley vérifient expérimentalement les prédictions de Maxwell (expérience de Michelson-Morley).
En 1905 Albert Einstein résout le paradoxe découvert par Maxwell en montrant que les lois de la mécanique classique doivent être remplacées par celles de la relativité restreinte[5].
En 1933 Walther Meissner et Robert Ochsenfeld découvrent qu'un échantillon supraconducteur plongé dans un champ magnétique a tendance à expulser celui-ci de son intérieur (effet Meissner).
En 1944 Lars Onsager propose le premier modèle (dit modèle d'Ising) décrivant le phénomène de ferromagnétisme.
En 1966 le docteur Karl Strnat découvre les premiers aimants samarium-cobalt, d'une énergie phénoménale (18 à 30 MGOe)[6].
En 1968 sont découverts les pulsars, cadavres d'étoiles extraordinairement denses, sièges des champs magnétiques les plus intenses existant aujourd'hui dans la nature (4 × 108 teslas pour le pulsar du Crabe, par exemple).
En 1983 une équipe internationale crée des aimants néodyme-fer-bore, les plus puissants aimants permanents connus à ce jour (35 MGOe, soit environ 1,25 T[6]).
En 1998 une équipe russe crée un champ magnétique pulsé par une explosion qui atteint 2 800 T[7].
Le 12 décembre 1999, une équipe américaine crée un champ magnétique continu d'une intensité de 45 T[8].
En 2006 des champs magnétiques pulsés ont atteint 100 T sans destruction[9].
Pour les champs statiques, le record obtenu en 2019 est de 45,5 T[10].
On note généralement le champ magnétique avec la lettre , écrite en caractère gras ou surmontée d'une flèche, ces deux notations indiquant qu'il s'agit d'un vecteur (ou en l'occurrence d'un pseudovecteur) :  ou . Cette lettre, empruntée à James Clerk Maxwell, vient de ses notations : il décrivait les trois composantes du champ magnétique indépendamment, par les lettres , , . Les composantes du champ électrique étant, dans les notations de Maxwell les lettres , , .
Le champ étant défini dans tout l'espace, c'est en fait une fonction des coordonnées, en général notées par le rayon vecteur , et éventuellement du temps , aussi est-il noté  ou . Cependant, on utilise souvent la notation , la dépendance spatiale et/ou temporelle étant implicite.
L'unité moderne utilisée pour quantifier l'intensité du champ magnétique est le tesla, défini en 1960[11]. C'est une unité dérivée du système SI. On définit un tesla par un flux d'induction magnétique d'un weber par mètre carré :
Pour diverses raisons historiques remontant aux travaux de Charles de Coulomb, certains auteurs préfèrent utiliser des unités hors du système SI, comme le gauss[e] ou le gamma[f]. On a :
Enfin, on utilise parfois l'œrsted (de symbole « Oe »), notamment pour quantifier la « force » des aimants naturels, dont l'équivalent SI est l'ampère par mètre (A m−1) par la relation :
Dans l'espace interplanétaire, le champ magnétique est compris entre 10−10 et 10−8 T[13]. Des champs magnétiques à plus grande échelle, par exemple au sein de la Voie lactée sont également mesurés, par l'intermédiaire du phénomène de rotation de Faraday, en particulier grâce à l'observation des pulsars. L'origine et l'évolution des champs magnétiques aux échelles galactiques et au-delà est à l'heure actuelle (2007) un problème ouvert en astrophysique. Les étoiles, à l'instar des planètes, possèdent aussi un champ magnétique, qui peut être mis en évidence par spectroscopie (effet Zeeman). Une étoile en fin de vie a tendance à se contracter, laissant à l'issue de la phase où elle est le siège de réactions nucléaires un résidu plus ou moins compact. Cette phase de contraction augmente considérablement le champ magnétique à la surface de l'astre compact. Ainsi, une naine blanche possède un champ magnétique pouvant aller jusqu'à 104 teslas, alors qu'une étoile à neutrons jeune, bien plus compacte qu'une naine blanche a un champ mesuré à 108 voire 109 teslas. Certaines étoiles à neutrons appelées pulsars X anormaux et magnétars semblent être dotées d'un champ magnétique jusqu'à 100 fois plus élevé[14],[15].
Un aimant NdFeB (néodyme-fer-bore) de la taille d'une pièce de monnaie (créant un champ de l'ordre de 1,25 T[6]) peut soulever un objet de 9 kg et effacer les informations stockées sur une carte de crédit ou une disquette. Les utilisations médicales, comme l’IRM, impliquent des champs d'intensité allant jusqu'à 6 T. Les spectromètres RMN peuvent atteindre jusqu'à 23,5 T (1 GHz résonance du proton).
Étant une composante du champ électromagnétique, l'intensité du champ magnétique décroît avec la distance à sa source, mais en restant de portée infinie. Ceci est intimement lié au fait que la particule élémentaire vecteur de l'interaction électromagnétique, le photon, est de masse nulle. Cependant, des chercheurs espagnols ont récemment montré[16] que – un peu de la même manière qu'une fibre optique peut transporter la lumière avec peu de pertes – un cylindre composé d'un matériau supraconducteur (cobalt-fer hautement magnétique dans le cas présent) peut transporter des champs magnétiques sur une distance plus longue (c'est-à-dire diminuer leur perte d'intensité selon la distance)[16].
En physique classique, les champs magnétiques sont issus de courants électriques. Au niveau microscopique, un électron en « orbite » autour d'un noyau atomique peut être vu comme une minuscule boucle de courant, générant un faible champ magnétique et se comportant comme un dipôle magnétique. Selon les propriétés des matériaux, ces structures magnétiques microscopiques vont donner lieu à essentiellement trois types de phénomènes :
Tout courant électrique, alternatif ou continu, génère un champ magnétique, ce qu'a montré l'expérience historique de Hans Christian Ørsted pour le courant continu.
La présence d'un courant permet donc d'influencer localement le champ magnétique, c'est le principe des électroaimants. Ce champ magnétique est d'autant plus intense que le courant l'est. Réciproquement, un champ magnétique variable est susceptible de générer un courant électrique. C'est le principe de l'induction magnétique qu'utilisent toutes les machines électriques.
La Terre, comme la plupart des planètes du Système solaire, possède un champ magnétique. Ce champ magnétique terrestre — qui protège la Terre en déviant les particules chargées issues du Soleil dans une région appelée magnétosphère — est principalement d'origine interne. On suppose qu'il est issu d'effets de convection de la matière située dans le noyau externe de la Terre, principalement composé de fer et d'un peu de nickel liquide. En particulier, des courants (bien que très faibles), parcourant le noyau induiraient ce champ magnétique, par un processus appelé effet dynamo.
La valeur moyenne du champ magnétique terrestre est d'environ 0,5 gauss (soit 5 × 10−5 T). Le champ magnétique terrestre fluctue au cours du temps : sa direction et son intensité ne sont pas constantes. De plus, il n'est pas homogène en tout point du globe[17].
En particulier, les champs magnétiques des planètes Jupiter et Saturne, les plus intenses après celui du Soleil[18] sont actuellement beaucoup étudiés afin notamment de comprendre le décalage entre l'orientation du champ magnétique et l'axe de rotation de la planète, ainsi que ses variations[19]. La mesure du champ magnétique de Saturne est l'un des objectifs de la mission Cassini-Huygens[20], tandis que celui de Jupiter est en cours d'étude par la sonde JUNO[21]. L'origine de ces champs est supposée liée aux mouvements du noyau d'hydrogène métallique qu'elles abritent.
Au niveau des pôles magnétiques de ces planètes, le champ a tendance à guider les particules chargées, issues par exemple du vent solaire. Celles-ci, très énergétiques, interagissent parfois avec l'atmosphère de la planète : c'est ce que l'on peut observer sous la forme des aurores polaires.Une des différences fondamentales entre le champ électrique et le champ magnétique est que l'on observe dans la nature des particules possédant une charge électrique, alors que l'on n'observe ni particule ni objet possédant une charge magnétique. En pratique, cela se traduit par l'absence de configurations possédant un champ magnétique purement radial, ce qui mathématiquement correspond au fait que le champ magnétique est de divergence nulle.
En particulier, tout aimant possède un pôle nord et un pôle sud magnétique. Si l'on casse cet aimant en deux, on se retrouve avec deux aimants ayant chacun un pôle nord et un pôle sud magnétique. Mathématiquement, cette propriété se traduit par le fait que la divergence du champ magnétique est nulle, propriété formalisée par l'une des équations de Maxwell. Des objets hypothétiques ne possédant qu'un seul pôle magnétique sont appelés monopôles magnétiques.
L'existence de monopôles magnétiques n'a pour l'heure pas été prouvée. D'un point de vue physique, rien n'interdit cependant leur existence. Dans cette hypothèse, l'électrodynamique quantique prédit certaines de leurs propriétés, à savoir que la charge électrique et la charge magnétique sont deux entités nécessairement discrètes, dont le produit de la plus petite valeur positive est égal au produit d'un nombre entier par la constante de Planck réduite. On parle dans ce cas de monopôles de Dirac, nommés en l'honneur du physicien anglais Paul Dirac qui a prouvé cette propriété de discrétisation.
Dans la théorie de Yang-Mills, on fait intervenir un monopôle de 't Hooft-Polyakov (en).
En 1905, Albert Einstein montra comment le champ magnétique apparaît comme un des aspects relativistes du champ électrique[22], plus précisément dans le cadre de la relativité restreinte.
Il se présente comme le résultat de la transformation lorentzienne d'un champ électrique d'un premier référentiel à un second en mouvement relatif.
Lorsqu'une charge électrique se déplace, le champ électrique engendré par cette charge n'est plus perçu par un observateur au repos comme à symétrie sphérique, à cause de la dilatation du temps prédite par la relativité. On doit alors employer les transformations de Lorentz pour calculer l'effet de cette charge sur l'observateur, qui donne une composante du champ qui n'agit que sur les charges se déplaçant : ce que l'on appelle « champ magnétique ».
On peut ainsi décrire les champs magnétique et électrique comme deux aspects d'un même objet physique, représenté en théorie de la relativité restreinte par un tenseur de rang 2, ou de manière équivalente par un bivecteur.
Le champ  peut être calculé dans le cas général en résolvant les équations de la magnétostatique qu'on peut écrire
où  est une constante fondamentale appelée perméabilité magnétique du vide, et  représente la densité de courant électrique.
Toutefois, et particulièrement dans le cas de l'étude des matériaux magnétiques, il est intéressant de décomposer phénoménologiquement la densité de courant  en deux composantes :
Il est alors possible d'introduire le vecteur aimantation  tel que , les équations précédentes deviennent :
Les deux sources du champ (courant de conduction et aimantation) doivent être connues pour pouvoir résoudre le système ci-dessus. Ce n'est pas toujours le cas en pratique car l'aimantation dépend souvent du champ et cette dépendance n'est pas toujours facile à modéliser.
Il est souvent commode pour résoudre les équations ci-dessus de définir un champ auxiliaire  par
Ce champ  est communément appelé excitation magnétique, mais parfois aussi champ magnétique, auquel cas le champ  sera appelé induction magnétique ou densité de flux magnétique.
Le champ  s'avère pratique notamment dans deux situations.
D'une part, lorsque ,  découle simplement de
On peut ainsi interpréter  comme étant le champ produit par le courant électrique. L'équation  montre que l'aimantation agit alors simplement comme une contribution supplémentaire à . Cette situation se rencontre notamment lorsqu'on aimante un matériau façonné en forme de tore à l'aide d'un bobinage enroulé autour de lui. Le champ  produit par le bobinage affecte l'aimantation du matériau, ce qui justifie le nom d’excitation magnétique donné à .
D'autre part, lorsque le champ est produit exclusivement par de la matière magnétique (des aimants), on a  et  découle de
Par analogie avec l'électrostatique, le terme  est appelé densité de charge magnétique. En pratique, la charge magnétique se trouve souvent sous forme de charge surfacique localisée sur les surfaces de l'aimant. Cette charge surfacique découle des discontinuités de la composante de  normale à la surface, où  est localement infini. Les surfaces ainsi chargées sont appelées pôles de l'aimant. La surface chargée positivement est le pôle nord, celle chargée négativement est le pôle sud. Le système d'équations ci-dessus exprime le fait que le champ magnétique est engendré par les pôles des aimants. Ce système peut être résolu numériquement en faisant dériver  d'un potentiel scalaire, alors qu'un potentiel vecteur serait nécessaire pour , ce qui vaut à  la faveur des analystes numériques.
Il faut remarquer qu'à la différence des charges électriques, les charges magnétiques ne peuvent être isolées. Le théorème de flux-divergence montre en effet que la charge magnétique totale d'un échantillon de matière est nulle. Un aimant a donc toujours autant de charge positive (pôle nord) que négative (pôle sud).
Dans le cas général où il y a à la fois des courants et des charges magnétiques, on peut décomposer  en une contribution engendrée par les courants et une contribution engendrée par les charges. Ces deux contributions sont calculées séparément. Une situation courante en physique expérimentale est celle où on utilise une bobine pour appliquer un champ sur un échantillon de matière. Dans ce cas le champ créé par la bobine est appelé champ appliqué et il est souvent connu à l'avance (il a été calculé par le fabricant de la bobine). Le champ total est alors donné par :
où  est le champ appliqué et  le champ créé par l'échantillon. Ce dernier est souvent appelé champ démagnétisant. Son calcul se ramène au cas où il n'y a pas de courant.
On peut remarquer d'abord que ces deux champs s'expriment dans des unités différentes :
Cette différence traduit le fait que  est défini par ses effets (force de Laplace) alors que  est défini par la façon de le créer avec des courants .
Dans le vide, puisque , on a
On peut alors interpréter la multiplication par  comme un simple changement d'unités et considérer que les deux champs sont identiques. L'ambiguïté qui découle du fait que l'un comme l'autre peut être appelé champ magnétique est alors sans conséquence. En pratique, beaucoup de matériaux, dont l'air, sont très faiblement magnétiques () et l'équation ci-dessus reste une très bonne approximation.
Cependant, dans les matériaux ferromagnétiques, notamment les aimants, l'aimantation ne peut être négligée. Il est important alors de distinguer les champs  et  à l'intérieur du matériau, bien qu'ils restent identiques à l'extérieur. Dans le cas d'un aimant barreau par exemple, les deux champs sont globalement orientés du pôle nord vers le pôle sud à l'extérieur de l'aimant. Cependant, à l'intérieur de celui-ci le champ  est globalement orienté du nord vers le sud (opposé à , d'où le nom de champ démagnétisant) alors que  va du sud vers le nord.
On peut remarquer que les lignes du champ  bouclent sur elles-mêmes, ce qui est une conséquence de , alors que les lignes de  ont toutes comme point de départ le pôle nord et comme point d'arrivée le pôle sud.
Par définition, les lignes de champ du champ magnétique sont l'ensemble des courbes « en tout point » tangentes à .
Ces lignes relient les pôles magnétiques, et par convention sont orientées de sorte que les lignes de champ d'un aimant entrent par le sud et ressortent par le nord. Leur expression locale est telle que :
où , de coordonnées (, , ), est un vecteur infinitésimal de déplacement.
Une équation paramétrique décrivant les lignes de champ se déduit de la formule ci-dessus en choisissant une variable d'intégration (par exemple  si la composante  est non nulle) et en intégrant les équations, qui en coordonnées cartésiennes donnent
Les lignes de champ permettent de visualiser qualitativement les forces magnétiques. Dans les substances ferromagnétiques comme le fer ou les plasmas, on peut visualiser ces forces en imaginant qu'il y a une tension le long des lignes de champ (qui agissent un peu comme un élastique), et au contraire une pression répulsive dans la direction perpendiculaire, qui tend à écarter ces lignes les unes des autres. Avec ceci en tête, on « voit » que les pôles magnétiques de signe contraires s'attirent, parce qu'ils sont directement reliés par de nombreuses lignes ; mais inversement on « voit » que les pôles de signes identiques se repoussent, parce que les lignes de champ qui en sont issues ne se rejoignent pas, mais les faisceaux s'écrasent l'un contre l'autre, ce qui engendre à la surface de contact une poussée répulsive entre les deux. Une description plus rigoureuse de cette visualisation fait appel au tenseur des contraintes de Maxwell.
Lorsqu'on approche un aimant d'une poudre de fer, on observe des formes géométriques particulières. Le ferromagnétisme de la limaille de fer fait qu'elle s'aimante légèrement en présence du champ magnétique. Ainsi, la limaille s'orientera de sorte qu'on observera les lignes de champ magnétique.
La forme précise de ces lignes dépend de la forme de l'aimant.
Dans une bobine suffisamment longue, on observe et on montre que le champ magnétique est pratiquement uniforme à l'intérieur : les lignes de champ sont portées par des droites parallèles et de même écart, selon l'axe du solénoïde.
Le champ magnétique étant de divergence nulle (on parle parfois de champ solénoïdal), il est possible de le décomposer en deux champs appelés champ toroïdal et champ poloïdal. Une telle décomposition est particulièrement appropriée dans les configurations de forme sphérique, et se trouve donc fréquemment utilisée en géophysique et en physique stellaire. Elle est également utilisée pour décrire le champ magnétique qui règne dans un tokamak.
Le champ magnétique influence les particules chargées au travers de la force de Lorentz.
En l'absence de champ électrique, l'expression de cette force est, pour une particule de charge  animée d'une vitesse  :
où ∧ représente le produit vectoriel, et où les quantités sont exprimées dans les unités du Système international.
On peut réécrire cette relation sous forme différentielle pour un fil, en introduisant le courant électrique :
avec  l'intensité du courant électrique,  le champ magnétique et  une portion infinitésimale de fil, symbolisée par un vecteur tangent à celui-ci.
Cette expression se généralise aux distributions de courants bidimensionnelles (surfaces et courants surfaciques) aussi bien que tridimensionnelles (volumes et courants volumiques). On introduit dans ces cas la notion d'« élément de courant » , définie par :
On a ainsi une expression générale :
La force de Laplace est simplement un cas particulier de la force de Lorentz, pour un barreau homogène et conducteur, parcouru par un courant électrique et placé dans un champ magnétique.
Contrairement à la force de Lorentz, elle ne traite pas des particules constituantes du barreau, mais de l'effet macroscopique : si son expression est similaire, le sens physique des objets considérés diffère. En particulier, la force n'est pas toujours orthogonale à la vitesse.
L'expression de la force de Laplace est :
où  est l'intensité du courant,  le champ magnétique et  un élément infinitésimal du barreau.
Les matériaux supraconducteurs ont la propriété intéressante de ne pas pouvoir être pénétrés par un champ magnétique : on parle d'expulsion du champ magnétique. On observe ce phénomène par exemple au travers de l'effet Meissner.
Une des interprétations possibles consiste à fournir une masse aux photons, porteurs du champ magnétique, ce qui diminue la portée de ce champ à l'intérieur du matériau. Il est ainsi possible de faire des analogies avec des processus comme le mécanisme de Higgs, qui explique la masse des porteurs des interactions nucléaires.
On traduit cela par une expression particulière du potentiel vecteur.
Cet effet ne saurait par ailleurs être observé entre deux aimants : la lévitation statique serait alors interdite par le théorème d'Earnshaw.
Dans la théorie BCS, qui traite des supraconducteurs, on peut montrer que le potentiel vecteur est de la forme :
ou  la profondeur de pénétration dans le supraconducteur et  est la longueur de pénétration caractéristique, qui vaut
où  est la masse d'un électron,  la charge élémentaire et  la densité superfluide du supraconducteur, supposée uniforme et constante. Ainsi, le potentiel vecteur — donc le champ magnétique — ne pénètre que sur une épaisseur de quelques  à l'intérieur du matériau.
Si le champ magnétique environnant le matériau supraconducteur est trop intense, celui-ci ne peut expulser le champ dans sa totalité. Certaines régions du matériau supraconducteur vont devenir non supraconductrices et canaliser le champ magnétique. Le supraconducteur a tendance à minimiser la taille de telles régions, qui prennent la forme de tubes alignés le long du champ magnétique. Ces régions sont appelées, pour des raisons évidentes, tubes de flux.
Le phénomène d'induction électromagnétique (ou, simplement, induction) a pour résultat la production d'une différence de potentiel aux bornes d'un conducteur électrique soumis à un champ électromagnétique variable. Cela s'exprime au travers de l'équation locale de Maxwell-Faraday :
 étant le champ électrique,  le champ magnétique.
Ce champ électrique peut à son tour engendrer un champ magnétique, propageant ainsi une onde électromagnétique.
Lorsqu'un matériau est placé dans un champ magnétique variant, il apparaît dans celui-ci un champ électrique (dont la circulation est appelée force électromotrice) qui génère à son tour des courants, appelés courants de Foucault. C'est d'une part le principe des alternateurs, qui produisent de l'électricité en déplaçant des aimants. C'est d'autre part le principe des chauffages et plaques à induction, car la dissipation par effet Joule de ces courants échauffe le métal.
Par ailleurs, deux systèmes magnétiques, comme des bobines, peuvent être couplés au travers du champ magnétique. On parle d'induction mutuelle (ou de mutuelle induction). Cet effet modifie le comportement individuel de chaque circuit.
On peut aborder cet effet par un modèle très simple : un conducteur ohmique de conductivité électrique  est parcouru par un champ magnétique sinusoïdal, d'intensité  et de pulsation . Ce champ est, à tout instant , d'intensité donnée par :
Ce champ induit dans le conducteur, d'après la loi de Faraday, un champ électrique  d'intensité  donnée par
D'après la loi d'Ohm, il se dissipe donc une puissance moyenne volumique, par effet Joule :
Un conducteur, parcouru par un courant électrique selon une direction, soumis à un champ magnétique dirigé dans une seconde direction, présente une différence de potentiel selon la troisième direction. Ce phénomène est connu sous le nom d'effet Hall, en l'honneur du physicien américain Edwin Herbert Hall.
On peut expliquer cet effet au travers de la physique classique, en considérant que les porteurs de charge (par exemple les électrons) qui se déplacent dans le corps du conducteur sont soumis à la force de Lorentz, donc déviés, de sorte que leur répartition est différente d'une part et d'autre du conducteur — d'où la différence de potentiel. On peut l'expliquer de manière plus fondamentale du point de vue de la mécanique quantique.
Cet effet est à la base de nombreux dispositifs de mesure du champ magnétique et du courant électrique.
En présence d'un champ magnétique, certains conducteurs voient leur résistance électrique varier. Cet effet est appelé magnétorésistance, et présente de nombreuses applications, par exemple dans les disques durs qui équipent les ordinateurs modernes.
Il n'existe pas à ce jour d'explication définitive de tous les phénomènes de magnétorésistance, mais des théories distinctes qui régissent les principales manifestations de cet effet : la magnétorésistance classique, « géante », « colossale » et la magnétorésistance à effet tunnel.
Parfois, on peut introduire la notion de moment magnétique, qui permet de travailler avec des dipôles.
En particulier, on utilise ce modèle au niveau microscopique, lorsqu'un ensemble de molécules ou de particules est parcouru par un courant. Pour une boucle ceinturant une surface orientée  et parcourue par un courant , on définit le moment magnétique  par :
Ceci revient à assimiler l'objet à un aimant droit infiniment fin. On peut alors introduire une énergie potentielle dipolaire :
Ainsi, elle est minimale lorsque le dipôle est aligné avec le champ. On montre de même que, dans une chaîne de dipôles, ils s'orientent tous dans une même direction pour minimiser leur énergie. Dans les cas (fréquents) où on ne sait pas modéliser la structure d'un dipôle magnétique par une boucle de courant, le moment magnétique est défini par la relation ci-dessus, c'est-à-dire par l'énergie qu'il faut fournir pour tourner un dipôle magnétique dans un champ magnétique donné.
Dans les matériaux, lorsqu'on considère des moments magnétiques de particules, le fait qu'ils s'orientent tous de la même manière ne peut être expliqué que d'un point de vue quantique (principe d'exclusion de Pauli et hamiltonien de Heisenberg).
Dans le cadre d'un dipôle magnétique de moment  soumis à un champ , lorsque le champ est homogène, le torseur des actions mécaniques se réduit au moment, car la résultante des forces est nulle. On a donc le couple :
où  est le moment résultant,  le moment magnétique du dipôle et  le champ magnétique.
Cela explique notamment l'effet d'un champ magnétique sur une boussole : il a tendance à aligner l'aiguille de celle-ci avec le champ.
Si en revanche le champ est non uniforme, alors le dipôle subit de plus une force, dont l'expression est :
avec les mêmes notations que précédemment.
Cela explique notamment le fait que deux aimants s'attirent : cette force s'exerce sur le premier de sorte à l'approcher des champs plus intenses, donc plus près de l'autre aimant. En supposant cette fois que les pôles sont ponctuels, alors l'intensité de la force F s'exerçant d'un pôle sur l'autre est donnée par[23] :
où  et  représentent l'intensité de ces pôles (en A m si elles sont exprimées dans le Système international d'unités),  la perméabilité magnétique du milieu, et  la distance entre les pôles.
Certaines roches sont riches en matériaux ferromagnétiques, qui sont sensibles au champ magnétique. En particulier, ils perdent leurs propriétés magnétiques au-delà d'une certaine température, dite température de Curie.
Les roches basaltiques issues par exemple des volcans ou des rifts océaniques, sont chauffées au-delà de cette température dans le magma. Lorsqu'elles refroidissent, elles regagnent leurs propriétés magnétiques, et figent l'orientation du champ magnétique terrestre. On observe cet effet au travers des anomalies magnétiques des roches. C'est par l'analyse de ces roches que l'on a observé les inversions du champ terrestre[24],[25].
Il existe également des roches, comme l'hématite, dont les propriétés magnétiques sont telles qu'on observe les variations de champ au cours de leur formation. L'étude de ces roches est également un élément déterminant qui appuie la tectonique des plaques.
Les différentes espèces connues ne sont pas identiquement sensibles aux champs électromagnétiques. Les données concernant les êtres humains sont encore sporadiques[26]. Les champs statiques inférieurs à 8 teslas n'ont vraisemblablement pas d'effets physiologiques notables, si ce n'est l'apparition chez certaines personnes de phosphènes lorsqu'ils sont exposés à des champs de plus de 4 T[27]. L'organisation mondiale de la santé mène encore aujourd'hui des études[28] sur les risques potentiels.
Des champs continus aussi intenses sont relativement difficiles à obtenir en dehors des laboratoires spécialisés, les applications courantes impliquant généralement des champs inférieurs au tesla.
Les recherches actuelles s'orientent davantage sur les champs non ionisants de très basse fréquence (EMF : extremely low frequency), qui ne sont pas statiques, mais semblent agir sur les systèmes biologiques ou parfois provoquer des cancers[29].
Les champs pulsés, que l'on peut créer beaucoup plus intenses, provoquent de plus par induction un rayonnement électromagnétique. Celui-ci peut interagir avec les systèmes biologiques, et son effet dépend de la radiorésistance des espèces exposées. Notamment, selon la fréquence, de tels champs peuvent provoquer des radiations ionisantes : ultraviolets, rayons X ou gamma. Ceux-ci sont dangereux pour la santé, et provoquent en particulier la brûlure des tissus.
Récemment, des médecines alternatives faisant intervenir des champs magnétiques faibles pulsés prétendent limiter les cancers ou la sclérose en plaques. Si de tels champs ne semblent pas dangereux, aucune étude scientifique sérieuse n'appuie à ce jour ces allégations[30],[31]. En revanche, les champs magnétiques pulsés peuvent influencer l'équilibre[32] et semblent diminuer les symptômes du trouble bipolaire[33].
Les effets, principalement liés à l'induction dans les nerfs, permettent ainsi via la stimulation magnétique transcranienne, le diagnostic de pathologies neurologiques.
Depuis une dizaine d'années, les champs magnétiques pulsés sont utilisés par certains centres anti-douleur dans des hôpitaux en France (notamment au CHU de Grenoble, au Centre Hospitalier de Perpignan, de Soissons ou encore[34] de Valence) pour soigner les maladies de Parkinson[35][source secondaire nécessaire] ou d'Alzheimer[36][source secondaire nécessaire].
La présence d'un champ magnétique s'exprime globalement par une énergie, dite « énergie magnétique ». Elle s'exprime par :
avec  la norme du champ magnétique et  la perméabilité magnétique en chacun des points considérés.
En pratique, on définit une énergie volumique, appelée dans ce contexte pression magnétique :
En tant que champ pseudovectoriel, le champ magnétique a un comportement particulier par rapport aux symétries. En effet, contrairement au champ (vectoriel) électrique, les champs magnétiques ne suivent pas la symétrie de leurs sources. On parle ainsi de vecteur « axial » ou de « pseudovecteur ».
Par exemple, pour une spire circulaire parcourue par un courant :
Respectivement,  et  sont un plan d'antisymétrie et de symétrie pour le champ magnétique.
Le calcul du champ magnétique créé par un système demande de résoudre des équations différentielles assez complexes. Il existe pour cela une multitude de méthodes numériques comme la méthode des éléments finis, la méthode des différences finies et la méthode des volumes finis pour ne citer que les méthodes les plus répandues. Toutefois, il est possible de calculer analytiquement le champ magnétique dans certains cas simples. Sauf mention contraire, les expressions données pour le calcul du champ magnétique sont exprimées dans les unités SI. Cela explique notamment le facteur .
À partir des observations révélant un lien entre courants électriques et champ magnétique, André-Marie Ampère énonça une loi d'abord phénoménologique, qui décrivait l'effet observé. Démontrée depuis, dans le cadre plus général de l'électromagnétisme, cette relation est devenue le théorème d'Ampère. Elle n'est valable, en toute rigueur, que dans les cas magnétostatiques.
La formulation originelle de ce théorème est la suivante :
 étant le champ magnétique,  une courbe fermée et orientée et  l'intensité qui traverse une surface délimitée par .
Cette équation peut être écrite localement, on a alors :
où  est la perméabilité magnétique du vide, et  le vecteur densité de courant.
Cette relation étant mise en défaut dans le cas de champs magnétiques ou électriques dépendant du temps, Maxwell introduisit en 1861 les « courants de déplacement », dont la variation corrigeait cette relation : c'est l'équation locale de Maxwell-Ampère[37]. On peut l'écrire localement sous la forme :
 étant le champ électrique et  la permittivité diélectrique du vide.
On peut a posteriori réécrire cette loi sous forme intégrale, également appelée théorème d'Ampère :
avec
où  est la surface délimitée par le contour .
Ceci se comprend aisément grâce au théorème de Stokes-Kelvin (en) : .
La loi de Biot-Savart permet de donner l'expression du champ magnétique dans un milieu de perméabilité magnétique isotrope et homogène.
Le champ  généré en un point de coordonnées  par une charge  en mouvement, située en un point  et se déplaçant à la vitesse , est donné par la relation suivante :
Si on a affaire à une distribution de courants, qui est connue en tout point, alors on peut intégrer la relation locale.
Avec les notations précédentes, cela donne :
L'absence de monopôles magnétique implique que la divergence du champ magnétique est nulle :
Ceci implique, d'après les théorèmes de l'analyse vectorielle, qu'il existe un champ vectoriel , dont le rotationnel est égal à  :
Un tel champ  est appelé potentiel vecteur, par analogie au potentiel électrique, dit « potentiel scalaire », du champ électrique.
Ce potentiel n'est toutefois pas unique : il est défini à un gradient près. En effet, le rotationnel d'un gradient est identiquement nul, aussi le potentiel vecteur  défini par :
vérifie-t-il également la relation :
De façon quelque peu étrange, la quantité fondamentale n'est pas le champ magnétique mais le potentiel vecteur, alors que ce dernier ne peut être défini de façon univoque. Une telle situation est appelée en physique invariance de jauge : des phénomènes identiques, ici le champ , peuvent être générés par plusieurs configurations, appelées pour diverses raisons historiques « jauges » de l'objet fondamental, ici le champ .
D'un point de vue mathématique, l'invariance de jauge est la cause d'une loi fondamentale de l'électromagnétisme, la conservation de la charge électrique. Cette loi, expérimentalement vérifiée à une très grande précision implique en effet que l'objet fondamental apparaissant en électromagnétisme n'est ni le champ magnétique ni le champ électrique, mais le potentiel vecteur et le potentiel électrique.
Connaissant , on peut facilement en déduire . Le fait que le potentiel vecteur soit plus fondamental que le champ magnétique transparaît en mécanique quantique, où en présence de champ magnétique, c'est en fait le potentiel vecteur qui apparaît dans l'équation de Schrödinger, qui décrit l'évolution des particules élémentaires. L'illustration la plus manifeste de la prééminence du potentiel vecteur se trouve dans l'effet Aharonov-Bohm, où l'on est amené à considérer des configurations dans lesquelles le champ  s'annule dans certaines régions alors que le potentiel vecteur  n'est pas nul (mais de rotationnel nul) et influence explicitement le comportement des particules.
Il est d'ailleurs possible de calculer le potentiel vecteur  directement à partir de la donnée des courants :
l'expression ci-dessus n'étant valable que lorsque les courants — donc les champs — ne dépendent pas du temps. En pratique, ces variations peuvent souvent être négligées tant que l'on n'étudie pas les ondes et leur propagation.
Dans ces derniers cas, il faut remplacer l'expression ci-dessus par une expression plus complexe, faisant appel au concept de potentiels retardés pour tenir compte du temps de propagation du champ magnétique.
On peut montrer qu'un champ magnétique affecte le déplacement de particules chargées, en infléchissant leur trajectoire. Il est ainsi utilisé pour courber leur trajectoire dans les accélérateurs de particules et, par exemple, exploiter le Rayonnement synchrotron résultant de cette déviation.
En effet, d'après la loi de Lorentz, la force  qu'exerce un champ magnétique  sur une particule de charge  se déplaçant à la vitesse  est :
Ainsi, cette force est toujours orthogonale à la vitesse, donc son travail  exercé lors d'un petit déplacement  est nul :
Par conséquent, la norme de la vitesse n'est pas directement influencée par le champ magnétique. En revanche, cette force modifie la direction de celle-ci dès que vitesse et champ magnétique ne sont pas colinéaires. Cette accélération latérale va faire perdre de la vitesse (on parlera plutôt, dans le cas des accélérateurs de particules, d'Énergie étant donné le caractère relativiste du problème) à la particule chargée, en raison du Rayonnement synchrotron.
Le champ magnétique dévie les particules chargées. Si, de plus, le milieu présente une certaine viscosité, alors ces particules décrivent des spirales, desquelles on peut déduire la charge électrique (le sens de l'enroulement) et la masse (au travers de la décélération) des particules.
C'est le principe des chambres à bulles, inventées au début du XXe siècle pour observer, en particulier, les constituants de la matière (protons, neutrons et électrons), les positrons et les neutrinos. On préfère cependant aujourd'hui, depuis leur invention dans les années 1970, utiliser les chambres à fils.
En pratique, il existe toujours un champ électrique, qui dévie les particules.
Une particule dans une chambre à bulles est idéalement soumise uniquement à la force magnétique et aux forces de frottement. Elle vérifie donc :
où  est le coefficient intervenant dans la force de frottement, colinéaire mais opposée à la vitesse. Cette équation peut se réécrire de façon équivalente :
La résonance magnétique est un phénomène qui apparaît lorsque certains atomes sont placés dans un champ magnétique et reçoivent un rayonnement radio adapté.
En effet, les atomes dont le noyau est composé d'un nombre impair de constituants — en particulier l'hydrogène, dont le noyau se résume à un proton — présentent une sorte de moment magnétique, appelé moment magnétique de spin. Lorsqu'un noyau est placé dans un champ magnétique statique — mécanique quantique oblige — il ne peut être observé que dans deux états distincts. On peut toutefois faire basculer un noyau d'un état à l'autre en appliquant brièvement un champ magnétique oscillant de pulsation adaptée : on parle de résonance[38]. Ce phénomène affectant le noyau d'un atome, on parle de résonance magnétique nucléaire.
Un noyau affecté retourne à l'équilibre par échange thermique avec son environnement. En parallèle, la valeur moyenne du moment magnétique est animée d'un mouvement de précession mesurable par induction. Le signal mesuré, en plus d'indiquer la présence du noyau, peut également informer sur son voisinage au sein d'une molécule. En effet, il se produit des couplages, qui influencent notamment sa fréquence. En RMN, on appelle ces écarts à un solvant de référence les « déplacements ».
L'imagerie par résonance magnétique nucléaire (IRM) est l'application de cet effet en imagerie médicale, permettant d'avoir une vue 2D ou 3D d'une partie du corps, notamment du cerveau.
Un transformateur électrique est un convertisseur, qui permet de modifier les valeurs de la tension et de l'intensité du courant délivrées par une source d'énergie électrique alternative en un système de tension et de courant de valeurs différentes, mais de même fréquence et de même forme. Il effectue cette transformation avec un excellent rendement. Il est analogue à un engrenage en mécanique (le couple sur chacune des roues dentées étant l'analogue de la tension et la vitesse de rotation étant l'analogue du courant).
Un transformateur est constitué de deux parties : le circuit magnétique et les enroulements. Les enroulements créent ou sont traversés par un flux magnétique que le circuit magnétique permet de canaliser afin de limiter les pertes. Dans le cas d'un transformateur monophasé parfait pour lequel toutes les pertes et les fuites de flux sont négligées, le rapport du nombre de spires primaires et secondaires détermine totalement le rapport de transformation du transformateur. Ainsi, si on note respectivement  et  le nombre de spires au primaire et au secondaire, on obtient :
Avec  la tension primaire et  la tension secondaire.
Une machine électrique est un dispositif permettant la conversion d'énergie électrique en travail ou énergie mécanique : les moteurs rotatifs produisent un couple par un déplacement angulaire tandis que les moteurs linéaires produisent d'une force par un déplacement linéaire.
Les forces engendrées par les champs magnétiques, formulées par la relation de Lorentz, permettent d'envisager des dispositifs qui utilisent un tel champ pour transformer l'énergie électromagnétique en énergie mécanique.
Le premier moteur électrique fut construit par Peter Barlow : une roue, soumise à un champ magnétique permanent, est parcourue par un courant électrique. Il s'exerce donc une force sur cette roue, qui se met alors en rotation : c'est la roue de Barlow. Elle constitue de fait le premier moteur électrique à courant continu.
Les liens entre champ magnétique et champ électrique, exprimés par les équations de Maxwell, font qu'il est possible de construire des systèmes qui créent un champ magnétique non permanent — à partir d'une source de courant, au moyen d'électroaimants.
Au sein de tels appareils, on crée un champ magnétique tournant[h], c'est-à-dire un champ dont la direction varie en tournant dans un sens ou dans l'autre avec une fréquence de rotation déterminée.
L'une des possibilités est de créer un tel champ à l'aide d'électroaimants fixes — ils constituent le « stator » — parcourus par un courant électrique d'intensité variable, par exemple triphasé. Au centre, une partie mobile et sensible au champ magnétique, constituée par exemple d'aimants permanents, est ainsi mise en mouvement : c'est le « rotor », dont le mouvement de rotation est transmis à un arbre. Ce principe est par exemple mis en œuvre pour les machines synchrones et les machines asynchrones.
Une autre possibilité est de créer un champ permanent au stator à l'aide d'aimants permanents ou d'enroulements parcourus par un courant continu et de réaliser un champ magnétique tournant au rotor par un système de connexions glissantes afin que ce champ rotorique reste en quadrature avec le champ statorique. C'est le principe mis en œuvre pour la machine à courant continu.
La recherche se poursuit depuis plus d'un siècle, avec la possibilité d'étudier des champs de plus en plus intenses.
Un laboratoire européen des champs magnétiques intenses est en cours de création[39] associant notamment la France (Laboratoire national des champs magnétiques intenses ou LNCMI), les Pays-Bas (High Field Magnet Laboratory ou HFML) et l'Allemagne (Dresden High Magnetic Field Laboratory ou DHMFL)[39]. Ce pôle européen dit European Magnetic Field Laboratory (EMFL), est hébergé à Grenoble par le LNCMI (CNRS, Université Joseph Fourier, INSA-Toulouse et Université Paul Sabatier), où l'on peut déjà travailler avec les champs les plus puissants d’Europe (jusqu'à 750 000 fois le champ magnétique terrestre)[39].
modifier La vitesse de la lumière dans le vide, habituellement notée c, est une constante physique de l'Univers qui est fondamentale dans plusieurs domaines de la physique.
L'étude de la lumière et de sa vitesse remonte à l'Antiquité. Des philosophes et des scientifiques, en s'appuyant sur des arguments théoriques ou des observations, affirment que sa vitesse est infinie, alors que d'autres prétendent que non. Ole Rømer démontre en 1676 qu'elle est finie. Les scientifiques s'attachent ensuite à déterminer sa valeur par divers moyens, la précision s'améliorant au fil des années. Dès la fin du XIXe siècle il est acquis qu'elle vaut environ 300 000 km/s ; en 1975, le résultat fiable le plus précis est 299 792 458 ± 1 m/s. En 1983, un accord international redéfinit le mètre de telle sorte que la vitesse de la lumière est d'exactement 299 792 458 m/s.
Dans la vie de tous les jours, la lumière (et donc les ondes électromagnétiques) semble se déplacer instantanément, mais sur de longues distances ou dans des instruments de mesure très précis, des effets permettent de déduire que sa vitesse est finie.
Dans les matériaux transparents et les conducteurs électriques, les ondes électromagnétiques se déplacent plus lentement que c.
Les vitesses de certains phénomènes ondulatoires et de certains objets célestes peuvent être plus grandes que c.
La vitesse d'expansion de l'Univers excède c hors de certaines limites géométriques.
Bien que cette vitesse soit le plus souvent associée à la lumière, c'est aussi celle de toute particule sans masse et de toute perturbation dans un champ situé dans le vide, incluant les ondes gravitationnelles et les ondes électromagnétiques (dont la lumière visible ne constitue qu'une minuscule partie).
Les particules dotées d'une masse au repos peuvent approcher de c, mais ne peuvent l'atteindre, peu importe le référentiel inertiel dans lequel leur vitesse est mesurée.
Au XXIe siècle, la vitesse de la lumière dans le vide est dénotée par la lettre minuscule c, initiale du mot latin celeritas (signifiant « rapidité, célérité ») ou encore de « célérité » en français, mais son symbole a varié dans le temps. En 1856, Wilhelm Eduard Weber et Rudolf Kohlrausch utilisent c pour une constante différente qui a été plus tard démontrée égale à √2 × c. En 1865, le symbole V est introduit par James Clerk Maxwell comme alternative pour indiquer la vitesse de la lumière dans le vide. En 1894, Paul Drude préfère c, tout en lui donnant sa définition moderne. Pourtant, Albert Einstein utilise V dans ses articles sur la relativité restreinte de 1905 ; c'est en 1907 qu'il commence à utiliser c, devenu entretemps le symbole courant pour la vitesse de la lumière dans le vide[1],[2],[3].
Parfois, c est utilisée pour indiquer la vitesse d'une onde lumineuse dans n'importe quel médium physique et c0 pour la vitesse de la lumière dans le vide[4].
Cette notation indicée, présente dans la littérature du SI[5], a la même forme que plusieurs constantes de l'électromagnétisme : μ0 pour la perméabilité du vide (ou constante magnétique), ε0 pour la permittivité du vide (ou constante électrique) et Z0 pour l'impédance caractéristique du vide. Dans la suite de cet article, seul c est utilisé pour désigner la vitesse de la lumière dans le vide.
Avant l'époque moderne (grossièrement, de 1500 à 1800), des scientifiques et des philosophes proposent soit que la lumière se déplace instantanément soit à une vitesse finie très grande. Le premier enregistrement connu d'un effort dans ce sens remonte à la Grèce antique. Les Grecs anciens, les érudits musulmans puis les scientifiques européens de l'époque moderne ont longuement débattu sur ce sujet, jusqu'à ce que Ole Rømer fournisse la première preuve que la vitesse de la lumière est finie. La relativité restreinte d'Einstein, proposée en 1905 et vérifiée expérimentalement par la suite, permet de conclure que c est constante, peu importe le référentiel où elle est mesurée. Au XXe siècle, des scientifiques ont continué à affiner la valeur de c.
Empédocle (c. 490-430 av. J.-C.) est le premier à proposer une théorie de la lumière[21] et déclare que la lumière a une vitesse finie[22].
Il affirme que la lumière est quelque chose en mouvement, et doit donc prendre du temps pour voyager.
Aristote, au contraire, argue que « la lumière est due à la présence de quelque chose, mais ce n'est pas un mouvement »[trad 1],[23].
Euclide et Ptolémée reprennent la théorie de l'émission d'Empédocle, où la lumière provient de l'œil, ce qui permet de voir. En se basant sur cette théorie, Héron d'Alexandrie affirme que la vitesse de la lumière doit être infinie puisque l'on voit des objets distants, telles les étoiles, dès que l'on ouvre les yeux[24].
Les premiers philosophes islamiques acceptent dans un premier temps la vision aristotéliciennephysique d'Aristote selon laquelle la lumière ne voyage pas. En 1021, Alhazen publie son Traité d'optique, où il présente un ensemble d'arguments contre la théorie de l'émission et en faveur de la théorie de l'intromission, c'est-à-dire que la lumière d'un objet entre dans l’œil[25].
Sa réflexion l'amène à proposer que la lumière doit voyager à une vitesse finie[23],[26],[27] et que cette vitesse peut changer selon le corps dans lequel elle se propage, étant plus lente dans les corps plus denses[27],[28].
Il soutient que la lumière est une matière faite d'une substance solide, sa propagation exige donc du temps, même si nos sens ne peuvent le percevoir[29].
Également au XIe siècle, Al-Biruni soutient que la vitesse de la lumière est finie et mentionne qu'elle est nettement plus grande que celle du son[30].
Au XIIIe siècle, Roger Bacon argue que la vitesse de la lumière dans l'air n'est pas infinie, recourant à des arguments philosophiques qui s'appuient sur les travaux d'Alhazen et d'Aristote[31],[32].
Dans les années 1270, Vitellion étudie la possibilité que la lumière voyage à une vitesse infinie dans le vide, mais ralentit dans les corps plus denses[33].
Au début du XVIIe siècle, Johannes Kepler pense que la vitesse de la lumière est infinie puisque l'espace vide ne présente aucun obstacle à sa propagation. René Descartes argue que si la vitesse de la lumière est finie, le Soleil, la Terre et la Lune ne seraient pas parfaitement alignés lors d'une éclipse lunaire. Puisqu'un tel manque d'alignement n'a pas été observé, Descartes conclut que la vitesse de la lumière est infinie. Il spécule que si la vitesse de la lumière était finie, tout son système philosophique pourrait être réfuté[23].
Lorsqu'il dérive les lois de Snell-Descartes, il accepte la contradiction que la lumière se déplace instantanément, alors que son système philosophique affirme que plus dense est le médium, plus rapide est la lumière[34].
Pierre de Fermat, qui soutient que la vitesse de la lumière est finie, dérive les mêmes lois en utilisant l'argument opposé que la lumière voyage moins vite dans les médiums plus denses[35].
En 1629, Isaac Beeckman propose une expérience où une personne observe l'éclair d'un coup de canon réfléchi sur un miroir à une distance d'environ 1 mile (1,6 km)[7].
En 1638, Galilée propose une autre expérience, qu'il aurait réalisée quelques années plus tôt, pour mesurer c en observant le délai entre l'exposition du hublot d'une lanterne allumée et de la détection de la lumière projetée à quelque distance de là. Il aurait été incapable de déterminer si c est infinie ou pas. Il conclut que si elle n'est pas infinie, elle doit être très grande[6],[7].
En 1667, l’Accademia del Cimento de Florence rapporte avoir effectué l'expérience de Galilée, avec des lanternes distantes d'environ 1 mile. Aucun délai n'est observé[9],[10],[note 2].
Ole Rømer est, en 1676, le premier à tenter de mesurer c.
Connaissant la période orbitale de la lune Io de Jupiter, il détermine qu'elle raccourcit lorsque la Terre approche de Jupiter et qu'elle allonge lorsque la Terre s'éloigne de Jupiter. Il conclut que la lumière voyage à une vitesse finie ; il estime qu'elle prend 22 minutes à franchir le diamètre de l'orbite terrestre[36],[11].
Pour sa part, Christian Huygens combine cette durée avec une estimation du diamètre de l'orbite terrestre et calcule que c égale 220 000 km/s[12],[note 3].
Dans son livre Opticks de 1704, Isaac Newton rapporte les calculs de Rømer et affirme que la lumière franchit la distance séparant le Soleil de la Terre en « sept ou huit minutes »[trad 2],[37],[note 4].
En 1729, James Bradley découvre l'aberration stellaire[38].
En s'appuyant sur cet effet, il détermine que c égale 10 210 fois la vitesse orbitale de la Terre[note 5] ou, de façon équivalente, qu'il faut à la lumière 8 min 12 s pour franchir la distance Soleil-Terre[38].
Au XIXe siècle, Hippolyte Fizeau développe une méthode pour déterminer c en effectuant des mesures terrestres du temps de vol de la lumière ; il rapporte la valeur de 315 000 km/s[39].
En 1856, Wilhelm Eduard Weber et Rudolf Kohlrausch, grâce aux décharges électriques d'une bouteille de Leyde, mesurent une unité de charge électromagnétique et une unité de charge électrostatique ; ils calculent le rapport des deux unités et obtiennent une vitesse proche de la valeur obtenue par Fizeau. L'année suivante, Gustav Kirchhoff calcule qu'un signal électrique voyage dans un fil sans résistance à cette même vitesse[40].
Au début des années 1860, James Clerk Maxwell démontre, dans le cadre de sa théorie de l'électromagnétisme, que les ondes électromagnétiques se propagent dans le vide[41],[42],[43] à une vitesse égale à celle calculée par Weber et Kohlrausch, tout en attirant l'attention sur la proximité numérique avec la vitesse de la lumière mesurée par Fizeau[44]. Pour lui, la lumière est une onde électromagnétique[45].
Améliorant la méthode de Fizeau, Léon Foucault obtient 298 000 km/s en 1862[13].
Aux XIXe siècle, les scientifiques pensent qu'un médium est nécessaire pour qu'un phénomène ondulatoire puisse se produire, peu importe que ce soit une vague ou une onde sonore par exemple. Puisque la lumière se propage dans le vide, il doit être rempli d'un médium qui sert à la propagation des ondes lumineuses. La Terre, qui se déplace dans ce médium immobile appelé « éther luminifère », est soumise à l'équivalent d'un vent[note 6],[46].
Reprenant cette hypothèse, des scientifiques du XIXe siècle pensent qu'il est possible de mesurer la vitesse de la Terre en détectant un changement dans la vitesse de la lumière. En effet, si la Terre s'éloigne ou se rapproche du Soleil par exemple, la vitesse de la lumière issue du Soleil change selon la loi de composition des vitesses[46].
Au début des années 1880, plusieurs expériences sont menées pour calculer la vitesse de la Terre[47]. La plus connue est l'expérience de Michelson–Morley de 1887[48].
Pendant cette expérience, la vitesse détectée est toujours plus petite que l'erreur d'observation[49],[50]. 
Des expériences menées au XXe siècle démontrent que l'erreur est inférieure à 6 nanomètres par seconde ; il faut donc conclure que la lumière se déplace à la même vitesse, peu importe la direction de propagation (elle est donc isotrope)[51].
À la suite de cette expérience, George FitzGerald et Hendrik Lorentz proposent de façon indépendante que les appareils utilisés se contractent dans le sens du mouvement, ce qui annulerait l'effet du vent d'éther. Lorentz indique de plus que le temps d'un système en mouvement, qu'il appelle « temps local », doit aussi être modifié par le même facteur, ce qui mène à la formulation des transformations de Lorentz. En se basant sur la théorie de l'éther de Lorentz, Henri Poincaré démontre en 1900 que ce temps local (une approximation d'ordre 2 du rapport v/c) est celui indiqué par les horloges qui se déplacent dans l'éther, qui sont synchronisées en faisant l'hypothèse que c est constante. En 1904, il spécule que c pourrait être l'ultime vitesse en dynamique, à la condition que toutes les hypothèses de la théorie de Lorentz soient validées. En 1905, il démontre, en se basant sur différentes expériences, que la théorie de l'éther de Lorentz explique complètement le principe de relativité[52],[53].
En 1905, le physicien Albert Einstein postule que la vitesse de la lumière dans le vide, telle que mesurée par des observateurs non accélérés, est indépendante du mouvement de la source et du mouvement des observateurs. En se basant à la fois sur cette invariance et le principe de relativité, il jette les bases de la relativité restreinte, où c est élevée au rang de constante fondamentale de l'Univers, constante qui apparaît dans des contextes où la lumière ne joue aucun rôle direct. Sa théorie rend caduque la notion d'éther luminifère (hypothèse que soutiennent encore Lorentz et Poincaré) et met en avant ce qui sera appelé l'« espace-temps », une façon de représenter l'espace et le temps comme deux notions inséparables[54],[55],[56].
Dans la seconde moitié du XXe siècle, des progrès techniques permettent de préciser encore plus la valeur de la vitesse de la lumière. En 1950, Louis Essen détermine que c égale 299 792,5 ± 3,0 km/s en utilisant une cavité résonnante[17].
Cette valeur est adoptée par la 12e assemblée de l'Union radio-scientifique internationale en 1957. En 1960, le mètre est redéfini en fonction de la longueur d'onde d'une ligne spectrale particulière du krypton 86. En 1967, c'est au tour de la seconde d'être redéfinie selon la fréquence de transition hyperfine de l'état fondamental du césium 133[57].
En 1972, en utilisant des techniques d'interférométrie par laser et les nouvelles définitions, un groupe du National Bureau of Standards détermine que la vitesse de la lumière dans le vide est de 299 792 456,2 ± 1,1 m/s. Cette mesure est 100 fois plus précise que la plus précise des mesures précédentes. L'incertitude est surtout attribuable à la définition du mètre[note 7],[19].
D'autres expériences ayant déterminé la même valeur de c, la 15e Conférence générale des poids et mesures (CPGM), tenue en 1975, recommande d'utiliser la valeur de 299 792 458 m/s pour la vitesse de la lumière[60].
En 1983, le 17e congrès de la CGPM conclut que, comparativement aux méthodes reconnues par les standards en vigueur, il est plus facile de reproduire certaines longueurs d'onde à partir de mesures de fréquences et d'une valeur connue de c. Le congrès retient la définition de 1967 pour la seconde, ce qui fait de la fréquence hyperfine du césium la base servant à définir la seconde et le mètre. Le congrès déclare que « le mètre est la longueur du trajet parcouru par la lumière dans le vide pendant l'intervalle temporel de 1/299 792 458 seconde »[trad 4],[20].
En conséquence, la valeur de c est définie exactement égale à 299 792 458 m/s[61],[62],[63] et devient ainsi une constante définie dans le Système international d'unités (SI)[64].
La décision du congrès impose que la valeur du mètre dépende dorénavant de mesures plus précises de fréquences ou de longueurs d'onde de la lumière, par exemple en mesurant plus précisément la longueur d'onde de la transition hyperfine du krypton 86 ou de toute autre source d'ondes électromagnétiques[65],[66].
En 2011, le CGPM déclare son intention de redéfinir les sept unités de base du SI en utilisant ce qu'il appelle « la formulation des constantes explicites »[trad 5], où chaque « unité est définie indirectement en spécifiant explicitement une valeur exacte pour une constante fondamentale bien connue »[trad 6], comme il a été fait pour c[67].
Une nouvelle définition du mètre, complètement équivalente, est proposée : [note 8].
Cette définition est inscrite dans le « SI révisé »[69].
La vitesse à laquelle se déplace la lumière dans le vide est à la fois indépendante de la vitesse de la source et du référentiel inertiel de l'observateur[70],[71],[note 9].
Cette invariance a été postulée par Albert Einstein en 1905[72], 
après une étude de la théorie de l'électromagnétisme de James Clerk Maxwell  et du manque de preuve de l'existence de l'éther luminifère[73].
Son hypothèse a été confirmée à maintes reprises par la suite[74],[note 10].
Il est seulement possible de vérifier expérimentalement que la vitesse d'un rayon de lumière effectuant un aller-retour (par exemple, d'une source à un miroir, et vice-versa) ne dépend pas du référentiel inertiel, parce qu'il est impossible de mesurer la vitesse de la lumière dans un seul sens (d'une source à un très lointain détecteur par exemple) sans avoir établi au préalable une convention pour synchroniser les horloges à la source et au détecteur. Toutefois, en adoptant la synchronisation d'Einstein, c dans un sens et c dans un aller-retour sont identiques par définition[75],[76].
La relativité restreinte, fondée par Albert Einstein en 1905, explore plusieurs conséquences de l'invariance de c.
Par exemple, c est la vitesse à laquelle toutes les particules sans masse et toutes les ondes électromagnétiques se propagent dans le vide[71],[70].
Cette théorie prédit des phénomènes contre-intuitifs, qui ont été vérifiés expérimentalement[77].
Parmi ceux-ci, il y a l'équivalence masse-énergie (exprimée par )[78], la contraction des longueurs[79] (les objets en mouvement sont plus courts dans le sens du mouvement)[note 11] et la dilatation du temps[82] (les horloges en mouvement avancent plus lentement). Le facteur de Lorentz, noté γ, permet de calculer la contraction de la longueur et la dilatation du temps d'un objet en mouvement[83] ; il est donné par la formule γ = , où v est la vitesse de l'objet et c, la vitesse de la lumière[84]. 
La valeur de γ est très proche de 1 aux vitesses beaucoup plus faibles que c, ce qui est le cas pour la plupart des vitesses observées dans la vie courante[84] — dans ces cas, les valeurs calculées par la relativité restreinte sont très proches de celles calculées par la relativité galiléenne. Il augmente sensiblement aux vitesses dites relativistes (donc, proches de c) et tend vers l'infini positif lorsque v est très proche de c[85]. Par exemple, le taux de contraction γ d'un objet en mouvement égale 2 lorsque sa vitesse relative atteint 86,6 % de c. Par ailleurs, un taux de dilation du temps γ = 10 apparaît lorsque v = 99,5 % c[note 12].
Les résultats de la relativité restreinte peuvent être résumés en regroupant l'espace et le temps dans une seule structure appelée « espace-temps », tout en exigeant que soit satisfaite l'invariance de Lorentz, dont la formulation mathématique comprend c[86] (elle permet de relier l'espace au temps puisque c comprend à la fois les unités de mesure de l'espace et du temps).
L'invariance de Lorentz, une symétrie, est une hypothèse de base régulièrement mentionnée dans les théories physiques fondamentales modernes, telles l'électrodynamique quantique, la chromodynamique quantique, le modèle standard de la physique des particules et la relativité générale. En conséquence, c apparaît en beaucoup d'endroits en physique. Par exemple, la relativité générale prédit que c est aussi la vitesse de la gravité et des ondes gravitationnelles[87],[note 13].
Dans les référentiels non inertiels (des espaces-temps courbés par la gravité ou des référentiels accélérés), la vitesse de la lumière locale est constante et égale c. Elle peut être différente sur une trajectoire de longueur finie selon la façon dont sont définis les distances et les temps[89].
La plupart des scientifiques pense que les constantes fondamentales, telle que c, sont identiques peu importe l'espace-temps choisi. Elles seraient donc indépendantes du lieu et du temps où elles seraient calculées ou observées. Néanmoins, des scientifiques ont produit des théories où c serait différente selon l'époque cosmologique[90],[91].
Aucune preuve concluante qui permettrait de valider ces théories n'a été trouvée jusqu'en 2013, et la recherche se poursuit[92],[93].
Également, c est régulièrement jugée isotrope, c'est-à-dire qu'elle a la même valeur peu importe sa direction de propagation. Les observations d'ondes émises (1) par des noyaux atomiques plongés dans un champ magnétique variable[94] et (2) par des résonateurs optiques en rotation, imposent des limites strictes et très faibles sur l'imprécision d'une anisotropie en fonction de l'angle d'observation[95],[96].
Selon la relativité restreinte, l'énergie d'un objet ayant une masse au repos m et une vitesse v est donnée par γmc2, où γ est le facteur de Lorentz (qui comprend le terme v). Quand v est nulle, γ égale un, ce qui mène à la « fameuse équation »[97]  (équivalence masse-énergie). γ tend vers l'infini positif lorsque v approche de c et il faudrait une énergie encore plus grande (jusqu'à une valeur infinie) pour accélérer encore plus un objet pesant pour lui faire atteindre c. La vitesse de la lumière dans le vide est donc l'ultime limite de vitesse pour les objets en mouvement dotés d'une masse au repos positive. Les photons individuels ne peuvent voyager plus vite que cette vitesse[98],[99],[100].
Ces hypothèses ont été confirmées expérimentalement[101].
Plus généralement, il est impossible aux signaux ou à l'énergie de voyager plus vite que c. Un argument en faveur de cette position provient de la relativité de la simultanéité, l'une des conséquences de la relativité restreinte. Si la distance spatiale des évènements A et B est plus grande que l'intervalle de temps entre les deux multiplié par c, alors il existe des référentiels où A précède B, d'autres où B précède A et d'autres où les deux sont simultanés.
En conséquence, si quelque chose voyageait plus vite que c relativement à un référentiel inertiel, il reculerait dans le temps relativement à un autre référentiel et la causalité serait violée[note 14],[103].
Dit autrement, un effet serait observé avant sa cause. Ce phénomène, qui n'a jamais été observé[76], mènerait par exemple à l'existence d'un antitéléphone tachyonique, c'est-à-dire un hypothétique appareil qui pourrait être utilisé pour envoyer un signal dans son passé[104].
Albert Einstein en 1907[105],[106] présente une expérience de pensée où des signaux supraluminiques pourraient provoquer un paradoxe de causalité.
En 1910, Arnold Sommerfeld et Einstein le décrivent comme un moyen de « télégraphier dans le passé »[trad 7],[107].
La même expérience de pensée a été décrite par Richard Tolman en 1917[108] ; des scientifiques peuvent faire allusion à cet appareil en mentionnant le « paradoxe de Tolman ».
Plus tard, il a été nommé « antitéléphone tachyonique »[trad 8] par Gregory Benford et al[109].
Dans les domaines de la physique où c apparaît régulièrement, comme la relativité restreinte et la relativité générale, il est courant d'utiliser des systèmes d'unités naturelles de mesures ou des systèmes d'unités géométriques dans lesquelles c = 1[110],[111],[note 15].
Certaines observations laissent penser, à tort, que la matière, l'énergie ou des signaux transportant des informations se déplacent à une vitesse supérieure à c. Par exemple, tel que discuté dans la section Dans un médium  ci-dessous, les vitesses de plusieurs caractéristiques d'ondes peuvent excéder c. Par exemple, les vitesses de phase des rayons X, lorsqu'ils traversent la plupart des verres, dépassent régulièrement c[112], mais aucune vitesse de phase n'influe sur la vitesse à laquelle les ondes transportent des informations[113].
Si un faisceau laser balaie rapidement un objet distant, la vitesse de la tache lumineuse peut se déplacer plus rapidement que c. Il y a un délai entre le moment où le faisceau initial quitte le laser et le moment où la réflexion du faisceau parvient à un observateur. Les seuls objets qui se déplacent sont le laser et le faisceau, la vitesse de ce dernier atteignant au plus c avant qu'il n'atteigne le site de réflexion. De la même façon, une ombre sur un objet lointain peut se déplacer plus rapidement que c, mais l'absence de lumière se déplace à c[114].
Dans ces deux cas, ni la matière, ni l'énergie et ni l'information ne voyagent plus rapidement que la lumière[115].
La vitesse de changement de la distance entre deux objets observée dans un référentiel distinct de ceux des deux objets peut dépasser c. Encore une fois, cela ne représente pas la vitesse d'un objet dans le même référentiel inertiel[115].
Quelques effets quantiques semblent être transmis instantanément et seraient dont plus rapides que c, comme par exemple dans le paradoxe EPR. Un exemple met en jeu les états quantiques de deux particules intriquées. Tant qu'elles ne sont pas observées, elles sont superposées dans deux états quantiques. Si les deux sont séparées et que l'état de l'une est observé, alors l'état de l'autre est déterminé instantanément. Néanmoins, il est impossible de vérifier dans quel état quantique se trouve la première particule sans l'observer au préalable ; donc, aucune information ne peut être transmise de cette façon[115],[116].
L'effet Hartman prédit l'existence de vitesses supérieures à c : sous certaines conditions, le temps nécessaire à une particule virtuelle de franchir une barrière grâce à un tunnel quantique est constant, peu importe l'épaisseur de la barrière[117],[118].
Si la barrière est suffisamment mince, la particule virtuelle franchit la barrière à une vitesse supérieure à c. Encore une fois, aucune information ne peut être transmise ainsi[119].
Des vitesses supraluminiques sont observées dans certains phénomènes astronomiques[120], tels que les jets relativistes de radiogalaxies et de quasars. Ces jets ne se déplacent pas à une vitesse supérieure à celle de la lumière : c'est la conséquence de la projection géométrique du mouvement apparent de ces objets qui voyagent à une vitesse proche de c et qui sont observés sous un petit angle de la ligne de mire. Ces objets s'éloignent de la Terre à une vitesse relativiste ; le temps d'observation de chaque rayon successif est plus grand que le précédent[121].
Selon les modèles inflationnistes de l'Univers, le plus loin se trouve une galaxie, le plus rapidement elle s'éloigne[122]. Cette récession n'est pas la conséquence du mouvement dans l'espace, mais plutôt de l'expansion de l'Univers[123].
Par exemple, les galaxies distantes de la Terre semblent s'en éloigner à des vitesses proportionnelles à leur distance[122]. Au-delà du volume de Hubble, la vitesse d'éloignement est plus grande que c[124].
En physique classique, la lumière est considérée comme une onde électromagnétique. Dans ce cadre, le comportement du champ électromagnétique est décrit par les équations de Maxwell qui prédisent que c, vitesse à laquelle les ondes électromagnétiques (dont la lumière visible) se propagent dans le vide, est fonction de la capacité du vide et de l'inductance du vide. Ces deux caractéristiques, appelées respectivement la permittivité du vide () et la perméabilité du vide (), sont reliées à la vitesse de la lumière dans le vide (c) par l'équation[125] :
En mécanique quantique, approche plus moderne, le champ électromagnétique est décrit par l'électrodynamique quantique (QED). La lumière y est décrite comme une excitation fondamentale (ou quanta) du champ électromagnétique ; elle est alors composée de photons, qui sont également des particules sans masse[126],[127].
Des extensions de QED où le photon est doté d'une masse ont été étudiées. Dans ces cadres théoriques, la vitesse du photon dépendrait de sa fréquence et l'invariant c de la relativité restreinte serait alors la limite ultime de la vitesse de la lumière dans le vide[89].
Toutefois, aucune variation de la vitesse de la lumière en fonction de la fréquence n'a été observée dans des conditions de laboratoire rigoureuses[128],[129],[130], lesquelles ont imposé des limites strictes sur la masse du photon.
La limite calculée dépend du modèle utilisé : si le photon massif est décrit selon l'approche de Proca par exemple[131],
alors la limite supérieure expérimentale pour sa masse est de 10−57 gramme[132].
Une autre raison qui militerait en faveur de la vitesse de la lumière en fonction de sa fréquence serait l'impossibilité d'appliquer la relativité restreinte à de très petites échelles arbitraires, tel que prédit par quelques théories s'appuyant sur la gravité quantique. En 2009, l'observation de sursauts gamma provenant du système stellaire GRB 090510 n'a pas démontré que la vitesse du photon dépend de son énergie, ce qui impose des limites strictes aux modèles de quantification de l'espace-temps qui s'appuient sur l'idée que cette vitesse est influencée par l'énergie du photon lorsque les énergies sont proches de l'échelle de Planck[133].
Au moins huit vitesses différentes peuvent être utilisées pour caractériser la propagation de la lumière, à savoir : (1) la vitesse de phase, (2) la vitesse de groupe, (3) la vitesse d'énergie, (4) la vitesse de signal, (5) la constante de vitesse relativiste, (6) la vitesse de rapport d'unités, (7) la centrovitesse et (8) la vitesse de corrélation[134],[135]. Dans le vide, toutes ces vitesses sont égales à c, alors que dans un autre milieu, seule la vitesse du front d'onde conserve cette valeur.
Par ailleurs, pour des fréquences différentes, les vitesses sont différentes. Dans une onde plane, chaque crête et chaque creux se propage à vp, la vitesse de phase. Un signal physique qui a une portée finie (une impulsion de lumière) voyage à une vitesse différente. La plus grande partie d'une impulsion voyage à vg, la vitesse de groupevitesse de groupe, alors que l'autre partie voyage à vf, la vitesse de front.
La vitesse de phase est importante pour déterminer comment une onde lumineuse se propage dans un matériau ou d'un matériau à un autre. Régulièrement, cette information est décrite par l'indice de réfraction qui est défini par le rapport de c à la vitesse de phase vp du matériel (plus grand est l'indice, plus faible est la vitesse de l'onde). L'indice de réfraction dépend de plusieurs facteurs, dont la fréquence de la lumière, son intensité, sa polarisation et sa direction de propagation. Néanmoins, dans plusieurs cas, il est traité comme une quantité invariable[136].
L'indice de réfraction de l'air est d'environ 1,0003[136].
Des médias plus denses, comme l'eau[137],
le verre[138]
et le diamant[139], présentent des indices de réfraction d'environ 1,3, 1,5 et 2,4 pour la lumière visible. Dans des matériaux exotiques, tel le condensat de Bose-Einstein maintenu à une température très proche du zéro absolu, la lumière peut se déplacer à quelques mètres par seconde. Dans ces cas, la durée prise par les atomes pour absorber puis émettre la lumière est significativement plus longue que si le processus d'absorption-émission avait été réalisé à 0 °C par exemple.
Deux équipes de physiciens ont affirmé avoir complètement arrêté la lumière en la faisant passer dans un condensat de Bose-Einstein de rubidium. L'énergie de la lumière est stockée dans les atomes (qui deviennent ainsi excités), puis émise plus tard sous forme lumineuse si les atomes sont illuminés par un faisceau laser. Le comportement absorption-émission retardée est en général vrai au niveau microscopique pour tous les médiums transparents qui « ralentissent » la lumière[140].
Dans les matériaux transparents, l'indice de réfraction est habituellement plus grand que 1, ce qui signifie que la vitesse de phase est plus petite que c. Dans certains matériaux, l'indice de réfaction peut être plus faible que 1 à certaines fréquences lumineuses ; dans quelques matériaux exotiques, l'indice peut être négatif[141].
L'exigence que la causalité ne soit pas violée implique que les parties réelle et imaginaire de la permittivité d'un matériau, qui correspondent respectivement à l'indice de réfraction et au coefficient d'extinction, soient reliées par les relations de Kramers-Kronig[142].
En pratique, dans un matériau qui présente un indice de réfraction inférieur à 1, l'absorption de l'onde lumineuse est si rapide qu'aucun signal ne peut être transmis plus vite que c.
Une impulsion lumineuse avec des vitesses de groupe (vg) et de phase (vp) différentes (qui survient lorsque la vitesse de phase change selon la fréquence des ondes de l'impulsion) s'étale avec le temps, un processus appelé dispersion. Quelque matériaux présentent une vitesse de groupe très faible (ou même nulle) pour les ondes lumineuses, un phénomène appelé lumière lente[143],[144],[145],[146].
L'opposé, des vitesses de groupe supérieures à c, a aussi été mis en évidence par des expériences[147].
En théorie, la vitesse de groupe pourrait être infinie ou négative, avec des impulsions voyageant instantanément ou à reculons dans le temps[148].
Néanmoins, toutes ces possibilités ne permettent pas de transmettre de l'information à une vitesse supérieure à c. Il est en effet impossible de transmettre de l'information avec une impulsion lumineuse plus rapide que la vitesse de la première partie d'une onde, la vitesse de front. Sous certaines conditions, elle est toujours égale à c[148].
Une particule peut voyager plus rapidement que la vitesse de phase de la lumière dans un médium (cette vitesse étant toujours plus faible que c). Quand une particule chargée se propage ainsi dans un matériau diélectrique, l'équivalent électromagnétique d'une onde de choc se produit, c'est l'effet Tcherenkov[149].
Dans un milieu biréfringent, la vitesse de la lumière dépend de son plan de polarisation, phénomène utilisé dans de très nombreux domaines, que ce soit la microscopie ou la fabrication de lunettes de soleil[150].
La vitesse de la lumière est importante dans le domaine des télécommunications : un aller simple et un aller-retour ne sont pas instantanés. Cette constatation s'applique à tous les objets connus dans l'Univers, que ce soit des atomes ou de lointaines galaxies. Quelques techniques s'appuient sur la finitude de c, notamment en métrologie.
Dans les superordinateurs, la vitesse de la lumière impose une limite à la vitesse de transmission de l'information entre les processeurs. Si un processeur opère à 1 gigahertz, un signal ne peut parcourir qu'une distance d'environ 30 cm par cycle. Pour une vitesse maximale de traitement, les processeurs doivent donc être logés les uns près des autres pour minimiser la latence de la communication ; cette contrainte peut réduire l'efficacité du refroidissement. Si la cadence de l'horloge du processeur augmente, la vitesse de la lumière devient alors une contrainte ferme lors de la conception d'une puce électronique[151],[152].
Les radars mesurent la distance en calculant le temps pris par un signal pour faire l'aller-retour entre une cible réfléchissante et l'instrument de lecture[153] : la distance entre les deux est proportionnelle au temps multiplé par c. Un récepteur GPS calcule de même, mais en s'appuyant sur les signaux émis par plusieurs satellites GPS qui émettent en continu. Puisque la lumière parcourt environ 300 000 kilomètres en une seconde, les détecteurs embarqués et les calculs doivent être d'une grande précision[154].
Le Lunar Laser Ranging Experiment, l'astronomie radar et le Deep Space Network calculent respectivement les distances à la Lune[155], 
aux planètes[156]
et aux vaisseaux spatiaux[157] en mesurant les temps d'aller-retour d'ondes électromagnétiques.
Puisque la circonférence équatoriale de la Terre mesure environ 40 075 km[note 16] et que c est d'environ 300 000 km/s, la durée minimale théorique pour qu'une information atteigne le point opposé de la Terre en circulant à sa surface seulement est d'environ 67 millisecondes. Quand la lumière circule dans une fibre optique autour du globe, le temps de transit est plus long, entre autres parce que la vitesse de la lumière y est diminuée d'environ 35 %, selon l'indice de réfraction n du matériau de la fibre[note 17]. De plus, le signal lumineux doit être régénéré régulièrement ou encore converti en signal électronique puis optique ; ces opérations durent plus longtemps que le temps pris par la lumière pour parcourir en ligne droite la distance entre l'entrée et la sortie de l'un de ces appareils[159].
Les communications entre la Terre et un vaisseau spatial ne sont pas instantanées. Plus les deux sont éloignés, plus le délai entre l'émission et la réception d'un signal est grand. Ce délai est devenu apparent lors des communications entre le Mission Control Center de la NASA et la capsule d'Apollo 8 qui orbitait autour de la Lune (en décembre 1968) : pour chaque question, le premier devait attendre au moins 3 secondes avant de recevoir une réponse[161].
Le délai de communication entre la Terre et Mars varie entre 5 et 20 minutes selon leur position relative[162].
En conséquence, si un robot sur Mars éprouve un problème, son contrôleur humain ne le sait pas avant 5 minutes et peut-être même après 20 minutes. Il faudrait encore au moins de 5 à 20 minutes pour inciter le robot à effectuer une manœuvre corrective.
La lumière qui provient d'objets astronomiques lointains prend encore plus de temps pour atteindre la Terre. Par exemple, l'un des objets célestes de l'image Hubble Ultra Deep Field a émis de la lumière qui a parcouru l'Univers pendant 13 milliards d'années avant d'être détectée par le télescope spatial Hubble[163],[164].
Cette image, construite aujourd'hui, capture l'état de cette lointaine galaxie voici 13 milliards d'années, quand l'Univers était âgé d'un milliard d'années[163].
Les distances astronomiques sont parfois exprimées en années-lumière, surtout dans les ouvrages de vulgarisation et dans les médias de masse[165].
Une année-lumière est la distance parcourue par la lumière dans le vide pendant une année, c'est-à-dire environ 9 461 milliards de kilomètres ou 0,306 6 parsec[166]. Proxima Centauri, l'étoile la plus près de la Terre après le Soleil, se trouve à environ 4,2 années-lumière[167].
La vitesse de la lumière est d'une certaine importance dans les transactions à haute fréquence, où des courtiers tentent de gagner de petits avantages financiers en effectuant des transactions une fraction de seconde avant leurs compétiteurs. Par exemple, des courtiers préfèrent utiliser des systèmes de communication à micro-ondes, parce que ces ondes circulent presque à c alors que la lumière dans les fibres optiques voyage de 30 à 40 % moins rapidement[168],[169].
On peut mesurer c de plusieurs façons. Par exemple, en observant la façon dont les ondes lumineuses se propagent grâce à des instruments astronomiques. On peut la mesurer en fonction de constantes connues, telles la permittivité du vide () et la perméabilité du vide (). On peut également calculer cette valeur en connaissant la longueur d'onde et la fréquence d'une onde lumineuse, puisque leur produit égale c.
Depuis 1983, le Système international d'unités (SI) fixe la vitesse de la lumière à exactement 299 792 458 m/s[170],[171],[64].
En tant que constante dotée d'unités de mesure, la valeur numérique de c diffère selon le système d'unités[note 18].
La question de la constance de la vitesse de la lumière dans le vide ne peut être tranchée puisqu'il est théoriquement possible que les photons aient une masse non nulle : les mesures ne peuvent que plafonner cette masse hypothétique et non prouver qu'elle est rigoureusement nulle. Toutefois, même s'il était avéré que les photons ont une masse, cela ne remettrait pas en cause le principe de la constante c, mais donnerait plutôt une limite de précision de son observabilité dans les modèles de référence[172].
Le milieu interstellaire est un lieu pertinent pour mesurer la vitesse de la lumière à cause de sa grandeur et de l'absence quasi totale d'obstacles sur de grandes distances. Historiquement, les scientifiques ont mesuré le temps de parcours de la lumière en fonction d'une distance connue dans le Système solaire, tel le diamètre de l'orbite terrestre.
Ole Christensen Rømer effectue en 1676 une mesure astronomique qui lui permet de prédire que la vitesse de la lumière est finie[36],[11].
Entre 1671 et 1673, Rømer observe une variation dans la durée de l'orbite de la lune Io de Jupiter[173] et déduit que la lumière prend de 10 à 11 minutes pour parcourir le diamètre de l'orbite terrestre[174],[175]. 
C'est néanmoins Christian Huygens qui calcule la vitesse de la lumière à partir des observations astronomiques de Rømer et de Jean-Dominique Cassini : 230 000 km/s, probablement parce que Rømer doutait de sa capacité à pouvoir calculer la valeur numérique d'une telle grandeur et que Cassini rejetait l'hypothèse de Rømer[176].
Une autre méthode pour mesurer c est d'utiliser l'aberration de la lumière, découverte et expliquée par James Bradley au XVIIIe siècle[38].
Cet effet s'explique par l'addition vectorielle de la vitesse de la lumière qui provient d'une source lointaine (comme une étoile) et la vitesse du télescope (voyez les explications du diagramme à la droite). Un observateur mobile voit un rayon de lumière d'une direction légèrement différente et, en conséquence, observe la source à une position décalée par rapport à sa position calculée. Cet effet est la source du mouvement apparent des étoiles dans le ciel puisque la direction de la vitesse de la Terre change continuellement (elle orbite autour du Soleil et tourne sur elle-même). En se basant sur la différence angulaire de la position des étoiles (20,5 secondes d'arc)[177], il est possible de déterminer c en fonction de la vitesse de la Terre autour du Soleil connaissant la durée d'une orbite terrestre complète. En 1729, Bradley détermine que c est 10 210 fois plus rapide que la vitesse orbitale de la Terre[note 19]. De façon équivalente, il faut 8 min 12 s pour que la lumière parcourt la distance Soleil-Terre[38].
Une unité astronomique (UA) est à peu près la distance moyenne entre la Terre et le Soleil. En 2012, elle a été fixée à exactement 149 597 870 700 m[178],[179].
Auparavant, elle n'était pas définie selon les unités du Système international d'unités, mais selon la force gravitationnelle exercée par le Soleil dans un référentiel de mécanique classique[note 20].
Auparavant, l'inverse de c exprimé en secondes par UA était mesuré en comparant le temps pour un signal radio à atteindre différents vaisseaux spatiaux dans le Système solaire, leur position étant calculée en fonction des effets gravitationnels du Soleil et de planètes. En calculant une moyenne de plusieurs mesures, une valeur du temps lumière par unité de distance s'obtient par meilleur ajustement. Par exemple, en 2009, la meilleure estimation, approuvée par l'Union astronomique internationale (UAI), est de[181],[182],[183] : 
L'incertitude relative est de 0,02 parties par milliard (2 × 10−11), équivalente à l'incertitude de la mesure terrestre d'une distance  par interférométrie[184].
Puisque le mètre est défini comme la distance parcourue par la lumière en un certain intervalle de temps, la mesure du temps de parcours selon la définition précédente peut aussi être interprétée comme la longueur de l'UA (ancienne définition) en mètres[note 21].
Une méthode pour déterminer c est de mesurer le temps pour un faisceau lumineux d'atteindre un lointain miroir et d'en revenir, méthode qu'utilise Hippolyte Fizeau en 1849[186],[187]. Le montage de Fizeau consiste en un faisceau de lumière dirigé vers un miroir à environ 8 km de la source et qui tombe sur les dents d'une roue dentelée en rotation. Lorsque cette roue atteint une certaine vitesse de rotation, le faisceau passe entre une paire de dents, est réfléchi puis passe entre la paire de dents suivante. Connaissant la distante entre la roue et le miroir, la distance entre deux dents et la vitesse de rotation, la vitesse de la lumière peut être calculée[13].
Léon Foucault préfère utiliser un miroir rotatif. Dans son expérience, réalisée en 1850, un faisceau de lumière tombe sur le miroir rotatif. Pendant que le faisceau se dirige vers le miroir fixe, puis est réfléchi, le miroir rotatif continue de tourner et le faisceau est réfléchi sur ce miroir rotatif à un angle différent de celui au début de son trajet. Connaissant la différence d'angle, la vitesse de rotation du miroir rotatif et la distance au miroir fixe, il est possible de calculer c[188],[189].
Au XXIe siècle, en utilisant un oscilloscope plus précis que la nanoseconde, c peut être déterminée en mesurant le temps pris par l'impulsion lumineuse d'un laser (ou d'une DEL) réfléchie par un miroir. La valeur obtenue est moins précise, de l'ordre de 1 %, que celles obtenues par un laboratoire proprement doté, mais elle a l'avantage de pouvoir être reproduite dans un laboratoire universitaire ordinaire[190],[191],[192].
En septembre 2011, la collaboration de physiciens travaillant sur l'expérience OPERA annonce que le temps de vol mesuré de neutrinos produits au CERN est inférieur de 60,7 ns à celui attendu pour des particules se déplaçant à la vitesse de la lumière[193],[194].
Le 8 juin 2012, la collaboration annonce que l'anomalie est en fait liée à une erreur de mesure due au branchement défectueux d’un câble de synchronisation optique des horloges atomiques, et que la vitesse mesurée des neutrinos est compatible avec celle de la lumière[195].
Une façon d'obtenir la valeur de c sans s'appuyer sur des mesures en lien avec la propagation d'ondes électromagnétiques, est d'utiliser la relation entre c, la permittivité du vide () et la perméabilité du vide () tel que démontré par Maxwell : [196],[197]. La permittivité peut être déterminée en mesurant la capacité électrique et les dimensions d'un condensateur, alors que la perméabilité est fixée à exactement 4π × 10−7 H/m à la suite de la définition de l'ampère. Rosa et Dorsey empruntent cette voie en 1907 et calculent 299 710 ± 22 km/s[14],[15].
c se calcule par la relation [198]. Donc, c peut être établie si l'on mesure de façon indépendante la fréquence () et la longueur d'onde () d'une onde électromagnétique dans le vide. 
Une autre façon de faire est de mesurer la fréquence de résonance dans une cavité résonnante. Si les dimensions de la cavité sont connues, on peut alors trouver la longueur d'onde. En 1946, Louis Essen et A. C. Gordon-Smith établissent plusieurs modes normaux des micro-ondes dans une cavité résonnante dont les dimensions sont connues avec une incertitude de ± 0,8 μm grâce à des mesures par interférométrie[14].
Puisque les longueurs d'onde des modes normaux sont connues grâce à l'électromagnétisme, la valeur de c peut être calculée pour plusieurs fréquences[14],[199].
Le résultat d'Essen–Gordon-Smith, 299 792 ± 9 km/s, est significativement plus précis que ceux obtenus par des méthodes optiques[14].
En 1950, Essen affirme, après avoir mené une suite d'expériences, avoir obtenu 299 792,5 ± 3,0 km/s pour c[17].
Une démonstration maison de cette technique peut se faire avec un four à micro-ondes et une substance fusible, telles que des guimauves ou de la margarine. Si la table pivotante est retirée de façon que l'aliment reste immobile, le four va cuire plus rapidement aux anti-nœuds (les points où l'amplitude de l'onde est la plus grande) ; donc, aux points où l'aliment fond le plus rapidement. La distance entre deux points est la moitié de la longueur d'onde des micro-ondes ; en mesurant cette distance et en multipliant par la fréquence des micro-ondes (inscrite sur la plaque signalétique du four, habituellement 2 450 MHz), la valeur de c peut être calculée « souvent avec une erreur inférieure à 5 % »[trad 9],[200],[201].
Un faisceau de lumière cohérente (par exemple un laser), d'une fréquence connue (), est divisé en deux. Chaque partie suit un trajet différent, puis elles sont recombinées. En modifiant et mesurant avec précision la longueur d'un trajet tout en observant le motif d'interférence, la longueur d'onde () peut être déterminée. La vitesse de la lumière est donnée par [202].Dans la moitié gauche, un faisceau de lumière, de fréquence connue (), est issu de la source cohérente. Il est en partie réfléchi vers le haut, en partie transmis à travers le miroir semi-transparent. La partie qui monte est ensuite réfléchie vers le bas par le miroir horizontal. Pour sa part, la partie de faisceau qui a traversé le miroir est réfléchie par le miroir vertical. Si la distance entre le bas du miroir oblique et le miroir horizontal est un multiple de la longueur d'onde (), alors les deux parties, qui sont des ondes, formeront une interférence constructive (la courbe rose dans le motif en bas à la gauche).
Dans la moitié droite, un faisceau lumineux subit les mêmes transformations, mais la distance entre le bas du miroir oblique et le miroir horizontal est un multiple de  plus . Les deux parties, toujours des ondes, formeront une interférence destructive (la courbe rose aplatie dans le motif en bas à la droite).
Lorsqu'il y a interférence constructive, l'observateur est illuminé par le faisceau recombiné. S'il y a interférence destructive, il ne voit que du noir.
Avant la démocratisation des lasers, les sources d'ondes radio cohérentes ont servi à déterminer c par interférométrie[18]. Toutefois, la détermination de la longueur d'onde par interférométrie devient de moins en moins précise au fur et à mesure que la longueur d'onde augmente. Dans la pratique, les expériences perdent beaucoup de précision lorsque les longueurs d'onde dépassent 4 mm. La précision peut être augmentée en utilisant de la lumière de plus courte longueur d'onde, mais il devient alors difficile de mesurer directement sa fréquence. Une voie de contournement est de commencer avec un faisceau de faible fréquence pour lequel celle-ci peut être mesurée. Ensuite, synthétiser des ondes de plus grandes fréquences qui peuvent être liées au faisceau de départ. Un laser peut ensuite être syntonisé sur cette fréquence ; sa longueur d'onde est alors mesurée par interférométrie[203]. Cette technique, mise au point par une équipe du National Bureau of Standards, a été utilisée en 1972 pour mesurer la vitesse de la lumière dans le vide à une précision relative de 3,5 × 10−9[203],[19].
(en) Cet article est partiellement ou en totalité issu de la page de Wikipédia en anglais intitulée « Speed of light » (voir la liste des auteurs).Sur les autres projets Wikimedia :
« Aliments » redirige ici. Pour l'obligation juridique, voir Obligation alimentaire.
Cet article adopte un point de vue régional ou culturel particulier et nécessite une internationalisation (9 mars 2018).
Merci de l'améliorer ou d'en discuter sur sa page de discussion ! Vous pouvez préciser les sections à internationaliser en utilisant {{section à internationaliser}}.
De manière générale, la nourriture désigne les aliments d'origine animale, végétale, fongique (parfois bactérienne ou minérale) ou chimique, consommés par des êtres vivants à des fins d'alimentation. Mais il existe aussi une nourriture festive, ou de récréation incluant des aliments dits facultatifs, c'est-à-dire non nécessaires à l'alimentation humaine. Ces aliments sont généralement classés parmi les aliments ultratransformés (ou UPF)[1] (qui eux-mêmes constituent l'essentiel de la « malbouffe »).
Les aliments liquides sont appelés « boissons ».
Dans toute l'Union européenne[2], la notion d'aliment désigne toute substance ou produit, transformé, partiellement transformé ou non transformé, destiné à être ingéré ou raisonnablement susceptible d’être ingéré par l’être humain. Ce terme recouvre les boissons, les gommes à mâcher et toute substance, y compris l’eau, intégrée intentionnellement dans les denrées alimentaires au cours de leur fabrication, de leur préparation ou de leur traitement. Il inclut l’eau au point de conformité défini à l’article 6 de la directive 98/83/CE[3].
Le terme « denrée alimentaire » n'inclut pas en Europe :
En Europe toujours, les « denrées alimentaires génétiquement modifiées » sont [4] et . Elles sont soumises à une traçabilité et un étiquetage spécifique[5].
Une partie importante des denrées alimentaires est dégradée avant d'être consommée. Beaucoup d'aliments sont gaspillées. La FAO (Food and Agriculture Organization) estime que nourrir toute la planète ne sera pas possible sans réduction du gaspillage et des déchets alimentaires[6]. La FAO a mené une analyse en 2011 et estime que 1,3 milliard de tonnes par an, soit environ un tiers de la production totale de denrées alimentaires destinées à la consommation humaine est gaspillé dans le monde[7]. En France le gaspillage alimentaire concerne plus de 10 millions de tonnes par an soit à peu près de 18 milliards de repas jetée à la poubelle.
Alimentaire ou spirituelle, la nourriture désigne ce qui entretient la vie d'un organisme en lui procurant au moins les substances à assimiler nécessaires à sa subsistance.
Les divers panthéons incluaient une déesse de la nourriture : ainsi Zywienia, épouse de Radegast, dieu de l'hospitalité, est-elle la déesse de la nourriture dans la mythologie slave.
En 2017, alors que depuis les années 1950 les aliments industriels gagnent du terrain, et que la diversité génétique et la diversité spécifique des plantes et animaux consommés se sont effondrées, la malbouffe a tué plus de gens dans le monde que tout autre facteur (tabagisme y compris)[8]
En 2020, plus de 2 milliards de personnes étaient en surpoids ou obèse (dans les réions du monde de culture occidentale essentiellement) alors que 811 millions de personnes ne mangeaient pas à leur faim (surtout dans les régions les plus pauvres)[8]. Au rythme de la démographie mondiale et des tendances alimentaires dans le monde, selon la FAO, la production de viande, de produits laitiers et d'œufs devrait augmenter d'environ 44% en 30 ans, de 2020 à 2050, ce qui est incompatible avec les objectifs climatiques et tous les objectifs de développement durable de l'ONU et de ses États-membres[8].
Le mot « nourriture », a de nombreux synonymes, techniques, familiers ou argotiques : produit alimentaire, bouffe, rata, casse-dalle, etc.
Dans l'Union européenne, dans le domaine de l'IAA (Industrie agroalimentaire), un Plan de Maîtrise Sanitaire, un agrément sanitaire ou une déclaration[9] avant l'ouverture peuvent être nécessaire pour les établissements produisant ou utilisant des POADAC (produits d'origine animale et denrées alimentaires en contenant) et POVDAC (produits d'origine végétale et denrées alimentaires en contenant).
On distingue plusieurs grandes familles d'aliments :
Ces aliments sont regroupés selon leur degré de transformation en 4 catégories par une classification dite « NOVA » .
La consommation de nourriture ultra-transformée (UPF, tels que définis par la classification NOVA) est en hausse forte et régulière depuis les années 1950.
Les multinationales agro-industrielles ont créé une nourriture « prête à consommer », artificiellement rendue hyper appétissants et goûteuse, qui pousse à la suralimentation[10]. [1].
Ces formulations industrielles d'aliments et de boissons à base de substances alimentaires et d'additifs (souvent controversés), contenant souvent peu ou pas d'aliments entiers (biscuits, confiseries, aliments à base de poulet, boissons sucrées, margarine et nombreux plats préparés) sont commercialement très rentables car utilisant des ingrédients peu coûteux, semblant pouvoir remplacer tous les autres groupes d'aliments[11].
Alors que les preuves de nocivité des UPF s'accumulent[12],[13],[10], leurs impacts environnementaux (par exemple lié à leurs hautes teneurs en sucre industriel, huile de palme et soja transgénique, mais aussi liés à leurs chaines de transformation, d'emballage [sources d'une énorme quantité de déchets ménagers et de la restauration] et de distribution) sont encore mal cernés. Or la nourriture hypertransformée est essentiellement produite par des multinationales ayant le pouvoir de dicter où et quand cultiver ses produits ensuite achetés à bas prix, transportés, préparés et introduits dans les systèmes alimentaires du monde entier, en s'appuyant sur un marketing agressif et des allégations trompeuses, nutritionnelle et de santé, instituant de nouvelles cultures alimentaires encourageant la poursuite de la production de masse de nourriture, de son transport planétaire et une production de déchets problématiques[14].
Faire évoluer ces aliments vers des régimes alimentaires soutenables est un enjeu pour le XXIe siècle.
Elle affecte l'environnement, et la santé via des effets de long termes (maladies chroniques souvent évitables)[14]. La mal bouffe tend à s'auto-entretenir via des addictions associées aux excès de sucre, de sel, d'alcool et à certains additifs exhausteurs de goût et autres auxiliaires technologiques, très utilisés par les producteurs de nourriture industrielle. Une mauvaise nourriture est aussi parfois source d'intoxications alimentaires, chroniques ou aiguës alors souvent causées par des bactéries, des toxines et des virus ; la nourriture peut aussi être source de parasites et de prions pathogènes. Il s'agit aussi de « maladies évitables »).
Intoxications alimentaires : déjà citées par Hippocrate, elles tuent encore environ 7 millions de personnes par an. Et environ 10 fois plus de personnes souffrent d'une intoxication non-mortelle. En France, environ 1,5 million de cas par an causent plus de 17 000 hospitalisations, et plus de 200 décès[15].
Elles résultent souvent d'une contamination croisée d'aliments prêts à consommer par d'autres aliments non-cuits, et/ou d'un mauvais contrôle de la température. Plus rarement, c'est une contamination chimique des aliments qui est en cause, par exemple à la suite d'un stockage inapproprié, dans un récipient inapte au contact alimentaire, en plastique libérant des perturbateurs endocriniens, ou recouvert d'un émail à base de plomb, alors source de saturnisme) ou à la suite de l'utilisation de savons et de désinfectants de qualité non alimentaire.
Les produits animaux se gâtant facilement (viandes, certains produits laitiers, fruits de mer nécessitent des procédures strictes et adéquates d'hygiène (chaîne du froid et conservation au chaud jusqu'au stockage après cuisson, au risque d'une prolifération bactérienne dangereuse, comme Salmonella ou Escherichia coli). Beaucoup d'aliments industriels peuvent aussi être falsifiés ou adultérés, dont lors de l'élevage, de la fabrication, de la cuisson, de l'emballage, de la distribution ou de la vente.
Divers biocontaminants ((ex. : microbes, moisissures, parasites et/ou leurs œufs) et corps étrangers (poils, mégots, bris de bois, métaux, verre, etc.) et polluants sont parfois retrouvés dans les aliments.
L'étiquetage, la traçabilité, les dates limites et divers contrôles d'hygiène visent a améliorer la sécurité alimentaire. Les procédures de type HACCP impliquent une traçabilité « de la fourche à la fourchette », le maintien d'une zone de préparation propre et de circuits séparés des aliments de différents types, avec maintien de la chaine du froid, et garanties de températures adéquates de cuisson ou de séchage, e réfrigération rapide des aliments après cuisson, etc.[16].
Maladies Chroniques : un régime alimentaire trop riche en sucre, huile, viande, sel, conservateurs et les « aliments ultra-transformés » (sodas sucrés, plats surgelés, viandes reconstituées) est l'une des premières causes de Diabète, d'obésité, de troubles cardiovasculaires et d'autres pathologies en augmentation, dont de nombreux cancers (environ 35 % des cancers selon une analyse épidémiologique publiée par Richard Doll et Richard Peto en 1981). L'alimentation est en effet source de substances cancérogènes et/ou mutagènes, soit naturellement présentes dans certains aliments ou boissons (alcools), soit radiotoxiques (iode 131 disséminé par la catastrophe de Tchernobyl par exemple, source de cancers de la thyroïde), ou encore issus de moisissures sources de mycotoxines (ex. : aflatoxines issues de maïs, blé ou arachides contaminés) ou de bactéries (ex. : certaines cyanophycées sources de cyanotoxines). La cuisson à haute température de la viande génère des amines hétérocycliques et/ou des hydrocarbures polyaromatiques dans ses parties carbonisée (comme dans le poisson fumé), et certains jambons ou viandes séchées de type saucissons, bacon… apportent des nitrosamines issues de nitrites utilisés comme conservateurs.
De nombreux fruits et légumes contiennent des vitamines, antioxydants et anticarcinogènes prévenant les maladies chroniques, mais ils tendent à être moins consommés ou à être dégradés dans les processus agro-industriels. Il est cependant souvent difficile d'identifier les composants spécifiques de l'alimentation qui servent à augmenter ou diminuer le risque de cancer car de nombreux aliments, tels que le steak de bœuf et le brocoli, contiennent de faibles concentrations à la fois de cancérigènes et d'anticarcinogènes. Il existe de nombreuses certifications internationales dans le domaine de la cuisine, telles que Monde Selection, AA Certification, iTQi. Ils utilisent des méthodes d'évaluation de haute qualité pour rendre les aliments plus sûrs.
Les progrès scientifiques récents (dont sur l'importance du microbiote intestinal, éclairent la diététique d'un jour nouveau et font reculer certaines idées reçues[17] ; en particulier les études épidémiologiques et leurs méta-analyses ont récemment fourni des preuves supplémentaires qu'[18].
Une Commission EAT-Lancet, financée par l'ONG Wellcome a réuni des experts-nutritionnistes pour, à partir d'une revue de la littérature scientifique, élaborer les bases d'un régime alimentaire répondant au mieux aux besoins du corps humain et respectant autant que possible actuellement les limites planétaires climatique et environnementales[19] ; c'est-à-dire tenant notamment compte des impacts de la production de la nourriture est termes d'émissions de carbone (au regard des données du GIEC publiées en 2018[20], sachant que la situation a empiré depuis), de perte de biodiversité, de consommation d'eau, de sols terre, d'azote[21] et de phosphore.
Pour un adulte de 30 ans et de corpulence moyenne, cette commission recommande des aliments diversifiés, surtout locaux et végétaux (légumes, légumes-racine et fruits frais) apportent 2 500 calories/jour. La viande rouge ne devrait pas dépasser 100 g/semaine (soit une portion de viande rouge, moins du quart de ce qu'un Américain typique consomme en 2020). Les aliments ultra-transformés sont  et le régime est à saisonnièrement décliner selon les produits régionaux[22]. Selon cette commission, ce régime varié, meilleur pour la santé (santé mentale y compris)[23] et soutenable dans l'anthropocène[24], permettrait de sauver la vie d'environ 11 millions de personnes par an[25] et de  commente Tim Lang (co-auteur du rapport EAT-Lancet, et chercheur en politique alimentaire à la City University de Londres) qui ajoute :  ; les données récentes semblent indiquer qu'il existe un seuil de cinq portions de fruits et légumes par jour au delà duquel le risque de cancer ne diminue plus, mais le risque de mortalité cardiovasculaire continue lui à diminuer pour chaque portion supplémentaire de fruits et légumes par jour.
Une critique faite à ce régime est qu'il serait excellent pour les pays riches aux habitudes alimentaires non-soutenables ; mais est-il suffisamment nutritif pour ceux qui vivent dans des milieux à faibles ressources. Selon Ty Beal, scientifique travaillant à Washington avec l'Alliance mondiale pour l'amélioration de la nutrition, d'après des calculs non publiés, ce régime ne fournirait que 78 % de l'apport aujourd'hui recommandé en zinc et 86 % de calcium pour les plus de 25 ans, et seulement 55 % des besoins en fer des femmes en âge de procréer. D'autres, comme Fanzo et Davis en 2010, se demandent si dans le contexte démographique et sociopolitique des années 2010, un régime alimentaire peut être [26].
En 2014, une étude prospective a estimé qu'au vu des tendances en termes d'urbanisation et de démographie mondiale, l'augmentation des émissions de GES en 40 ans (entre 2010 et 2050) va entraîner + 80 % des émissions liées à l'alimentation[27], or ces émissions sont déjà majeures (voir plus bas).
Tout aliment, qu'il soit local ou non, d'origine végétale/fongique, ou d'origine animale, cru ou cuit, congelé ou non, préparé ou non… a une empreinte eau, une empreinte carbone, une empreinte énergétique et climatique. Les régimes alimentaires ont des empreintes extrêmement différentes selon leur type, de plus en plus élevées dans le modèle industriel dominant contemporain. , alerte en 2021 Sam Myers (directeur de la « Planetary Health Alliance », un consortium mondial basé à Boston (Massachusetts), qui étudie les impacts sanitaires des changements environnementaux[8].
En 2020, la production le transport, la consommation et le gaspillage de la nourriture industrielle contribue au dépassement des limites planétaires (ce qui à terme conduit à un effondrement écologique et à effondrement des sociétés rendant la planète inhospitalière pour les humains)[28],[29]. La production de nourriture est en 2021 source d'un peu plus du quart (26%) des émissions de gaz à effet de serre dans le monde[30].
La production de nourriture dans son ensemble [8].
Selon une estimation de 2014, si le terrien moyen mangeait plus de végétaux et moins de viande/poisson, et si les émissions de tous les autres secteurs étaient stoppées, le monde aurait 50 % de chances d'atteindre l'objectif climatique de 1,5 °C[27] ; et si conjointement à ce choix alimentaire, des changements plus larges du secteur agroalimentaire incluant la réduction et une meilleure gestion des déchets, nos chances de ne pas dépasser 1,5 °C en 2100 passeraient à 67 %[8]. Les économies d'énergie, une consommation générale plus locale et une réduction des transports suffiraient alors pour stabiliser le climat.
Bien que ces scenarii n'éliminent pas la viande, et qu'il existe des modèles d'élevage plus soutenable, ces données scientifiques n'ont pas été appréciées par le lobby de l'industrie de la viande[8].
En 2020, les aliments agro-industriels et industriels représentent environ 70 % de la consommation d'eau douce[8] et 40 % des surfaces terrestres émergées[8] dans le monde (au détriment d'une grande partie de la faune, flore et fonce sauvage ainsi privé d'une partie de leurs habitats naturels, et notamment des sols le plus riches. La surpêche surexploite une grande partie des océans.
Pour produire les aliments industriels, les cultures industrielles exigent (pour la plupart) des engrais et pesticides destructeurs de biodiversité. Les engins et pratiques de l'agriculture industrielles dégradent les sols (érosion, tassement, lessivage, pollution, épuisement des nutriments…). Les engrais azotés perturbent le cycle de l'azote et les phosphates perturbent le cycle du phosphore, et sont en grande partie responsable de la pollution chronique des rivières et des eaux littorales[32].
Les aliments sont composés de plusieurs types d'ingrédients, qu'on peut classer selon leur origine :
Dans cette famille, on trouve tous les minéraux fréquemment utilisés dans les processus de fabrication. Le plus fréquent est le Chlorure de Sodium (le sel). Cette catégorie est répartie en sels minéraux (calcium, sodium, potassium) et en oligo-éléments (fer, magnésium selon les cas, cuivre, cobalt, etc.).
Il est important de distinguer le potentiel hydrogène (pH) des aliments de leur effet sur le corps humain une fois digérés. Par exemple, certains aliments basiques (pH élevé) auront pour effet de diminuer l'acidité (augmentation du pH, potentiel alcalinisant), alors que certains aliments acides (pH faible) auront pour effet d'augmenter l'acidité (diminution du pH, potentiel acidifiant)[réf. nécessaire].
Les aliments non transformés sont des ressources primaires provenant de l'agriculture (élevages et cultures) et de la pisciculture ou de la nature (cueillette, pêche, chasse) ou de situations intermédiaires (chasse en enclos ou de gibier d'élevage, pêche en étangs de pêche ou à la suite de rempoissonnements issus de piscicultures).
La qualité des denrées varie selon l'environnement, les modes de productions agricoles mis en œuvre, la fraîcheur du produit, d'éventuelles contaminations (métaux lourds, pesticides, biocides, bactéries spécifiques, radionucléides, etc.) ou ruptures de la chaîne du froid. Dans la plupart des pays, des systèmes plus ou moins poussés de contrôle et surveillance existent, y compris pour les contaminations radioactives[33].
Pour mettre en surbrillance les différentes natures des aliments dévolus au commerce, il existe quantité de labels sur lesquels le consommateur peut s'appuyer avec plus ou moins de certitude pour avoir une indication sur leurs vertus organoleptiques, sociales, environnementales ou/et sanitaires.
Des désignations (AOP), des identifications (IGP, STG, LR) et des marques collectives de certification officielles (AB) décernés par des organismes d'État permettent aux consommateurs de faire leur choix en fonction de critères objectifs et répondant à un cahier des charges précis.
En parallèle, des organisations privées ont créé des marques ou des signes distinctifs (Max Havelaar, Produit de l'année , etc.).
Dans la plupart des pays existe un corpus de législation alimentaire, spécifique, incluant des dispositions législatives, réglementaires et administratives. Cette réglementation régit les denrées alimentaires et leur sécurité, à échelle communautaire (en Europe par exemple) et/ou nationale. Elle concerne toutes les étapes de la production, de la transformation et de la distribution des denrées alimentaires et aussi les aliments destinés ou donnés à des animaux producteurs de denrées alimentaires.
Dans l'Union européenne, un aliment ou denrée alimentaire est  ; le terme « denrée alimentaire » ne couvre pas :
C'est l'ensemble des cinq règlements communautaires fixant des exigences relatives à l’hygiène des denrées alimentaires et des denrées animales[réf. nécessaire].
Il impose notamment un système de [35], y compris, dans une certaine mesure pour l'alimentation animale[36]. Pour l'alimentation humaine, la traçabilité doit être assurée de la fourche à la fourchette, via :
En 2002, un règlement a rappelé[37] que la législation alimentaire inclut aussi des exigences relatives aux aliments pour animaux, notamment à leur production et à leur utilisation, lorsque ces aliments sont destinés à des animaux producteurs de denrées alimentaires et ce, [37].
Un règlement européen (CE 178/2002[37]) vise à renforcer et entretenir le  dans l'exécution des politiques communautaires, tout en permettant une  dans la Communauté européenne. Il inclut :
Pour les articles homonymes, voir Sucre (homonymie).
Le sucre est une substance de saveur douce extraite principalement de la canne à sucre ou de la betterave sucrière. Le sucre est une molécule de saccharose (glucose + fructose). Il est également possible d'obtenir du sucre à partir d'autres plantes.
Toutefois, d'autres composés de la même famille des saccharides ont également une saveur douce : le glucose, le fructose… qui sont de plus en plus utilisés par l'industrie agroalimentaire et dans d'autres secteurs[1]. Sur un étiquetage nutritionnel, l'information dont sucres, située sous la ligne Glucides qu'elle complète, désigne tous les glucides « oses » ayant un pouvoir sucrant, essentiellement le fructose, saccharose, glucose, maltose et lactose. Les autres glucides ayant un pouvoir sucrant sont les « polyols » (sorbitol, maltitol, mannitol) mais ils sont étiquetés séparément, en tant que « polyalcools », qui sont des glucides et non des sucres.
Le terme « sucre » vient probablement du sanskrit « çârkara » (signifiant « gravier » ou « sable »[2]).
Outre le miel et les fruits (comme la pomme) qui servent de complément glucidique depuis la Haute Antiquité, divers végétaux contiennent des quantités importantes de sucres et sont utilisés comme matière première d'où l'on extrait ces sucres, souvent sous la forme de sirop :
Les sucres ont une saveur que l'on a dit être une des quatre saveurs de base (sucré, salé, amer, acide).
Sur le plan cognitif et neurologique, les saveurs sucrées semblent indiquer aux primates, humains ou non humains, la valeur énergétique des végétaux, d'où le plaisir qui lui est associé[3]. Le premier aliment de l'homme est légèrement sucré (lactose). La plupart des plantes toxiques sont amères, le choix d'un aliment sucré serait donc sans danger.
Certaines saveurs sucrées sont reconnues par une famille de récepteurs, situés sur la langue, couplés à la protéine G T1R1, T1R2 et T1R3 ; ils s’assemblent en homodimères ou hétérodimères et permettent la reconnaissance des sucres naturels ou des édulcorants.
À part les sucres, de nombreuses autres molécules, artificielles ou naturelles, possèdent un pouvoir sucrant, mais celles-ci ne sont pas toutes reconnues par l'ensemble des animaux.
Parmi les molécules d'origine naturelle, on trouve les acides aminés (glycine), les protéines (thaumatine, mabinline), des hétérosides (stéviosides), etc.
Parmi les molécules de synthèse, on trouve, des dipeptides (aspartame), des sulfamates (acésulfame potassium), etc.
Les premières traces de cultures sucrières associées à une plante naturelle se trouvent en Asie du Sud-Est et sur les îles du Pacifique : on y mâchait la tige de la canne à sucre pour en extraire le suc. La fabrication du sucre par extraction aurait commencé dans le Nord-Est de l’Inde ou dans le Pacifique Sud respectivement vers 10000 ou 6000 av. J.-C. Vers 325 av. J.-C., Néarque, l'amiral d'Alexandre le Grand, lors d'une expédition en Inde, évoque un « roseau donnant du miel sans le concours des abeilles », reprenant par là une expression des Perses[4].
En Europe occidentale, chez les Anciens Grecs notamment, on utilisait principalement la saveur sucrée du miel, comme en témoignent les nombreuses jarres découvertes durant les campagnes archéologiques de Cnossos, Mycènes et de Paestum. Pour autant, le sucre de canne n'y est pas inconnu (les Anciens Égyptiens la cultivent), du fait des échanges maritimes : cependant, il est encore rare et cher. Sous l'Empire romain, le coût faiblit grâce à l'annexion de l’Égypte et d'une partie de l'ancienne Perse, mais l'usage du miel est très largement dominant.
D’autres découvertes archéologiques effectuées au début du XXe siècle associent la culture de la canne avec la civilisation de la vallée de l'Indus[5], cultures qui remonteraient au deuxième millénaire avant notre ère.
En Inde, on aurait réussi à purifier et cristalliser le sucre pendant la dynastie des Gupta vers l’an 350.
Partis de Bagdad, de Damas et de Tunis, dès le Xe siècle les premiers voyageurs arabes découvrent la canne sucrière, notamment en Inde. Au fur et à mesure de l’expansion musulmane en Asie, en retour la canne à sucre est acclimatée dans les pays méditerranéens, depuis la Syrie jusqu'à l'Espagne du sud, et les techniques de production indiennes y sont adoptées et améliorées[6]. Le sucre, en pain ou en poudre, est ainsi facilement transportable par les caravanes. La route des épices est aussi celle du sucre. Les Arabes sont également à l'origine des premières sucreries, raffineries, et plantations de type quasi-industriel[7].
Au Moyen Âge, l'Occident découvre le sucre de canne lors des croisades face aux califats fatimides et almoravides : la canne arrive en Italie, dans les îles de la Méditerranée (Crète, Chypre) et dans le Sud de la France[8].
Vers 1390, une meilleure technique de pressage est créée, permettant de multiplier par deux la quantité de jus obtenu à partir de la canne, et inaugure l’expansion économique des plantations de sucre en Andalousie et en Algarve. Vers 1420, la production de sucre de canne fut étendue aux îles Canaries, Madère et aux Açores.
Au XVe siècle, Venise contrôle le commerce de la Méditerranée orientale, y compris celui du sucre, et fonde la première raffinerie d’Europe. La route des Indes, ouverte par Vasco de Gama, permit aux Portugais de s’assurer d’importantes ressources sucrières et de devenir les premiers fournisseurs du marché européen. Dès le milieu du XVe siècle, ils installèrent des plantations et des raffineries à Madère.
Produit exotique et rare, il est d'abord réservé aux apothicaires et aux élites chez qui il est utilisé comme monnaie d'échange, épice et médicament jusqu'au XVIIe siècle, ne devenant réellement un ingrédient pour la cuisine qu'au XVIIIe siècle : avant cette époque, le sucre de canne est associé au chaud et au sec selon la théorie des humeurs, il soigne le lymphatique ou l'atrabilaire, purge le phlegme, entre dans la fabrication de sirop (chaud et sec) contre le rhume (froid et humide). Dans plusieurs pays où il existe une nette séparation du sucré et du salé, le sucre apparaît plutôt en fin de repas puis en entremets comme dans le blanc-manger[9].
Les Portugais importèrent au milieu du XVIe siècle le sucre au Brésil. L'aventurier Hans Staden témoigne qu’
Après 1625, les Hollandais importèrent la canne à sucre d’Amérique du Sud vers les îles des Caraïbes, aux îles Vierges et à la Barbade. De 1625 à 1750, le sucre devint une matière première très prisée, et les Caraïbes, la principale source mondiale grâce à la main-d’œuvre fournie par l’esclavage.
Au début du XVIIe siècle, les Antilles françaises sont des colonies de peuplement. Les premières plantations de canne ne voient le jour qu’en 1643, après l’échec de la culture du tabac. Les sucreries se multiplient à la Martinique, la Guadeloupe et Saint-Domingue. En métropole, ce sont les raffineries qui fleurissent sous l’impulsion de Colbert, à Nantes et Bordeaux. Le siècle des Lumières est aussi le siècle de la domination française du marché du sucre colonial[10] : le sucre devient un élément important de l’économie et donc de la politique européenne mercantiliste.
Au milieu du XVIIIe siècle, le sucre de canne devient très populaire dans la bourgeoisie, on l'appelle « canamelle ». Le marché du sucre connait une forte croissance, la production devenant de plus en plus mécanisée. Une machine à vapeur alimente un premier moulin à sucre en Jamaïque en 1768, et peu après, la vapeur servit d'intermédiaire au feu comme source de chaleur.
Ce n’est qu’au début du XIXe siècle que le sucre de betterave va connaître un réel essor. Si, dès 1600, l’agronome français, Olivier de Serres, remarque que la « bette-rave » donne en cuisant un jus « semblable au sirop de sucre », il faut attendre 1747 pour qu’Andreas Sigismund Marggraf, chimiste berlinois, prouve que le sucre de betterave et le sucre de canne sont identiques. Les écrits de Marggraf sont ensuite traduits en français[11].
Franz Karl Achard, élève de Marggraf, produit en 1798 le premier pain de sucre de betterave[12]. En 1810, face au blocus continental qui suspend le commerce colonial maritime, l’intérêt pour la betterave est soudain ravivé en France sous l’impulsion de Jean-Antoine Chaptal, qui travaille dans la commission de l’Institut de France, laquelle est chargée de vérifier les expériences d’Achard. Cette commission informe Napoléon de l’intérêt que la France aurait à produire elle-même son sucre car la culture betteravière est rentable et l'extraction en cristaux possible.
Fin 1811, le Normand Jean-Baptiste Quéruel, engagé chez Benjamin Delessert à sa manufacture de Passy, invente la méthode permettant la fabrication industrielle de sucre cristallisé (extraction du jus, filtration, compactage en pains coniques). Napoléon Ier, via Chaptal, incite derechef les agriculteurs français à ensemencer les champs en plants de betterave et les industriels à améliorer les procédés. Dès lors, la France se mobilise pour extraire le sucre à partir de la betterave. En 1812 naît l’agro-industrie sucrière française. Delessert présente à l'empereur en personne ses premiers pains de sucre : celui-ci ordonne aussitôt la mise en culture de 100 000 hectares[13].
La fin de l’Empire permet le retour sur le continent du sucre de canne et met un temps en péril le développement de la betterave sucrière. Mais la récession ne va cependant pas durer. En 1828, la France compte 585 sucreries implantées dans 44 départements.
En 1900, le sucre de betterave représente 53 % de la production mondiale de sucre. La Première Guerre mondiale, en transformant les grandes plaines betteravières européennes en champs de bataille, stoppe toute la production et la fait redescendre à 26 %. S'il remonte pour atteindre 40 % dans les années 1950, le sucre de betterave représente actuellement 22 % de la production mondiale de sucre.
En 1949, Louis Chambon met au point la technique de moulage des « dominos » de sucre par compression, mais les premiers morceaux de sucre blanc, certes grossièrement, sont inventés en 1855.
La démocratisation de la consommation en Europe a lieu lors de la révolution industrielle, la production de sucre étant multipliée par 1 000 entre le XVIIIe et le XXe siècle[9].
Aujourd'hui une sucrerie de betteraves produit entre 1 500 et 2 000 tonnes de sucre au cours d'une journée avec un effectif permanent d'environ 150 personnes.
Le mot « sucre » désigne plus d'une centaine de produits édulcorants différents formé des mêmes éléments chimiques : carbone, hydrogène et oxygène. Sa teneur en carbone est variable. En revanche, il contient toujours deux fois plus d'hydrogène que d'oxygène. À ce titre, le sucre est un hydrate de carbone[14]. Le sucre le plus courant est le saccharose.
Certains types de sucre sont normalisés au niveau mondial par le Codex Alimentarius.
Sucre blanc, sucre roux de canne, sucre de canne complet et vergeoise.
Sucre en morceaux.
Bûchettes de sucre en poudre.
Quelques types de sucre (normalisés ou non) :
La canne à sucre contient environ :
L'extraction n'étant pas parfaite, 1 tonne de canne fournira environ 115 kilogrammes de saccharose.
Les champs de canne à sucre sont généralement brûlés et les cannes ramassées mécaniquement. Le brûlage sur pied, qui diminue la masse végétale inutile (les feuilles) et concentre le sucre dans la tige par évaporation, est une technique aussi ancienne que la culture de la canne. Cette technique est toutefois abandonnée par certains producteurs afin de réduire la production de CO2 associée à la culture de la canne[18].
Ensuite, le procédé d’extraction du sucre de canne[19] est identique à celui du sucre de betterave, à l'exception de la première phase où le jus de canne est extrait par broyage, tandis que celui de betterave est extrait par diffusion. À leur entrée dans la sucrerie, les cannes sont découpées en petits morceaux puis pressées et broyées dans plusieurs moulins. Séparé de la bagasse (la canne écrasée), le jus de canne obtenu (le vesou) contient 80 à 85 % d'eau, 10 à 20 % de sucre et 0,7 à 3 % de composés organiques et minéraux. Il suit ensuite les mêmes étapes que le jus de betterave. Le sirop recueilli après cristallisation et essorage du sucre de canne ou de betterave, également appelé « eau mère », est encore chargé de sucre. Il subit alors une nouvelle cuisson et un nouvel essorage qui donnent le sucre dit de « deuxième jet », plus coloré et moins pur que le sucre de premier jet.
Puis ce sirop de deuxième jet, toujours riche en sucre, est à son tour réintroduit dans le cycle pour donner un sucre de troisième jet, brun et chargé d’impuretés (le sucre roux), ainsi qu'un dernier sirop visqueux et très coloré, appelé mélasse.
La bagasse est utilisée de différentes façons, le carburant pour la chaudière de la sucrerie étant la plus commune.
Pour les sucres « biologiques », obtenus à partir de cannes de l'agriculture biologique, on distingue plusieurs types de sucres, dont :
Le rhum est obtenu à partir du jus fermenté.
La betterave sucrière contient environ :
Pour la canne comme pour la betterave, l'extraction[19] doit se faire rapidement car les plantes continuent à respirer et consomment du sucre pour leur métabolisme. En moyenne, on chiffre de 100  à   130 g de sucre perdu par tonne de betterave et par jour[20]. Les usines sucrières sont ainsi toujours à moins de trente kilomètres des champs. Une autre partie du sucre se retrouve dans la mélasse ou reste dans la pulpe. L'obtention du sucre blanc se fait par adjonction de lait de chaux et de gaz carbonique, puis par centrifugation après cristallisation[21]:9-10, 33-43.
La mélasse produite au cours de l'extraction du sucre de betterave est souvent utilisée pour la fermentation ou la nourriture du bétail[21]:11, 44.
Le sucre roux de betterave, appelé vergeoise ou cassonade, est obtenu par chauffage prolongé du sucre blanc qui provoque la formation de colorants de type caramel[22].
De fabrication artisanale, ce sucre est extrait des inflorescences des palmiers à sucre. Le jus obtenu est filtré, puis cuit afin de le transformer en sirop. Il est enfin battu pour amorcer la cristallisation. Le sucre obtenu est brun, naturellement riche en fructose et oligo-éléments.
En 2011, les cinq premiers producteurs de sucre étaient le Brésil, l'Inde, l'Union européenne, la Chine et la Thaïlande. Cette même année, le principal exportateur de sucre était le Brésil, suivi à distance par la Thaïlande, l'Australie et l'Inde. Les principaux importateurs étaient l'Union européenne, les États-Unis et l'Indonésie[23],[24]. Dans la dernière décennie (2000-2009), la part du Brésil dans les exportations mondiales de sucre brut est passée de 7 à 62 %[25].
Sur 112 pays producteurs, 35 cultivent la betterave sucrière, et fournissent environ 20 % de la production en 2017.
En 2016-17, la France, avec un rendement de treize tonnes de sucre à l'hectare, a produit 4,7 millions de tonnes, et exporté 2 millions de tonnes. Elle est le premier producteur mondial de sucre de betterave[27]. En 2016-2017, la Belgique a produit 683 000 tonnes[28].
Au niveau de l'Union européenne, l'organisation commune de marché du sucre (OCM sucre) est réformée en 2006[29]. Trois impératifs président à cette réforme : intégrer les principes de la nouvelle PAC dans l'OCM sucre, tenir compte de l'ouverture accrue du marché européen résultant d’engagements pris par l'UE auprès de pays en développement et appliquer une décision de l'Organisation mondiale du commerce (OMC) obligeant l'UE à réduire ses exportations de sucre. Une nouvelle réforme d'envergure a lieu le 30 septembre 2017 lorsque le système fondé sur un quota de production réparti entre les différents États membres prend fin[30],[31]. L'Europe met ainsi fin à un dispositif existant depuis les années 1960 ; la même année, les principaux producteurs de sucre augmentent également leurs productions, réduisant les cours, et permettant d'alimenter de nouveaux marchés en sucre[32].
Il existe deux manières d'évaluer la consommation de sucre : par les données de ventes et par les études de consommation.
Il existe aussi différent indicateurs, selon l'objet auquel on s'attache :
Les ventes de sucre sont passées de 5 kg par an et par habitant en 1850 à 30-35 kg dans les années 1960. Depuis, elles sont stables[33] (environ 33 kg par an et par habitant en 2017), avec quelques variations (maximum de 39 en 2013, minimum de 33 en 2017)[34].
Ces quantités vendues sont utilisées en partie pour des usages alimentaires (consommation des ménages, usage par les professionnels, usages industriels) et en partie dans des usages de transformation chimique ou culinaire (fabrication de médicaments, homéopathie, chaptalisation du vin, vins effervescents). Il existe aussi des pertes (par les industriels au cours de leurs processus de fabrication) et du gaspillage. Elles ne représentent donc pas la consommation stricto sensu (les ventes de sucre reflètent la notion de disponibilité ou de volumes de sucre mis sur le marché, à l’échelle d’un pays ou d’une population).
La consommation est mesurée par des enquêtes de consommation individuelles menées par l'Agence nationale de sécurité sanitaire de l'alimentation, de l'environnement et du travail (ANSES) et le Centre de recherche pour l'étude et l'observation des conditions de vie (CREDOC).
L'ANSES évalue la consommation de sucres totaux (sucre naturellement présent dans les fruits et légumes + sucres ajoutés dans la cuisine ou dans des produits industriels), hors lactose, à 75 g par jour et par personne en 2006-2007[35] (il n'existe pas d'étude plus récente), tout en fixant une recommandation pour les apports maximums en sucres (hors lactose) à 100 g par jour et par personne. Selon l'ANSES, 20 à 30 % des enfants et des jeunes adultes dépassent cette recommandation.
Ces données concernent les sucres totaux et ne doivent pas être confondues avec celles des sucres libres (ensemble des sucres ajoutés, ainsi que le sucre des jus de fruits et le miel), sur lesquels porte la recommandation de l'Organisation mondiale de la santé. L'OMS recommande de ramener l'apport en sucres libres à moins de 10 % de la ration énergétique totale chez l’adulte et l’enfant, ce qui représente une consommation d'environ 50 g de sucres libres par jour et par personne. Il s'agit de sa recommandation dite « forte ». L'OMS a établi une deuxième recommandation, « avec réserve » pour éventuellement baisser l'apport en sucres libres à 5 % de la ration énergétique[36].
En 2006-2007, l'apport en sucres libres en France a été mesuré à 52 g par jour et par adulte, soit 9,5 % des apports énergétiques, par l'étude INCA2, 41 % dépassant cette recommandation[37].
En Belgique, les ventes par habitant sont équivalentes à 34 kg par habitant et par an.
La consommation de sucre fournit de l'énergie chimique à court terme, mais ce n'est pas une forme de stockage d'énergie pour l'organisme. Une partie du sucre consommé peut être utilisée tout de suite pour fournir de l'énergie si nécessaire, dans les minutes qui suivent ; une autre partie sera emmagasinée dans le foie et les muscles (sous forme de glycogène) pour utilisation dans les heures qui suivent ; et, en cas d'excès, une partie sera transformée en graisses (triglycérides) qui seront stockées dans les cellules du tissu adipeux[38].
Dès que l'on consomme du glucose, composant du sucre, l'insuline est sécrétée : son rôle principal est de favoriser l'utilisation du glucose par toutes les cellules de l'organisme. Par ailleurs l'insuline stimule la glycolyse, bloque la lipolyse (utilisation des graisses stockées) et favorise la lipogenèse par l'intermédiaire d'une enzyme (la triglycéride synthase), c'est-à-dire la fabrication de graisses dans le tissu adipeux. En effet, le stock de glycogène hépatique est limité et le glycogène musculaire n'est utilisable que par les muscles eux-mêmes.
Cette régulation du glucose, avec un système de stockage et de libération, permet de fournir un apport continu en glucose au cerveau. S'il ne représente que 2 % du poids du corps, le cerveau utilise 20 à 30 % du glucose disponible, qui est sa seule source d'énergie (en dehors des corps cétoniques synthétisés en cas de jeûne prolongé)[39].
Le sucre de betterave est toujours raffiné pour en retirer le goût désagréable, tandis que le sucre roux de canne peut être consommé tel quel[40],[41]:19[21]:10. Le sucre de canne cristallise avec une coloration qui va du blond au brun, due à des pigments présents uniquement dans la canne. Pour devenir blanc, le sucre roux de canne est refondu et débarrassé de ses colorants dans une raffinerie, sans modification chimique.
Lorsqu'il provient de la canne à sucre, le sucre roux est composé de 95 à 98 % de sucre (saccharose). Le sucre blanc lui, qui vient soit de la canne (après raffinage) soit de la betterave, contient plus de 99,7 % de saccharose. Le reste est constitué de traces d’eau, de minéraux et de matières organiques[42],[43].
En outre, le sucre complet (non raffiné) contient quarante fois plus d’éléments minéraux que le sucre roux de betterave et vingt fois plus d'éléments minéraux que le sucre roux de canne[44].
Cependant, l'apport en minéraux par le sucre, qu'il soit blanc ou roux, reste très minime au regard des portions de sucres réellement consommés et des apports nutritionnels conseillés pour ces minéraux, et ces types de sucre ont les mêmes effets sur le métabolisme[45].
En France, d'après l'enquête INCA2, les apports quotidiens en glucides (amidon et sucres) sont chez les adultes de 230 g/j en moyenne ; chez les enfants, ils sont de 207 g/j. Les adultes consomment 95 g/j de sucres totaux tandis que les enfants en consomment 99 g/j[46]. Les apports quotidiens recommandés en glucides sont de 200 à 250 grammes (voir Apports nutritionnels conseillés).
Une nouvelle étude a analysé les données INCA2 afin de connaitre la consommation en « sucres libres » (sucres ajoutés et sucres naturellement présents dans les jus de fruits), l'Organisation mondiale de la santé recommandant un apport inférieur à 10 % de la ration énergétique totale (50 g de sucre pour une ration énergétique de 2 000 Cal). La consommation de sucres libres, chez les adultes en France, est estimée à 51,9 g par jour en moyenne pour une ration énergétique moyenne de 2 151 Cal par jour, 41 % des adultes français dépassant la recommandation de l'OMS[47].
Au Canada, en 2004, les apports quotidiens moyens étaient de 110 g par jour[48], avec de fortes variations suivant l'âge et le sexe. Aux États-Unis, la consommation moyenne de sucres est proche de 120 g par jour[49].
Qu'il soit blanc ou complet, il contient toujours quatre kilocalories (4 kcal ou 4 Cal) par gramme, soit 16 760 joules. Consommé sans modération, il peut conduire au diabète, à l'obésité, et peut déséquilibrer la régulation du taux de glucose dans le sang par hyperglycémie. Les avis médicaux récents suggèrent une limitation de l'apport de sucres à un niveau beaucoup plus faible que la consommation effective (voir Sucre/Avis du corps médical).
Les glucides complexes ou polysaccharides sont généralement plus difficiles à décomposer au cours de la digestion que les glucides simples oses ou diholosides, de sorte qu'on les qualifie parfois de « sucres lents », tandis que les glucides simples sont qualifiés de « sucres rapides ». Un glucide complexe peut toutefois être plus rapide à digérer que certains glucides simples comme le fructose, de sorte que les nutritionnistes préfèrent se référer à l'indice glycémique des glucides[50].
L'ANSES rappelle en 2016 qu'à proprement parler le terme « sucres » (au pluriel) désigne seulement les glucides simples[51].
Les glucides sont plutôt à classer selon leur pouvoir « glycémiant », c'est-à-dire leur action sur la glycémie (taux de glucose dans le sang), ou plus récemment encore, selon la rapidité de la réaction insulinique qu'ils induisent[52].
La vitesse d'assimilation des glucides n'est pas liée à leur type : les glucides simples n’ont pas tous un indice glycémique élevé et les glucides complexes un indice glycémique faible. Par exemple, la pomme de terre est un féculent (source de glucides complexes) mais son index glycémique est élevé[53].
Un régime à faible indice glycémique est recommandé pour prévenir le diabète, les maladies cardiovasculaires et probablement l'obésité[54].
Le sucre ingéré est hydrolysé en glucose et fructose[55] dans l'intestin. Les monosaccharides sont ensuite absorbés soit par diffusion passive (transporteur de glucose et de fructose), soit par transport actif faisant intervenir des transporteurs spécifiques (transporteur sodium-glucose)[56]. Ces produits passent rapidement dans le sang puis sont véhiculés vers le foie et le reste de l'organisme. Le taux de glucose dans le sang (glycémie) est régulé par la production d'insuline ; le taux de fructose dans le sang n'est pas régulé. Le métabolisme du glucose est la glycogénogenèse qui intervient dans le foie pour reconstituer les réserves de glycogène. La glycolyse, à l'inverse, est le procédé métabolique permettant la dégradation du glucose en énergie. Le métabolisme du fructose prend place essentiellement dans le foie où il peut être transformé en glucose, lactate, glycogène et en triglycérides[57],[58].
Une étude[59] de la Harvard School of Public Health (États-Unis) a conclu que l’excès de glucose dans le sang est la cause de plus de trois millions de décès par an dans le monde, dont 960 000 directement à cause du diabète et 2,2 millions en raison de troubles cardiovasculaires (1,5 million de décès par infarctus du myocarde soit 21 % du total des infarctus) et 709 000 décès dus à un accident vasculaire cérébral (13 % du total des décès par AVC). Selon un commentaire paru dans la presse[60], . D'autres sources médicales soulignent le lien entre la consommation de boissons sucrées et les maladies cardiovasculaires[61].
Chez l'Homme, [62].
Le taux de glucose dans le sang est régulé par le pancréas :
On parle de diabète quand la glycémie à jeun est supérieure ou égale à 1,26 gramme par litre de sang (à deux reprises et en laboratoire)[62]. Selon l’Organisation mondiale de la santé, quelque 356 millions de personnes sont diabétiques en septembre 2012 dans le monde[63]. Le diabète de type 2 représente la majorité des diabètes dans le monde, et est en grande partie le résultat d’une surcharge pondérale et de la sédentarité[63]. La sur-consommation de sucres ajoutés en général ou de fructose et de boissons sucrées en particulier sont une des causes du diabète de type 2. La consommation de sucres à des niveaux inatteignables avec des produits naturels non préparés nourrit l'épidémie de diabète de type 2[64]. À ce titre réduire sa consommation de sucres ajoutés ou préférablement de fructose ajouté pourrait se traduire par une réduction de la mortalité due au diabète[64]. La consommation de nourriture à fort indice glycémique est associée au diabète de type 2[65]. La consommation de boissons sucrées augmente le risque de diabète[66],[67]. Par exemple, boire une à deux boissons sucrées par jour entraîne une augmentation de 26 % du risque de diabète de type 2[65]. Dans le monde, il est estimé que 133 000 morts du diabète sont imputables à la consommation de boissons sucrées[68].
L'excès de fructose semble constituer une cause de l'accumulation de graisse dans le foie[69] ou stéatose hépatique, qui peut conduire à une inflammation chronique du foie.
La carie est un problème qui peut être lié à la consommation répétée de glucides. En effet, ils favorisent la métabolisation d’acides par des bactéries, qui détruisent l’émail dentaire. Le facteur déterminant dans la formation des caries est moins la quantité que la fréquence et la durée de séjour en bouche du sucre absorbé, ainsi que la texture plus ou moins collante de l'aliment. Selon l'Agence française de sécurité sanitaire des aliments (AFSSA, devenue ANSES), les aliments contenant du saccharose ou de l'amidon interviennent dans la propagation des caries dentaires[70]. Elle préconise donc de limiter la consommation entre les repas de féculents (pâtes, pommes de terre, etc.), boissons et produits sucrés, et d'avoir une bonne hygiène bucco-dentaire.
Beaucoup d'études et d'experts scientifiques affirment que l'apport excessif en sucre et/ou en fructose joue un rôle important dans l'obésité et le diabète[71],[72],[73]. Plusieurs études établissent le lien entre la consommation de sucre et/ou de fructose et l'augmentation de la graisse intra-abdominale (ou viscérale)[74].
Par ailleurs, il semblerait que l’organisme comptabilise moins bien « l'énergie liquide » consommées en excès. Ainsi les boissons sucrées (jus de fruits, sodas, nectars, sirops…) régulièrement consommées pendant ou en dehors des repas, apporteraient un excès d'énergie préjudiciable à terme et constituent un facteur de risque d'obésité[75],[76],[77],[78],[66].
La réduction de la consommation de sucres réduit le poids et, inversement, l'augmentation de la consommation entraîne une prise de poids[79].
Une consommation d'une boisson sucrée par jour entraîne une prise de poids moyenne de 0,12 kg par an chez les adultes[65].
En 2010, l'Autorité européenne de sécurité des aliments n'a pas établi de relation directe entre consommation de sucres — en dehors d’apports caloriques excessifs — et prise de poids[80] en s'appuyant sur quatre études dont deux financées par les industriels du sucre[81]. Le rapport de l'EFSA est aussi critiqué du fait que la majorité des experts aient des liens avec l'industrie[81].
En revanche, l'EFSA recommande de favoriser les glucides complexes plutôt que les glucides simples dont le sucre.
Il arrive que l'industrie laisse penser que l'exercice physique est aussi important que l'alimentation. Par exemple, dans une de ses communications, Coca-Cola associe leur produit au sport en suggérant que ce n'est pas un problème de consommer leur boisson du moment que l'on fait de l'exercice. Or ce n'est pas corroboré par les données scientifiques puisqu'une synthèse des connaissances scientifiques a montré que réduire sa consommation de sucres est le plus efficace pour réduire le syndrome métabolique et que les bénéfices s'en font ressentir avant même la perte de poids[82].
L'ANSES a conclu en 2016 que les études d'intervention ainsi que les données épidémiologiques ne montrent pas d'association de la prise de poids avec la consommation de sucres lorsque l'apport énergétique est contrôlé[51].
La consommation importante de sucre, de produits sucrés pauvres en vitamines, sels minéraux et fibres, peut favoriser des carences nutritionnelles si par ailleurs l'alimentation est peu diversifiée.
La consommation de sucres est un facteur d'augmentation de l'indice de masse corporelle, qui favorise l'émergence de certains cancers (sein, côlon, pancréas, œsophage, utérus, rein, vésicule biliaire)[68]. De ce fait, la consommation de boissons sucrées serait responsable de 6 450 décès par cancers chaque année dans le monde[68].
Il existe aussi un lien direct entre syndrome métabolique et survenue du cancer du sein[83].
Une synthèse de onze études montre qu'une consommation d'aliments à indice glycémique élevé est associée à une augmentation de 6 % du risque de cancer du sein[84].
La consommation de fructose peut théoriquement engendrer des cancers du pancréas mais l'analyse de populations ne permet pas de corroborer cet effet[85].
En revanche, la consommation de fructose est responsable de carcinomes hépatocellulaires mais pour les autres cancers du foie, les conclusions sont contradictoires[85]. Les conclusions sont contradictoires quant à un lien entre consommation de sucre et cancer colorectaux[85].
De plus, un excès de consommation de ces produits pourrait favoriser l'obésité ou l'insulinorésistance qui, elles-mêmes, favoriseraient le risque de cancer[86].
La consommation de boissons sucrées a augmenté dans le monde au cours des dernières décennies. Leur impact sur la santé cardiométabolique a fait l’objet de nombreuses études et est aujourd’hui bien établi. Cependant, leur association avec le risque de cancer a été moins étudiée : très peu d’études prospectives ont été menées sur l’association entre les boissons sucrées et le risque de cancer. Pourtant, ces boissons ont été associées au risque d’obésité, à son tour reconnu comme un facteur de risque important pour de nombreux cancers. Des mécanismes inflammatoires ou liés au stress oxydant pourraient aussi intervenir, ce indépendamment du lien avec la prise de poids.
Cette étude, publiée le 10 juillet 2019 dans le BMJ (British Medical Journal), visait à étudier les associations entre la consommation de boissons sucrées et le risque de survenue de cancer. Au total, 101 257 participants de la cohorte française NutriNet-Santé (suivie entre 2009 et 2018) ont été inclus. La consommation alimentaire habituelle a été évaluée grâce à des enregistrements de 24 h répétés (6 en moyenne par participant) portant sur plus de 3 300 aliments différents (dont 109 types de boissons sucrées ou édulcorées)[87].
Le risque de maladie cardiovasculaire augmente en moyenne de 17 % par boisson sucrée supplémentaire consommée chaque jour[65].
Après prise en compte des autres facteurs de risque, il y a une augmentation moyenne de 16 % du risque d'accident vasculaire entre les plus gros consommateurs de boissons sucrées et les moins gros consommateurs[65]. D'après une autre étude, la mortalité par maladie cardiovasculaire est plus que doublée pour les personnes qui consomment plus de 25 % de leurs calories à partir de sucres ajoutés, par rapport aux personnes qui consomment moins de 10 % des calories à partir de sucres ajoutés[88].
Remplacer des graisses saturées par des glucides hautement raffinés ne fait pas diminuer le risque de maladie cardiovasculaire, alors que remplacer ces graisses par des graisses polyinsaturées fait diminuer le risque[65].
Chaque année, environ 45 000 décès par maladie cardiovasculaires dans le monde sont imputables aux boissons sucrées[68].
Une méta-analyse de 1995 conclut que le comportement des enfants n’est pas modifié par l’absorption de sucre[89].
Les résultats des études récentes sont contradictoires. Selon une étude américaine publiée dans le Journal of Biological Chemistry en décembre 2007, le sucre contribuerait au développement de la maladie d'Alzheimer[90]. Une autre étude parue en 2012 dans la revue Aging Cell a établi un effet protecteur du glucose vis-à-vis de la neurodégénerescence[91].
Les travaux scientifiques les plus récents concluent que le sucre présente un pouvoir addictif chez l'humain.
En 2007, une expérience menée sur des rats indique que les rats peuvent développer une addiction au sucre dans certaines circonstances, et qu'il est possible que ce genre de phénomène se produise aussi chez les humains[92].
En 2010, une revue d'études affirme qu'il n'y a pas de preuve d'addiction physique au sucre chez les humains et que le sucre ne joue pas de rôle dans les troubles des conduites alimentaires[93].
Chez le rat, une exposition prolongée au goût sucré (sous forme de sucre ou d'édulcorant) induit une dépendance caractérisée par des modifications comportementales et cérébrales comme celles des drogues dures[94]. Des expériences ont montré que des rats et des souris préfèrent la consommation d'eau sucrée à celle de cocaïne en intraveineuse[95]. Cela peut constituer un facteur explicatif de la tendance de l'industrie agroalimentaire à sucrer ses préparations[96].
Selon Serge Ahmed, directeur de recherche en neurosciences au CNRS, l'extrapolation de ces études à l’homme reste délicate et , il ajoute que le manque de données [94]. Trois ans plus tard, en 2013, le doute persiste avec une revue d'études menée par Serge Ahmed : [98]. En 2019, Serge Ahmed estime que l'addiction au sucre toucherait 5 à 10 % de la population aux États-Unis, au Canada et en Allemagne, et que son pouvoir addictif est comparable à celui de drogues dures comme l'alcool, la cocaïne, l'héroïne et les méthamphétamines. Par ailleurs, il observe que l'association entre sucre et matières grasses crée un stimulus gustatif puissant dans le cerveau[99].
Une revue des études sur l'addiction au sucre de 2016 affirme qu'il y a peu de preuves que le sucre crée une dépendance, et qu'il est plus rationnel de penser qu'il n'en crée pas. Il est expliqué aussi que lors des expériences sur les animaux, la dépendance apparait seulement si l'accès au sucre est intermittent[100].
Une revue systématique de 52 études liées aux « addictions alimentaires », publiée en 2018, conclut que la dépendance alimentaire existe, et suggère que certains aliments, notamment les aliments transformés contenant du sucre ou des matières grasses ajoutés, présentent le potentiel addictogène le plus élevé[101].
Une expérience menée sur 29 rats montre que les rats ayant un régime sucré ont eu des capacités mémorielles significativement inférieures à celles des rats ayant un régime sans sucre[102].
Une étude menée sur 737 portoricains de 45 à 75 ans a mesuré avec plusieurs tests les fonctions cognitives des participants et trouve un lien de corrélation entre la consommation de sucre et les mauvais résultats aux tests mais précise que la cause de ce lien est inconnue[103].
Plusieurs études suggèrent qu'une consommation élevée de sucre et/ou d'HFCS (donc de fructose) est associée à une moindre capacité d'apprentissage et/ou de mémorisation[104].
La consommation d'aliments sucrés est associée au développement de symptômes dépressifs[105]. Des analyses prospectives ont montré une augmentation à 5 ans de 23 % du nombre de personnes atteintes de troubles dépressifs chez les hommes consommant une quantité importante de sucre. Les études confirment un effet négatif de la consommation de sucre sur la santé psychologique à long terme[105]. Certaines études ont montré une corrélation hautement significative entre la consommation de sucre et le taux annuel de dépression dans six pays différents[106]. Une étude menée en Australie a montré que les individus buvant un demi-litre de soda sucré par jour avait environ 60 % plus de risques de développer des troubles dépressifs[107].
Il n'y a pas d'avis médical contre les glucides en général, mais la sous-catégorie du sucre fait depuis quelques années l'objet d'avis plus tranchés. En plus de l'effet incontestable sur les caries, plusieurs spécialistes associent soit le sucre soit le fructose avec l'épidémie d'obésité et de diabète de type 2. Une campagne se développe pour limiter la consommation de sucre aux États-Unis[108], en Australie[109] et au Royaume-Uni[110]. L'association de cardiologues American Heart Association fait le lien entre une consommation de sucre élevée et les maladies cardiovasculaires, et a récemment produit des recommandations pour limiter la consommation de sucre[111]. Les limites sont 20 g de sucres ajoutés par jour pour les femmes et 36 g pour les hommes (une canette de soda contient 33 g de sucre ajouté). Au Royaume-Uni, les autorités médicales conseillent clairement de diminuer la consommation de sucre[112],[113] et ont recommandé au Parlement d'introduire une taxe pour limiter la consommation de sucre[114]. En France, l'ANSES recommande depuis peu de réduire de 25 % la consommation de glucides simples[115],[116] (actuellement de 100 g environ par jour et par personne), tout en augmentant les glucides complexes. En 2004, le rapport exhaustif de l'ANSES sur les glucides ne donnait pas de recommandations sur les sucres simples[70].
En 2003, l'Organisation mondiale de la santé préconise de limiter les apports en sucres libres (sucres ajoutés + sucres des jus de fruits et sirops) à moins de 10 % des apports énergétiques, soit environ 50 g de sucres libres par jour pour un apport quotidien de 2 000 kcal/j[117]. En France, la consommation actuelle en sucres totaux est d’environ 100 g/j, dont environ la moitié de sucres libres, selon l’enquête INCA 2[46]. En 2014, une révision de la recommandation de l'OMS suggère une limitation à moins de 5 % des apports énergétiques, soit environ 25 g de sucre[118].
En France, les distributeurs automatiques de boissons sont interdits dans les écoles en 2005, et une taxe spécifique sur les boissons sucrées et/ou édulcorées est introduite en 2012[119]. Selon une étude commanditée par l'industrie des boissons, la taxe n'aurait pas eu l'effet recherché[120]. En 2018, la taxe sur les boissons sucrées a été triplée[121], celle sur les boissons édulcorées a été baissée[122].
D'autres pays ont introduit une taxe sur les boissons sucrées comme le Mexique[123] (un des pays les plus touchés par le diabète de type II dans le monde, et où la consommation de sodas est la plus élevée), la ville de Berkeley en Californie[124], et le Royaume-Uni pourrait le faire dans les années qui viennent[125].
Dès les années 1950, la Sugar Research Foundation (SRF), une organisation industrielle fondée en 1943, était consciente du rôle du sucre dans les caries. Mais elle va sélectionner les recherches à financer pour éviter que les restrictions sur le sucre soit un moyen de contrôler les caries. Entre 1967 et 1970, la SRF va financer, avec les industries du chocolat et des bonbons, le projet 269 visant à rendre la bactérie Streptococcus mutans moins destructive pour les dents après que du sucre a été consommé. Ce même projet visera également à développer un vaccin contre les caries pour que les gens puissent continuer à consommer du sucre. Ces recherches ne donneront finalement pas de résultat concluant. Influencé par l'industrie, le National Institute of Dental Research des États-Unis, va financer très peu de recherche pour étudier le risque de carie associé à chaque aliment[126].
Des documents révélés en 2013 ont montré que l'industrie du sucre a cherché à « forger l'opinion publique » dès les années 1970 pour minorer les craintes d'effets du sucre sur la santé. En 1977, la Sugar Association a réservé 230 000 dollars pour financer des recherches, notamment des scientifiques dans de prestigieuses universités américaines. Les fonds provenaient de diverses industries dont Coca-Cola, General Foods ou General Mills[127].
En 2006, à la suite de travaux de l'Organisation mondiale de la santé (OMS) pour promouvoir une limite de 10 % de calories issues de sucres, une campagne de lobbying aux États-Unis a visé les sénateurs d'États producteurs de sucre et de sirop de maïs pour menacer l'OMS de couper ses fonds[128].
Un lobbying de la World Sugar Research Organisation, une organisation regroupant des intérêts économiques (dont Coca-Cola), a bloqué avec succès une recommandation de 2003 conjointe entre l'OMS et l'Organisation des Nations unies pour l'alimentation et l'agriculture (FAO). Les recommandations quantitatives qu'elle contenait ont été remplacées par des limites non spécifiques[126].
Coca-Cola a financé le Global Energy Balance Network (en) dont les chercheurs considéraient que le manque d'exercice, plutôt que la consommation de calories, était responsable de l'obésité, à l'opposé des conclusions scientifiques[128],[82].
Les chercheurs recevant des financements de l'industrie du sucre ont tendance à avoir des conclusions à allant plus en faveur de l'industrie[129]. Par exemple, une analyse de 88 études sur la consommation de sodas a montré que les études financées par l'industrie trouvaient une taille d'effet quasi nulle pour la prise d'énergie, alors que les études non financées par l'industrie trouvaient une taille d'effet modérée[130]. D'autres chercheurs ont étudié les différentes synthèses réalisées sur le lien entre consommation de boissons sucrées et gain de poids. Parmi 18 résultats de ces synthèses, 12 n'avaient pas de lien mentionné avec l'industrie et 10 considéraient que la consommation de sodas pouvait être un facteur de risque pour la prise de poids. À l'inverse, parmi les 6 financées par l'industrie, 5 concluaient que les preuves n'étaient pas suffisantes pour soutenir un tel lien. Les synthèses dont les auteurs avaient un conflit d'intérêts avaient donc cinq fois plus de chance d'avoir une conclusion allant dans le sens de l'industrie[131].
Une étude de 2016 a révélé que l'industrie du sucre, à travers la Sugar Research Foundation, rebaptisée depuis « Sugar Association (en) », a financé des recherches afin de minorer les effets du sucre sur les maladies cardiovasculaires et de reporter la faute sur les graisses saturées[132].
Selon le journaliste Michael Moss (en), le 8 avril 1999, les dirigeants des onze plus grandes entreprises agroalimentaires américaines se réunissent dans l'auditorium de la Pillsbury Company à Minneapolis pour fixer le cap de leur secteur dans les années à venir. Michael Mudd, vice-président de Kraft Foods, les alerte sur l'image négative de leur groupe auprès des institutions liées à la santé publique et des organismes de recherche qui les jugent en partie responsables de l'épidémie d'obésité qui touche le pays, due à . Il recommande de diminuer l'incorporation de sel, de sucre et de matière grasse dans la nourriture industrielle. Le PDG de General Mills, Stephen Sanger (en), rejette cette responsabilité et encourage ses pairs à faire de même. Les céréales de petit-déjeuner sucrées que produisent son groupe sont régulièrement condamnées par les associations de consommateurs, mais il considère que les produits qu'il fabrique répondent aux souhaits des consommateurs, qui selon lui se préoccuperaient plus du goût que des qualités nutritionnelles des aliments qu'ils achètent[133].
Par photosynthèse, les plantes produisent du glucose ou éventuellement d’autres sucres, comme le fructose. Ces sucres sont majoritairement transportés dans la sève des plantes sous forme de saccharose. Suivant les plantes, le saccharose est ensuite stocké comme réserve énergétique sans modification (ex. : canne, betterave sucrière) ou bien est modifié et transformé en amidon (ex. : pommes de terre, céréales)[134].
Le glucose en solution est essentiellement sous cette forme cyclique avec moins de 0,1 % des molécules sous forme de chaîne ouverte.
Les oses peuvent se grouper par liaisons covalentes osidiques et former des diholosides tels que saccharose (sucrose), ou former des polyosides tels que l’amidon. Les liaisons osidiques doivent être hydrolysées (c’est-à-dire qu’une molécule d’eau vient « casser » ou rompre le lien.) Cette réaction est catalysée par une enzyme (protéine) pour que les molécules puissent être métabolisées. Après digestion et absorption par un animal, les oses présents dans le sang et les tissus sont le glucose, le fructose, et le galactose.
Le préfixe « glyco- » indique la présence de sucre dans une substance non glucidique : par exemple, une glycoprotéine est une protéine à laquelle un ou plusieurs oses se sont connectés. De même, un glycolipide est un lipide lié à des résidus osidiques.
Fructose, glucose, galactose et mannose sont des sucres simples (oses) de formule C6H12O6.
Parmi les diholosides, les plus courants sont le saccharose (sucre de canne ou de betteraves, formé d’un glucose et d’un fructose), le lactose (un glucose et un galactose) et le maltose (deux glucoses). La formule de ces diholosides est C12H22O11.
En industrie, le saccharose peut être hydrolysé pour obtenir une solution contenant du fructose, du glucose et du saccharose et appelée « sucre inverti », utilisée en confiserie et en pâtisserie.
Le sucre entre dans la composition de nombreuses recettes, notamment en pâtisserie.
Mélangé à de l'eau et cuit, il devient du caramel.
Le sucre ne périme jamais car il ne contient pas d'eau et les bactéries ne peuvent pas se développer. Conserver le sucre dans un endroit frais et sec permet de le stocker très longtemps[135].
En janvier 2018, la Novège a augmenté la taxe sur les aliments sucrés de 83 % ; elle s'est alors élevée à 36,92 couronnes (environ 3,7 €) par kilogramme. En conséquence, la dépendance au sucre a diminué, et la consommation norvégienne de confiserie, de 5 kilogrammes par personne en 1960 et 15 kg en 2008, a été réduite à 12 kilogrammes par personne en 2018. La consommation de boissons sucrées est passée de 93 litres à la fin des années 1990 à 47 litres par personne en 2018. La branche aliment-boisson de la Confédération des entreprises norvégiennes (en) milite pour la suppression de la taxe sur le sucre[136]. La mesure a été abrogée un peu plus tard[137].
« The existing basic science evidence, observational data, and clinic trial findings suggest that reducing consumption of added sugars, particularly added fructose, could translate to reduced diabetes-related morbidity and potentially premature mortality. […] At current levels, sugar consumption and fructose consumption in particular—in concentrations and contexts not seen in natural whole foods—are fueling a worsening epidemic of type 2 diabetes. Even without existing data for the duration of diabetes’ 20-year incubation period, shorter-term basic science evidence, observational data, and clinical trial findings present compelling evidence to suggest that added sugar and especially added fructose (provided from HFCS and sucrose) present a serious and increasing public health problem. »« Robust data from systematic reviews and high-quality randomized controlled trials (RCTs) support a harmful effect of highly refined, high–glycemic load (GL) carbohydrates. A meta-analysis of observational studies indicated that high–glycemic index (GI) foods are associated with T2DM.[…]T2DM risk in individuals with the highest GL and lowest cereal fiber is 2.5-fold that of those with the lowest GL and highest cereal fiber diet. […] A meta-analysis of 310,819 participants and 15,043 cases of T2DM reported a 26% increased T2DM risk among those consuming 1 to 2 SSB servings/day compared with nonconsumers. »« Coca Cola, who spent $3.3 billion on advertising in 2013, pushes a message that ‘all calories count’; they associate their products with sport, suggesting it is ok to consume their drinks as long as you exercise. However science tells us this is misleading and wrong. It is where the calories come from that is crucial. Sugar calories promote fat storage and hunger. Fat calories induce fullness or ‘satiation’. causation. A recently published critical review in nutrition concluded that dietary carbohydrate restriction is the single most effective intervention for reducing all the features of the metabolic syndrome and should be the first approach in diabetes management, with benefits occurring even without weight loss. »Sur les autres projets Wikimedia :Différents sucres :
Édulcorants :
Sucre et santé :
Divers :
Ne doit pas être confondu avec Arpentage.
La géométrie est à l'origine la branche des mathématiques étudiant les figures du plan et de l'espace (géométrie euclidienne). Depuis la fin du XVIIIe siècle, la géométrie étudie également les figures appartenant à d'autres types d'espaces (géométrie projective, géométrie non euclidienne ). 
Depuis le début du XXe siècle, certaines méthodes d'étude de figures de ces espaces se sont transformées en branches autonomes des mathématiques : topologie, géométrie différentielle et géométrie algébrique. Si l'on veut englober toutes ces acceptions, il est difficile de définir ce qu'est, aujourd'hui, la géométrie. C'est que l'unité des diverses branches de la « géométrie contemporaine » réside plus dans des origines historiques que dans une communauté de méthodes ou d'objets. 
Le terme géométrie dérive du grec de γεωμέτρης (geômetrês) qui signifie « géomètre, arpenteur » et vient de γῆ (gê) « terre » et μέτρον (métron) « mesure ». Ce serait donc « la science de la mesure du terrain ».
Sans qualificatif particulier et sans référence à un contexte particulier (par opposition à la géométrie différentielle ou la géométrie algébrique), la géométrie ou encore géométrie classique englobe principalement :
Les géométries ci-dessus peuvent être généralisées en faisant varier la dimension des espaces, en changeant le corps des scalaires (utiliser des droites différentes de la droite réelle) ou en donnant une courbure à l'espace. Ces géométries sont encore dites classiques.
Par ailleurs, la géométrie classique peut être axiomatisée ou étudiée de différentes façons : 
Il est remarquable que l'algèbre linéaire (espaces vectoriels, formes quadratiques, formes bilinéaires alternées, formes hermitiennes et antihermitiennes, etc.) permette de construire des modèles explicites de la plupart des structures rencontrées dans ces géométries. Cela confère donc à la géométrie classique une certaine unité.
Il y a des branches des mathématiques qui sont issues de l'étude des figures des espaces euclidiens, mais qui se sont constituées en branches autonomes des mathématiques et qui étudient des espaces qui ne sont pas nécessairement plongés dans des espaces euclidiens :
Les différents espaces de la géométrie classique peuvent être étudiés par la topologie, la géométrie différentielle et la géométrie algébrique.
La géométrie admet de nombreuses acceptions selon les auteurs. Dans un sens strict, la géométrie est « l'étude des formes et des grandeurs de figures »[1]. Cette définition est conforme à l'émergence de la géométrie en tant que science sous la civilisation grecque durant l'époque classique. Selon un rapport de Jean-Pierre Kahane[2], cette définition coïncide avec l'idée que se font les gens de la géométrie comme matière enseignée : c'est « le lieu où on apprend à appréhender l'espace ».
En 1739, Leonhard Euler étudie le problème des sept ponts de Königsberg ; ses travaux sont considérés comme l'un des premiers résultats de géométrie ne dépendant d'aucune mesure, des résultats qu'on qualifiera de topologiques. Les questions posées durant le XIXe siècle ont conduit à repenser les notions de forme et d'espace, en écartant la rigidité des distances euclidiennes. Il a été envisagé la possibilité de déformer continûment une surface sans préserver la métrique induite, par exemple de déformer une sphère en un ellipsoïde. Étudier ces déformations a conduit à l'émergence de la topologie[réf. nécessaire] : ses objets d'étude sont des ensembles, les espaces topologiques, dont la notion de proximité et de continuité est définie ensemblistement par la notion de voisinage. Selon certains mathématiciens, la topologie fait pleinement partie de la géométrie, voire en est une branche fondamentale. Cette classification peut être remise en cause par d'autres.
Selon le point de vue de Felix Klein (1849-1925), la géométrie analytique « synthétisait en fait deux caractères ultérieurement dissociés : son caractère fondamentalement métrique, et l'homogénéité »[3]. Le premier caractère se retrouve dans la géométrie métrique, qui étudie les propriétés géométriques des distances. Le second est au fondement du programme d'Erlangen, qui définit la géométrie comme l'étude des invariants d'actions de groupe.
Les travaux actuels, dans des domaines de recherche portant le nom de géométrie, tendent à remettre en cause la première définition donnée. Selon Jean-Jacques Szczeciniarcz[4], la géométrie ne se construit pas sur « la simple référence à l'espace, ni même [sur] la figuration ou [sur] la visualisation » mais se comprend à travers son développement : « la géométrie est absorbée mais en même temps nous parait attribuer un sens aux concepts en donnant par ailleurs l'impression d'un retour au sens initial ». Jean-Jacques Sczeciniarcz relève deux mouvements dans la recherche mathématique qui a conduit à un élargissement ou à un morcellement de la géométrie : 
Dans le prolongement, la géométrie peut être abordée non plus comme une discipline unifiée mais comme une vision des mathématiques ou une approche des objets. Selon Gerhard Heinzmann[5], la géométrie se caractérise par « un usage de termes et de contenus géométriques, comme « points », « distance » ou « dimension » en tant que cadre langagier dans les domaines les plus divers », accompagné par un équilibre entre une approche empirique et une approche théorique.
L'invention de la géométrie remonte à l'Égypte antique[6].
Pour Henri Poincaré[7], l’espace géométrique possède les propriétés suivantes :
Les géométries euclidienne et non euclidienne correspondent à cette définition stricto sensu de l'espace. Construire une telle géométrie consiste à énoncer les règles d'agencement des quatre objets fondamentaux : le point, la droite, le plan et l'espace. Ce travail reste l'apanage de la géométrie pure qui est la seule à travailler ex nihilo.
La géométrie plane repose d'abord sur une axiomatique qui définit l'espace ; puis sur des méthodes d'intersections, de transformations et de constructions de figures (triangle, parallélogramme, cercle, sphère, etc.).
La géométrie projective est la plus minimaliste, ce qui en fait un tronc commun[8] pour les autres géométries. Elle est fondée sur des axiomes :
Distinguer dans la géométrie projective des éléments impropres caractérise la géométrie arguésienne. Puis la géométrie affine naît de l'élimination de ces éléments impropres. Cette suppression de points crée la notion de parallélisme puisque désormais certaines paires de droites coplanaires cessent d'intersecter. Le point impropre supprimé est assimilable à la direction de ces droites. De plus, deux points ne définissent plus qu'un segment (celui des deux qui ne contient pas le point impropre) et rend familière la notion de sens ou orientation (c'est-à-dire, cela permet de distinguer  de [9]).
Le cinquième axiome ou « postulat de parallèles » de la géométrie d'Euclide fonde la géométrie euclidienne :
Par un point extérieur à une droite, il passe toujours une parallèle à cette droite, et une seule.
Voir l'axiomatique de Hilbert ou les Éléments d'Euclide pour des énoncés plus complet de la géométrie euclidienne.
La réfutation de ce postulat a conduit à l'élaboration de deux géométries non euclidiennes : la géométrie hyperbolique par Gauss, Lobatchevski, Bolyai et la géométrie elliptique par Riemann.
Dans la conception de Felix Klein (auteur du programme d'Erlangen), la géométrie est l'étude des espaces de points sur lesquels opèrent des groupes de transformations (appelées aussi symétries) et des quantités et des propriétés qui sont invariantes pour ces groupes. Le plan et la sphère, par exemple, sont l'un comme l'autre des espaces de dimension 2, homogènes (pas de point privilégié) et isotropes (pas de direction privilégiée), mais ils diffèrent par leurs groupes de symétrie (le groupe euclidien pour l'un, le groupe des rotations pour l'autre)[10].
Parmi les transformations les plus connues, on retrouve les isométries, les similitudes, les rotations, les réflexions, les translations et les homothéties.
Il ne s'agit donc pas d'une discipline mais d'un important travail de synthèse qui a permis une vision claire des particularités de chaque géométrie. Ce programme caractérise donc plus la géométrie qu'il ne la fonde. Il eut un rôle médiateur dans le débat sur la nature des géométries non-euclidiennes et la controverse entre géométries analytique et synthétique.
Il y a en géométrie différentielle et en géométrie algébrique des groupes de Lie et des groupes algébriques, qui eux ont des espaces homogènes, et la géométrie classique se ramène souvent à l'étude de ces espaces homogènes. Les géométries affine et projective sont liées aux groupes linéaires, et les géométries euclidienne, sphérique, elliptique et hyperbolique sont liées aux groupes orthogonaux.
Lorsqu'il y a des classifications explicites des groupes de Lie ou algébriques ou des leurs espaces homogènes vérifiant certaines hypothèses (groupes de Lie ou algébriques simples, espaces symétriques, variétés de drapeaux généralisées, espaces de courbure constante, par exemple), les principaux éléments de ces classifications sont parfois issus de la géométrie classique, et les groupes auxquels sont associés ces géométries classiques sont liés aux groupes dits classiques (groupes linéaires, orthogonaux, symplectiques, par exemple).
La plupart des géométries classiques sont liées aux groupes de Lie ou algébriques simples, dit classiques (ils sont issus de l'algèbre linéaire). Il y a d'autres groupes de Lie ou algébriques simples, et ils sont dits « exceptionnels » et ils donnent lieu à la géométrie exceptionnelle, avec certaines analogies avec la géométrie classique. Cette distinction est due au fait que les groupes simples sont (sous certaines hypothèses) classés en plusieurs séries infinies (souvent quatre) et en un nombre fini d'autres groupes (souvent cinq), et ce sont ces derniers groupes qui sont exceptionnels, et ils ne relèvent pas de l'algèbre linéaire (du moins pas de la même manière) : ils sont souvent liés à des structures algébriques non associatives (algèbres d'octonions, algèbres de Jordan exceptionnelles, par exemple).
Aux groupes de Lie ou algébriques simples sont associés des diagrammes de Dynkin (des sortes de graphes), et certaines propriétés de ces géométries peuvent se lire dans ces diagrammes.
La géométrie riemannienne peut être vue comme une extension de la géométrie euclidienne. Son étude porte sur les propriétés géométriques d'espaces (variétés) présentant une notion de vecteurs tangents, et équipés d'une métrique (métrique riemannienne) permettant de mesurer ces vecteurs. Les premiers exemples rencontrés sont les surfaces de l'espace euclidien de dimension 3 dont les propriétés métriques ont été étudiées par Gauss dans les années 1820. Le produit euclidien induit une métrique sur la surface étudiée par restriction aux différents plans tangents. La définition intrinsèque de métrique fut formalisée en dimension supérieure par Riemann. La notion de transport parallèle autorise la comparaison des espaces tangents en deux points distincts de la variété : elle vise à transporter de manière cohérente un vecteur le long d'une courbe tracée sur la variété riemannienne. La courbure d'une variété riemannienne mesure par définition la dépendance éventuelle du transport parallèle d'un point à un autre par rapport à la courbe les reliant.
La métrique donne lieu à la définition de la longueur des courbes, d'où dérive la définition de la distance riemannienne. Mais les propriétés métriques des triangles peuvent différer de la trigonométrie euclidienne. Cette différence est en partie étudiée à travers le théorème de Toponogov, qui permet de comparer du moins localement la variété riemannienne étudiée à des espaces modèles, selon des inégalités supposées connues sur la courbure sectionnelle. Parmi les espaces modèles :
La géométrie complexe porte sur les propriétés d'espaces pouvant localement s'identifier à . Ces objets (variété complexe) présentent une certaine rigidité, découlant de l'unicité d'un prolongement analytique d'une fonction à plusieurs variables.
La géométrie symplectique est une branche de la géométrie différentielle et peut être introduite comme une généralisation en dimension supérieure de la notion d'aire orientées rencontrée en dimension 2. Elle est liée aux formes bilinéaires alternées. Les objets de cette géométrie sont les variétés symplectiques, qui sont des variétés différentielles munie d'un champ de formes bilinéaires alternées. Par exemple, un espace affine attaché à un espace vectoriel muni d'une forme bilinéaire alternée non dégénérée est une variété symplectique.
La géométrie de contact est une branche de la géométrie différentielle qui étudie les variétés de contact, qui sont des variétés différentielles munies d'un champ d'hyperplans des espaces tangents vérifiant certaines propriétés. Par exemple, l'espace projectif déduit un espace vectoriel muni d'une forme bilinéaire alternée non dégénérée est une variété de contact.
Longtemps, géométrie et astronomie ont été liées. À un niveau élémentaire, le calcul des tailles de la lune, du Soleil et de leurs distances respectives à la Terre fait appel au théorème de Thalès[réf. nécessaire]. Dans les premiers modèles du système solaire, à chaque planète était associé un solide platonicien. Depuis les observations astronomiques de Kepler, confirmées par les travaux de Newton, il est prouvé que les planètes suivent une orbite elliptique dont le Soleil constitue un des foyers. De telles considérations de nature géométrique peuvent intervenir couramment en mécanique classique pour décrire qualitativement les trajectoires.
En ce sens, la géométrie intervient en ingénierie dans l'étude de la stabilité d'un système mécanique. Mais elle intervient encore plus naturellement dans le dessin industriel. Le dessin industriel montre les coupes ou les projections d'un objet tridimensionnel, et est annoté des longueurs et angles. C'est la première étape de la mise en place d'un projet de conception industrielle. Récemment, le mariage de la géométrie avec l'informatique a permis l'arrivée de la conception assistée par ordinateur (CAO), des calculs par éléments finis et de l'infographie.
La trigonométrie euclidienne intervient en optique pour traiter par exemple de la diffraction de la lumière. Elle est également à l'origine du développement de la navigation : navigation maritime aux étoiles (avec les sextants), cartographie, navigation aérienne (pilotage aux instruments à partir des signaux des balises).
Les nouvelles avancées en géométrie au XIXe siècle trouvent des échos en physique. Il est souvent dit que la géométrie riemannienne a été initialement motivée par les interrogations de Gauss sur la cartographie de la Terre. Elle rend compte en particulier de la géométrie des surfaces dans l'espace. Une de ses extensions, la géométrie lorentzienne, a fourni le formalisme idéal pour formuler les lois de la relativité générale. La géométrie différentielle trouve de nouvelles applications dans la physique post-newtonienne avec la théorie des cordes ou des membranes.
La géométrie non commutative, inventée par Alain Connes, tend à s'imposer pour présenter les bonnes structures mathématiques avec lesquelles travailler pour mettre en place de nouvelles théories physiques.
La géométrie occupe une place privilégiée dans l'enseignement des mathématiques. De nombreuses études pédagogiques prouvent son intérêt[réf. souhaitée] : elle permet aux élèves de développer une réflexion sur des problèmes, de visualiser des figures du plan et de l'espace, de rédiger des démonstrations, de déduire des résultats d'hypothèses énoncées. Mais plus encore, « le raisonnement géométrique est beaucoup plus riche que la simple déduction formelle », car il s'appuie sur l'intuition née de l'« observation des figures ».
Dans les années 1960, l'enseignement des mathématiques en France insistait sur la mise en pratique des problèmes relevant de la géométrie dans la vie courante. En particulier, le théorème de Pythagore était illustré par la règle du 3, 4, 5 et son utilisation en charpenterie[11]. Les involutions, les divisions harmoniques, et les birapports étaient au programme du secondaire. Mais la réforme des mathématiques modernes, née aux États-Unis et adaptée en Europe, a conduit à réduire considérablement les connaissances enseignées en géométrie pour introduire de l'algèbre linéaire dans le second degré. Dans de nombreux pays, cette réforme fut fortement critiquée et désignée comme responsable d'échecs scolaires[réf. souhaitée]. Un rapport de Jean-Pierre Kahane[2] dénonce le manque d'« une véritable réflexion didactique préalable » sur l'apport de la géométrie : en particulier, une « pratique de la géométrie vectorielle » prépare l'élève à une meilleure assimilation des notions formelles d'espace vectoriel, de forme bilinéaire…
L'utilisation des figures dans l'enseignement d'autres matières permet de mieux faire comprendre aux élèves les raisonnements exposés. N.B. En didactique des Mathématiques, on fait habituellement la différence entre les notions de « dessin » (réalisé avec des instruments comme règle, compas…), de « schéma » (réalisé à main levée et servant de support concret au raisonnement abstrait à effectuer) et de « figure » (objet géométrique abstrait sur lequel porte en définitive le raisonnement, et dont chacun possède sa propre représentation mentale : par exemple on peut avoir une représentation mentale différente, à une similitude près, de la « figure » triangle équilatéral). Avec ces distinctions, ce qui est représenté graphiquement évoquerait donc une « figure », mais n'en serait pas une.[réf. souhaitée].
Pour les articles homonymes, voir Vis.
Une vis de fixation, appelée communément vis, est une pièce mécanique, comportant une tige filetée et une tête ; elle est destinée à réaliser la fixation d'une ou de plusieurs pièces par pression. La fixation par vis crée une liaison plan sur plan démontable, par placage précontraint des deux pièces à assembler. Tant que les efforts de traction appliqués sur la liaison n'excèdent pas la tension exercée au repos par les vis (dite ), l'assemblage bénéficie de la raideur des pièces assemblées.
Les premières apparitions connues d'un organe mécanique utilisant une surface hélicoïdale remontent à la vis d'Archimède, (mais il s'agit d'une utilisation dynamique de type « hélice »). Il fallut attendre la Renaissance pour voir des vis comme , dans les horloges, les machines de guerre et d'autres constructions mécaniques.
Léonard de Vinci développa alors des méthodes pour leur usinage. Toutefois, elles continueront à être fabriquées à la main et sans normalisation, même après le début de la Révolution industrielle.
Les vis sont fabriquées industriellement depuis le XIXe siècle. Les vis fabriquées au XIXe siècle étaient à tête fendue. Au début du XXe siècle, pour éviter que le tournevis ne glisse sur la tête de la vis, le Canadien Peter L. Robertson inventa la vis à tête carrée. Au même moment, un Américain inventa la vis à tête étoilée pour résoudre le même problème.
La vis à tête carrée est très populaire au Canada alors que la vis à tête étoilée est très populaire ailleurs dans le monde. Cependant, aucun type de vis n'a su s'imposer complètement et on retrouve tous les types de vis dans tous les pays.
Les vis de fixation sont constituées des parties suivantes :
Les vis de fixation sont habituellement fabriquées à base d'acier. Lorsqu'une grande résistance au temps ou à la corrosion est requise, comme pour les petites vis de fixation ou implants médicaux, des matériaux tels que l'acier inoxydable, le laiton, le titane, et le bronze peuvent être utilisés.
Les vis de fixation se divisent en deux grands groupes selon le mode de pression :
Les vis d'assemblage sont les plus fréquentes, elles traversent généralement les pièces assemblées. La liaison se fait  pour une translation et deux rotations (appui plan sur plan) et  pour la rotation autour de l'axe de la vis et deux translations dans le plan, sauf pour les vis à tête fraisée ou équivalentes qui reprennent ces translations en réalisant une sorte de . Quand les efforts de cisaillement excèdent les capacités de reprise par frottement, on complète la liaison d'un pion de centrage (deux pions s'il faut reprendre aussi le moment de torsion). Les vis ajustées permettent d'assurer le rôle des pions. Dans ce cas les perçages doivent être effectués après présentation des pièces en vis-à-vis.
Quand la vis traverse les pièces assemblées et que le serrage est obtenu par l'action conjuguée d'une vis et d'un écrou, on parle de liaison boulonnée (un boulon = une vis + un écrou). Sinon la vis peut se serrer directement dans une des pièces à assembler qui présente alors un trou taraudé, éventuellement équipé d'un insert (filets rapportés).
Les différentes vis se subdivisent selon la forme de l'extrémité, qui permet un plus ou moins bon guidage pour l'introduction dans le trou taraudé :
La tête de la vis est un élément fonctionnel indispensable pour les vis d'assemblage, puisque c'est cette partie qui maintient la pression, ce qui n'est pas le cas des vis de pression, qui bien souvent n'ont pas de tête. Toutefois, la tête assure une deuxième fonction : celle de permettre l'application d'un couple de serrage, au moyen d'un outil approprié : tournevis ou clef.
Les formes choisies vont déterminer les différents types d'entraînement :
Indépendamment du système d’entraînement, il existe plusieurs formes de tête de vis, entre autres : vis à tête fraisée et plate ; vis à tête fraisée bombée ; vis à tête bombée ; vis à tête cylindrique ; vis à tête plate ; vis trompette…
Ces vis sont parfois également appelées HC, ou CHC (cylindriques hexagonales creuses) ou BTR.
4 tailles actuellement identifiées : 2 mm, 2,3 mm, 2,6 mm et 3 mm.
Rencontrées dans des productions asiatiques.
Les vis Torx sont caractérisées par leur empreinte sur la tête de vis en forme d'étoiles à six branches. Elles peuvent être préférées aux vis traditionnelles cruciformes pour leur meilleur prise avec une clé.
La particularité de cette vis est d'offrir un entraînement crénelé sous tête et donc une face plate pouvant être décorée ou peinte sans risque que le décor ne soit abîmé par le tournevis. Ce type de vis nécessite un outil spécifique pour le vissage.
Ces vis possèdent la particularité de ne pas avoir l'empreinte sur la tête mais du côté du filet. Elles sont utilisées pour les assemblages aéronautiques, où elles permettent un serrage de l'écrou et le maintien de la vis d'un seul côté et simultanément. Cette propriété est particulièrement intéressante pour les assemblages d'avion, où la taille des structures ne permet pas à un seul opérateur de maintenir la tête de vis et de serrer l'écrou.
L'empreinte ASTER a été développée par LISI Aerospace, et introduite au début des années 2010, en remplacement des vis à empreinte hexagonale, qui n'offrent pas une résistance en couple suffisante lors du serrage des structures composites.
Vis auto-taraudeuse.
Vis à bois moderne.
Vis auto-foreuse.
Boulon (vis + écrou).
Vis diverses.
Diverses vis pour boîtier de PC.
Il existe deux types de vis auto-taraudeuse : vis à bout pointu (type A), utilisée pour les tôles minces (d'épaisseur inférieure à 5 mm, environ), et vis à bout plat (type B), utilisée pour les tôles (ou supports) plus épais (ép. > 5 mm). Comme son nom l'indique, la vis auto-taraudeuse est destinée à tarauder le trou dans le support qu'elle va assembler. Avant la mise en œuvre, les éléments sont simplement percés et la vis va tarauder l'orifice. Il est donc inutile d'avoir un taraud, ou un écrou, pour ce type de vis: ceci permet un gain de temps lors de la mise en œuvre. Aujourd'hui[Quand ?], les vis auto-taraudeuses sont largement utilisées dans les différents domaines de l'industrie et de la construction.
La pièce réceptrice comporte un trou non fileté, appelé avant-trou, ou n'en comporte pas: le filet de la vis et son extrémité sont adaptés pour réaliser le taraudage à la première utilisation (extrémité pointue, filetage conique en bout, filet tranchant, etc.). Dans ce groupe, on trouve : la vis à bois traditionnelle, dont les tire-fonds, et les vis à bois et aggloméré, qui tendent à les remplacer ; et la vis à panneaux de plâtre, ou « vis en trompette ».
Les vis à métaux ne peuvent être utilisées sans un écrou ou une pièce taraudée avec le bon filetage. La tige filetée est de forme cylindrique et le filetage est identique du début à la fin de la vis.
Une variante, la vis à collet carré, possède une section carrée à la base de sa tige sous la tête. Elle s'utilise exclusivement avec un écrou et une rondelle.
La vis n'est filetée qu'en partie. La partie épaulée est destinée à rester à l’extérieur de la pièce réceptrice. Cette vis peut donc assurer à la fois une mise en position précise et un maintien en position ; cela permet d'éviter d'utiliser un pion de positionnement.
Il existe aussi des vis autoperceuses, dont l'extrémité a la fonction d'un foret. Elles permettent de visser directement dans l'acier ou l'aluminium sans avoir à effectuer d'opérations de perçage, ni de taraudage préalable. Ces vis percent jusqu'à 16 mm d'acier. Elles peuvent être en acier, en inox ou bimétal (le corps en inox et l'extrémité en acier) type SX
Les vis autoperceuses sont essentiellement utilisées dans le bâtiment :
et dans le transport pour la fabrication des planchers de camion.
La désignation des vis permet d'identifier le type de filetage, le diamètre de la vis et la classe de résistance (dite classe de qualité) du matériau utilisé. Par exemple, pour la classe de qualité 8.8, le premier 8 indique une résistance à rupture Rm supérieure ou égale à 800 MPa (8×100) et le second 8 que la limite élastique vaut 80 % de Rm (8×10 %, ou bien 8×0,1), en l'occurrence Re = 0,8×800 = 640 MPa. Ces vis sont plus ductiles (moins cassantes) que des vis de classe 10.9 ou 12.9 en cas de choc.
Il ne serait pas raisonnable de représenter une vis en respectant les mêmes règles que pour les autres pièces. La figure ci-dessous, réalisée par ordinateur, montre ce que donnerait la représentation explicite d'une vis : non seulement cela prendrait beaucoup de temps, mais de plus, pour les vis de petite taille, d’usage courant, cela deviendrait illisible.
Représentation réaliste d'une vis à métaux tête hexagonale.
Représentation normalisée d'une vis
En dessin technique, la vis est représentée de manière normalisée. La convention utilisée représente le creux du filet par une ligne fictive (trait continu fin) situé du côté de la matière. Un trait fort traversant la tige indique la position du dernier filet complet, équivalent à une surface de butée contre laquelle s'arrêtera l'écrou. Ici, une vis à tête hexagonale.
Les vis sont utilisées pour :
Pour chaque utilisation, et plus spécialement pour les vis d'assemblage le dimensionnement sera pris en compte dès la conception de l'assemblage. L'inventaire et le niveau des contraintes permet de déterminer les dimensions minimales en fonction des matériaux en jeu.
Il est couramment conseillé d'avoir pour longueur d'implantation :
Ces ordres de grandeur ne remplacent pas le calcul.
Pour le jeu vidéo, voir Transistor.
Le transistor est un composant électronique à semi-conducteur permettant de contrôler ou d'amplifier des tensions et des courants électriques. C'est le composant actif le plus important des circuits électroniques aussi bien en basse qu'en haute tension : circuits logiques (il permet, assemblé avec d'autres, d'effectuer des opérations logiques pour des programmes informatiques), amplificateur, stabilisateur de tension, modulation de signal, etc. Les transistors revêtent une importance particulière — le plus souvent en tant qu'interrupteurs marche/arrêt — dans les circuits intégrés, ce qui rend possible la microélectronique. 
Ce dispositif comporte trois électrodes actives permettant de contrôler un courant ou une tension sur l'électrode de sortie (le collecteur pour le transistor bipolaire et le drain sur un transistor à effet de champ) grâce à une électrode d'entrée (la base sur un transistor bipolaire et la grille pour un transistor à effet de champ). Le transistor est isolant sans tension sur la borne base, et conducteur avec une tension sur la borne base.
Le terme transistor provient de l'anglais transfer resistor (résistance de transfert). Il a été sélectionné par un comité directeur de vingt-six personnes[source insuffisante][1] des Bell Labs le 28 mai 1948[2], parmi les noms proposés suivants : semiconductor triode, surface states triode, crystal triode, solid triode, iotatron, transistor. Pour des raisons commerciales, il fallait un nom court, sans équivoque avec la technologie des tubes électroniques, et le mot Transistor fut retenu[3][source insuffisante],[1].
Ce nom correspond dans la fonction de résistance électrique pouvant être commandée par une tension ou un courant électrique.
Par métonymie, le terme transistor désigne souvent les récepteurs radio équipés de transistors (originellement appelés poste à transistors).
À la suite des travaux sur les semi-conducteurs, le transistor bipolaire a été réalisé pour la première fois le 23 décembre 1947 par les américains John Bardeen, William Shockley et Walter Brattain, chercheurs des Laboratoires Bell[note 1]. Ces chercheurs ont reçu pour cette invention le prix Nobel de physique en 1956[4].
Herbert Mataré et Heinrich Welker, deux physiciens allemands, ont aussi développé parallèlement et indépendamment le « transistor français » en juin 1948 alors qu'ils travaillaient à la Compagnie des Freins et Signaux à Paris[5]. Ils déposent leur première demande de brevets pour un transistor le 13 août 1948. Les études menées par les commissaires montrent qu'ils ne se sont pas appuyés sur l'annonce du transistor du laboratoire américain mais qu'ils ont bien eu l'idée en même temps[5]. Le 18 mai 1949, cette invention européenne est présentée par la presse au public sous le nom de « Transistron »[6],[7]. L'objectif est alors de conquérir le marché mondial en premier. A l'époque, la presse technique donne l'avantage au transistron considéré plus résistant et plus stable[5]. Néanmoins le gouvernement français étant focalisé sur la technologie nucléaire, le transistron est mis à l'écart et perd son avantage face au transistor[5]. En 1952, Herbert Mataré crée l'entreprise Intermetall qui est la première à produire des transistors et qui atteindra son apogée un an plus tard avec la présentation de la première radio à transistor un an avant celle de Texas Instrument. En 1954, Texas Instrument met au point son prototype de poste radio à transistor qui sera industrialisé par la société IDEA (Industrial Development Engineering Associates)[8].
Avant cela, Herbert Mataré avait déjà approché l'effet transistor alors qu'il travaillait pour l'armée allemande durant la seconde guerre mondiale dans le but d'améliorer les radars. L'urgence de la guerre l'empêcha de se pencher davantage sur le sujet et il qualifia ce phénomène d'« interférences ». Lorsque la Russie reprit le village où il travaillait en Pologne, Herbert Mataré dut brûler toutes ses notes de peur qu'elles ne tombent entre les mains de l’ennemi[réf. nécessaire][5].
Le transistor est considéré comme un énorme progrès face au tube électronique : beaucoup plus petit, plus léger et plus robuste, fonctionnant avec des tensions faibles, autorisant une alimentation par piles, il fonctionne presque instantanément une fois mis sous tension, contrairement aux tubes électroniques qui demandaient une dizaine de secondes de chauffage, généraient une consommation importante et nécessitaient une source de tension élevée (plusieurs centaines de volts).
Une fois le transistor découvert, l'ouverture au grand public ne fut pas immédiate. La première application du transistor fut, pour la radio, en 1954[9], soit 7 ans après la découverte du transistor. Mais à partir de ce moment son influence sur la société augmenta de façon exponentielle, en particulier chez les scientifiques et les industriels. En effet, à partir du milieu des années 1950, on commence à utiliser le transistor dans les ordinateurs, les rendant assez fiables et relativement petits pour leur commercialisation. À partir de 1957, IBM construisait tous les nouveaux ordinateurs avec des transistors au lieu des tubes à vide[8].
Après l'invention du circuit intégré en 1958, groupant en un petit volume plusieurs transistors et composants, en 1969 est inventé le microprocesseur, permettant à des milliers de transistors de fonctionner en harmonie sur un support, ce qui est encore une fois une révolution pour l'informatique moderne[10]. L'Intel 4004, sorti en mars 1971 et commandité par Busicom, intègre 2 250 transistors et exécute 60 000 opérations par seconde[8].
De nos jours, le transistor est omniprésent dans la plupart des appareils de notre quotidien. Le nombre de transistors dans un microprocesseur a considérablement augmenté pendant que sa taille diminuait, suivant en cela la loi de Moore prédisant durant plusieurs décennies un doublement du nombre de transistors des microprocesseurs, donc un doublement de la puissance de calcul de ces derniers, tous les 18-24 mois. Au 24 mars 2023, date du décès de Gordon Moore, la plus grande intégration pour une puce commerciale combinant CPU, GPU et moteur neuronaux est de 114 milliards de transistors pour environ 864 mm2[11]. Elle est de 2 600 milliards de transistors pour une super-puce de type "wafer-scale intégration" (interconnexion de toutes les puces d'une même galette). Le transistor a contribué au développement d'une grande variété de domaines[12]. Il est présent dans tout ce qui contient un tant soit peu d'électronique, de notre cafetière à nos voitures en passant par nos smartphones ou les feux de signalisation. Dès qu'il y a un choix plus complexe que ouvert/fermé dans un appareil électronique, un transistor entre en jeu[13].
Un transistor bipolaire est un dispositif électronique à base de semi-conducteur dont le principe de fonctionnement est basé sur deux jonctions PN, l'une en direct et l'autre en inverse.
Contrairement au transistor bipolaire, la grille agit par « effet de champ » (d'où son nom) et non par passage d'un courant électrique.
Parmi les transistors à effet de champ (ou FET, pour Field Effect Transistor), on peut distinguer les familles suivantes :
Le transistor dit à unijonction n'est quasiment plus utilisé, mais servait à créer des oscillateurs à relaxation.
L'IGBT est un composant hybride bipolaire et de MOSFET, principalement utilisé en électronique de puissance.
Les deux principaux types de transistors permettent de répondre aux besoins de l'électronique analogique et numérique mais aussi à ceux de l'électronique de puissance et haute tension.
Un mélange des deux technologies est utilisé chez les IGBT.
Les substrats utilisés sont le germanium (série AC, aujourd'hui obsolète), le silicium, l'arséniure de gallium, le silicium-germanium et plus récemment le carbure de silicium, le nitrure de gallium, l'antimoniure d'indium. Le 10 mai 1954, Texas Instruments sort le premier transistor au silicium[8].
Pour la grande majorité des applications, on utilise le silicium alors que les matériaux plus exotiques tels que l'arséniure de gallium et le nitrure de gallium sont plutôt utilisés pour réaliser les transistors hyperfréquences.
Les trois connexions sont appelées :
Dans les deux types de transistors bipolaires, l'électrode traversée par l'ensemble du courant s'appelle l'émetteur. Le courant dans l'émetteur est égal à la somme des courants du collecteur et de la base.
La flèche identifie l'émetteur et suit le sens du courant ; elle pointe vers l'extérieur dans le cas d'un NPN, vers l'intérieur dans le cas d'un PNP. L'électrode reliée au milieu de la barre centrale figure la base et la troisième électrode figure le collecteur.
Dans le cas de l'effet de champ, la flèche disparaît, car le dispositif est symétrique (drain et source sont échangeables). Les traits obliques sont habituellement remplacés par des traits droits.
Pour le transistor MOS, la grille se détache des autres électrodes, pour indiquer l'isolation due à la présence de l'oxyde.
En réalité, il existe une quatrième connexion pour les transistors à effet de champ, le substrat (parfois appelé bulk), qui est habituellement relié à la source (c'est la connexion entre S et les deux traits verticaux sur le schéma).
Les premiers transistors utilisaient le germanium comme semi-conducteur. Ce matériau, de nouveau utilisé pour certaines applications, a vite été remplacé par le silicium plus résistant, plus souple d'emploi, moins sensible à la température. Il existe aussi des transistors à l'arséniure de gallium utilisés en particulier dans le domaine des hyperfréquences.
Les transistors à effet de champ sont principalement utilisés en amplification grand gain de signal de faible amplitude, très basse tension. Ils sont très sensibles aux décharges électrostatiques.
Les évolutions technologiques ont donné les transistors ou commutateurs MOS de puissance, ils sont de plus en plus utilisés dans toutes les applications de commutation de forte puissance (classe D), basse tension, vu qu'ils n'ont presque plus de résistance de drain contrairement aux transistors, ils ne s'échauffent pas et n'ont donc pas besoin de refroidissement (radiateurs).
Le graphène, nouveau matériau très prometteur et performant, pourrait remplacer le silicium dans les transistors de future génération et permettre de prolonger la loi de Moore en termes de miniaturisation des transistors pour la microélectronique et la nanoélectronique de nouvelle génération[14],[15],[16].
On a réussi à produire des transistors au bisulfure de molybdène (MoS2) atomiquement minces[17], mais la miniaturisation restait difficile[18]. Récemment (en 2022) des transistors MoS2 à paroi latérale à canal atomiquement mince et dotés d'une longueur de grille physique de moins de 1 nm ont été produits sur le bord d'une couche de graphène utilisée comme électrode de grille, grâce à de films de graphène et de MoS2 développés par dépôt chimique en phase vapeur pour la fabrication de transistors à paroi latérale sur une tranche de 2 pouces[19].
Les transistors MOS et bipolaires fonctionnent de façons très différentes :
Chacun de ces transistors est caractérisé par une tension de seuil, correspondant à la tension de grille qui fait la transition entre le comportement bloqué du transistor et son comportement conducteur. Contrairement aux transistors bipolaires, dont la tension de seuil ne dépend que du semi-conducteur utilisé (silicium, germanium ou As-Ga), la tension de seuil des transistors à effet de champ dépend étroitement de la technologie, et peut varier notablement même au sein d'un même lot. Le transistor à effet de champ à déplétion à canal N est le semi-conducteur dont les caractéristiques se rapprochent le plus des anciens tubes à vide (triodes). À puissance égale, les transistors N sont plus petits que les P. À géométrie égale, les transistors N sont également plus rapides que les P. En effet, les porteurs majoritaires dans un canal N sont les électrons, qui se déplacent mieux que les trous, majoritaires dans un canal P. La conductivité d'un canal N est ainsi supérieure à celle d'un canal P de même dimension.
La plupart des circuits intégrés numériques (en particulier les microprocesseurs) utilisent la technologie CMOS qui permet d'intégrer à grande échelle (plusieurs millions) des transistors à effet de champ (à enrichissement) complémentaires (c'est-à-dire qu'on retrouve des N et des P). Pour une même fonction, l'intégration de transistors bipolaires consommerait beaucoup plus de courant. En effet, un circuit CMOS ne consomme du courant que lors des basculements. La consommation d'une porte CMOS correspond uniquement à la charge électrique nécessaire pour charger sa capacité de sortie. Leur dissipation est donc quasiment nulle si la fréquence d'horloge est modérée ; cela permet le développement de circuits à piles ou batteries (téléphones ou ordinateurs portables, appareils photo…).
Sauf dans le domaine des fortes puissances, il est devenu rare de n'avoir qu'un seul transistor dans un boîtier (pour les fortes puissances, on optera pour un montage Darlington, permettant d'obtenir un gain en courant plus important).
Les circuits intégrés ont permis d'en interconnecter d'abord des milliers, puis des millions. L'intégration de plus d'un milliard de transistors sur un seul composant a été atteinte en juin 2008 par Nvidia avec la puce GT200. Cette puce, utilisée comme processeur graphique (GPU), atteint 1,4 milliard de composants électriques gravés en 65 nanomètres, sur une surface d'environ 600 mm2.
Ces circuits intégrés servent à réaliser des microprocesseurs, des mémoires et la plupart des composants actifs.
Pour les articles homonymes, voir Moteur à combustion, MCI et ICE.
Un moteur à combustion interne ou MCI (en anglais : internal combustion engine ou ICE) est un type de moteur à combustion, c'est-à-dire un moteur permettant d'obtenir un travail mécanique à partir d'un gaz en surpression, cette dernière étant obtenue à l'aide d'un processus de combustion. Dans le cas d'un moteur à combustion interne, cette combustion a lieu à l'intérieur du moteur.
Il existe deux grands types de moteurs à combustion interne : les moteurs produisant un couple sur un arbre mécanique et les moteurs à réaction éjectant rapidement un fluide par une tuyère.
Moteur à combustion :
Turbine à gaz, turbopropulseur, turbomoteur.
Selon le type de comburant utilisé, il existe deux types de moteur à réaction :
Ne doit pas être confondu avec Gunpowder.
Cet article ne cite pas suffisamment ses sources (octobre 2011).
La poudre noire, parfois dénommée poudre à canon ou poudre à fusil, est le plus ancien explosif chimique connu. De couleur noire, elle est constituée d'un mélange déflagrant de soufre, de nitrate de potassium (salpêtre) et de charbon de bois.
Inventée en Chine probablement vers le IXe siècle, la poudre noire s'est progressivement diffusée en Europe et en Asie jusqu'au XIIIe siècle. Utilisée pour les canons et les fusils, c'était le seul explosif chimique connu jusqu'au XIXe siècle. La poudre noire n'est plus utilisée de nos jours pour les armes modernes et pour les applications industrielles, en raison de sa faible efficacité comparée à celle des explosifs plus récents. Son usage est aujourd'hui limité à des armes anciennes de chasse et de tir sportif (armes authentiques ou répliques), aux pétards et aux feux d'artifice.
Certaines sources situent l'invention de la poudre noire durant la dynastie Han (206 av. J.-C. à 220 ap. J.-C), mais la plupart des historiens pensent que la poudre à canon fut inventée en Chine vers le VIIe siècle, durant la Dynastie Tang (618-907). En chinois, la poudre à canon est désignée par le terme Huoyao (chinois : 火药 ; pinyin : huǒyào ; litt. « substance à feu ») et est d'abord utilisée pour ses propriétés médicinales.
Au milieu de la dynastie Xixia, vers 1044, il est conseillé d'éviter ce type de mélange qui risque d'exploser. Le Wujing Zongyao (武经总要, « Principes généraux du classique de la guerre ») donne une méthode de fabrication de grenades à poudre noire, dont l'effet principal semble encore être le bruit. Au Xe siècle, apparaissent les premières lances de feu, au départ de simples tubes de bambou contenant de la poudre noire et un projectile, dispositif fixé sur une lance chinoise (Qiang). Il semble qu'aux alentours de 1130, des tubes de bambou remplis de poudre noire servirent de lance-flammes.
Les techniques de fabrication de la poudre auraient été transmises au monde arabo-perse entre le VIIIe siècle et le IXe siècle, car des échanges de techniques d'alchimie existaient déjà entre le monde musulman et le monde chinois[1]. Cependant l'usage « connu » de la poudre semble plutôt dater du XIIIe siècle, avec des mentions écrites de composition à base de salpêtre, lors des guerres entre la dynastie Yuan et les pays musulmans d'Asie centrale. La poudre à canon a été introduite en Europe à partir du XIIIe siècle, principalement par l'intermédiaire d'échange commerciaux avec le monde arabe [2]. Il semble également que les Mongols aient joué un rôle déterminant.
À partir du XIVe siècle, la poudre noire est utilisée pour animer les spectacles équestres du Maghreb organisés par les tribus berbères.
Roger Bacon et Albert le Grand en mentionnent la recette, en s'inspirant de celle donnée par Marcus Graecus dans son manuscrit Liber ignium ad comburandos hostes (1230)[3], mais le but en reste incendiaire. Selon d'autres sources, le moine allemand Berthold Schwarz est considéré comme celui qui redécouvrit ses secrets en Occident. Ibn Khaldoun mentionne un usage de la poudre pour l'artillerie en 1273 au siège de Sidjilmesa. Les premières armes à feu utilisables apparaissent environ cinquante ans plus tard.
C'est en 1617 qu'est attestée la première utilisation de la poudre noire en Europe pour l'extraction de minerais dans une mine de cuivre du Thillot[4] grâce aux travaux archéologiques menés par la Société d'étude et de sauvegarde des anciennes mines (SESAM) depuis 1987 dans ces mines.
En 1829, Samuel Colt est le premier à faire déflagrer une charge de poudre sous l'action d'un courant électrique.
En 1886 est inventée la poudre pyroxylée, qui dégage beaucoup moins de fumée et peu de résidus lors de sa combustion. Cette poudre est aujourd'hui utilisée dans toutes les armes contemporaines, car la quasi absence de résidus ne les encrasse pas.
Aux XIVe siècle et XVe siècle, la composition était (en masses) : 6 parties de salpêtre (75 %) pour une partie de soufre (12,5 %) et une partie de carbone sous forme de charbon de bois (12,5 %). Mais ultérieurement, on trouve des compositions variables selon les usages.
Par exemple (pourcentages massiques) :
Dans les pièces d'artifices, on trouve généralement la composition (charbon 15 %, soufre 10 %, salpêtre 75 %). Cette poudre est un mélange de deux éléments très combustibles (le soufre et le charbon), avec un corps très oxydant : le salpêtre. La qualité de la poudre est due en grande partie au charbon utilisé. Il provient du bois d'arbres des plantes telles que Rhamnus Frangula, Solanum Mauritianum, Prunus domestica, Salix Caprea ou Fraxinus americana ; par pyrolyse à 500 °C, on obtient du charbon noir (poudre de guerre), tandis que la pyrolyse à 300 °C donne du charbon roux (poudre de chasse).
Pour que la combustion se déroule efficacement, le soufre et le charbon doivent être broyés en poudres fines (moins de 80 nanomètres), avant de le mélanger avec un moulin à billes. Par la suite, un mélange de nitrate de potassium et d'alcool est ajouté et le tout est mélangé dans un mélangeur pour obtenir un mélange très homogène. Enfin, le mélange est séché à basse température et doucement réduit en poudre à l'aide d'un pilon. On obtient une poudre noire qui brûle comme un flash.
La poudre noire craint beaucoup l'humidité, contrairement à ses descendantes modernes (poudres pyroxylées).
La poudre noire, contenant du salpêtre, a un goût salé en raison de ce constituant (nitrate de potassium KNO3). Pendant les différents conflits européens de la fin du XVIIIe et du début du XIXe siècle, les soldats utilisaient de la poudre noire pour assaisonner et conserver leurs aliments lorsque le sel venait à manquer. Le salpêtre est encore utilisé de nos jours comme conservateur (on en trouve dans la charcuterie par exemple).
L'équation de la combustion de la poudre à canon (charbon 15 %, soufre 10 %, salpêtre 75 %) est la suivante :
Les résidus solides sont appelés « calamine ».
Parmi les avantages de la poudre noire, notons qu'elle est peu onéreuse, stable et qu'une faible quantité d'énergie en provoque la combustion. Ainsi, peut-on l'enflammer à l'aide d'une flamme, d'une friction, d'une étincelle, ou même d'un laser. Il en résulte que sa manipulation est dangereuse.
Elle produit :
Pour ces raisons, on lui préfère aujourd'hui la poudre sans fumée.
Au cours du XIXe siècle, les chimistes mirent au point un procédé permettant d'obtenir de la poudre noire en grains, dont la taille peut être modulée selon l'usage prévu : plus les grains sont petits, plus la poudre obtenue est dite « vive », c'est-à-dire qu'elle présente une vitesse de combustion élevée. Ce conditionnement permet également de mieux conserver et de mieux doser la poudre noire.
La poudre noire contient à la fois un combustible et un comburant. Le comburant est le salpêtre qui libère de l'oxygène au cours de la réaction, venant oxyder les combustibles (soufre et carbone).
Stable à température ambiante, un petit apport d'énergie localisé suffit à amorcer la réaction.
Du fait de sa vitesse de combustion à l'air libre, on dit que la poudre noire « déflagre », ce qui signifie que l'onde de combustion (front de flamme) se déplace moins vite que les gaz générés, ne produisant donc pas d'onde de choc. Placée dans un endroit confiné qui permet une élévation de la pression des gaz, elle détone (génération d'une onde de choc) et produit un effet de souffle assez important en raison du volume de gaz produit.
Considérée comme un explosif à effet de souffle, par opposition aux explosifs à effet brisant, elle a longtemps été utilisée à cette fin.
La température de la réaction est assez élevée (plus de 2 000 K) mais reste nettement inférieure à celle obtenue avec des explosifs modernes (TNT, dynamite, poudres pyroxylées), limitant les risques de brûlures. L'ajout de certains composés chimiques ou de corps simples (particules métalliques, oxydes, etc.) permet de modifier la couleur de la flamme obtenue pour les feux d'artifice par exemple. La fumée dégagée par la réaction chimique est blanche, assez dense, en raison de nombreuses particules issues de la combustion.
Lors de son élaboration, la poudre noire suit un protocole précis, pratiquement identique dans les différents pays de production. En France, et plus particulièrement à la poudrerie de Vonges (21), le protocole est le suivant :
Les éléments entrants dans la fabrication de la poudre noire sont stockés séparément. Le charbon de bois issu de la bourdaine produit en Europe, arrive en vrac. Il est stocké dans des poches en tissu (coton) dans un hangar chauffé l’hiver et ventilé l’été pour obtenir le meilleur séchage possible. Le soufre et le salpêtre importés très souvent du Mexique et du Chili, sont livrés en sacs (papier ou polyester) de 25 kg. Dans la composition de certaines variétés de poudres noires, il y est ajouté quelquefois du lignite importé régulièrement d’Allemagne, en quantité précise.
Tonnes binaires
Dans un premier atelier nommé « tonnes binaires » (tonneaux en acier de 2 m de Ø et 2 m de hauteur équipés d’une porte d’accès, placés horizontalement sur un axe rotatif contenant un poids précis de billes en bronze pur d’environ 2 cm de Ø, entraînés par des courroies), le charbon de bois et le soufre sont malaxés ensemble. L’atelier est équipé d’un nombre variable de « tonnes », généralement une dizaine.
Après plusieurs heures de pulvérisation des composants, les tonnes sont vidées sur un tamis très fin et leur contenu stocké dans des étouffoirs (tonneaux en matière plastique).
Remarque importante : à partir de l’étape de fabrication suivante, tous les ateliers dans lesquels les préparations seront traitées ou entreposées seront soumis à des règles de sécurité drastiques afin d’éviter tous risques de surchauffe, d’étincelle, etc. (port de sabots en caoutchouc, sols en asphalte le plus souvent maintenus mouillés, utilisation d’outils en bronze, en bois ou en aluminium, interdiction formelle de fumer ou d’utiliser une flamme nue, etc.). La sécurité des employés est primordiale.
Préparations
Dans cette deuxième phase, le produit arrivant des tonnes binaires est mélangé au salpêtre, en respectant des poids très précis suivant la variété de poudre à fabriquer. Une certaine humidité est observée à ce mélange en y ajoutant un peu d’eau. Tout en restant compact, le mélange est entreposé dans des bacs plastiques individuels de 15 à 25 kg.
Ces bacs sont entreposés dans un hangar fermé attenant aux ateliers des préparations.
Meules
Le mélange provenant de l’atelier des ateliers des préparations entre dans sa troisième phase de fabrication. Entre 120 et 200 kg de matière, sont déposés sur une piste en fonte sur laquelle tournent autour d’un axe entraîné par courroies, deux meules en fonte également, d’environ 1,50 m de Ø, pesant plusieurs tonnes (voir photo ci-contre).
Le mélange, malaxé 30 à 50 min par les meules, est sorti en plusieurs morceaux de poids variables (galettes). Ces morceaux (galettes) sont répartis dans les bacs provenant des ateliers des préparations et entreposés dans un autre hangar dédié, fermé, attenant aux ateliers des meules.
Grenoirs
Les galettes provenant des ateliers des meules entrent dans la quatrième phase de fabrication. Environ 100 kg de galettes sont déposés dans une tonne grenoir (sorte de tonneau fait de deux disques en bois cerclé de bronze, de 1,5 m de Ø dont un est équipé d’une porte d’accès à son centre, placés horizontalement sur un axe rotatif à 1 m d’écart, équipés sur leurs circonférences d’un grillage quadrillé en inox les reliant entre eux, entraînés par des courroies). Un poids précis de boules de bois (gaïac) d’environ 10 cm de Ø, sont placées dans ce tonneau. Sous l’effet des boules bondissantes lors de la rotation, les galettes sont cassées en de nombreuses particules qui passent au travers d’une toile tamis (en nylon) de différentes grosseurs selon la variété de poudre fabriquée. Ces grains sont récupérés à l’extrémité du tamis et entreposés dans des sacs de toile (coton).
Lorsque la tonne grenoir est vidée de son contenu, elle est de nouveau approvisionnée de galettes. Cette phase dure de 40 min à plus d’une heure.
Les grains fabriqués avec les galettes provenant des ateliers des meules, sont entreposés en sacs dans un autre hangar dédié, fermé, attenant aux ateliers des grenoirs.
Lissoir
La poudre constituée des grains fabriqués dans les ateliers grenoirs, arrive dans sa cinquième phase de fabrication. Plusieurs centaines de kilos de poudre sont déposés dans une tonne lissoir (sorte de tonneau en bois cerclé de bronze, de 1,5 m de Ø, équipé d’une porte d’accès, placé horizontalement sur un axe rotatif, entraîné par des courroies). On ajoute à ces grains avant de refermer la porte quelques centaines de grammes de poudre de graphite. Ce graphite augmente la fluidité des grains et les protège de l'humidité.
Les grains provenant des ateliers grenoirs, mélangés plusieurs heures (4 à 5 heures), sont déversés sur un tamis de la grosseur du grain voulu, puis entreposés en sacs dans un autre hangar dédié, fermé, attenant aux ateliers des grenoirs. Ce tamis élimine les poussières restantes et les plus gros grains.
Séchoir
La poudre de l’atelier lissoir arrive dans sa sixième phase de fabrication. Plusieurs centaines de kilos de poudre sont déposés sur un grand châssis de bois (environ 15 m²) recouvert d’une toile de coton, sur une épaisseur d’une dizaine de centimètres. Cette poudre est séchée par de l’air chauffé à 45 °C, arrivant sous la toile en coton durant vingt-quatre heures.
La poudre noire provenant de sa dernière phase de fabrication, est entreposée en sacs dans un autre hangar dédié, fermé, attenant au séchoir.
Il existe quelques variantes dans la fabrication de la poudre noire, elle ne subit pas forcément toutes les différentes phases décrites ci-dessus. Par exemple, le pulvérin ou le MCHA destinés à la fabrication des artifices ou utilisé dans les carrières d’extraction de pierres, ne subissent pas de phase de séchage.
La poudre noire ainsi obtenue après avoir subi toutes ces phases de fabrication, est destinée à l’emballage, l’emboîtage, ou à la fabrication de pastilles ou de cartouches. Au cours des dernières phases de finition, des échantillons de poudre noires sont prélevés pour être analysés en laboratoire pour déterminer toutes ses caractéristiques techniques ainsi que son taux d’humidité.
Il convient de noter qu’avant l'utilisation de l'électricité, les différents ateliers de fabrication de la poudre noire étaient tous alimentés par la force hydraulique. Lorsque les ateliers ont été modernisés, les axes de transmissions qui étaient entraînés par des courroies, l'ont été par des moteurs électriques. Il convient de noter que pour leur sécurité, tous les ouvriers de fabrication de la poudre noire, qui conduisent les ateliers (excepté les tonnes binaires et les préparations), se trouvent, durant le fonctionnement des ateliers, derrière des murs forts de plus d'un mètre d'épaisseur. Ils doivent tenir, hiver comme été, les sols humides de leurs ateliers en les aspergeant régulièrement à l'eau.
Les principales différences entre les différents types de poudres noires que l’on peut trouver sur le marché, résident dans leurs granulométries, ce qui confère à chacune sa vivacité propre. Plus les grains sont petits, plus la poudre est dite « vive », plus elle brûlera vite, plus la montée en pression sera rapide et inversement.
Plus la différence entre les tailles de grains est faible, plus la poudre est homogène, plus la montée en pression et la vitesse des balles seront reproduites à l'identique d'un tir à l'autre et donc, plus la précision sera accrue.
En effet, une poudre hétérogène, comme la SNPE Poudre Noire Chasse, contient des grains qui vont de FFFFFg (pulvérin) à FFg (poudre lente).
Comme les grains de granulométries différentes ne sont jamais parfaitement mélangés dans un bidon ou dans une doseuse, il y aura forcément des charges plus vives que d'autres, ce qui engendrera des pressions et des vitesses de balles différentes.
Le tableau supra permet de visualiser la granulométrie de chaque poudre noire et leur classement en conséquence. Il concerne les poudres noires disponibles actuellement, sur le marché français. Les poudres françaises sont représentées en gris et les poudres suisses en noir. Elles sont classées dans l'ordre décroissant, selon leur granulométrie moyenne. En fond, un code de couleurs symbolise l'échelle américaine de granulométrie, de Fg à FFFFg. Les poudres pourront donc être comparées avec cette échelle et donc, avec les poudres américaines.
Ceci est particulièrement intéressant pour transposer en France, avec les PN suisses et françaises, les recommandations américaines, qui sont faites avec leurs PN.
La fabrication, le stockage, le transport et la manipulation de poudre à canon ont été source de nombreux accidents.
En outre, la poudre à canon sous forme agglomérée assure de mauvais groupements et engendre des risques supplémentaires lors de l'utilisation.
En Allemagne la poudre noire est vendue exclusivement en armurerie pour les particuliers. Les quantités admises à la vente sont limitées et c'est la SNPE qui fournit le marché allemand[réf. nécessaire]. Vendue dans des bidons contenant 500 g ou 1 kg de poudre, son utilisation est très réglementée. La loi n'autorise pas de posséder plus de 2 kg chez soi.
En France, les lois sont similaires : la poudre noire est vendue en bidons généralement en plastique (antistatique), les quantités sont similaires à l'Allemagne.
Les tireurs utilisant la poudre noire en Allemagne utilisent deux types de poudre : l'« allemande » et la « suisse ». La poudre noire allemande (PNA) est réputée moins intéressante pour le tir que la poudre noire suisse « Poudrerie d'Aubonne (Vaud) », mieux dosée et plus régulière.
En France, les tireurs utilisent également deux types de poudre : la « française » et la « suisse ». Les poudres noires françaises (PNF1, PNF2 et PNF4P) sont différentes des poudres noires suisses, en dosage et en granulométrie, à l'usage. La poudre suisse « Poudrerie d'Aubonne (Vaud) » est moins salissante (moins de résidu) et légèrement plus puissante [moins de salpêtre (76 %), plus de charbon de pin (12 %) et de soufre (12 %)].[réf. souhaitée]
Pour les articles homonymes, voir Colisée (homonymie).
Le Colisée (Colosseo en italien), à l'origine amphithéâtre Flavien (amphitheatrum Flavium en latin), est un immense amphithéâtre ovoïde situé dans le centre de la ville de Rome, entre l'Esquilin et le Cælius, le plus grand jamais construit dans l'Empire romain. Il est l'une des plus grandes œuvres de l'architecture et de l'ingénierie (en) romaines.
Témoignage monumental de la propagande flavienne, sa construction, juste à l'est du Forum Romain, a commencé entre 70 et 72 apr. J.-C., sous l'empereur Vespasien, et s'est achevée en 80 sous Titus. D'autres modifications ont ensuite été apportées au cours du règne de Domitien (81-96)[2]. Le nom d'amphithéâtre Flavien dérive du nom de famille (gens Flavii) de l'empereur Vespasien et ses fils Titus et Domitien.
Pouvant accueillir probablement 50 000 spectateurs (les estimations plus anciennes de 80 000 spectateurs, soit un douzième de la population romaine, étant exagérées)[3], le Colisée, témoin de l'évergétisme impérial, a été utilisé pour les venationes (combats d'animaux sauvages), les munera (combats de gladiateurs) et autres spectacles publics, tels que des exécutions de condamnés à mort, des reconstitutions de batailles célèbres et des drames basés sur la mythologie romaine. Il est resté en service pendant près de 500 ans, les derniers jeux se prolongeant jusqu'au VIe siècle. Pour l'inauguration du Colisée, en 80 apr. J.-C., Titus donne une naumachie dans le Colisée transformé en bassin reconstituant la bataille navale de Corinthe contre Corcyre. Le bâtiment a finalement cessé d'être utilisé au cours du haut Moyen Âge. Il a plus tard été réutilisé pour des usages variés tels que des habitations, des ateliers d'artisans, le siège d'un ordre religieux, une forteresse, une carrière et un sanctuaire catholique chrétien.
Le Colisée est actuellement en état de ruine, en raison des dommages causés par les tremblements de terre (443, 508, 801, 847 et 1349) et la récupération des pierres, mais il continue à donner la mesure de l'ancienne puissance de la Rome Impériale. Aujourd'hui, il est l'un des symboles de la Rome moderne, une de ses attractions touristiques les plus populaires avec 7,6 millions de visiteurs, et a encore des liens étroits avec l'Église catholique romaine : chaque Vendredi saint, le pape mène une procession aux flambeaux sur un chemin de croix aboutissant à l'amphithéâtre. Le Colisée est représenté sur la pièce de monnaie italienne de 5 centimes d'euro.
Le nom latin initial du Colisée était amphitheatrum Flavium (en français « amphithéâtre Flavien »). Le monument a été construit par les empereurs de la dynastie Flavienne pour offrir aux citoyens des spectacles, d'où son nom d'origine[5]. Ce nom est encore fréquemment utilisé dans les ouvrages spécialisés, mais il est peu connu du grand public. Dans l'Antiquité, les Romains ont parfois évoqué le Colisée sous le nom d'Amphitheatrum Caesareum, dans un contexte poétique[6],[7].
Le nom de Colosseum (du bas latin colossus qui vient lui-même du grec κολοσσός, « colosse, grande statue[8] ») est probablement[9] dérivé de celui d'une statue colossale de Néron érigée à proximité[2] et initialement ornant l'entrée de la Domus aurea[10]. Alors que le palais impérial a été démantelé après la mort de Néron frappé de damnatio memoriae, cette statue a été remodelée par les successeurs de l'empereur en une figure d'Hélios (Sol ou Apollon), dieu du soleil, par l'ajout de la couronne solaire. La tête de Néron a été remplacée à plusieurs reprises par celles de divers empereurs. En dépit de ses liens païens, la statue est restée debout une bonne partie de l'époque médiévale, et était créditée de pouvoirs magiques[réf. nécessaire]. Elle fut finalement considérée comme un symbole iconique de la permanence de Rome[11].
Au VIIIe siècle, Bède le Vénérable (ca. 672-735) écrivit une célèbre épigramme célébrant la signification symbolique de la statue : Quandiu stabit coliseus, stabit et Roma ; quando cadet coliseus, cadet et Roma ; quando cadet Roma, cadet et mundus (« Tant que durera le Colosse, Rome durera ; quand le Colosse tombera, Rome tombera ; quand Rome tombera, le monde tombera »)[12]. Ceci est souvent mal traduit, en se référant au Colisée plutôt qu'au colosse (par exemple, dans le fameux poème de Byron Childe Harold's Pilgrimage) : à l'époque de Bède, le nom masculin coliseus était appliqué à la statue plutôt qu'à ce qui était encore connu sous le nom d'amphithéâtre Flavien.
Le colosse de Néron a fini par tomber, probablement jeté bas en vue de la réutilisation de ses éléments de bronze. Le nom de Colosseum (nom neutre) a été utilisé vers l'an 1000, pour désigner l'amphithéâtre. La statue elle-même a été en grande partie oubliée, et seule sa base survécut, entre le Colisée et le Temple de Vénus et de Rome tout proche[13].
Le nom a été corrompu en Coliseum au cours du Moyen Âge. En Italie, l'amphithéâtre est toujours connu sous le nom de il Colosseo, et d'autres langues romanes en sont venues à utiliser des formes similaires, telles que le Colisée en français, el Coliseo en espagnol, o Coliseu en portugais ou Colosseumul en roumain.
Colisée est devenu un nom commun, synonyme d'amphithéâtre et arènes : colisée d'El Jem, colisée de Pula, colisée de Vérone...
Après le grand incendie de Rome en 64 apr. J.-C, Néron se fit construire un somptueux palais. S'étant saisi de terrains au fond d'une vallée basse au fond de laquelle courait un ruisseau canalisé, entre le Cælius, l'Esquilin et le Palatin, il fit édifier la magnifique Domus aurea. Devant des pavillons, jardins, et portiques, il créa un lac artificiel qui constituait la partie centrale du complexe palatial et fit placer le Colosse de Néron non loin de l'entrée du domaine. L'aqueduc préexistant de l'Aqua Claudia fut prolongé pour l'approvisionnement en eau de cette zone[13],[14].
À sa mort en 68, Néron fit l'objet d'une damnatio memoriae. La zone fut transformée par Vespasien et ses successeurs  qui, par un geste de propagande antinéronienne, voulaient rendre aux plaisirs du peuple romain une partie de l'espace urbain que Néron avait confisqué pour sa propre jouissance, dans le cadre de l'évergétisme impérial[15]. La statue colossale fut conservée, mais on démolit une grande partie de la Domus aurea dont les vestiges servirent de fondations aux thermes de Trajan. Le lac d'agrément fut comblé et le terrain réutilisé pour la construction du nouvel Amphithéâtre Flavien qui était destiné à remplacer l'amphithéâtre de Statilius Taurus ravagé lors du grand incendie de Rome[16]. La construction du Colisée commença autour de 70-72 (le début et la durée des travaux de construction restent incertains, mais selon la plupart des historiens, ils durèrent environ dix ans, avec deux années supplémentaires pour la finition[17]) sous le règne de l'empereur Vespasien dont la décision peut être vue comme un geste populiste de retour au domaine public d'un quartier annexé par Néron pour son propre usage[18].
Selon l'inscription portée sur un bloc de marbre trouvé sur le site, telle que reconstituée par l'épigraphiste Géza Alfödy, « l'empereur Vespasien a ordonné que l'on édifie ce nouvel amphithéâtre sur sa propre part de butin [19] ». Compte tenu des montants en cause, il ne peut s'agir que des trésors saisis par les Romains à la suite de leur victoire dans la première guerre judéo-romaine de 70, notamment lors du sac de Jérusalem et du pillage de son temple[20],[21]. Il est peu probable que les 30 000 prisonniers juifs emmenés à Rome à la suite de cette campagne militaire aient été employés à la construction du monument, bien que certains, réduits en esclavage, aient pu être affectés aux travaux les moins qualifiés (ouvriers posant le béton plutôt que maçons spécialisés dans le travail du travertin)[22]. Le Colisée peut donc être interprété comme un grand monument triomphal construit dans la tradition romaine de célébration des grandes victoires, ce qui expliquerait la présence d'un arc de triomphe ornant l'entrée principale du Colisée et l'ajout par Domitien de boucliers de bronze sur l'attique[13].
Contrairement à beaucoup d'autres amphithéâtres situés à la périphérie des villes, le Colisée fut construit littéralement et symboliquement au cœur de Rome. Des écoles de gladiateurs et d'autres bâtiments annexes furent construits à proximité. À la mort de Vespasien, en 79, le troisième étage du Colisée était achevé. Le dernier niveau fut inauguré par son fils Titus, en 80. Dion Cassius rapporte que 9 000 bêtes sauvages furent tuées lors des jeux inauguraux. Le bâtiment fut ensuite rénové par Domitien, fils cadet de Vespasien, empereur nouvellement désigné, qui ajouta l'hypogée, réseau de souterrains utilisés pour abriter les animaux et les gladiateurs. Il adjoignit également une galerie tout au sommet du Colisée, destinée à accroître encore le nombre de places.
Sous le règne de Trajan, en 107, 11 000 animaux et 10 000 hommes auraient été impliqués durant 123 jours de fête. Côté technique, 2 000 marins étaient employés pour manœuvrer au-dessus du Colisée l'immense velarium qui donnait de l'ombre aux 55 000 spectateurs. Ceux-ci pouvaient alors admirer à la belle saison, et environ six fois par an, le combat des gladiateurs dont l'âge dépassait rarement 22 ans.
En 217, le Colisée fut gravement endommagé par un incendie majeur (causé par la foudre, selon Dion Cassius[23]) qui détruisit les étages supérieurs des gradins construits en bois. Il ne fut entièrement réparé que vers 240 et subit d'autres réparations en 250 ou 252, puis en 320. Une inscription enregistre la restauration de diverses parties du Colisée sous Théodose II et Valentinien III (règne : 425-450), peut-être pour réparer les dommages causés par un tremblement de terre majeur, en 443 ; d'autres travaux furent entrepris en 484 et en 508.
L'arène continua d'être utilisée pour des concours jusqu'au VIe siècle au moins, avec les derniers combats de gladiateurs vers 435. Les chasses aux animaux sauvages se poursuivirent au moins jusqu'en 523[13].
Le Colisée a connu bien des changements au cours du Moyen Âge. Une petite église fut construite à l'intérieur de la structure, à la fin du VIe siècle, et l'arène devint un cimetière.
Les nombreux espaces voûtés, sous les gradins, furent utilisés comme habitations ou comme ateliers, et on relève encore des locataires au XIIe siècle, époque où les Frangipani fortifièrent l'édifice, apparemment pour en faire une forteresse.
Le Colisée eut à souffrir de plusieurs tremblements de terre, dont ceux de 443, 508, 801, 847 et surtout celui de 1349 qui provoqua l'effondrement de tout un pan du mur extérieur du côté sud bâti sur une couche d'alluvions argileuse[24]. Avec l'autorisation du pape, une grande partie des pierres fut alors récupérée pour la construction des palais, églises, hôpitaux et autres bâtiments. Ainsi les façades du palais de Venise et de la basilique Saint-Pierre sont issues du réemploi des blocs de travertin du Colisée[25]. Les placages de marbre alimentèrent les fours à chaux[13]. Les agrafes de fer ou de bronze scellées au plomb servant à assujettir les pierres furent systématiquement pillées en creusant au burin entre les joints, laissant les innombrables cicatrices (trous) aujourd'hui visibles sur tous les murs intérieurs et extérieurs, et affaiblissant encore l'édifice qui souffrit des tremblements de terre de 1703 et de 1812[26].
Un ordre religieux s'installa dans les ruines au milieu du XIVe siècle, pour s'y maintenir jusqu'à la fin du XIXe siècle[27].
Au cours des XVIe et XVIIe siècles, les fonctionnaires de l'Église cherchèrent à donner un rôle productif au grand monument à l'abandon. Sixte Quint (1585-1590) envisagea de transformer l'édifice en filature de laine où l'on emploierait les prostituées, mais cette proposition ne fut pas suivie d'effet par suite de son décès[28]. En 1671, le cardinal Altieri autorisa son utilisation pour des courses de taureaux, ce qui provoqua un tollé.
Au début du XVIIIe siècle, un moine Carme, le père Angiolo Paoli, intervint auprès du pape Clément XI pour préserver ce lieu qui « avait été imprégné du sang des martyrs » et avait été laissé à l'abandon. Le Pape approuva le projet du moine. Le Carme, avec l'aide de quelques volontaires, se transforma en maçon et fit fermer les arcs avec des murs épais, et les portes avec de grosses traverses en fer. À l'intérieur, il érigea trois grosses croix de bois[29].
En 1749, Benoît XIV décida que la politique officielle de l'Église serait de faire du Colisée le lieu sacré où les premiers chrétiens ont été martyrisés. Il interdit l'utilisation du Colisée comme carrière, et consacra l'édifice à la Passion du Christ et fit installer un chemin de croix, le déclarant sanctifié par le sang des martyrs chrétiens qui y périrent. Plus tard furent entrepris divers projets de restauration et de stabilisation : sous l'Empire français, la façade fut renforcée par des étais de brique en 1807 et 1827, Napoléon faisant employer 1 800 hommes à la restauration et aux fouilles des principaux monuments de Rome de 1811 à 1814. À la suite de l'occupation de Rome, Napoléon III poursuivit les travaux de restauration et de fouilles[30]. L'intérieur fut restauré en 1831, 1846 et 1930. Avant le nettoyage et les restaurations effectuées au XIXe siècle par des archéologues et ingénieurs, les voyageurs avaient une vision romantique du site, les arcs et les ruines devenues un jardin rempli de fleurs et de verdure leur rappelant la splendeur passée de Rome[31].
L'arène fut partiellement fouillée en 1810-1814 et en 1874, puis totalement déblayée dans les années 1930[13]. En 1995, débuta un chantier de restauration, le plus important depuis 1836, dont l'objectif était de diminuer le nombre de fragments du monument se détachant et d'ouvrir 85 % du monument au public (contre 15 % en 1995)[32]. Les restaurations se poursuivent encore actuellement.
Contrairement aux amphithéâtres antérieurs construits entre deux collines, le Colisée est une structure autonome. Il est de plan ovoïde (courbe polycentrique très proche d'une ellipse)[33], orienté OSO-ENE, de 189 m de long et 156 m de largeur, avec une superficie de 2,4 ha. La hauteur de la paroi extérieure est de 48 m. Le périmètre d'origine mesure 545 m. L'arène centrale est un ovale de 86 m de long et 54 m de largeur, entouré par un mur de 4,5 m de hauteur, qui s'élève jusqu'au niveau des premiers gradins[34].
Avec ses 250 000 tonnes de pierres, la construction du Colisée a nécessité près de 100 000 m3 de travertin (dont 45 000 pour la paroi extérieure en opus quadratum), montés sans mortier, mais solidarisés par 300 t d'agrafes de fer[13]. Cette roche, issue d'une carrière près de Tibur, fut transportée au Colisée par une route spécialement aménagée à cet effet. Une quantité similaire de blocs de tuf, de briques et de béton en opus caementicium ont également été employés afin d'adapter la résistance des matériaux aux charges et poussées selon les structures (principaux piliers en travertin, murs radiaux en travertin et tuf, voûtes en briques de béton, plus léger que la pierre et plus solide que le mortier traditionnel)[35]. Cependant, l'ensemble de la structure a subi d'importants dommages au cours des siècles, avec de larges segments effondrés à la suite de tremblements de terre. Le côté nord du mur d'enceinte est toujours debout ; les rampes de brique à chaque extrémité ont été ajoutées au XIXe siècle pour consolider le mur. Le reste de l'actuel extérieur du Colisée est en fait le mur intérieur d'origine.
Le bâtiment repose sur une base de deux marches. La partie survivante de la paroi extérieure de la façade monumentale se compose de trois niveaux d'arcades superposés, surmontés d'une plate-forme sur laquelle se dresse un attique de grande hauteur, percé de fenêtres à intervalles réguliers. Seulement 31 arcs de l'anneau extérieur, numérotés de XXIII à LIV, sont restés intacts. Les 80 arcades de chaque niveau sont respectivement encadrées de demi-colonnes de style dorico-toscan (style spécifiquement romain), ionique et corinthien, tandis que l'attique est orné de pilastres composites[36], les styles de ces trois niveaux devenant l'archétype des amphithéâtres romains postérieurs[37]. Les arcs au rez-de-chaussée font 7,05 m de hauteur pour 4,20 m de largeur. Les arcs aux premier et deuxième étages, qui ne font que 6,45 m de hauteur, étaient ornés de 160 statues en bronze doré hautes de cinq mètres probablement en l'honneur des divinités et des autres personnages de la mythologie classique, tandis que 40 boucliers en bronze ajoutés par Titus rythmaient l'attique et symbolisaient les conquêtes militaires romaines avec le bouclier pris à l'ennemi. Il est possible que les boucliers soient un rappel de ce décor déjà employé dans la basilique Æmilia[38].
La construction est favorisée par la répétition d'un motif architectural, le fornix (travée formée d'une arcade et de deux piliers, répétée 80 fois pour constituer le périmètre et trois fois pour l'élévation) rappelant la prostitution qui se déroulait sous ces arcades[39].
Deux cent quarante mâts d'une vingtaine de mètres de hauteur étaient dressés en encorbellement autour du sommet de l'attique. Ils soutenaient un vaste auvent rétractable, connu sous le nom de velum ou velarium (toile de lin ou de chanvre qui pouvait être colorée pour donner des ambiances particulières), qui abritait les spectateurs du soleil. C'était une immense toile soutenue par un anneau de cordes en filet, avec un trou au centre, entouré d’un anneau de fort cordage (du chanvre de 80 mm de diamètre environ)[2]. Il couvrait deux tiers de l'arène, en pente vers le centre pour capter le vent et en diriger une brise vers les spectateurs. Des esclaves et marins, spécialement enrôlés au siège de la marine à Misène et basés à la proche caserne du Castra Misenatium, étaient chargés de la manœuvre du velarium[40]. À noter que des esclaves, armés de vaporisateurs, envoyaient des brumes rafraîchissantes et parfumées (les spartiones[41]) sur les notabilités. Des projections de suaves effluves (eau
mêlée de safran ou de baume) par des pompes à piston pouvaient avoir lieu avant le spectacle. Pour neutraliser l'odeur des bêtes et les relents d'écurie, des brûle-parfums étaient répartis dans l'amphithéâtre[42].
Le Colisée était entouré d'une place de 17,5 m de large pavée de travertin et délimité par les bornes fixées[43] dans le sol dont la fonction est discutée (ancrage des cordes de rappel du velarium, portes pour filtrer et réguler les accès au monument)[44]. Les jours de spectacle, cette place regorgeait de colporteurs et de pickpockets[45]. L'énorme capacité du Colisée rendait indispensable un dispositif d'accès et d'évacuation rapide, pour lequel les architectes mirent au point des solutions similaires à celles que nous connaissons dans nos stades modernes. Quatre-vingts entrées s'ouvraient sur l'extérieur au rez-de-chaussée, dont soixante-seize, numérotées (de même que chaque escalier) étaient destinées aux spectateurs ordinaires[2]. Des grilles sous chaque arc (il ne reste plus que leurs gonds dans le mur) permettaient de réguler le flot de spectateurs. La porte principale nord-ouest (appelée « porta triumphalis » ou « porte de la vie ») était l'entrée principale réservée à la parade inaugurale des gladiateurs et à la sortie des combattants vainqueurs, la porte sud-est (la « porta libitinensis » ou « porte de la mort »)[46] pour emmener les mortellement blessés au spoliarium alors que les deux autres entrées axiales étaient destinées à l'élite (porte sud-ouest empruntée par l'empereur et ses proches, par les sénateurs et les vestales ; porte nord-est par les magistrats et riches patriciens)[47]. Les quatre entrées axiales étaient richement décorées de peintures et de reliefs en stuc, dont des fragments nous sont parvenus. Le « passage de Commode » (passage secret du nom de l’empereur qui, selon les sources historiques, y subit un attentat)[48] qui reliait la loge impériale méridionale à l'extérieur (probablement un palais) est le témoin de ce riche décor. Bon nombre des entrées originales extérieures ont disparu avec l'effondrement du mur extérieur, mais les entrées XXIII à LIV survivent encore. Tout l'édifice était probablement peint[13].
Les spectateurs recevaient des billets sous forme de fragments de poterie numérotés (jeton d'entrée, ou tessera, distribué gratuitement la veille[49]) qui leur donnaient les instructions nécessaires de section et de rangée de sièges. Ils accédaient à leurs places par des vomitoria qui s'ouvraient sur les gradins, le public étant installé en une heure. Dès la fin des jeux ou en cas d'urgence, l'évacuation ne prenait que quelques minutes[50].
Façade originelle du Colisée.
Entrée LII du Colisée, avec chiffres romains encore visibles.
Les trous et 240 consoles marquent l'emplacement des mâts qui supportaient le velum[51].
L'ensemble des gradins forme la cavea. Les travées inférieures forment un angle de 30°, les supérieurs de 40°, ce qui permet au public en hauteur d'avoir une vue dégagée de l'arène[53].
Les spectateurs étaient assis dans un arrangement hiérarchisé qui reflète la nature rigide et stratifiée de la société romaine. Des inscriptions gravées sur les gradins indiquent à quelle catégorie de personnes ils étaient destinés, par exemple equitibus romanis (c'est-à-dire « pour les chevaliers romains »), ou encore pædagogis puerorum (« pour les maîtres d'école »).
Des loges spéciales étaient réservées respectivement au sud et nord à l'empereur et aux Vestales, offrant les meilleures vues sur l'arène. Une large plate-forme ou podium, au même niveau, accueillait les spectateurs de classe sénatoriale, autorisés à apporter leur propre chaise. Les noms de certains sénateurs du Ve siècle sont encore gravés dans la pierre.
Le niveau situé juste au-dessus de celui des sénateurs, connu sous le nom de primum mænianum, consistait en neuf travées en marbre occupées par la classe des chevaliers (ordre équestre : equites, noblesse non sénatoriale). Le niveau suivant, le mænianum secundum, était à l'origine réservé aux simples citoyens romains (les plébéiens) et était divisé en deux sections. La partie inférieure (l'immum) était destinée aux citoyens riches, alors que la partie supérieure (le summum) était dévolue à la classe moyenne. Des secteurs spécifiques étaient attribués à d'autres groupes sociaux : par exemple, les garçons avec leurs tuteurs, les soldats en permission, les dignitaires étrangers, les scribes, les hérauts, les prêtres et ainsi de suite. Certaines zones étaient réservées à des groupes spécifiques. Les sièges de pierre, et plus tard de marbre étaient rendus plus confortables par les coussins que chacun apportait pour son propre usage[54].
Deux niveaux supplémentaires, le mænianum summum secundum et le mænianum secundum in ligneis, ("2e étage en bois") furent ajoutés au sommet de l'édifice sous le règne de Domitien. Ce dernier consistait en une galerie destinée aux pauvres, aux esclaves et aux femmes, avec des places debout ou aménagées succinctement sur des tribunes de bois en pente très raide. Certains groupes étaient totalement exclus du Colisée, notamment les fossoyeurs, les acteurs et les anciens gladiateurs[13].
Chaque niveau, divisé en sections (mæniana) par des passages en courbe (larges couloirs appelés praecinctiones) et des murets baltei), était subdivisé en cunei, ou portions, et par les allées et les marches des vomitoria. Chaque rangée (gradus) avait ses sièges numérotés, permettant de désigner précisément chaque siège par son gradus, son cuneus et son numéro propre[55].
L'arène mesure 83 × 48 m (280 × 163 pieds romains)[13]. Elle est composée d'un plancher de bois recouvert de sable (le mot latin arena ou harena signifie « sable ») qui évitait aux combattants de glisser, absorbait facilement le sang répandu et pouvait être rapidement remplacé[56]. L'arène couvre une vaste structure souterraine appelée « hypogée » (nom masculin d'origine grecque, littéralement "le sous-sol": hupo (sous) et gê (la terre)). Il reste peu actuellement de l'arène originale, mais l’hypogée est encore bien visible : à la suite des fouilles entreprises en 1803, les souterrains sont envahis par les eaux d'égouts et de pluie, et il a fallu attendre les années 1880 pour que des pompes performantes puissent évacuer ces eaux et permettre la reprise des fouilles. Les souterrains sont édifiés quelques années après l’inauguration de l’amphithéâtre, à l’époque de Domitien (81-96 apr. J.-C.)[57].
L'hypogée (du grec hupo, « sous », et gê, « terre ») construit sous l'empereur Domitien était divisé en 15 couloirs réalisés en brique et blocs de tuf, bâtis parallèlement à une galerie centrale qui suivait le grand axe de l’ellipse (est-ouest). Il était constitué d'un réseau à deux niveaux souterrains de tunnels et de cages situés sous l'arène, où gladiateurs et animaux se tenaient prêts avant le spectacle. Quatre-vingts puits verticaux fournissaient un accès instantané à l'arène pour les animaux en cage et les accessoires de scène ; des plates-formes à charnières de plus grandes dimensions, appelées hegmata, permettaient l'accès des éléphants et autres grands animaux. L'hypogée a été restructuré à maintes reprises au cours des cinq siècles de fonctionnement du Colisée, et l'on peut distinguer au moins douze différentes phases de construction[13].
L'hypogée était relié par des tunnels souterrains à un certain nombre de points en dehors du Colisée. Les animaux et leurs dresseurs pouvaient rejoindre par un tunnel les écuries situées à proximité, de même que les gladiateurs pouvaient rallier sans peine à partir du couloir central leur caserne du Ludus Magnus, toujours visible, juste à l'est du Colisée. Des tunnels spéciaux étaient réservés à l'empereur et aux Vestales, afin qu'ils puissent rejoindre leurs loges sans avoir à se mêler à la foule[13].
Des quantités de machines de toutes sortes étaient entreposées dans l'hypogée. Toute une machinerie scénique actionnée par des cordages reliés à des treuils, des palans et des cabestans (des socles en pierre dans laquelle était attelée la pièce en bronze de ces treuils sont encore visibles au sol)[58] mis en mouvement par des centaines esclaves, permettait de hisser jusqu'à la surface de l'arène, les cages des fauves, les plateaux mobiles supportant des décors et des accessoires, ou des plates-formes contenant des gladiateurs. 28 élévateurs (structures en bois faisant office d'ascenseurs[59], étaient montés et descendus à l'aide de ces cordages et de poulies dans la partie supérieure de ces monte-charge[60]. Ces effets qui augmentent la spectacularisation de l'événement, étaient aussi rendus possibles par la nature de la couverture du sous-sol, en grande partie constituée d'un plancher de bois composé d'éléments amovibles, de rampes et de trappes[61]. Il existe des preuves de l'existence de grands mécanismes hydrauliques permettant d'inonder rapidement l'arène, vraisemblablement par le biais d'une connexion à un aqueduc situé à proximité[13].
Modèle en coupe de l'hypogée montrant la machinerie scénique.
Reconstitution d'un élévateur.
Le Colisée attira dans son orbite toutes sortes d'activités annexes dans le voisinage : en plus de l'amphithéâtre lui-même, de nombreux autres bâtiments implantés à proximité étaient liés aux jeux. Immédiatement à l'est se trouvent les importants vestiges du Ludus Magnus, une école d'entraînement des gladiateurs, reliée au Colisée par un passage souterrain. Le Ludus Magnus avait sa propre arène, qui était elle-même une attraction populaire pour les spectateurs romains. D'autres écoles d'entraînement s'étaient installées dans la même zone, le Ludus Matutinus ("école du matin"), où étaient formés les chasseurs d'animaux, en plus des écoles des Daces et des Gaulois.
À l'intérieur du Colisée se trouve l'Eglise Santa Maria della Pietà al Colosseo qui est un lieu de culte catholique. De taille modeste, elle est insérée dans une des arcades de l’amphithéâtre Flavien et a probablement été construite entre le VIe et le VIIe siècle bien que la première information certaine concernant son existence ne remonte qu'au XIVe siècle. Cette église représente depuis toujours un lieu de culte en mémoire des martyres chrétiens ayant perdu la vie à l'intérieur du Colisée. Elle fut fréquentée par de nombreux saints notamment Ignace de Loyola, Philippe Néri et Camille de Lellis. L'archéologue romain Mariano Armellini a déclaré que cette chapelle : « (...) était destinée à l'origine à être la garde-robe de la compagnie de théâtre qui donnait des représentations dans l'arène de l'amphithéâtre, notamment le drame la Passion de Jésus-Christ, usage qui fut maintenu jusqu'aux temps de Paul IV. » Plus tard en 1622, l'édicule fut acheté par la Confraternité du Gonfalone qui le transforma en oratoire. Il lui appartint jusqu'en 1936 puis il changea de main et fut confié au Cercle San Pietro à partir de 1955. Cette organisation le dirige toujours et y célèbre des messes tous les samedis et dimanches[réf. souhaitée].
À proximité également se tenait l'Armamentarium, comprenant une armurerie pour stocker les armes, le Summum Choragium, où les machines étaient entreposées, le Sanitarium, où l'on soignait les gladiateurs blessés, et le Spoliarium, où les corps des gladiateurs morts étaient dépouillés de leurs armes et évacués.
Sur le périmètre du Colisée, à une distance de 18 m du périmètre, était disposée toute une série de grosses bornes de pierre, dont cinq subsistent du côté est. Diverses explications de leur présence ont été avancées : elles pourraient avoir marqué une frontière religieuse, ou bien une limite extérieure pour le contrôle des billets, ou encore des points d'ancrage pour le velarium[13].
Juste à côté du Colisée se dressait la Meta Sudans, et plus tard vint l'Arc de Constantin.
Le velum, appelé également velarium, était une toile de protection, qui était ténue au-dessus du colisée afin de protéger les spectateurs du soleil ou de la pluie. Il se composait d’un grand nombre de bandes trapézoïdales, déployées à l’aide des cordes roulées par des treuils. La toile descendait le plus bas possible afin de mieux protéger les spectateurs contre le soleil. Les nombreux travaux réalisés par les archéologues pendant les dernières années ont pu démontrer que l’heure pendant laquelle la protection était moins favorable était quand le soleil reste le plus bas dans le ciel (alors le matin ou le soir). Des études récentes, ont examiné l’influence du velum sur l’acoustique de l’amphithéâtre. Ces études ne peuvent pas apporter des résultats très concrets car la taille même du velum n’est pas connue.
La découverte du velum a eu lieu en 1876, quand les archéologues ont  mis au jour une fresque dans une maison de Pompéi, qui illustrait l’amphithéâtre de la ville avec une grande pièce de tissu qui couvrait la semi-totalité des cavea. Jusqu’à nos jours, aucune image de cette période ne montre le velum déployé. Après que l’existence des voiles ait été établie par les études archéologiques, cette conviction a été confirmée par un graffiti « publicitaire » déjà connu, trouvé  à Rome, qui finissait par la phrase suivante : « et uela erunt », littéralement « et il y aura des voiles ».
Les représentations des vela romains ont commencé dès la fin du XIXe siècle. Les premières tentatives étaient plutôt fantaisistes, sans avoir comme base des faits archéologiques. En 1999, Philippe Fleury, un archéologue français, a proposé une représentation du velum assez différente : un immense anneau central était levé 18 mètres au-dessus de l’arène afin de soutenir les 240 pans du velum, qui étaient déployés à l’aide d’un ensemble de treuils et de cordages (qui pesaient 68 tonnes). Le velum avait alors, la forme d’un entonnoir. Jusqu’à nos jours, cette représentation reste une des plus respectées dans le domaine de l’archéologie mais elle ne présente presque aucune ressemblance avec le velum de la peinture de Pompéi[62],[63].
Le Colisée a été utilisé pour accueillir des combats de gladiateurs et d'autres jeux très variés. Le matin, après un tour initial sur l’arène de tous les participants pour se présenter au public (la pompa gladiatoria), avait lieu un type de spectacle très populaire : la chasse aux animaux sauvages, ou venatio, qui faisait appel à une grande variété de bêtes sauvages, principalement importées d'Afrique, telles que rhinocéros, hippopotames, éléphants, girafes, lions, panthères, crocodiles, gnous et autruches. Des batailles et des chasses étaient souvent mises en scène parmi des décors comprenant des arbres et des bâtiments. Pendant l’après-midi se déroulaient les spectacles appelés munera qui ont toujours été donnés par des individus (appelés munerarii ou editores) plutôt que par l'État. Ils avaient une forte connotation religieuse, mais démontraient aussi la puissance et le prestige de la famille, auprès de la population qui les appréciait immensément[64]. Ces fêtes prenaient parfois une ampleur exceptionnelle : il est rapporté que Trajan, en 107, a fêté ses victoires sur les Daces par des jeux impliquant 11 000 animaux et 10 000 gladiateurs, durant 123 jours.
Les écrivains anciens rapportent que le Colisée a vu se dérouler, dès les premiers jours (et avant la construction de l'hypogée), des naumachies, plus communément appelées navalia proelia (reconstitutions et mise-en-scène de combats navals avec de vrais morts).
Il est consigné dans les comptes des jeux inauguraux offerts en 80 par Titus que l'arène remplie d'eau a alors accueilli des courses de chevaux et de taureaux spécialement entraînés à nager. Il est également fait état de la reconstitution d'une très fameuse bataille navale entre les Grecs de Corfou et de Corinthe.
Cela a fait l'objet d'un débat entre les historiens, bien que l'approvisionnement en eau n'eût pas été un problème, de savoir comment on avait pu rendre l'arène suffisamment étanche et trouver assez d'espace pour y faire évoluer des navires de guerre. On peut penser que ces grands spectacles navals avaient lieu dans le volume vide occupé par la suite par l'hypogée, tel que nous le voyons encore aujourd'hui[13].
Des sylvae ou recréations champêtres ont également eu leur place dans les Jeux de l'amphithéâtre. Des peintres, techniciens et architectes s'appliquaient à reconstituer toute une forêt, avec de vrais arbres et arbustes plantés dans le sable de l'arène. Cette forêt apparaissait progressivement peuplée d'animaux introduits tour à tour pour le plus grand plaisir de la foule. Ces scènes pouvaient simplement montrer à la population urbaine les scènes de la nature sauvage, ou bien devenir la toile de fond de chasses ou de scènes illustrant des épisodes de la mythologie.
Pendant l'intervalle du déjeuner, sur l'arène on exécutait des condamnations à mort. Lors de l'exposition aux bêtes, la damnatio ad bestias, le condamné était généralement lié à un poteau et poussé vers les bêtes. Occasionnellement, les décors ont pu être utilisés pour des exécutions dans lesquelles le héros de l'histoire - joué par un malheureux condamné - était tué de la façon dont le relataient les récits mythologiques -, dévoré par des fauves ou d'une autre façon[65]. Cette partie du spectacle était la moins prisée, de nombreux spectateurs en profitant pour s'alimenter ou se rafraîchir à la cinquantaine de fontaines disposées dans le Colisée[66].
En raison de son abandon et de ses utilisations successives, le Colisée est abondamment colonisé par différentes communautés végétales. Plusieurs botanistes se sont intéressés à l'analyse de cette flore particulière, la première datant de 1643 par Panaroli (it)[67]. Au XIXe siècle, des listes de la flore spontanée sont compilées en 1815 par Sebastiani (d)[68], en 1855 par Deakin (d)[69] et en 1874-1878 par Mazzanti (it)[70]. En 1951, Anzalone (d) entreprend une vaste analyse de la flore des murs de Rome, dont celle du Colisée[71]. Enfin, après des désherbages, dont certains avec des biocides chimiques, un nouveau relevé floristique est effectué en 2001[72].
Le regard porté sur ces ruines colonisées par une végétation spontanée change selon les époques et les botanistes. Par exemple, lorsque l'érudit italien Poggio Bracciolini le visite en 1430, il se lamente sur le site des ruines et s'exclame.  Mais d'autres ont vu une beauté romantique de l'arène réensauvagée comme Charles Dickens, dans ses Images d'Italie de 1846[73] :
« Le voir s'écrouler là, d'un pouce par an ; ses murs et ses arches envahis par la verdure ; ses couloirs ouverts au jour ; l'herbe haute qui pousse dans ses porches ; les jeunes arbres d'hier, qui poussent sur ses parapets déchiquetés, et qui portent des fruits : produit fortuit des graines que laissent tomber les oiseaux qui construisent leurs nids dans ses fentes et ses recoins ; voir sa fosse de combat remplie de terre... est le spectacle le plus impressionnant, le plus majestueux, le plus solennel, le plus grandiose, le plus majestueux, le plus triste qu'on puisse concevoir. »
Ces relevés comportent un maximum de 418 espèces, issues de 366 genres et de 84 familles principalement des Astéracées, des Poacées, des Fabacées, des Lamiacées et des Brassicacées. Le Colisée présente un faciès sec et chaud sur son côté sud et un faciès frais et humide au nord. Aux XVIIIe et XIXe siècles, les ruines sont pacagées par de petits troupeaux de brebis et de chèvres et quelques paysans y récoltent du foin. Deux petites centaines d'espèces sont très communes, et 135 sont fréquentes à toutes les périodes. Il s'agit principalement de plantes herbacées rudérales, largement répandues dans les pâturages plus ou moins nitrophiles, ou pionnières sur les rochers et les grèves. Les ligneux sont particulièrement représentés par de nombreux figuiers, oléastres, pistachiers et ronces à feuilles d'orme. Il constituent un préalable aux plantes forestières qui restent cependant rares[72],[74].
L'inventaire de 2001 comptabilise 242 espèces. 117 ont disparu de la flore du Colisée depuis le XIXe siècle tout comme de la Flore de Rome. 70 espèces ne sont répertoriées qu'à partir de 1951. Ces dernières sont des néophytes principalement d'origine américaine, issues de plantes cultivées et naturalisées comme Ailanthus altissima, Celtis australis, Ulmus minor, Acer negundo, Broussonetia papyrifera, Platanus hybrida et Robinia pseudoacacia. Parmi les herbacées, les annuelles sont plus représentées que les pérennes ; ceci étant certainement lié aux désherbages plus fréquents. De même, les espèces propres aux habitats riches en azote sont plus présentes, alors que les espèces liées aux prairies et champs cultivées diminuent. C'est également le cas des espèces liées au climat sec et chaud qui dominent au XXIe siècle et suivent les contraintes du changement climatique. Tout de même, certaines plantes trouvent refuge au sein des ruines du Colisée comme Asphodelus fistulosus et Sedum dasyphyllum qui se révèlent très rares dans l'environnement urbain alentour[72],[74].
Le Colisée est devenu une attraction touristique majeure de Rome. Au XIXe siècle, le peintre Camille Corot le représente À travers les arcades de la Basilique de Constantin dans un tableau de 1825[75], et du haut des jardins Farnèse en 1826[76]. Ces tableaux sont conservés au Musée du Louvre.
Le Colisée a également été le site de cérémonies catholiques depuis le XXe siècle. Par exemple, le pape Jean-Paul II y a inauguré une nouvelle forme de processions du chemin de croix qui ont lieu chaque Vendredi saint[77],[78].
Actuellement avec des milliers de touristes chaque année qui paient leur billet pour voir seulement l'arène de l'intérieur, bien que l'entrée pour les citoyens de l'UE soit partiellement subventionnée et que les moins de 18 ans, ainsi que les plus de 65 ans ressortissants de l'UE soient admis gratuitement[79]. En 2001, un musée consacré à Éros est situé à l'étage supérieur du bâtiment. Une partie du plancher de l'arène est reconstituée la même année.
Variations de température et d'humidité, secousses telluriques, trombes d'eau, pollution urbaine rongeant la pierre, surfréquentation... le Colisée est un géant malade. Chaque année, 500 000 euros lui sont alloués par l'État italien pour parer au plus urgent. Ce qui ne suffit pas pour des travaux plus ambitieux. Avec 7,6 millions de visiteurs par an (ce qui en fait le monument le plus visité d'Italie) et seulement 35 % du monument accessible au public en 2010, le Colisée poursuit ses restaurations pour éviter l'engorgement. En 2010, il ouvre ainsi une partie de l'hypogée à des visites guidées[80]. Face à la réduction du budget du ministère des Biens culturels, le site a dû se tourner vers le mécénat privé pour boucler le budget : en 2011, un accord signé avec Diego Della Valle, le PDG de la marque de chaussures Tod's, permet à ce groupe de se prévaloir d'être « le sponsor unique du Colisée » en finançant entièrement les travaux (nettoyage de la pierre noircie par la pollution, colmatage des fissures et brèches, remplacement des barrières métalliques obturant les arches du rez-de-chaussée, restauration de l'hypogée, mise en place d'un nouveau système d'illumination et construction d'un centre de services touristiques), soit 25 millions d'euros dont un tiers peut être déduit fiscalement[81]. En échange, l'association « Amici del Colosseo » créée par le sponsor a le droit exclusif d'utiliser l'image du monument pour ses publicités[82]. Le Colisée reste ouvert au public pendant ces travaux, prévus de 2013 à 2016, les échafaudages ne couvrant qu'un tiers du monument à la fois. Le nombre de visiteurs du Colisée est passé en une dizaine d'années d'un million par an à environ six millions en 2013, entre autres grâce au succès du film Gladiator de Ridley Scott en 2000[83].
Cet article concerne l'opéra occidental. Pour l'opéra chinois, voir opéra chinois. Pour les autres significations, voir Opéra (homonymie).
Dans la musique occidentale, un opéra est une œuvre musicale et théâtrale pour un orchestre et des chanteurs. L'opéra est l’une des formes de l'art lyrique du théâtre musical occidental.
L'œuvre, chantée par des interprètes possédant un registre vocal déterminé en fonction du rôle et accompagnés par un orchestre, parfois symphonique, parfois de chambre, parfois destiné au seul répertoire d'opéra, est constituée d'un livret mis en musique sous forme d'airs, de récitatifs, de chœurs et d'intermèdes souvent précédés d'une ouverture et parfois agrémentés de ballets. Comme pour le théâtre, le livret met en scène les personnages et leur histoire, mais les roles sont chantés.
Le genre musical est décliné selon les pays et les époques et recouvre des œuvres d’appellations et de formes différentes. Aujourd’hui, les œuvres sont jouées dans des salles d’opéra spécifiquement affectées ou tout simplement sur des scènes de théâtre ou dans des salles de concerts, voire en plein air.
Les représentations sont organisées par des institutions du secteur public ou privé, parfois désignées sous le vocable de « maison d'opéra », qui peuvent regrouper les compagnies d’artistes (orchestre, chœur et ballet) et les services administratifs et techniques nécessaires à l’organisation des saisons culturelles.
Selon Serge Dorny, directeur du Bayerische Staatsoper, 
L’opéra occidental est né en Italie à Florence au XVIIe siècle. Parmi les ancêtres de l'opéra figurent les madrigaux italiens, qui mirent en musique des situations avec des dialogues mais sans jeu de scène. Les mascarades, les ballets de cour, les intermezzi, ainsi que d'autres spectacles de cour de la Renaissance, faisant intervenir des figurants, de la musique et de la danse, sont autant de précurseurs. L’opéra proprement dit émane d’un groupe de musiciens et d'intellectuels humanistes florentins qui s'étaient donnés le nom de Camerata (« salon » en florentin). La Camerata, appelée aussi Camerata fiorentina ou encore Camerata de' Bardi, du nom de son principal mécène, s’était fixé deux objectifs principaux : faire revivre le style musical du théâtre grec antique et s’opposer au style contrapuntique de la musique de la Renaissance. En particulier, ils souhaitaient que les compositeurs s'attachent à ce que la musique reflète, simplement et mot pour mot, la signification des textes, les mette en valeur et non les rende incompréhensibles par la complexité des architectures sonores de son accompagnement. La Camerata pensait reprendre en cela les caractéristiques de la musique grecque antique. Pour atteindre ce but, on utilise la monodie accompagnée par la basse continue, les chœurs madrigalesques et les ritournelles et danses instrumentales.
En 1598 à Mantoue, Jacopo Peri écrit Dafne (en), que l'on considère alors comme l'un des premiers opéras (on parle à cette époque de dramma per musica). L'Orfeo (1607) de Claudio Monteverdi est également cité de nos jours.
Le premier grand compositeur d’opéras fut Claudio Monteverdi. Ses opéras (L'Orfeo, 1607 ; Ariane, 1608 ; Le Retour d'Ulysse dans sa patrie, 1640 ; Le Couronnement de Poppée, 1642) appliquaient les bases de l’opéra, définies à Florence par la Camerata de Bardi à la fin du XVIe siècle, en réaction contre les excès de la polyphonie de la Renaissance. Si Claudio Monteverdi n'est pas le premier compositeur à traduire ce programme (le premier opéra, Dafne, étant attribué à Jacopo Peri en 1598 chez le comte Bardi), c'est lui qui porta dès ses débuts l'opéra à un état de perfection qui suscita l'émulation des autres musiciens et la faveur du public.
L’opéra se répandit rapidement dans toute l’Italie, mais assez vite, les intentions initiales des créateurs de l'opéra sont dévoyées, le chant prenant progressivement la primauté sur la déclamation. La diffusion du nouveau type de spectacle touche d'abord Rome (Stefano Landi, Luigi Rossi) et plus encore Venise devenue le principal centre de l’opéra en Italie au milieu et à la fin du XVIIe siècle (Cavalli, Cesti, Legrenzi, plus tard Caldara, Lotti, Vivaldi, etc.). En 1637, dans cette ville, l'ouverture pour la première fois du théâtre San Cassiano à un public payant a eu pour conséquence d'élargir l'audience de l'opéra au-delà des cours fréquentées uniquement par la noblesse et d'accroître son importance artistique et sociale. L'opéra de tradition vénitienne mêle souvent aspects tragiques et comiques voire burlesques, fait intervenir magie et merveilleux, multiplie les personnages et les genres musicaux. La dernière école à apparaître est celle de Naples, elle finira par imposer son style à toute la péninsule italienne et à presque toute l'Europe. L'opéra napolitain va être l'objet d'une lutte d'influence continuelle entre librettistes, musiciens et chanteurs. Les librettistes considèrent que la musique doit être au service du texte, les musiciens, que seule la musique donne vie et consistance à l'œuvre ; les chanteurs usent de leur étonnante virtuosité pour imposer l'évolution de l'opéra vers une simple succession d'arias, les récitatifs n'étant que des intermèdes permettant un enchaînement logique des arias. La tradition napolitaine connaît ainsi plusieurs « réformes » visant à retourner aux fondamentaux. Fondée par Provenzale, son héros principal est Alessandro Scarlatti, qui introduit l'ouverture à l'italienne, réduit la structure musicale à l'alternance « récitatif/aria da capo », met en vedette la virtuosité des chanteurs (et surtout celle des castrats) et favorise l'évolution du dramma per musica vers l’opera seria (qui se différencie alors de l’opera buffa) composé sur les livrets d'Apostolo Zeno et surtout de Pietro Metastasio, partisans d'une épuration inspirée par l'exemple des poètes classiques français (Corneille et plus encore Racine) : respect des trois unités, élimination des éléments comiques et merveilleux, limitation du nombre de personnages... L'école napolitaine brilla particulièrement au XVIIIe siècle avec A. Scarlatti, Porpora, Vinci, Leo, Jommelli, etc.
Le genre fut adopté par les musiciens allemands ayant séjourné en Italie, rivalisant alors avec les Italiens eux-mêmes : Haendel composa 43 opéras, Hasse pas moins de 56 opéras. Le genre fut ensuite importé dans les autres pays d'Europe. En France, Jean-Baptiste Lully et Jean-Philippe Rameau composèrent des opéras.
La production d'opéras italiens est énorme au XVIIIe siècle, et réutilise à l'infini les mêmes livrets des auteurs les plus appréciés, en particulier Métastase.
Au XIXe siècle, l’opéra italien continua de laisser une place de choix à la voix. Gioachino Rossini composa des opéras-bouffes comme Le Barbier de Séville (1816) et La Cenerentola (1817), qui ont éclipsé ses œuvres plus dramatiques, comme Guillaume Tell (1829). Le style du bel canto, caractérisé par des airs coulants, expressifs et souvent spectaculaires, s’est également épanoui dans les œuvres de Vincenzo Bellini, dont Norma (1831), La sonnambula (1831) et I puritani (1835), ainsi que dans les opéras de Gaetano Donizetti, Lucia di Lammermoor (1835), ou dans ses comédies L'Élixir d'amour (1832) et Don Pasquale (1843). La deuxième moitié du XIXe siècle, en Italie, laissera la place à Verdi puis aux véristes, dont Puccini sera le principal vecteur.
L’homme qui a personnifié l’opéra italien est sans conteste Giuseppe Verdi : il a insufflé à ses œuvres une vigueur dramatique et une vitalité rythmique inégalées. Il composa nombre d’opéras dont Nabucco (1842), Ernani (1844) Rigoletto (1851), Il trovatore (Le Trouvère, 1853), La traviata (1853), Un ballo in maschera (Un bal masqué, 1859), La forza del destino (La Force du destin, 1862) et Aïda (1871), qui associe les splendeurs visuelles du grand opéra aux subtilités musicales d’une histoire d’amour tragique. Néanmoins, les opéras de Verdi restent profondément italiens, utilisant la voix humaine comme principal moyen d’expression.
L'opéra italien arrive en France en 1645 : le cardinal Mazarin avait fait venir de Venise une troupe qui interpréta La finta pazza à la cour de Louis XIV : le succès est immédiat. Mais il faut attendre 1671 pour voir le premier opéra réellement « français » : Pomone, de Robert Cambert et Pierre Perrin.
Au début du XVIIe siècle, le style napolitain s’établit dans pratiquement toute l’Europe, sauf en France où le compositeur Jean-Baptiste Lully, musicien de Louis XIV, fonda une école française d’opéra : la tragédie lyrique. Ses compositions reflétaient le faste de la cour de Versailles. Le ballet avait une place beaucoup plus importante dans les opéras français (tragédies lyriques) de Lully que dans les opéras italiens. Lully créa également un type d’ouverture, l’ouverture à la française. Alceste (1674), Atys (1676), Roland (1685), Armide (1686), Acis et Galatée (1686) restent ses chefs-d’œuvre.
Jean-Philippe Rameau avec Hippolyte et Aricie (1733), Castor et Pollux (1737) et Dardanus (1739), Les Indes galantes (1735), et Les Boréades (1764) ; Marc-Antoine Charpentier avec Médée (1693) et David et Jonathas (1684) ; André Campra avec Achille et Déidamie (1735) enrichirent à leur tour l’héritage de Lully. Après la mort de Rameau en 1764, s'ouvre une période pendant laquelle sont repris les livrets écrits par Quinault pour Lully et viennent s'installer à Paris de prestigieux compositeurs étrangers comme Gluck, Piccinni, Salieri, Sacchini ou Jean-Chrétien Bach.
Au cours du XIXe siècle, le romantisme se développa en France, en Allemagne et en Italie, et gagna l’opéra. Paris était alors le berceau du « grand opéra », combinaison de spectacle à grands effets, d’actions, de ballets et de musique. La plupart des opéras de ce style furent écrits par des compositeurs étrangers installés en France : La Vestale (1807) de Gaspare Spontini et Lodoïska (1791) de Luigi Cherubini, tous deux Italiens, ainsi que Masaniello, ou La Muette de Portici (1828), de Daniel-François-Esprit Auber (1782-1871). Ce style atteignit son apogée dans les œuvres fluides du compositeur Giacomo Meyerbeer, comme Robert le Diable (1831) et Les Huguenots (1836). Faust (1859), de Charles Gounod, fut l’un des opéras français les plus populaires du milieu du XIXe siècle et il est toujours très présent à l'affiche au XXIe siècle.
Le compositeur français le plus productif de la dernière partie du XIXe siècle fut Jules Massenet, auteur notamment de Manon (1884), Werther (1892) et Thaïs (1894). Les autres œuvres caractéristiques de la période sont Mignon (1866) d’Ambroise Thomas, Carmen de Bizet (1875), Samson et Dalila (1877) de Camille Saint-Saëns et Lakmé (1883) de Léo Delibes. On peut aussi parler du travail de Jacques Offenbach (auteur des Contes d'Hoffmann), compositeur parisien né en Allemagne qui s’imposa comme le maître de l’opéra-comique français du XIXe siècle, appelé opéra-bouffe. En 1900, Gustave Charpentier composa Louise, opéra réaliste d’un style très différent, mettant en scène des ouvriers de Paris.
Au tout début du XXe siècle, Claude Debussy renouvela le genre de l’opéra avec Pelléas et Mélisande (1902).
Parmi les autres compositeurs notables on peut noter Maurice Ravel (L'Heure espagnole, L'Enfant et les Sortilèges), Paul Dukas (Ariane et Barbe-Bleue), Albert Roussel, Darius Milhaud (La Mère coupable), Arthur Honegger (Antigone) et Francis Poulenc (Dialogues des Carmélites).
Pour la période contemporaine on peut retenir le long drame sacré Saint François d'Assise (1983) d'Olivier Messiaen.
Heinrich Schütz écrit, en 1627, le premier opéra sur des paroles allemandes, Dafne, dont la musique est perdue[2].
C’est en Angleterre que le compositeur d’origine allemande Georg Friedrich Haendel (1685-1759) fut le plus apprécié. Il écrivit quarante opéras dans le style italien pendant les années 1720-1730, après quoi il se tourna vers l’oratorio.
Christoph Willibald Ritter von Gluck (1714-1787) fut le réformateur de l'opéra classique allemand en introduisant le dramatique dans ses compositions. Il fut à l'origine de la Querelle des Gluckistes et des Piccinnistes alors qu'il était à Paris (1775-1779).
Wolfgang Amadeus Mozart (1756-1791) a écrit lui aussi des opéras, une vingtaine en tout si l'on compte les « actions théâtrales » mises en musique. Mozart composa son premier opera seria (œuvre sérieuse en italien) à l'âge de 14 ans, en 1770, pour une commande milanaise. Ce fut Mitridate, re di Ponto (Mithridate, roi du Pont), d'après une tragédie de Racine.
Dans les années 1780, l'empereur d'Autriche voulut créer un genre théâtral national, dans lequel les opéras seraient évidemment chantés en allemand. C'est dans ce contexte que fut composé le Singspiel Die Entführung aus dem Serail (L'Enlèvement au sérail). Néanmoins, l'empereur ne donna pas suite à sa lubie, et l'opéra allemand dut attendre Wagner pour se faire un nom.
Mozart composa vers la fin de sa vie cinq de ses opéras les plus joués. Les trois premiers (Le nozze di Figaro, Don Giovanni et Così fan tutte) sont considérés comme une trilogie, car leur livret a été écrit par le même auteur, Lorenzo da Ponte, un aventurier aux mœurs légères (il était l'ami de Casanova, et à la fin de sa vie, exilé aux États-Unis, il fera donner l'un des premiers opéras chantés sur le sol américain, à savoir Don Giovanni). Don Giovanni avait été créé en 1787 à Prague.
En 1791, l'année de sa mort, Mozart composa deux opéras : le premier, La clemenza di Tito (la Clémence de Titus), est aujourd'hui considérée comme l'un des meilleurs operas serias jamais écrits[réf. nécessaire]. Le deuxième, la Flûte enchantée, a notamment été filmée par Ingmar Bergman. Ce dernier opéra doit son livret à Schikaneder, un organisateur de spectacles alors lourdement endetté qui vit dans la Flûte enchantée l'occasion de se refaire une santé. Le plus redoutable de ses opéras pour la technique et les suraigus qu'il exige. Un air interprété par la Reine de la Nuit qui s'intitule Der Hölle Rache kocht in meinem Herzen (Les flammes de l'enfer me dévorent le cœur) monte au contre-fa, sommet de la voix humaine.
L'opéra romantique allemand de Ludwig van Beethoven, Carl Maria von Weber, Richard Wagner et Richard Strauss, est l'héritier musical du Singspiel, qui devient rapidement obsolète au XIXe siècle.
Le premier grand opéra allemand du XIXe siècle est Fidelio (1805) de Ludwig van Beethoven (1770-1827).
Carl Maria von Weber (1786-1826) composa les opéras romantiques allemands Der Freischütz (1821) et Euryanthe (1823).
L’opéra allemand atteignit l’un de ses sommets avec Richard Wagner (1813-1883) qui donna naissance à ce qu’il a appelé le « drame en musique », dans lequel le texte (dont il était l’auteur), la partition et la mise en scène étaient inséparables. Ses premiers opéras, tels que Le Vaisseau fantôme (1843), Tannhäuser (1845) et Lohengrin (1850), conservèrent des éléments de l’ancien style. Ses plus grandes œuvres furent Tristan et Isolde (1865), les quatre opéras composant l’Anneau du Nibelung (1852-1874, comprend L'Or du Rhin, La Walkyrie, Siegfried et Le Crépuscule des dieux), Les Maîtres chanteurs de Nuremberg (1868), où il décrivit les guildes médiévales, et Parsifal (1882). Les œuvres de Wagner font un grand usage du leitmotiv, terme musical identifiant un personnage ou une idée revenant régulièrement dans toute l’œuvre.
L’influence de Wagner se poursuivit dans pratiquement tous les opéras. Un des rares opéras à se détacher du lot est Hänsel et Gretel d’Engelbert Humperdinck (1893), inspiré du conte du même nom.
Au début du siècle la figure dominante est Richard Strauss, qui utilise une orchestration et des techniques vocales similaires à celles de Wagner dans Salomé (1905) et les poussa à l'extrême dans Elektra (1909). Le Chevalier à la rose (1911) devint son œuvre la plus populaire. Cet opéra fut suivi, entre autres, d’Ariane à Naxos (1912), de La Femme sans ombre (1919) et d’Arabella (1933).
L'opéra fut introduit en Russie dans les années 1730 par des troupes italiennes et fit bientôt partie des divertissements de la cour impériale et de l'aristocratie. De nombreux compositeurs étrangers, comme Baldassare Galuppi, Giovanni Paisiello, Giuseppe Sarti et Domenico Cimarosa furent invités en Russie et reçurent des commandes d'opéras, principalement en langue italienne. Parallèlement à cela, quelques musiciens natifs (par exemple Maxim Berezovski et Dmitri Bortnianski) étaient envoyés en Europe occidentale pour y étudier la composition musicale. Le premier opéra composé en langue russe fut Céphale et Procris (en) du compositeur italien Francesco Araja (1755). Les compositeurs Vassili Pachkevitch (en), Evstigneï Fomine (en) et Alexeï Verstovski contribuèrent au développement de l'opéra de langue russe.
Toutefois le véritable acte de naissance de l'opéra russe est dû à Mikhail Glinka et à ses deux opéras, Une vie pour le tsar (1836) et Rouslan et Ludmila (1842). Parmi les principaux successeurs de Glinka on peut citer Alexandre Dargomyjski (La Roussalka (en) et Le Convive de pierre), Modeste Moussorgski (Boris Godounov (1874) et La Khovantchina), Alexandre Borodine (Le Prince Igor (créé en 1890, de manière posthume), Nikolaï Rimski-Korsakov (La Demoiselle des neiges (Sniegourotchka), Sadko et Le Coq d'or (1909)) et Tchaïkovski (Eugène Onéguine et La Dame de pique).
La grande majorité de ces œuvres montre l'importance croissante du nationalisme russe, composante d'un mouvement slavophile plus vaste, dans l'ensemble de la création artistique. L'œuvre de Pouchkine, considéré comme le fondateur de la littérature russe, a fourni l'intrigue d'une grande partie de ces opéras (notamment Rousslan et Ludmilla, Eugène Onéguine, Boris Godounov, Le Convive de pierre, Le Chevalier avare, La Roussalka, La Dame de pique, Le Coq d'or).
L'opéra russe continua fortement d'exister au XXe siècle. Citons notamment[3] :
L’opéra espagnol a produit des centaines voire des milliers d’ouvrages depuis le début du XVIIe siècle jusqu’à nos jours. L’Espagne constitue même historiquement, après l’Italie, le premier pays où l’art lyrique a éclos.
Au cœur du Siècle d'or espagnol, qui connaît une floraison théâtrale, est représenté le premier opéra espagnol : La gloria de Niquea[4] (sur une musique de Matheo Romero, Juan de Palomares, Juan Blas de Castro et Álvaro de los Ríos) au Palais d'Aranjuez. Cette création en 1622 suit de peu Rome, mais précède Venise dans l’expérimentation du genre lyrique. La France et l’Allemagne devront encore attendre. Succède, en 1627, La selva sin amor[5], autre pièce théâtrale entièrement chantée sur un livret de Félix Lope de Vega. L’œuvre fut exécutée au château royal de l’Alcazar de Madrid. Peu après, c’est le Palais du Buen Retiro qui devient le réceptacle habituel des ouvrages lyriques de la cour espagnole. Ce palais madrilène, aujourd’hui disparu (à la suite de son incendie par l’Armée napoléonienne en déroute en 1808)[6], comportait un théâtre couvert à l’image de la toute nouvelle mode italienne. Car en ces temps, la péninsule italienne était en large partie sous la domination politique de l’empire espagnol. D’où, des échanges culturels et artistiques. Il sera même des compositeurs d’origine italienne qui écriront des œuvres lyriques pour l’Espagne à partir de livrets en espagnol, comme plus tard Francisco Corradini (1700-1769) ou Luigi Boccherini (1743-1805).
Peu après la création de l’opéra en Espagne, naît un genre dérivé : la zarzuela, en 1648 avec El jardín de Falerina[7], au Palais royal de la Zarzuela (aux environs de Madrid). La zarzuela se distingue de l’opéra (intitulé qui n’existait pas encore, en Espagne ni même en Italie) par l’introduction de passages parlés parmi les scènes chantées (comme pour l’opéra-comique français ou le Singspiel allemand, genres qui eux n’apparaîtront que plus d’un siècle après). Mais il est difficile de faire des catégories tranchées entre opéra espagnol et zarzuela, tant les hybrides abondent.
Durant les XVIIe et XVIIIe siècles, il est une multitude d’ouvrages lyriques sur des livrets en espagnol (plus d’un millier, bien que beaucoup de partitions aient disparu, notamment dans l’incendie du Palais du Buen Retiro 
[8]
). Au XVIIe siècle, se distinguent, parmi tant d’autres, les compositeurs lyriques Cristóbal Galán, Juan de Navas, Juan de Serqueira et surtout Juan Hidalgo (1614-1685). De ce dernier, a été conservé Celos aun del aire matan[9], créé en 1660 sur un livret de Pedro Calderón de la Barca. Au XVIIIe siècle, les compositeurs marquants sont Sebastián Durón (1660-1716) – auteur de la première œuvre répertoriée à porter l’intitulé espagnol « ópera », La guerra de los gigantes, datée de 1700 –, Antonio de Literes (1673-1747), José de Nebra (1702-1768) et Antonio Rodríguez de Hita (1724-1787).
Aux XIXe et XXe siècles, se comptent environ six cents opéras espagnols (différents, donc, des zarzuelas). Avec par exemple, de 1880 à 1910, plus de cinquante créations d’opéras, dont une trentaine pour la seule décennie 1890. De ces deux siècles, parmi les compositeurs espagnols d’opéras les plus connus, peuvent être cités : Emilio Arrieta (1821-1894, auteur de Marina), Ruperto Chapí (1851-1909, auteur de Margarita la Tornera), Antonio Reparaz (1831-1886), Tomás Bretón (1850-1923, auteur de La Dolores), Valentín Zubiaurre (1837-1914), Emilio Serrano (1850-1939), Felipe Pedrell (1841-1922), Enrique Granados, Isaac Albéniz, Manuel de Falla, Joaquín Turina, Conrado del Campo (1878-1953, auteur de La tragedia del beso), Amadeo Vives (1871-1932, auteur de Artús et Maruxa), Manuel Penella (1880-1939, auteur de El gato montés), Jesús Guridi (1886-1961, auteur de Mirentxu), Federico Moreno Torroba (1891-1982, auteur de El poeta), José Serrano Simeón (1873-1941, auteur de La venta de los gatos), Pablo Sorozábal (1897-1988, auteur de Adiós a la bohemia et Juan José)… Mais à côté des quelques titres d’opéras qui sont ici mentionnés, les uns et les autres de ces compositeurs auront aussi composé nombre de zarzuelas[10].
L'opéra polonais naît en 1628 avec la représentation de Galatea de Sante Orlandi et de La liberazione di Ruggiero dall'isola d'Alcina de Francesca Caccini à Varsovie donnée à l'initiative du prince Ladislas IV Vasa. Après son accession au trône, celui-ci crée au sein du palais royal un théâtre d'opéra où l'on présente fréquemment les œuvres de Marco Scacchi, sur les livrets de Virgilio Puccitelli. Subsiste du premier opéra en polonais le manuscrit de Heca (La Drôlerie), créé entre le XVIIe et le XVIIIe siècle. On peut trouver dans les poèmes d'Adam Korczyński (XVIIIe siècle) plusieurs influences de l'opéra. En 1748 le roi Auguste III de Pologne fonde un nouvel opéra à Varsovie ou sont représentées les œuvres de Johann Adolf Hasse, parmi lesquelles Zenobia, sur un livret de Pietro Metastasio, créé pour le théâtre polonais. Durant le règne de Stanislas II de Pologne l'opéra varsovien est dirigé par Wojciech Bogusławski qui, en collaboration avec les tchèques polonisés Maciej Kamieński et Jan Stefani, donne plusieurs opéras en polonais avec des allusions contre les futurs occupants de la Pologne (principalement la Russie). Le plus important opéra de ce temps est Cud mniemany, albo Krakowiacy i Górale (Miracle supposé ou les Cracoviens et les montagnards). Après la chute de la Pologne, le théâtre de Varsovie continue à présenter des opéras de Karol Kurpiński et Józef Elsner. Le Faust du prince Antoni Henryk Radziwiłł est la première réalisation opératique de l'œuvre de Johann Wolfgang von Goethe. Les plus importants opéras polonais de la période romantique sont ceux de Stanisław Moniuszko, ancrés dans le cœur de toute la nation polonaise, sans toutefois devenir populaires dans le monde. La représentation de Manru de Ignacy Paderewski au Metropolitan Opera en 1902 est un épisode important pour l'histoire de l'opéra polonais. La renaissance de la Pologne en 1918 est dominée par Karol Szymanowski avec son opéra Król Roger (Le Roi Roger), sur un livret de Jarosław Iwaszkiewicz. L'opéra moderne polonais est représenté par Tadeusz Baird (dont l'œuvre la plus considérable est Jutro - Demain), Krzysztof Penderecki (Les Diables de Loudun, Paradise Lost, Le Masque noir, Ubu Rex), Zygmunt Krauze (Yvonne, princesse de Bourgogne) et Paweł Mykietyn (L'Ignorant et le Fou). En 2006 est représenté à Moscou l'opéra Pasażerka (La Passagère, d'après le roman polonais de Zofia Posmysz), composé par Mieczyslaw Weinberg, polonais juif russifié, dont le sujet s'intéresse aux bourreaux et aux victimes du camp d'Auschwitz. Il sera ensuite présenté en 2010 à Varsovie.
Dans les Pays-Bas espagnols, l'opéra italien fut introduit en 1650 par Giuseppe Zamponi, maître de chapelle à la cour de Bruxelles[11]. C’est à Bruxelles, au palais du gouverneur Léopold-Guillaume de Habsbourg, que fut représenté l’opéra de style vénitien Ulisse nell'isola di Circe de Zamponi, dont le prologue et les actes étaient entrecoupés du Ballet du monde du maître de danse Giambattista Balbi. Ce fut la première fois qu'un véritable opéra a été mis en scène aux Pays-Bas[12].
À Amsterdam, l'opéra ne fut pas introduit avant 1677[11]. Le premier opéra réalisé en république des Provinces-Unies, en 1677, est Isis de Giovanni Battista Lulli et de Philippe Quinault, représenté au Théâtre d'Amsterdam[13]. La plus ancienne production d’art lyrique en langue néerlandaise se rapprochant du genre de l’opéra, et créée aux Pays-Bas septentrionaux en 1686, est l'opéra sur la devise « Sans le vin et la bonne chère, plus d'amour »[14] dont les paroles sont de Govert Bidloo et dont la musique est de Johann Schenck. L'opéra De triomfeerende min (en français : Le Triomphe de l'amour), composé par Carolus Hacquart en 1678, après la conclusion du traité de Nimègue sur un livret de Dirck Buysero, n’aurait jamais été réalisé[15].
L'engouement pour l'opéra a permis de produire les premières œuvres littéraires en wallon, qui contribuèrent à conférer un statut respectable à cette langue. Les quatre livrets de Simon de Harlez, de Cartier, Fabry et Vivario, connus sous le nom de « théâtre liégeois », furent créés en 1756, et joués régulièrement sous l'Ancien Régime devant les princes invités en Principauté de Liège. Ils furent republiés par François Bailleux en 1854 et contribuèrent à la naissance de la Société de langue et littérature wallonnes en 1856.
En 1757, Jean-Noël Hamal, formé à Liège et à Rome, a mis en musique ces opéras en wallon, dont Li Voyedje di Tchofontaine (Le Voyage de Chaudfontaine)[16].
Pour les articles homonymes, voir Karaté (homonymie).
Cet article ne cite pas suffisamment ses sources (novembre 2015).
Le karaté (空手道, karate-dō?) est un art martial, dit japonais. Cependant, il est originaire du royaume de Ryūkyū, un ancien royaume indépendant annexé par le Japon en 1879. Il s'est développé sur l'île Okinawa, île principale de l'archipel Ryūkyū, en dérivant de la boxe de la grue blanche, pratiquée dans la province du Fujian, lors d'échanges avec la Chine impériale des Qing.
En 2021, le karaté est un sport olympique provisoire lors des Jeux de Tokyo.
Il existe plusieurs styles de karaté dont le Shotokan-ryu, le Wado-ryu, le Shito-ryu, le Shorin-ryu ou encore le Goju-ryu.
En japonais, le kanji kara (空?) signifie le « vide », plus précisément la « vacuité » au sens bouddhique du terme ; té (手?) est la main et, par extension, la technique avec laquelle on la réalise. Dō (道?) signifiant « voie », karate-dō peut être traduit par « la voie de la main vide » ou « la voie de la main et du vide », compris dans le sens « la voie de la vacuité (au sens bouddhique/zen), réalisée par la main (les techniques) » ou dans le sens « combat à mains nues », les différentes interprétations ne s'excluant pas mutuellement.
À l'origine, « karaté » était écrit avec les kanjis 唐手 (tō-de : « main Tang » ou « main de Chine »). Le 25 octobre 1936, en raison de la montée du nationalisme japonais, et aussi à cause de l'antagonisme avec la Chine, et pour faciliter la reconnaissance et la diffusion du karaté, les principaux grands maîtres (dont Hanashiro Chomo, Motobu Chôki, Chôshin Chibana  et Chojun Miyagi)[3] ont remplacé ces kanjis pour « gommer » l'origine chinoise, sacrifiant ainsi à l'usage japonais du moment (remplacement par des kanjis de prononciation équivalente, d'« origine » japonaise).
Le karaté est une discipline martiale dont les techniques visent à se défendre uke (受け?), puis à répondre par une attaque (atemi (当て身?)) au moyen des différentes parties du corps : doigts (nukite), mains ouvertes (shuto) et fermées (tsuki), avant-bras (ude), pieds (geri), coudes (enpi), genoux [ex. : hiza geri]). Les 20 préceptes du karaté voudraient qu'il n'existe pas d'attaque pure et dure de la part d'un karatéka. 
Ainsi distingue-t-on chez le combattant nippon deux types de répliques à l'attaque: le go no sen, qui consiste en une contre-attaque une fois que l'adversaire a déjà porté ou tenté de porter son coup; et le sen no sen, généralement qualifié "d'attaque dans l'attaque", qui consiste quant à lui à porter la contre-attaque avant même que l'adversaire ait eu le temps d'exercer sa technique. Un sen no sen parfaitement maîtrisé pourrait laisser penser, d'un point de vue extérieur, à une anticipation, ou à une inversion des rôles attaquant / défenseur, du fait que le spectateur ne perçoit pas nécessairement les signes de déclenchement d'une attaque. Pour autant, la finesse d'exécution du sen no sen ne saurait remettre en cause la notion de riposte: le karatéka travaille toujours ses attaques sur un mode défensif.
Des nuances de contenus techniques et philosophiques sont relativement marquées en fonction du style (Shōrin-Ryu, Shōtōkan, Shōtōkai, Wadō-ryū, Shitō-ryū, Gōjū-ryū, etc.).
Pour acquérir la maîtrise de ces techniques en combat, l'enseignement comporte trois domaines d'étude complémentaires : le kihon (基本), les katas (型 ou 形) et le kumite (組手). Mais d'autres domaines d'étude font partie de l'apprentissage. Le placement et la maîtrise de la respiration sont essentiels à la compréhension des techniques de karaté. En outre, certains maîtres pratiquent la méditation zen.
En 480 ou 520, un moine nommé Bodhidharma quitta l’Inde pour s’installer dans le monastère Shaolin dans le nord de la Chine[4].
La tradition affirme que les bonzes, faméliques parce que mal nourris, ne pouvaient supporter l’immobilité que leur imposait la méditation. Bodhidharma se souvint alors de diverses formes gymniques, plus ou moins guerrières, qu’il avait étudiées pendant son jeune âge sous la direction de son père. Ce dernier était en effet, en plus d'être roi, un haut initié de la caste des kshatriya et connaissait donc l’art du combat, proche de ce qui est, actuellement en Inde, le kalaripayat. Il mit donc au point une méthode connue sous le nom évocateur de « Nettoyage des muscles et des tendons, purification de la moelle et des sinus », le yijing kingyi suijing, parfois écrit i chin ching, méthode connue également sous les dénominations de shi ba lo han she (shih pa loran sho) et de ekkinkyo (ekki kin kyo jya) en japonais.
Cette méthode mi-gymnique, mi-martiale provoqua de nombreuses réactions, puisqu’elle était considérée par certains comme étant à l’origine même des diverses pratiques martiales réputées du monastère de la Petite Forêt, donc de la plupart des arts martiaux chinois, et ce faisant des origines profondes des arts martiaux japonais (bujutsu et budō).
La diffusion de ces enseignements a été possible lors de l’invasion du temple Shaolin qui a forcé les moines à fuir dans toute la Chine et donc à diffuser ces techniques, y compris aux delà des frontières par le biais d'échanges. De nos jours, beaucoup de styles se disent toujours d’inspiration Shaolin.
Selon la tradition bouddhique, Bodhidharma serait le 28e descendant de Bouddha[5] et le fondateur du chán (zen en japonais), bouddhisme influencé par le taoïsme et le plus répandu en Chine (à l'exception du Tibet et de la Mongolie-Intérieure), enrichi par la culture coréenne avant d'arriver enfin au Japon[6].
Ces récits historiques de la création du karaté semblent néanmoins teintés du désir japonais de minimiser l’influence chinoise[4]. Il s’avère que des pratiques guerrières, ou martiales, étaient déjà très développées en Chine bien avant la venue de Bodhidharma.
Sunzi, général chinois, dans ses Treize Chapitres sur l’Art de la Guerre, ouvrage écrit au IVe siècle av. J.-C., traite, par exemple, de l’« art du poing » (quanfa ou chuan fa) et en conseille l’usage aux officiers, 800 ans avant la venue de l’Illuminé en Chine.
Les historiens japonais de la période nationaliste attribuaient la paternité des arts martiaux à Bodhidharma, donc au courant bouddhiste zen. Ils en avaient ignoré les origines taoïstes à dessein, à l'instar des shoguns et autres daimyos pendant toute l'histoire du Japon, depuis l'époque Kamakura, le zen étant ce qui convenait le mieux à une « caste guerrière ».
Ils passaient ainsi sous silence les autres versions issues d’une tradition chinoise, avec laquelle le Japon impérial avait historiquement peu d’affinités.
Ceux d'aujourd'hui ne font guère mieux, en attribuant au karaté ancestral okinawaïen les modifications qu'ils ont eux-mêmes apportées à certaines techniques ainsi qu'à leur nom, ou en qualifiant de « traditionnelles » les écoles modernes les plus récentes, maitre Gichin Funakoshi étant le « père » du karaté « moderne ». (Opposition des termes « traditionnel » et « moderne ».)
En outre, il semble qu'il y avait bel et bien cinq temples portant l'appellation Shaolin en Chine. Le moine bouddhiste aurait trouvé refuge non pas dans le monastère Shaolin du Quangzhou (d'où proviennent bien les applications martiales apparentées au kung-fu), mais dans celui de Songchan dans le He Nan, au centre de la Chine. Le monastère de Quangzhou étant situé bien plus au sud, son influence sur la pratique martiale d'Okinawa est incontestable. Beaucoup de biographies de grands maîtres du karaté attestent d'ailleurs de très longs séjours réalisés dans le sud de la Chine. C'est le cas notamment de Kanryō Higaonna, le maître du naha-te, et de Chojun Miyagi, son meilleur disciple et père du Goju-ryu, qui furent plutôt influencés par les traditions martiales taoïstes (travail basé sur la respiration abdominale, entre autres), mais aussi de Sakugawa Kanga ou Tode Sakugawa et de Sokon Matsumura, père du Shōrin-ryū, ancêtre du shōtōkai qui, eux, ont voyagé dans presque toute la Chine et ont été plutôt influencés par les Shaolin quan (« poings de Shaolin ») mais aussi, plus près de nous, de Kanbun Uechi, ce qui indique la persistance des échanges.
Après avoir été importé de Chine, le karaté a été développé et perfectionné dans le royaume de Ryūkyū (1429–1879), principalement à Okinawa[7]. Au début de l'expansionnisme de l'ère Shōwa, le Japon conquiert le royaume (en) entre 1872 et 1879. Les plus grands experts de la fin du XIXe siècle et du début du XXe, dont Hanashiro Chomo, Chotoku Kyan, Azato Yasutsune (le premier maître de Funakoshi), Kentsu Yabu, Ankō Itosu (le second maître de Funakoshi), Chibana Chōshin (l'un des condisciples de Funakoshi), Gichin Funakoshi, Kanryō Higaonna, Chōjun Miyagi (disciple du précédent), Kenwa Mabuni (autre condisciple de Funakoshi), entre autres, sont tous originaires d’Okinawa[8]. À part Kanryō Higaonna et Chōjun Miyagi, son disciple et successeur, tous les autres, sans exception, sont des disciples, directs ou indirects, de Sokon Matsumura (1809-1896).
Il n’y a pas de traces écrites de la transmission de ces techniques à Okinawa, qui est le berceau du karaté tel qu'il est pratiqué aujourd’hui. Mais ce dont on est sûr, c’est que ces techniques ont été importées en grande partie de Chine, la culture d'Okinawa étant encore plus sinisée que la culture japonaise. Les Okinawaïens avaient aussi des techniques martiales qui leur étaient propres, comme la rotation axiale du poing dans les coups de poing et les blocages.
En 1409, le roi Sho Hashi unifie les territoires d’Okinawa. Sous son règne se développe l'art du ti (ou te, ou di), cependant déjà présent chez les classes guerrières et nobles. Deux cents ans plus tard, soit en 1609, l'invasion de l'île par le clan Satsuma appauvrit la noblesse okinawaïenne, la contraignant à exposer une de ses dernières richesses : le te. Les armes sont encore confisquées par le nouveau gouvernement japonais ; cependant, les armes à feu ayant supplanté les armes blanches, l'autorité se soucie peu du contrôle des villageois. L'art martial des îles Ryūkyū (Ryūkyū no ti ou te) existait déjà, mais était enseigné en vase clos et n'est pas apparu à cette période. Aucune source historique ne justifie la pose arbitraire de la création du te à cette date : les classes paysannes ne repoussaient pas des samouraïs en armure et équipés d'armes à feu à mains nues et n'avaient pas accès au savoir du te. En revanche, les classes de guerriers, de la police, de l'administration (peic hin) ou des nobles participent au développement du te. On remarque que les maîtres de cet art sont tous d'origine sociale aisée (marchands, nobles, officiers), pratiquant de ce fait entre eux.
Pour ces raisons, les classes aisées d’Okinawa ont adapté les méthodes de combat chinoises reprises sous le nom de Okinawa-te (nom donné au tō-de à partir de la seconde moitié du XIXe siècle, en réaction à la domination japonaise) en développant des techniques de combat à mains nues. Te signifiant « main », Okinawa-te signifiait donc les techniques de combat à mains nues d’Okinawa. Dans le dialecte okinawaïen (uchinaguchi), le terme tōdi était également employé.
De nombreux facteurs ont permis le développement du karaté (initialement tō-de ou to-te ou to-di, « main chinoise ») ou encore plus simplement appelé de ou te par les Okinawaïens) :
Bien que le te ne puisse être distingué en « styles », étant un ensemble, deux grands courants principaux sont apparus, liés aux deux principales villes d'Okinawa : Shuri (Shuri-te) et Naha (Naha-te). Un troisième courant (Tomari-te) s'est également développé, combinant certaines techniques des deux précédents, mais malgré tout plus proche du Shuri-te, ceci s'expliquant en partie par la situation géographique de sa ville d'origine, Tomari, située entre Shuri et Naha.
Du XVIIe siècle au XIXe siècle, du fait que la pratique de cet art était interdite par l'occupant japonais, les cours avaient lieu en secret, de nuit et dans des jardins fermés. Il s'est « ouvert » au milieu du XIXe siècle grâce à Sokon Matsumura, héritier du Shuri-te et créateur du Shōrin-ryū, qui fut le garde du corps personnel des trois derniers rois d'Okinawa et entraîneur officiel de leur garde.
À la suite du choix fait par Chōshin Chibana pour satisfaire la demande de Jigoro Kano (créateur du judo), c'est maître Funakoshi qui introduisit le karaté en 1922 sur l'archipel japonais en réalisant une démonstration devant l'empereur du Japon.
Le développement des techniques du karaté et leur enseignement s'est fait aussi grâce à des maîtres tels que Sōkon Matsumura (1809-1896), ainsi que son principal disciple et successeur Ankō Itosu (1832-1916).
Ce dernier a développé une véritable pédagogie du karaté Shōrin Ryu, créant les cinq premiers katas de base (pinan shodan, pinan nidan, pinan sandan, pinan yodan, pinan godan), à partir de plusieurs katas d'origines, longs et compliqués dont, entre autres, kosokun dai (ou kushanku dai ou encore kanku dai en japonais). Il fut, en 1901, l'instigateur de l'introduction du karaté comme « matière » obligatoire dans le cursus scolaire d'Okinawa. C'est d'ailleurs pour faciliter son enseignement à de jeunes enfants qu'il a créé les pinan.
Ce fut Chōjun Miyagi, le père fondateur du Gōjū-ryū, qui présenta le premier l'examen officiel de maître bushido devant les autorités du Dai Nippon Butokukai, organisme d'État japonais créé dans le but de contrôler tous les arts martiaux du pays. C'était la première fois qu'un maître de karaté faisait cette démarche. Il obtint le titre de kyōshi (« maître »), le plus haut titre qui sera jamais donné à l'époque à un maître de karaté présentant cet examen. Grâce à lui, cet art martial faisait, en 1935, sa véritable entrée dans le budō japonais.
La même année fut décidée l'adoption du terme « karaté » (dans le sens de « main vide ») par l'assemblée générale des « grands maîtres d'Okinawa ».
Un an plus tard, en 1936, sans doute sous la pression du Dai Nippon Butokukai, maître Funakoshi, après avoir modifié la forme et les techniques des katas eux-mêmes (pour sacrifier au développement du « sport spectacle » de l'époque, permettant ainsi au public ainsi qu'à des arbitres néophytes de comprendre ce qui se passe en compétition), en a changé et le nom (de naihanchi en tekki, et de pinan en heïan, de la prononciation chinoise à la prononciation japonaise pour les mêmes raisons que celles citées plus haut) et l'ordre des pinan, le premier étant devenu le deuxième et inversement.
En parallèle du karaté s'est développé le kobudō (combat avec des outils de la vie quotidienne, agraires ou autres ustensiles de cuisine faisant office d'armes : tonfa, nunchaku, bō, jō, saï, etc.) : l’interdiction d’utiliser des armes a été contournée par l’utilisation d’outils traditionnels. C’est ainsi qu’on retrouve parmi les armes traditionnelles d’Okinawa : le bō (le bâton de l’éleveur a de multiples usages), le nunchaku (utilisé pour battre le blé, le riz), le saï (trident qui servait à faire un trou pour planter le plant de riz), le tonfa (manche de meule), l'eku (la rame de barque).
L'école de kobudō la plus connue dans le monde est du courant de maître Matayoshi.
Comme dit plus haut, le karaté vient du Japon. Cet art de combat était connu à Okinawa sous le nom de tō-de depuis le XVe siècle jusqu'à la fin du XIXe siècle, puis d'Okinawa-te.
En 1935 ou 1936, le 25 octobre, les grands maîtres d'Okinawa ont organisé une « assemblée générale » pour décider de la politique à adopter pour favoriser le développement de leur art et en faciliter la reconnaissance et la diffusion au Japon. C'est lors de cette réunion que, à cause de la montée du nationalisme japonais et surtout de l'antagonisme sino-japonais du fait de la guerre récente entre les deux pays, perdue par la Chine, mais aussi pour montrer leur « japonisation », qu'ils ont décidé de modifier l'idéophonogramme et le pictogramme 唐手 (« main de la dynastie Tang ») qui étaient prononcés tō-te en okinawaïen et « karaté » en japonais par l'idéophonogramme et le pictogramme 空手 (« main vide » dans le sens bouddhique de vacuité) prononcés également « karate », suivant en cela les préconisations de l'un d'entre eux, Hanashiro Chomo, qui avait déjà fait cette modification en 1905.
Envoyé près de 15 ans plus tôt par les mêmes pour satisfaire la demande de Jigorō Kanō, Gichin Funakoshi, venu faire une démonstration, est resté au Japon pour enseigner le karaté. Jigorō Kanō lui apporta son aide pour s'installer et a adopté à son tour cette modification.
Depuis 2005, la préfecture d'Okinawa et les fédérations locales, célèbrent le 25 octobre comme« la journée du karaté », Karate no hi.
Le karaté est officiellement admis aux Jeux olympiques de Tokyo en 2020[9]. En outre, la Fédération mondiale de karaté (WKF) est reconnue par le Mouvement olympique[10] et il est au programme des Jeux mondiaux, des Jeux asiatiques, des Jeux panaméricains et des Jeux méditerranéens, critères d'évaluation pour devenir sport olympique.
Le taekwondo est discipline olympique à partir des Jeux de 2000 à Sydney, sous l'impulsion de Juan Antonio Samaranch, président du Comité international olympique (CIO) de l'époque.
En 2005, lors de la 117e session du CIO à Singapour, il est décidé que le baseball et le softball ne seraient plus au programme des Jeux à partir des 2012[11]. Il reste donc deux places disponibles pour de nouvelles disciplines. Cinq sports non olympiques sont examinés par la commission du programme olympique : le roller, le squash, le golf, le karaté et le rugby à sept. Après un premier vote, le squash et le karaté sont retenus comme finalistes pour être au programme des Jeux de Londres, mais ils n'obtiennent pas la majorité des deux tiers requise à l'époque pour devenir un sport olympique[12].
Le karaté est une nouvelle fois en lice pour être sport olympique lors des Jeux de 2016 et la majorité absolue des voix suffit alors[13]. Cependant, il n'est une nouvelle fois pas retenu, au contraire du golf et du rugby à sept[14] qui deviennent sports olympiques après le XIIIe congrès olympique, qui s'est tenu du 3 au 5 octobre 2009 à Copenhague[15].
En parallèle, des contacts ont lieu entre les fédérations de handisport et le Comité international paralympique pour que le handikaraté (notamment sa pratique en chaise) soit en démonstration dès les Jeux de Londres en 2012, mais la démarche n'aboutit pas.
Lors de la 123e session du CIO, à Durban en juillet 2011, sept sports sont retenus pour une éventuelle admission au programme sportif des Jeux de 2020 : le softball, le baseball, le karaté, le squash, le roller, le wushu, le wakeboard et l’escalade sportive. Il est décidé que la question de l'admission d’un ou plusieurs de ces sports au programme des Jeux de 2020 sera entérinée lors de la 125e session du CIO à Buenos Aires en septembre 2013[16]. Néanmoins, le programme olympique est limité à 28 sports[17] et toutes les places sont occupées : l'adhésion du karaté nécessiterait donc d'enlever un sport existant ou de faire passer le nombre de sports olympiques à 29. La Fédération mondiale de karaté lance à cette occasion une importante campagne de promotion du karaté[18]. À la suite de la décision du CIO le 29 mai 2013, à Saint-Pétersbourg, le karaté n'est pas choisi pour figurer dans la liste restreinte des trois sports pouvant prétendre à participer aux JO de 2020. Toutefois, le CIO prend la décision d'étendre le nombre de disciplines en refaisant un grand appel à candidatures pour étendre le panel des sports proposés.
En août 2015, une série de fédérations présentent leurs disciplines au bureau du CIO réuni pour cette raison à Tokyo. À ce moment, il reste neuf sports en lice, dont le karaté. Le 1er juin 2016, la commission exécutive du CIO se déclare favorable à l'intégration du karaté[19]. Le 3 août 2016, la décision finale est prise lors de la 129e session du CIO à Rio : le karaté ainsi que quatre autres sports (baseball/softball, escalade sportive, skateboard et surf[20]) sont intégrés au programme à Tokyo.
Le contenu de cet article ou de cette section est peut-être sujet à caution et doit absolument être sourcé (septembre 2022).
Si vous connaissez le sujet dont traite l'article, merci de le reprendre à partir de sources pertinentes en utilisant notamment les notes de fin de page. Vous pouvez également laisser un mot d'explication en page de discussion.
Plusieurs écoles ou styles différents se sont créés au cours du XXe siècle. Ils varient tous les uns des autres, dans bien des domaines : frappes, positions de combat, utilisation d'armes, applications martiales…
Les quatre grands styles officiels du karaté sont : le Shōtōkan, le Gōjū Ryu, le Wado Ryu et le Shito Ryu. Toutefois, au cours de l'histoire, nombre d'écoles ont été créées et ont grandi avec plus ou moins de réussite.
À part le Gōjū Ryu et le Shito Ryu, les deux autres styles sont issus exclusivement du Shōrin Ryu de Sōkon Matsumura.
Bien qu'aujourd'hui il y ait beaucoup de différents karatés pratiqués en tant que sports, à l'origine il n’y en avait qu’un seul et unique. Le Premier Karaté ou Traditionnel (karate-dō) était le karaté « original », auquel ces différents sports, qui sont arrivés plus tard, ont emprunté le nom « karaté », comme il est généralement et largement utilisé aujourd'hui.
Après la Seconde Guerre mondiale, la valeur du karaté pour l'autodéfense, la forme physique, la compétition et le développement général mental et physique est devenue de plus en plus reconnue. Cependant, en tant qu’art martial, le karaté nécessite de longues études approfondies. La pratique du karaté allait connaître un boom de popularité, et les exigences de longues études approfondies finirent par être ignorées à cause de la demande du monde d’aujourd’hui, qui veut des résultats et un développement plus rapides.
La conséquence a été l'apparition de beaucoup de nouveaux sports utilisant le nom de karaté. Pour éviter la confusion avec ces récents enseignements, le public a commencé à distinguer le karaté originel en tant que « karaté traditionnel ».
Shōtōkan-ryū est un style de karaté japonais fondé en 1938 et issu du Shorin Ryu d'Okinawa introduit par Gichin Funakoshi.
Il fut l'un des premiers à promouvoir cet art martial et fut choisi afin de représenter le karate-dō lors de la première démonstration nationale d'athlétisme à Tokyo en 1922, sur invitation de Jigorō Kanō, fondateur du judo.
C'est le fils de Gichin Funakoshi, Yoshitaka, qui fut à l'origine du style tel qu'on le connaît désormais. Ce style est considéré comme l'un des plus puissants. Les coups de poing sont directs, les coups de pied bas et les katas sont longs (comme dans le Shorin Ryu dont il est issu).Le Gōjū-ryū est un style de karaté prenant son origine dans le Naha-te (puis Shōrei-ryū) d’Okinawa et fondé par Chojun Miyagi, en 1926. C'est ce dernier qui concrétisa le passage du Naha-te au Goju Ryu et qui décida de l'appellation. La véritable branche japonaise du Gōjū Ryu connut toutefois son essor avec un de ses élèves, Gogen Yamaguchi, un maître légendaire du karaté qui fut surnommé « le chat ».
Le style Goju Ryu a été celui de la casse par excellence, exercice pratiqué précédemment afin de voir le degré de force et de résistance des meilleurs élèves. Style de karaté resté assez traditionnel, il marie des techniques issues de différentes écoles chinoises (mêmes concepts techniques, même importance donnée au travail de l'énergie interne) ainsi que les bases ancestrales d'Okinawa.
Caractérisé par des positions naturelles, il comprend des modes de frappes et des déplacements souvent circulaires, visant les points vitaux, les coups de pied bas.
Le représentant en France au sein de la Fédération française de karaté est maître Oshiro Zenei.
Le Wadō-ryū (和道流?, « l'école de la voie de la paix ») est un style japonais de karaté créé en 1939 par Hironori Ohtsuka. Celui-ci était maître de ju-jitsu lorsqu’il découvrit le karaté sous la férule de Gichin Funakoshi. Il complétera quelques lacunes grâce à ses connaissances initiales et à la pratique du Shito Ryu afin de créer son style plus proche du budō. Initialement pratiquant de karaté Shōtōkan, Ohtsuka perçoit les limites de ce style après une sévère défaite que lui inflige un pratiquant de boxe chinoise. Il modifia le Shōtōkan original en développant un style moins rigide, visant à éviter les coups de l'adversaire plutôt qu'à les bloquer comme le fait le karaté Shōtōkan. C'est ce style qui a été choisi pour le taihojutsu, méthode d'intervention de la police japonaise et du Kidotai (escouade anti-émeute).
Shitō-ryū est un style de karaté d’Okinawa créé en 1939 par Kenwa Mabuni. Le fondateur a été un élève brillant des 2 grands maîtres de l’île : Anko Itosu du Shuri-te, et Kanryo Higashionna (ou Higaonna) du Naha-te. Ce style possède officiellement 60 katas. Le Shito Ryu est le style possédant le plus de katas. Maître Mabuni, créateur du style, rajouta au Naha-te et au Shuri-te des techniques souples de mains comme des blocages circulaires et des attaques de poings à courte distance qui lui furent nécessaires dans l'exercice de son métier de policier. Ce style utilise des coups de poing souples et les coups de pied visent les parties médianes du corps. 
Le shinkai jeetkidokai (截拳道館) traduit en français par « la voie du combat intégrale », plus connu sous le nom de jeetkïdô, est un art martial basé sur les traditions japonaises et vietnamiennes. Créé en 1945 à Osaka, au Japon, par Sosai Nguyen Luxuha, il est introduit en Europe à Lausanne en Suisse en 1984 par Kancho Shirigsu Ogama d'où il s'étend en Europe, sur le continent africain, au Brésil et aux États-Unis. Introduit en France en 1992 par Claude Santaguiliana, son développement est plus discret et placé sous l'autorité de la Fédération française de karaté et disciplines associées et de l'association nationale France jeetkidokaï karaté. Il fait partie des nombreux styles de karaté japonais et représente un karaté moderne dont la finalité est le combat libre ou free fight, dit aussi MMA Mixed Martial Arts. Le jeetkidokaï est un style de karaté de MMA ou Free Fight, sa particularité étant de toujours coller l'adversaire en combat pour l'amener au sol ; 80 % des combats en jeetkidokaï se terminent au sol, et seulement 20 % se terminent debout. Comme tous les styles de karaté, le jeetkidokaï travaille aussi les katas, les bunkai et d'autres techniques qui lui sont propres ; on lui donne le surnom de Free Fight Martial Art, ou Luxuha Karaté Intégrale "La Voie de l'efficacité Total."
Seigokan (正刚馆) est un style traditionnel de karatedō Goju Ryu créé par Seigo Tada hanshi (8e dan), en 1945 (Kyoto, Japon). À un moment donné, en vie du grand maître Seigo Tada, dans les années 1960[pas clair], a été la plus grande organisation (Kai-Ha) du Goju Ryu au Japon, avec plus de 200 000 membres.
Il s'agit d'une association fondée en 1935 par les disciples de Gichin Funakoshi, mais qui ne devient un style de karaté à part entière qu’en 1957, sous l’égide de Shigeru Egami. Ce style se veut être le prolongement des recherches de Yoshitaka Funakoshi (Shōtōkan) et intègre des techniques et notions propres à l’aïkido afin de rendre la méthode davantage en rapport avec les traditions martiales japonaises (budō). Deux courants prédominent : le Shōtōkaï actuel, celui de Tetsuji Murakami (également subdivisé en plusieurs associations du fait de sa mort et de la dispersion de ses élèves : International karate do shotokaï, Aïki-karate-do, Kiseikai, Shōtōkaï Europe, Mushinkai, Shōtōkaï Egami Do…), et celui de Mitsusuke Harada qui est revenu à une pratique plus classique. L’appellation de ce style, bien que significative du style par rapport à l'association créée en 1935, n'est pas dans son contexte exact.
En effet, jusque dans les années 1965-1966, on parlait uniquement du style Shotoikai qui fut importé du Japon avec la venue d'un 4e dan français (nommé par Egami senseï) porteur d'une lettre dans laquelle Egami demandait que l'on enseigne son style spécifique dès réception de cette lettre. Cette lettre fut remise à Marc Bassis qui a alors commencé à enseigner les changements du style Shōtōkan spécifique au groupe Egami de l'association Shōtōkaï NKS (Nippon Karate Shōtōkaï). Dans cette association, il y avait en ce temps-là deux courants : celui du Shōtōkan de son créateur Funakoshi Gichin représenté par Hironishi et celui de Egami qui avait commencé à changer les formes de son maître depuis quelques années.
On parlait ainsi du karaté Shōtōkaï pour se référer uniquement au style transformé par Egami. Ce n'est qu'en 1995, afin de se distinguer du style Shōtōkaï des élèves de Murakami (qui a, par ailleurs, enseigné le style sans l'avoir jamais pratiqué et qui n'a pas transmis les formes d'Egami de façon exacte mais adaptés à sa compréhension), qu'un élève de Harada, A. Schneider (qui fut l'un des derniers à avoir suivi au Japon un stage avec Egami senseï) a créé l'association AKSER et a appelé officiellement le style Shōtōkaï Egamiryū (nom enregistré à l'INPI). Après la mort de ce dernier, toutes les associations issues des élèves de Murakami ont créé leur propre organisation — sauf celle créée par un ancien membre de l'AKSER — et, pour garder le nom d'Egami dans son appellation, ils l'ont baptisée Egami-do puisque le nom Egami Ryu était protégé.
Le Shotobudō est un nouveau style de karaté du XXIe siècle, créé par senseï Pascal Ninot. Ce style met en exergue dans le karaté, les armes du kobudō d'Okinawa (bâton, tonfa, saï etc.). Le shotobudō karaté utilise notamment le bō (bâton long) dans les kihon, les katas et les bunkai du karaté Shōtōkan. Dans les bunkai, tori (l'attaquant) utilise principalement le sabre. C'est peut-être un paradoxe pour l'art du combat aux mains vides, mais cette forme de travail originale, qui équipe le karatéka avec les armes du kobudō, est un travail innovant qui met en relation directe le karaté et le kobudō. Le shotobudō est en quelque sorte, un prolongement du karaté Shōtōkan et il permet en outre, de retrouver les gestes authentiques du karaté et du budō de l'époque féodale d'Okinawa ou encore de la Chine ancienne.
Kyokushinkai (l’école de « l’Ultime Vérité ») est un style créé en 1964 par Masutatsu Oyama à partir du Goju Ryu et de quelques éléments du Shōtōkan. Le karaté Kyokushin est basé sur le combat au contact, ce qui en fera d'ailleurs sa particularité. Pour les plus enhardis de ses karatékas, maître Oyama a créé une épreuve que chacun peut présenter quand il le désire : l'épreuve des 100 combats.
Selon la légende, maître Oyama aurait vaincu 52 taureaux et en aurait tué 3, se contentant le plus souvent de briser leurs cornes du tranchant de la main. Son fondateur inscrira son école dans la légende en participant à différentes formes de démonstrations et de casses spectaculaires. Avec plus de douze millions de pratiquants à travers les 5 continents[réf. nécessaire], le Kyokushinkai est le style de karaté le plus pratiqué.
Cette organisation a été fondée en juillet 2004 par hanshi Manny Matias, 8e dan et instructeur-chef de l'administration centrale à Danbury CT (États-Unis), par senseï Robert Underhill, président également de l'administration centrale, et par senseï Denis Cordeiro, directeur, de Montréal (Canada). haitam aouam karate
Dans le milieu des années 1970, après une recherche approfondie, hanshi Manny Matias choisit de poursuivre sa pratique des arts martiaux sous la direction de Shigeru Oyama Soshu. Il a renoncé à ses écoles, à sa ceinture noire, et à son titre en tant que senseï d'un autre style de former sous le grand Shigeru Oyama Soshu[pas clair]. Soshu Shigeru a ensuite été saiko shihan de l'Organisation Karate Kyokushinkai fondée au Japon, par Mas Oyama. Après une période d'entraînement intensif, hanshi Manny a ouvert son dojo Kyokushinkai dans le Connecticut. Lorsque Shigeru Oyama Soshu, séparé de Kyokushinkai, a formé World Oyama Karate, hanshi Manny l'a loyalement suivi et est resté avec lui jusqu'à la démission de Soshu Shigeru en tant que directeur de l'organisation en 2004. Après sa démission, il y eut certains débats quant aux techniques du style Oyama. À la suite de cela, une séparation eut lieu qui donna naissance au style Kanreikai.
Peu de temps après la formation de World Kanreikai Karate, shihan Shlomi Lévy, qui exploite cinq écoles de karaté en Israël, a rejoint l'organisation. Ensuite, le New York dojo, nouvellement consolidé et dirigé par shihan Jose Coton, senseï David Sheeger, senseï Maria Van Dessel et senseï Michelle Gay, a également rejoint, renforçant encore la nouvelle organisation.[pas clair]
Ces gens sont les pionniers qui ont partagé la vision du monde de karaté Kanreikai à ses débuts. Ensemble, ils et beaucoup d'autres ont développé une organisation qui promeut le karaté traditionnel japonais Contact en poursuivant les enseignements des grands maîtres de Mas Oyama et Shigeru Oyama Soshu.[pas clair]
Le Shidokan est une discipline moderne en constante évolution, qui sait perpétuer la philosophie et l'éthique des arts martiaux ancestraux. Apparaissant comme l'un des styles les plus durs, les plus efficaces et les plus intransigeants, il devient incontournable pour les guerriers d'aujourd'hui.
Son fondateur, Maître Yoshiji Soeno, est né le 29 septembre 1947 à Tokorozawa, département de Saitama. Il est le descendant direct d’une famille de samouraïs. Il expérimente au Japon les arts martiaux suivants : judo, wado kaï, kendo, boxe et fait ses armes au karaté Kyokushinkaï sous la tutelle de Masutatsu Oyama et de trois de ses disciples : Tadashi Nakamura, Kenji Kurozaki et Akio Fujihara, pendant de nombreuses années.
Style de karatedō okinawaïen hérité de l'enseignement de Kanbun Uechi, qui naquit à Okinawa le 5 mai 1877. Son père était un paysan et la famille Uechi vivait très modestement. Homme tranquille et très doux, le père se faisait souvent importuner par ses voisins. Aussi, le jeune Kanbun décida-t-il d’apprendre les arts martiaux pour devenir fort et se faire respecter. À l’âge de vingt ans, pour éviter la conscription, il partit en Chine. Là, en 1897, dans la province de Fujian, il fit la connaissance d’un maître chinois Zhou Zihe (Shu Shiwa en okinawaïen). Shu Shiwa, expert d’une école de boxe chinoise du nom de pangainoon, enseignera ce style à Kanbun pendant dix ans. Ce type d'art martial est basé sur les boxes du tigre, de la grue et du dragon. Son originalité est le travail main ouverte, les coups portés avec la pointe des orteils, des piques aux yeux, des blocages circulaires…
Kanbun Uechi, sous la surveillance de son maître Shu Shi Wa, obtint son menkyo kaiden (diplôme de professeur), et fonda ensuite un dojo en Chine dans lequel il enseigna pendant trois ans. Il retourna finalement à Okinawa, en 1909, après avoir passé 13 ans en Chine. Son style associe l'attaque et la défense dans un même mouvement et favorise l'endurcissement du corps pour l'attaque et la défense, notamment lors du kata sanchin.
En 1987, Yves Déry fonda un style de karaté qu'il nomma Shinkudo[21]. Cette discipline est un mélange de disciplines et de diverses expériences vécues dans le domaine des arts martiaux par celui-ci au cours de ses 33 ans d'expérience dans les arts de combat (karaté Shōtōkan, karaté Kyokushin, boxe, ju-jitsu, aiki ju-jitsu, kendo, etc.).
Le mot shinkudo signifie la « voie de l’esprit libre », shin voulant dire « esprit », ku « libre » et do « la voie ».
Le Shinkudo est non compétitif. Cette discipline, d'origine canadienne, est éducative et a une approche individualisée. Elle recherche le développement de chaque individu au niveau mental, physique et spirituel.
L'adepte du Shinkudo se perfectionnera par la pratique de katas (formes), par la pratique du kumite (combat), du shiwari (cassage) et du karate-jitsu (autodéfense), ce dernier aspect prenant une importance primordiale. Certains katame waza (techniques de contrôle), issus du ju-jitsu et de l’aïkido, ainsi que les atemi (frappes) et certains nage waza (projections) font partie des techniques à maîtriser pour l'adepte du shinkudo.
Le combat se fait avec contact, avec peu de protections (fondé sur la méthode Kyokushinkai), mais de façon graduelle et sécuritaire et le shinkudo a été reconnu par la WKF (World Kobudo Federation) en 2011.
En 2013, Yves Déry se retire de l'enseignement et laisse la place à son fils, Vincent. Ce dernier s'installe dans un nouveau local et poursuit la pratique du Shinkudo selon la philosophie de son père.
Sous l'impulsion de plusieurs élèves de Gogen Yamaguchi, certaines écoles Gōju ont abandonné l'usage de la dureté au profit d'une plus grande fluidité. C'est le cas du Goju Ryu KuYuKai enseigné par maître Osamu Hirano.
À titre d'exemple, sanchin, le kata respiratoire symbole du Goju (du même nom que la position des pieds : en sanchin rachi), y perd en contraction et sonorité pour y gagner en profondeur. Cette amplitude à l'inspiration et l'expiration lui a d'ailleurs valu de se voir rallongé. De façon générale, les frappes elles-mêmes se sont déliées avec pour conséquence une fluidité accrue des enchaînements.
De nos jours, le Goju Ryu KuYuKai compte de nombreux clubs à travers le monde.
(少林流, style de Shaolin) est le style le plus ancien fondé par Sōkon Matsumura, et connaissant de multiples variantes. C'est le style le plus pratiqué à Okinawa. Il provient du shuri-te d'Okinawa.
Tous les styles de karaté modernes, sans aucune exception sont issus de son enseignement, y compris, en partie, le Gōjū Ryu, et le Uechi Ryu (les deux autres styles traditionnels okinawaïens).
Matsumura eut de nombreux disciples, dont plusieurs furent très éminents, en particulier Itosu Ankō, son successeur officiel, qui jeta les bases du développement du karaté tel que nous le connaissons aujourd'hui, et qui est un père du karaté moderne.
« L'école du temple de la petite forêt », en référence au temple de Shaolin, provenant du style Shuri-te sur l'île d'Okinawa. Créé par Joen Nagazato, lui-même élève de Chotoku Kyan, cette école contient à l'origine 9 katas que Kyan a enseigné à Nagazato et que ce dernier a voulu préserver tels quels. Ce style a connu par la suite quelques variantes selon qu'il fut enseigné par des Okinawaïens ou des Japonais de Honshu. En France, le Shorinjiryu a été développé sous l'influence du senseï Richard Kim par le senseï Richard Lee ; le style a intégré des techniques provenant du Naha-te comme du Tomari-te tout en conservant la fluidité du Shuri-te. Certains katas de kobudō sont également enseignés.
Shinan Masayoshi Kori Hisataka, créateur du Shorinjiryu kenkoken, est né le 22 avril 1907 sur l’île d’Okinawa. Il étudia avec de nombreux grands maîtres des arts martiaux tout le long de sa vie. Ces premiers enseignants d’art martiaux furent son père, son grand-père et son oncle qui lui enseignèrent la pratique familiale du Kudaka Ryu. Par la suite, alors qu’il était encore jeune, il commença son étude des arts martiaux dans les écoles du maître Anko Azato. Le maître Anko Azato était alors considéré un expert du karaté d’Okinawa et de la manipulation du sabre. Il est dit que celui-ci aurait appris à Shinan Masayoshi Kori Hisataka une forme particulière du kata nijushiho. Certains affirment également que shihan Masayoshi Kori Hisataka aurait étudié avec les maîtres Anko Itosu, Kanryo Higaonna et Chojun Miyagi alors que ceux-ci faisaient l’introduction du karaté dans les écoles d’Okinawa.
Lors de son adolescence, Shinan Masayoshi Kori Hisataka aurait passé quelque temps sur l’île japonaise de Kyūshū où il aurait appris le jujustu. Cependant, très peu d’informations sont disponibles à ce sujet. Il étudia également la manipulation des armes avec Ufuchiku Kanegushiku à la demande de la famille Hisataka. Sa pratique des armes se concentra alors particulièrement sur les saï, le bō et le jō. Cependant, son principal instructeur dans les arts martiaux fut le maître Chotoku Kyan, lui-même un étudiant du maître Anko Azato et l’un des meilleurs maîtres des arts martiaux d’Okinawa à l’époque. Maître Chotoku Kyan aurait enseigné à l’époque plusieurs caractéristiques clefs du Shorinjiryu d’aujourd’hui à Shinan Masayoshi Kori Hisataka tels que l’utilisation d’un poing vertical, le déhanchement et les esquives. Il commença son étude du karatedō avec ce dernier en 1919.
En 1929, il fit une tournée à Taïwan en compagnie du maître Chotoko Kyan et du maître Ryosei Kuwae. Ils firent alors plusieurs démonstrations et apprirent de différents adeptes locaux des arts martiaux. Une légende dit d’ailleurs qu’il ne perdit aucun combat lors de cette tournée.
Voulant toujours améliorer ses habiletés, il partit en Chine perfectionner l’art du Shorinjiryu Kempo. Au début des années 1930, il voyagea dans différents pays dont la Thaïlande, la Corée, la Birmanie, l’Afghanistan, la Russie et la Mongolie, perfectionnant dans chaque endroit sa connaissance des arts martiaux.
Puis il se rendit à Tokyo afin d’étudier le judo sous la tutelle du maître Sanpo Toku. En une seule année, Shinan Masayoshi Kori Hisataka serait parvenu au rang de ceinture noire quatrième dan. Il étudia également le kendo lors de cette période et aurait fait une tournée du Japon en compagnie du maître Chotoku Kyan.
Vers la fin des années 1930, après le début des hostilités entre la Chine et le Japon, Shinan Masayoshi Kori Hisataka fut posté en Mandchourie. Lors de son séjour, il eut l’opportunité de s’entraîner avec le maître Minoru Mochizuki, un étudiant du créateur du judo, maître Jigoro Kano, et du fondateur de l’aïkido, maître Morihei Ueshiba. Il est dit que certaines des techniques du karatedō Shorinjiryu auraient été influencées par maître Minoru Mochizuki. Il aurait également étudié un art martial chinois connu sous le nom de Baji Quan lors de son séjour.
Quelques années plus tard, à la fin de la guerre et à la suite du décès de son principal instructeur, maître Chotoku Kyan, il fonda les écoles de karatedō Shorinjiryu Kenkokan afin de promouvoir la santé et la discipline. Shinan Masayoshi Kori Hisataka créa ainsi son propre style de karaté, le Shorinjiryū kenkoken, dérivé du karaté qui lui avait été enseigné par maître Anko Azato et maître Chotoku Kyan, ainsi que du judo, du jujutsu, de l’aikijutsu et de différents arts martiaux chinois. Il ouvrit sa première école de karatedō Shorinjiryu en 1947. Il continua alors à développer son art en utilisant ses connaissances de différents arts martiaux japonais, chinois et d’Okinawa et ses nombreuses années de pratique. Il mit alors l’accent sur le développement de l’individu tant sur le plan physique que mental, principe qui devint en quelque sorte sa devise.
Il mit également l'emphase sur les différents aspects qui caractérisent aujourd’hui le Shorinjiryu, tels que l’utilisation de toute la force du corps dans les techniques (le déhanchement), l’utilisation du talon lors de certains coups de pied, la position verticale du poing lors des coups de poing, l’apprentissage et l’exécution de kumite, les positions relativement hautes, l’utilisation d’esquives en préférence aux blocages et l’utilisation de bogus[Quoi ?] pour plus de sécurité. En 1964, à la demande spéciale du gouvernement japonais, il introduit le Shorinjiryū aux États-Unis. Il envoie alors plusieurs de ses meilleurs étudiants, dont son fils, shihan Masayuki Kukan Hisataka, ouvrir des écoles de karaté Shorinjiryū kenkokan à New York, à Baltimore et à Montréal.
En 1974, Shinan Masayoshi Kori Hisataka se retira de l'enseignement quotidien du karatedō et céda sa place à son fils. Il est mort en 1988, laissant les écoles de karatedō Shorinjiryū kenkokan dans les mains de son descendant.
Hanshi Michel Laurin a toujours été passionné par les sports de combat. À quatre ans, son père l'initie à la boxe. À 12 ans, influencé par les exploits de Bruce Lee, il commence l'apprentissage du karaté. À 19 ans, il se rend au Japon pour deux ans où il s'entraîne sous la supervision de shihan Masayoshi Kori Hisataka et de son fils, shihan Masayuki Kukan Hisataka. Lors de son séjour, il remporte 3 fois le championnat du Japon. Il remporta également 6 fois le championnat mondial de karaté Koshiki (style de combat du Shorinji ryu). Il fonde par la suite sa propre branche de karaté Shorinjiryū, le Shorinjiryū shindo des écoles budō kwai. Hanshi Michel Laurin est actuellement 9e dan et dirige un dojo à Santa Clarita, en Californie. Hanshi Laurin a enseigné et formé plusieurs personnes qui sont aujourd'hui responsables de ses écoles du Québec, dont le shihan feu Ghislain Doré, le kyōshi Gilles Labelle, le shihan Patrick Panneton et le renshi Larry Foisy. Ceux-ci ont tous leur propre école aujourd'hui. Respectivement à Saint-Jérôme, Sainte-Adèle, Sainte-Agathe-Des-Monts et Sherbrooke.
Fondé par Tadashi Nakamura, 9em dan. Tadashi Nakamura a commencé son étude du karaté en 1956 avec Masutatsu Oyama, le fondateur du style Kyokushin. En 1962 Tadashi Nakamura devient une sensation locale au Japon en mettant KO un champion de kick boxing ou boxe thaï dans un combat pour déterminer quel pays possède l'art martial le plus efficace.
À cette époque, Nakamura enseigne également le karaté et occupe les fonctions de chef instructeur a la base navale US de camp Zama près de Tokyo. Alors qu’il reçoit son 7e Dan en karaté Kyokushin, il est également le chef instructeur du dojo de Kyokushin de Honbu à Tokyo.
En 1966, Nakamaru est personnellement choisi par Masutatsu Oyama pour apporter « l’esprit vrai » du karaté aux États-Unis. Il enseignera le karaté Kyokushin pendant une dizaine d’années à New York et formera plusieurs étudiants talentueux.
En 1976 Nakamura se retire respectueusement du karaté Kyokushin. La même année il fonde l’organisation mondiale du karaté Seido, qui reflète sa vision de la signification réelle du karaté.
Il crée le karaté Seido qui signifie « la voie sincère », et vise à développer non seulement les techniques de karaté mais également à aider les individus à se réaliser et a aider leurs communautés. Le Seido repose en partie sur les bases techniques du Kyokushin, tout en apportant une dimension spirituelle supplémentaire ainsi qu’un enracinement dans la communauté ou il est enseigné. Rendre à sa communauté ce que l'on a reçu, transmettre les valeurs du Seido dans le désintéressement, aider les étudiants à se réaliser en tant qu'individu et se forger l'attitude du « non-renoncement » sont les valeurs à la base de l’enseignement du Seido.
Techniques de combat corollaires au karaté utilisant des ustensiles de la vie quotidienne en tant qu'armes, comme le sansetsukon, le nunchaku, les tonfa (manivelle de moulin à moudre) et le bō. Ces armes étaient utilisées par les agriculteurs pour se défendre contre les envahisseurs et les pirates[22],[23].Fondé par Yoshinao Nanbu, qui est également le fondateur de l'école Sankukaï.
Fondé par soke Pierre Myre au Canada, surtout répandu en Amérique du Nord. Soke Pierre Myre a donc fondé son style en regroupant trois maisons : Shoto-kan /Chito Ryu et le jeet-kun-do pour n'en faire qu’un où l’on pratique le traditionalisme et les techniques rapides de la main ouverte tout en respectant les origines.
Art martial fondé par maître Hiroo Mochizuki, le Yoseikan budō est une des disciplines associées de la Fédération française de karaté (FFKDA).
Le Yoseikan karate-dō est un style de karaté fondé au Québec par Giancarlo Borelli-Lucchesi[24]. Le maître du style est Louise Chevalier, kyōshi, 8e dan. La Fédération Yoseikan Karaté-Do compte plus de 30 écoles au Québec et plusieurs écoles en Côte d'Ivoire.
Fondé au Québec, par maître Aymé Favre. Puis le style a été développé par maître Jacques Marleau et maître Jim Hartnell. L'Association Yoseikan Ryu opère principalement dans la région de Longueuil, Pointe-aux-Trembles et à La Prairie.
Il s'agit d'une branche du Shito ryū, créée par maître Kenji Kusano. Voir à ce sujet le site.
Synthèse de différentes écoles chinoises et japonaises, le Tokitsu Ryu Jiseidō est une méthode fondée sur l'intégration de la respiration, de l'énergie et de l'action martiale via l'art de la percussion. Développée en plus de 40 ans d'études et de recherches menées par Maître Kenji Tokitsu, 10e Dan WUKO La méthode fait sienne le concept profondément ancré dans la culture orientale selon lequel le corps et l'esprit forment une unité indissoluble. Le Jiseidō est le produit d’une conjugaison du Karaté, du TaiChi , du Yi Chuan, du Da Cheng Chuan, de l’Art du sabre japonais, et du Kikô de la méthode du Dr Yayama. Voir le site à ce sujet.
Le kenpō, ou « loi du poing », renvoie à un certain nombre de pratiques martiales d'origine japonaise, qui se sont étendues dans le monde, par l'intermédiaire d'Okinawa, puis de Hawaii, par maitre Chow. Certains l'assimilent au karaté japonais. Cependant, c'est un système qui a son identité propre, identité qui s'est renforcée au fil du temps.
Le grand maître Mitose James va introduire ce système dans l'île, puis certains de ses élèves, dont le professeur William K. S. Chow, vont le modifier. Ce dernier formera quelques élèves à l'origine d'autres systèmes de kenpō. Parmi les plus connus, on trouve : le maître Ed Parker, fondateur de l'American Kenpo ; le maître Emperado, fondateur du kajukenbo ; le maître Nick Cerio, élève de maître Chow, qui crée son propre style de kenpō, le système Nick Cerio's kenpō.
Plusieurs, comme Nick Cerio, se sont approprié le style pour le modifier. Ce système enseigne les blocages, les coups circulaires ainsi que les frappes. Les katas sont nombreux et très importants et les coups de pied sont très présents. C'est aussi très efficace en autodéfense.
Aujourd'hui appelé simplement Kudo, c'est une forme de karaté dur, héritier du Kyokushinkai, fondé en 1981 par maître Takashi Azuma. Maître Azuma est né en 1949, à Kenennuma, dans la province de Miyagi au Japon. Il commence les arts martiaux par la pratique du judo. En 1971, il découvre le karaté Kyokushinkai et deviendra le disciple de maître Oyama Masutatsu, fondateur du karaté Kyokushin. Il semble qu'après une altercation dans la rue, Azuma a compris l'utilité des frappes au visage (interdites en Kyokushinkai) et créa donc son style, le Kudo Daido Juku. C'est un style complet qui pratique le combat au sol, les projections et, bien sûr, le combat pieds poings avec droit de frapper au visage. Environ 2/3 du travail est constitué de frappes : poings, pieds, genoux, coudes et 1/3 du travail est constitué de projections et de combats au sol.
Ce qui a fait l'originalité du Kudo daido juku est le casque que portent les combattants lors des combats, casque qui a été mis au point par maître Masayuki Kukan Hisataka, fondateur du Karaté Contact Koshiki, puis repris et modifié par maître Azuma, pour éviter que les pratiquants aient des troubles cérébraux (comme certains pratiquants en ont après leur carrière, tel que Mohamed Ali), et se rapproche sur ce point du karaté Mumonkai.
La compétition se déroule au KO, on y utilise toute la panoplie du Kudo daido juku, le combat au sol est limité à trente secondes, les frappes au sol sont autorisées sauf à partir de la position montée où elles sont simulées. Un décompte des points a lieu si aucun des participants n'est KO ou a abandonné. Le ippon rapporte des points, tout comme les amenées au sol, etc.
Le Mumonkai est un style de karaté contemporain, fondé par maître Togashi Yoshimoto en 1973, qui s'illustra dans sa jeunesse en remportant défis et tournois de karaté. Il fut particulièrement apprécié par maître Oyama Masutatsu lui-même, après avoir notamment remporté la 5e place du All Japan Kyokushinkai de 1973 en toutes catégories, juste derrière celui qui deviendra deux ans plus tard le 1er champion du monde de karaté Kyokushinkai, Sato Katsuaki. Le Mumonkai est l'art du duel, où le poing est une flèche prête à être décochée.
Tout a commencé au Japon, en 1950, dans le temple Jozenji dans la préfecture de Yamagata, au nord-ouest du Japon, où Togashi Yoshimoto voit le jour. Élevé à la campagne, il ne se passionne pour le karaté que tardivement, vers ses 19 ans. Son intérêt le pousse à fréquenter différents dojos à la recherche du karaté absolu. Véritable samouraï des temps modernes, Togashi Yoshimoto est en quête d'un art martial authentique s'inspirant de méditation et dépassement de soi. En 1970, il se retire 10 jours dans les montagnes d'Ontake (nord de Nagoya), puis 100 jours en 1973 dans les monts Okutama. Entretemps, il effectue le yakunin-kumite (épreuve des 100 combats) et continue à participer à différents tournois open de karaté.
Très perplexe devant les règles de combat qui n'autorisent pas les coups aux parties vitales, il s'interroge alors sur le réalisme des méthodes de karaté qu'il rencontre et décide de s'isoler du monde durant un an, afin de trouver l'inspiration. C'est donc à l'issue de cette retraite, qu'il créé un style radicalement différent, auquel il donne le nom ésotérique Mumonkai (« École aux portes de la vacuité ») et en détermine les règles de combat basées sur l'authenticité.
Ce style de karaté met l'accent sur l'ichigeki (le « coup fatal ») en travaillant les tsuki (attaques directes) sur la base du ju-soko (coups de poing en flèche), afin que ceux-ci puissent mettre KO un adversaire en une seule frappe. En outre, c'est la garde très spécifique, qui permet à ses adeptes d'avoir un tsuki fulgurant et qui le caractérise dès le premier regard. Alors que de nombreux styles de karaté sont similaires et souvent dérivés les uns aux autres, le Mumonkai cultive quant à lui sa différence et ne se laisse pas aspirer par la mondialisation des arts martiaux.
Les passages de grade en kudo se décomposent en 3 parties: une partie physique (un certain nombre de pompes ou de développé-couché et de flexions selon le grade), une partie kihon et ido (mouvements de base) et une partie combat. Les règles et la durée des combats dépendent du grade, il y en a 4 types : kyokushin, boxe thaïlandaise, judo/jjb et kudo.
On peut également passer des grades en fonction de ses résultats en compétition (les combats en compétition remplacent alors les combats du passage de grade).
Lors du premier passage de grade, on peut directement « sauter » des kyus et devenir 7e ou 6e kyu par exemple, puis on passe les grades un par un sauf si on obtient de bons résultats en compétition.
La compétition Mumonkai se pratique avec un casque à bulle, pour éviter les traumatismes au visage, autorise les techniques de poings, coudes, genoux et de jambes et admet les kin geri (frappes aux parties génitales) avec coquille. Cette forme de budō-karate garde donc, même en compétition, son aspect martial basé sur le combat réel.
Il existe à ce jour cinq villes en France qui propose des clubs de Kudo: Paris[25], Nice[26] et Rennes[27],[28], Tours et Niort.
Le zendokan dénommé Shōtōkan dharma, a été créé vers la fin de 1950 par le défunt Michael Kelly, qui avait étudié le Okinawa-te et le judo après son retour de la Seconde Guerre mondiale. Ce style ayant beaucoup évolué depuis sa création, plusieurs mouvements et positions le distinguent des autres styles de karaté.
Ne doit pas être confondu avec Shitō-ryū.
Le style Chitō Ryu, fondé en 1946, a été développé par le Dr Tsuyoshi Chitose, gynécologue et obstétricien[29]. Chitose est né à Naha, sur l'île d'Okinawa, le 18 octobre 1898. À l'âge de 7 ans, il commence sa formation en karaté sous la gouverne du fameux maître de karaté d'Okinawa Arigaki Seisho en 1905. Il a notamment pratiqué les arts martiaux avec les grands maîtres suivants :
Choyu Motobu, Choki Motobu, Hanashiro Chomo, Kanryū Higashionna, Chotoku Kyan, Moden Yabiku, Sanda Chinen, Anko Itosu, Gichin Funakoshi (Shōtōkan Ryu), Kenwa Mabuni (Shito Ryu), Kanken Toyama (Shudo-kan), Yasuhiro Konishi (Ryobu-kai), Chojun Miyagi (Goju Ryu), Gogen Yamaguchi (Goju-kai).
Alors que Chitose enseignait au premier dojo de Funakoshi, il a enseigné à un autre homme, Masatoshi Nakayama, qui deviendra plus tard l'instructeur chef de l'Association japonaise de karaté (JKA). Le Dr Tsyuoshi Chitose est décédé le 6 juin 1984. Dans le milieu des années 1970, il y avait plus 40 000 étudiants et instructeurs de ce style au niveau mondial.[réf. nécessaire]
Le Gembukan-Tōde est une école de karate-dō issue de l'enseignement de senseï Ogura Tsuneyoshi au Gembukan dojo (créé à Kofu en 1944) et faisant référence aux racines chinoises de ce qui allait devenir karate-dō à Okinawa, puis au Japon, à partir des années 1920-1930.
Cette école, absente de la Fédération française de karaté, se situe hors du cadre sportif, compétitif et administratif généralement admis. Elle insiste sur la fluidité, la flexibilité et la continuité des mouvements au travers des katas, kihon et kumite, visant un épanouissement du pratiquant à travers une démarche à long terme. La progression propre à cette école n'utilise plus depuis 2006 le système hiérarchique en dan, mais uniquement les certificats de transmission et de compétence (menkyo-jō).
Décédé en 2007, senseï Ogura Tsuneyoshi a laissé trois personnes dans sa succession : Roland Habersetzer et Pierre Portocarrero en France, Hisanori Ogura son deuxième fils au Japon. Senseï Ogura a décerné le titre de shihan (1984), puis le certificat de transmission et d'enseignement menkyo kaiden (1988) à Pierre Portocarrero, lui accordant l'autorisation d'utiliser le nom du dojo « Gembukan » pour ultérieurement baptiser son ryūha (« courant », « style »). Ce fut chose faite en 2006 en bonne harmonie avec senseï Roland Habersetzer, soke (titre décerné de son vivant par senseï Ogura) de son propre ryūha, Tengu-no-michi.
Au karaté, même en dehors du Japon, on emploie des termes japonais pour désigner les parties du corps, les techniques, les postures, etc.
Souvent, les cours commencent et se terminent par une courte méditation et par le salut (rei), les élèves faisant face au professeur, ou senseï. Parfois, les plus gradés ou anciens (sempai) sont situés légèrement à part et saluent le senseï séparément en plus du salut à tous. Le salut se fait avec un respect mutuel.
Les séances d'apprentissage commencent habituellement (même si ce n'est pas codifié) par un échauffement qui prépare les muscles et les articulations à l'entraînement proprement dit. On adaptera l'échauffement à l'entraînement qui suivra en insistant sur la souplesse, l'endurance ou la force physique.
L'entraînement peut se composer de kihon, constitué de répétitions et/ou d'enchaînements de mouvements ; de l'apprentissage d'un ou de plusieurs katas (combat imaginaire codifié contre un ou plusieurs adversaires) ; de l'assimilation de bunkai (application du kata au combat) et enfin de kumite (combat), lequel peut être souple (ju-kumite) pour s'échauffer et tester des techniques ou plus codifié pour apprendre la prise de distance et les tactiques de combat (ippon kumite, sambon kumite, gohon kumite, pinan kumite, oyo kumite).
Ici sont expliquées les règles de la compétition selon la Fédération mondiale de karaté (WKF)[32], reconnue par le mouvement olympique. Des compétitions d'autres styles suivent des règles différentes, par exemple les compétitions de Kyokushinkai, régies en France par la Commission nationale Kyokushinkai au sein de la Fédération française de karaté[33]. D'autres compétitions sont régies par les règles de la World Kickboxing and Karate Association (WKA)[34], notamment en Amérique du Nord.
Les compétiteurs portent une tenue qui diffère selon le style mais chaque pratiquant porte un gi (uniforme), généralement blanc (mais qui peut être d'une autre couleur) et une ceinture de couleur rouge ou bleu selon qu'il est aka ou ao.
Selon les règles de la compétition, les participants portent ou non des protections. Dans certains cas, les combattants n'ont aucune ou très peu de protection. Dans d'autres cas, il peut être autorisé ou obligatoire de porter des gants, un casque, un protecteur buccal, des protège-tibia, protège-facial, coquille, pied, etc.
Les protections ne sont pas obligatoires ou plus importantes si la compétition est de type full contact. Dans certains tournois où les coups à plein contact sont permis, il n'y a aucune protection. Dans d'autres, les participants sont extrêmement protégés.
Dans la plupart des compétitions, il n'est pas permis de viser des cibles vitales sous la ceinture tels que les parties génitales ou les genoux. Il est souvent permis de faucher ou de balayer une jambe, soit pour déséquilibrer, déranger ou tout simplement faire chuter l'adversaire.
Il est rarement permis que des coups à mains ouvertes soient portés (avec le tranchant de la main par exemple, ou la pointe des doigts). De façon générale, il est également interdit de s'acharner sur un adversaire qui se retrouve au sol.
Dans certains cas, le compétiteur doit accumuler des « points », en touchant des cibles permises (sans contact ou avec contact selon le cas) pendant une période de temps déterminée. Dans bien des cas, une fois qu'on a accumulé le nombre de points maximum (8 points d'écart), on remporte la victoire, avant que la limite de temps soit écoulée.
Dans d'autres types de compétitions, l'objectif est de battre l'adversaire en le faisant abandonner ou en le mettant KO (c'est ce qu'on voit normalement en compétition Kyokushin). Le vainqueur est celui qui a mis hors combat son adversaire ou qui l'a fait tout au moins chuter pendant l'affrontement.
Certaines compétitions utilisent l'aide de juges qui sont aux quatre coins de la surface de combat. Ces juges détermineront le vainqueur ou aideront l'arbitre à prendre la décision finale. Dans certaines compétitions, ces juges prennent des notes au fur et à mesure du combat. Dans d'autres, ceux-ci lèvent un drapeau à chaque bon coup porté, durant le combat. L'arbitre arrête le combat dès qu'une frappe légale et efficace est portée si cela est la règle (compétition aux « points »).
Le comptage des points est très variable selon les règles de la compétition. Dans certaines compétitions, il n'y a aucun point à compter, l'objectif étant de mettre l'adversaire au sol ou hors combat par des frappes réelles. Dans d'autres cas, les points sont accordés à des combattants qui portent des atemi :
Au corps, le contact est autorisé jusqu'au KO (règle des 10 secondes).
Les points sont accordés en fonction de la sévérité de l'arbitre et des juges, sévérité qui varie selon le niveau des pratiquants qui combattent : on sera souvent plus sévère avec des pratiquants ayant un haut niveau d'expérience (ceintures noires) qu'avec des débutants.
Dans certaines compétitions, dès que l'on atteint le nombre de points nécessaires à la victoire, le combat est stoppé, peu importe s'il n'a pas duré le temps prévu.
Des infractions aux règles établies peuvent amener des pertes de points ou, dans certains cas, une disqualification.
Il y a infraction lorsque des coups interdits sont portés, et des gestes ou propos offensants ou inadéquats sont souvent considérés comme des infractions (gestes ou propos vis-à-vis l'adversaire ou un arbitre, par exemple).
L'infraction est souvent punie par la perte de points ou par un avertissement, mais les infractions peuvent amener, même sans disqualification, une défaite du combattant (par exemple s'il ne reste que cette façon pour établir un gagnant lors d'un match).
Selon les règles de la compétition, le combat peut durer une, deux ou trois minutes voire beaucoup plus.
Généralement, les compétiteurs masculins et féminins ne combattent pas dans la même catégorie. Il y a aussi bien souvent, même si cela n'est pas automatiquement le cas, des catégories de poids et de grades.
On attend des combattants qu'ils démontrent de la précision, de l'efficacité et de la combativité. La notion d'efficacité varie selon les tournois. Dans un tournoi où il y a un contact réel, on cherche à voir si le coup porté est puissant (si le combattant tombe, cela en est souvent la preuve évidente). Dans un tournoi où on compte les points, on cherche à voir si la frappe est vive, précise et contrôlée.
Le combat se déroule selon le temps établi, et on calcule le nombre de points portés, selon le cas. Dans d'autres cas, on arrête le combat dès que le nombre de points maximum est atteint (deux ou trois points bien souvent).
Dans certains types de tournois dits « avec contact », on arrête le combat dès qu'un adversaire est hors combat.
Les couleurs des ceintures de karaté sont blanc, jaune, orange, vert, bleu, (mauve) marron et noir. L'ordre des couleurs et le nombre de ceintures peuvent varier dans certains styles de karaté.
À l'origine, au Japon, les pratiquants portaient un kimono blanc, similaire à leur habit de travail, ainsi qu'une ceinture blanche pour fermer la veste. La ceinture blanche devenant marron puis noire au fur et à mesure de l'entraînement, la ceinture noire fut considérée comme l'ultime étape à atteindre.
Le style Shōtōkan de maître Ohshima a gardé ce principe originel de trois couleurs, blanche du 9e au 4e kyu, marron du 3e au 1er kyu, puis noire à partir de shodan (premier dan).
Même si à l'origine, le karaté et les autres arts martiaux n'utilisaient la ceinture que pour tenir le pantalon, il devint vite courant de différencier le pratiquant initié (et non « accompli ») du débutant en ceignant une ceinture noire (initié) ou blanche (débutant). Par la suite, la ceinture marron apparut. Elle désignait l'élève sur le point d'obtenir la ceinture noire. De nos jours, une classification large et variée existe et varie dans certains styles.
Néanmoins, les différents pratiquants s'entendent en général sur les éléments suivants :
Les ceintures bicolores (blanche et jaune, jaune et orange, etc.) sont parfois utilisées comme ceintures intermédiaires et remplacent les barrettes. Selon les styles, les couleurs (sauf blanche, marron et noire) peuvent être différentes, et leur succession, différer. Dans des styles voisins, la ceinture marron est parfois remplacée par une ceinture rouge.
Pour le Kyokushin, les couleurs sont (shidokan, kudo daido juku pas d'orange)[pas clair]
Ensuite, le pratiquant porte la ceinture noire à partir du 1er dan. Toutefois, dans certains styles, il est permis de porter une ceinture à barrettes rouge et blanche à partir du 6e dan, et une ceinture rouge ou blanche (pour marquer le fait que l'on ne cesse d'apprendre et boucler la boucle) à partir du 9e dan.
L'article L. 212-5 du Code du sport français prévoit que
« Dans les disciplines sportives relevant des arts martiaux, nul ne peut se prévaloir d'un dan ou d'un grade équivalent sanctionnant les qualités sportives et les connaissances techniques et, le cas échéant, les performances en compétition s'il n'a pas été délivré par la commission spécialisée des dans et grades équivalents de la fédération délégataire ou, à défaut, de la fédération agréée consacrée exclusivement aux arts martiaux. »
À ce titre et en ce qui concerne le karaté, seules les ceintures noires et les dans délivrés par la Commission spécialisée des dans et grades équivalents de la Fédération française de karaté seraient reconnus en France.
Enfin, dans les combats des compétitions de la WKF, les pratiquants portent une ceinture rouge (aka) ou bleue (ao) [qui remplace maintenant la blanche (shiro)], pour permettre au public et aux arbitres de les différencier plus facilement et de les désigner au moyen de drapeaux bleus ou rouges.
On traduit le plus souvent le mot "karaté" par « main vide », au sens de lutter à mains nues, mais il est à noter que les origines bouddhiques du karaté nous renvoient également à des conceptions plus philosophiques et méditatives. C'est ainsi que l'on peut relever sous cette appellation l'idée d'avoir les mains vides au sens de se décharger de son quotidien. On associera alors la pratique de cet art martial au concept de lâcher prise. En ce sens, le karaté devient un outil de gestion du stress et un moyen de mettre une distance entre soi et ses soucis.
Les méthodes de travail respiratoire que l'on peut retrouver dans certains katas (notamment Sanchin et Tenshō, qui sont des katas du Gojū Ryu) reposent sur des préceptes de modulation et de positionnement de la respiration issus du yoga.
Enfin, la pratique du karaté constitue un cheminement au plus profond de son être. Il confronte les gens à leurs instincts de violence, à leurs fantasmes de domination ou à leur peur de la confrontation et amène chacun, pas à pas, à gérer les conflits qui naissent chaque jour dans la vie en les purgeant de leur caractère dramatique. « Le karaté, chemin vers un plus grand moi », est alors la voie de la sérénité.
Néanmoins, le karaté peut être pratiqué comme une activité gymnique. Ainsi, chaque partie du corps peut être sollicitée, et la pratique adaptée à chaque morphologie.
CAMI Sport & Cancer propose des programmes de thérapie sportive incluant des cours de Karaté.
Le karaté est utilisé comme accompagnement chez les personnes atteintes de cancer soit
en cours de traitement , soit après traitement et pendant les remissions.
Cette pratique sportive améliore la qualité de vie, le risque de rechute et diminue les effets indésirables des traitements.
Même si le karaté est actuellement pratiqué comme un sport par bon nombre d'adeptes sur la planète, beaucoup de pratiquants ont encore à cœur le code du bushido (« la voie des techniques du guerrier ») et n'hésitent pas à le mettre de l'avant dans leur pratique[35]. Dans cette dernière voie, le karaté devient plus un art de vivre qu'un simple sport et tend vers la maîtrise du corps et de l'esprit.
Historiquement, ce code bushido est en fait le code d'honneur de la caste militaire japonaise des samouraïs, qui émergea véritablement pendant la période Heian au XIIe siècle. L'apparition du terme "bushido" date du XVIe siècle, au moment où le Japon était ravagé par les guerres civiles. Il fut standardisé au XVIIe siècle, sous la période Tokugawa. Le samouraï y était alors considéré comme un exemple vivant. Ce code est imprégné de nombres d'influences bouddhiques et taoïstes.
Les grandes lignes en sont les suivantes :
C’est la qualité essentielle. Nul ne peut se prétendre budōka (guerrier au sens noble du terme) s’il n’a pas une conduite honorable. Du sens de l’honneur découlent toutes les autres vertus. Il exige le respect du code moral et la poursuite d’un idéal de manière à toujours avoir un comportement digne et respectable. Il conditionne notre attitude et notre manière d’être vis-à-vis des autres.
Il n’y a pas d’honneur sans fidélité et loyauté à l’égard de certains idéaux et de ceux qui les partagent. La fidélité symbolise la nécessité incontournable de tenir ses promesses et de remplir ses engagements.
La fidélité nécessite la sincérité dans les paroles et dans les actes. Le mensonge et l’équivoque engendrent la suspicion, qui est la source de toutes les désunions. En karatédo, le salut est l’expression de cette sincérité ; c’est le signe qu'on ne déguise ni ses sentiments, ni ses pensées, qu'on se sait authentique.
La force d’âme qui fait braver le danger et la souffrance s’appelle le courage. Ce courage qui nous pousse à faire respecter, en toutes circonstances, ce qui nous paraît juste et qui nous permet, malgré nos peurs et nos craintes, d’affronter toutes les épreuves. La bravoure, l’ardeur et surtout la volonté sont les supports de ce courage.
La bonté et la bienveillance sont des marques de courage qui dénotent une haute humanité. Elles nous poussent à l’entraide, à être attentif à notre prochain et à notre environnement, à être respectueux de la vie.
La bonté et la bienveillance ne peuvent s’exprimer sincèrement sans modération dans l’appréciation de soi–même. Savoir être humble, exempt d’orgueil et de vanité, sans faux-semblant est le seul garant de la modestie.
Faire preuve de droiture, c’est suivre la ligne du devoir et ne jamais s’en écarter. Loyauté, honnêteté et sincérité sont les piliers de cette droiture. Elle nous permet de prendre sans aucune faiblesse une décision juste et raisonnable.
La droiture engendre le respect à l’égard des autres et de la part des autres. La politesse est l’expression de ce respect dû à autrui, quelles que soient ses qualités, ses faiblesses ou sa position sociale. Savoir traiter les personnes et les choses avec déférence et respecter le sacré est le premier devoir d’un budōka, car cela permet d’éviter de nombreuses querelles et conflits.
Le contrôle de soi doit être la qualité essentielle de toute ceinture noire. Il représente la possibilité de maîtriser nos sentiments, nos pulsions et de contrôler notre instinct. C’est l’un des principaux objectifs de la pratique du karatédo, car il conditionne toute notre efficacité. Le code d’honneur et la morale traditionnelle enseignée dans le karatédo sont fondés sur l'acquisition de cette maîtrise.
Fort de ses racines philosophiques, le karaté moderne a pu se tourner aussi vers des pratiquants dont les handicaps physiques ou mentaux ne permettaient pas une pratique stricte des différents exercices préconisés dans les styles et les écoles officielles. Certains maîtres se sont attardés sur ces pratiques et en ont parfois fait un style à part entière.
D'autres se sont penchés sur la pratique de ce sport en fauteuil roulant pour en faire un handisport très attractif : on trouvera des pratiquants en France, au Canada, en Irlande, en Allemagne et en Belgique. Le Goju-ryu Kuyukai a ainsi revisité tous les katas du style afin de les transposer pour une pratique en fauteuil. C'est le senseï belge et champion du monde WKF Franck Duboisse qui a réalisé cette tâche alors qu'il rencontrait de graves problèmes de mobilité.
La Fédération française de karaté et disciplines associées a créé un groupe de travail handikaraté. Elle a également créé un DVD, Karaté et Langue des signes en collaboration avec des associations de personnes sourdes.
Les compétitions officielles commencent à connaître l'organisation d'épreuves de katas et de kumite en chaise. C'est le cas de l'International Goju Karate Cup (Belgique) ou encore de l'Open d'Istanbul qui ouvrent leurs portes aux pratiquants en chaises actives. La liste des événements du calendrier s'est étoffée considérablement ces dernières années.
La WKF vient d'ouvrir plusieurs catégories pour un public de karatékas ayant un handicap lors des derniers championnats mondiaux à Brême en novembre 2014.
Trois catégories étaient ouvertes :
Plus de 20 pays ont envoyé des athlètes pour cette première édition historique dans le monde sportif où athlètes valides et athlètes handicapés se sont côtoyés pendant plusieurs jours.
En Belgique encore, une nouvelle association transversale, l'IKF est maintenant très active et a mis sur pied des événements sportifs en intégration avec des valides où l'on voit se confronter des personnes autistes ou des personnes ayant une déficience mentale.
Depuis 2015, pour des raisons de compréhension internationale et pour mettre l'accent sur la pratique intégrée, l'appellation « handikaraté » est délaissée au profit du concept de « i-karaté ».
1957 : Transmission et développement en France du Karaté Shōtōkan-ryū par Maître Hiroo Mochizuki 
1963 : Transmission et développement en France et en Europe du Karaté Wadō-ryū par Maître Hiroo Mochizuki
L'association France shotokan karaté est créée en 1964 par Tsutomu Ohshima.
1975 : Fondation de la Fédération Française de Karaté et Disciplines Assimilées (FFKDA) par Jacques Delcourt 
1976 : La FFKDA devient la Fédération Française de Karaté et Arts Martiaux Affinitaires (FFKAMA) 
2006 : La FFKAMA devient la Fédération Française de Karaté et Disciplines Associées (FFKDA[36]) 
Aujourd'hui, la pratique du karaté est notamment représentée par la Fédération Française de Karaté et Disciplines Associées (FFKDA). 
En 2009, elle rassemblait environ 200 000 licenciés, dont 30 % de femmes et 70 % d'hommes[37].
En 2017, elle rassemblait précisément 199 086 licenciés, dont 36 % de femmes et 64 % d’hommes[38].
Saison 2018-2019, elle rassemblait 244 443 licenciés, dont 36 % de femmes et 64 % d’hommes
À la suite de la Seconde Guerre mondiale, des militaires américains ont appris le karaté au Japon. En 1945, Robert Trias ouvre le premier dojo de karaté (Shuri Ryu) aux États-Unis.
«  Anciens Grecs » redirige ici. Ne pas confondre avec Grec ancien.
La Grèce antique est une civilisation de l'Antiquité des peuples de langue et de culture grecque développée en Grèce et dans la partie occidentale de l'Asie Mineure, puis, à la suite de plusieurs phases d'expansion, dans d'autres régions du bassin méditerranéen (Chypre, Sicile, Italie du sud, Égypte, Cyrénaïque) et du Proche-Orient (Syrie, Palestine), constituant des points d'implantation jusque dans les actuelles Espagne et France à l'ouest et sur le territoire de l’actuel Afghanistan (Bactriane) à l'est.
Cette civilisation de culture grecque prend forme durant les « siècles obscurs » (v.  1200-800 av. J.-C.), à partir des décombres de la civilisation mycénienne, et se développe en particulier durant l'époque archaïque (v.  800-480 av. J.-C.), et s'épanouit pleinement durant l'époque classique (480-323 av. J.-C.) et l'époque hellénistique (323-31 av. J.-C.). La conquête romaine (entre 220 et 31 av. J.-C.) marque la fin de l'indépendance politique grecque, mais la culture grecque antique a conservé un réel dynamisme sous domination romaine, évoluant progressivement vers la civilisation byzantine à partir du IVe siècle.
À compter de l'époque archaïque et plus fortement durant l'époque classique, la Grèce voit le développement d'une civilisation novatrice par bien des aspects, qui se singularise par rapport aux plus anciennes civilisations antiques des pays orientaux, et entame une phase d'expansion maritime (la colonisation grecque). Au sein de la cité (polis) se met en place une vie politique, sociale et culturelle très dynamique, appuyé de plusieurs centres (Ionie, Athènes, Sparte, Syracuse), dans un contexte marqué par de nombreux conflits entre cités où se forme un art de la guerre spécifique sur lequel elle s'appuie par la suite pour son expansion. Les cités grecques élaborent des formes politiques originales (comme la tyrannie et la démocratie), ont une vie religieuse dynamique aux aspects locaux très marqués tout en reconnaissant un groupe de divinités et des sanctuaires panhelléniques, leurs citoyens donnent naissance à une littérature variée (épopées, mythes, poésie, histoires, etc.), à des réflexions sur le monde et la vie en société dans le cadre de la philosophie, aussi un développement des sciences et des techniques, et à des réalisations artistiques et architecturales s'affranchissant des modèles orientaux pour devenir à leur tour des références, d'où vient leur statut « classique ». Les revers de ce dynamisme sont l'existence de nombreuses destructions créées par les guerres et des inégalités traversant les sociétés civiques, excluant les femmes des responsabilités politiques et reposant souvent sur l'existence d'une importante population ayant un statut d'esclave ou de dépendant.
La civilisation grecque antique a exercé une influence considérable dans le monde antique, en particulier après les conquêtes d'Alexandre le Grand et durant l'époque hellénistique quand elle a dominé et influencé les civilisations du Moyen-Orient, où se sont constitués d'importants centres de culture grecque (Alexandrie, Antioche). Dans le bassin méditerranéen, la culture grecque a joué un rôle décisif, par l'influence qu'elle a exercée sur la civilisation de la Rome antique, où le grec devient la langue du savoir utilisée par les élites, au point qu'on parle régulièrement de culture « gréco-romaine ». C'est par ce biais que beaucoup de productions politiques et culturelles du monde grec antique ont eu un rôle majeur dans le développement de la civilisation occidentale. Le monde grec reste donc très dynamique culturellement sous la domination romaine, et les cités restent le cadre fondamental de la vie politique et sociale. Durant l'Antiquité tardive (v.  250-700) le monde romain oriental, de culture grecque, prend progressivement son autonomie autour de Constantinople, qui devient la capitale de l'Empire romain d'Orient (ou Empire byzantin) et le nouveau pôle culturel du monde grec. La christianisation, qui conduit à la disparition de la religion polythéiste grecque, et le délitement progressif des institutions des cités grecques antiques, à la même époque, marquent la fin de la civilisation grecque antique.
L'influence culturelle grecque s'est aussi exercée sur la vie intellectuelle du monde arabo-musulman médiéval, et surtout en Occident où son statut de référence a été confirmé à de nombreuses reprises par la suite, plusieurs des aspects de la culture grecque antique ayant servi de sources d'inspiration. Souvent idéalisée, elle est couramment investie du statut de culture fondatrice pour le monde occidental. Les redécouvertes archéologiques effectuées sur le sol grec depuis le XIXe siècle et de nouveaux regards sur les textes antiques grecs transmis jusqu'à l'époque moderne ont permis d'approfondir et de renouveler la connaissance et la compréhension de cette civilisation.
Le terme français « Grec » est dérivé du latin Graecus qui lui-même vient du grec Γραικός / Graikós. Plusieurs étymologies ont été proposées pour Graecus. Les Grecs s'appellent eux-mêmes les « Hellènes » à partir de l'époque archaïque, se considérant comme les descendants d'Hellen, un roi mythologique, et appelaient leur pays (d'origine) Hellas, qui correspond à la république grecque actuelle plus la côte occidentale de l'Asie Mineure[1].
De nos jours, le terme « Grèce antique » renvoie de manière implicite à l'actuelle Grèce (ou République hellénique), dont les îles des mers Égée et Ionienne, en incluant la « Grèce d'Asie » (Ionie, Éolide et Doride sur la côte anatolienne) ainsi que des territoires colonisés par les Grecs en Thrace et en Grande-Grèce (Italie du Sud et Sicile). D'autres régions colonisées par les Grecs, comme les côtes de la Mer Noire ou de la Gaule, en sont traditionnellement exclues même si des cités grecques y ont été fondées. C'est pourquoi les historiens modernes préfèrent la notion de « monde grec » qui prend en compte le fait que les Grecs ont implanté des colonies dans l'ensemble du bassin méditerranéen à l'époque archaïque, et en Asie occidentale et en Égypte durant l'époque hellénistique[2],[3].
Au plus large, l'Antiquité grecque s'étend de l'époque des palais minoens, au XVIe siècle av. J.-C., et se prolonge jusqu'à la période romaine, soit au IIIe siècle[4], ou jusqu'en 400 en s'arrêtant à la christianisation[5], ou plus loin encore en incluant l'époque de transformation de l'Empire romain d'Orient en Empire byzantin. Cette ampleur chronologique implique qu'il serait anachronique de traiter comme un tout cet ensemble, en raison des nombreuses évolutions qu'il connaît sur cette très longue période[6].
Au sens restreint, le plus courant, les historiens spécialistes de la Grèce antique se focalisent sur la période qui va en gros de 1200 à 31 av. J.-C., durant lequel se développent les spécificités de la civilisation grecque antique (cité, philosophie, théâtre, sciences, art, etc.)[7]. Cela recouvre quatre époques de l'Antiquité grecque : les siècles obscurs (v. 1200-800 av. J.-C.), l'époque archaïque (v. 800-480 av. J.-C.), l'époque classique (480-323 av. J.-C.) et l'époque hellénistique (323-31 av. J.-C.). Le VIIIe siècle av. J.-C. est vu comme une rupture majeure avec l'émergence de la cité grecque, mais le renouvellement des études sur les siècles le précédant a nuancé cette impression[8]. À l'autre bout, le IIe siècle av. J.-C., quand Rome a mis la main sur la majeure partie du monde grec, est traditionnellement vu comme une séparation entre les champs d'étude des spécialistes d'histoire grecque et romaine, mais cela a évolué et la Grèce romaine est de plus en plus étudiée comme une partie de l'Antiquité grecque[9]. Il peut aussi être considéré que c'est le triomphe du christianisme qui met fin aux Grecs « antiques » en bouleversant leur univers religieux, et plus largement culturel et social, en établissant de nouvelles références qui font un autre type d'individu[10].
Définir ce que signifie « être grec » dans l'Antiquité est extrêmement complexe voire impossible. À l'image de toute identité, la « grécité » se manifeste suivant différentes perspectives, elle est instable et évolue selon les époques, anciennes ou modernes, notamment parce qu'elle a des usages variables. Les historiens modernes ont forgé une conception de l'identité grecque antique en partant des notions modernes d’État-nation, de théorie raciale, puis d'identité ethnique et d'auto-définition par construction d'un « Autre » opposé, en l'occurrence le « Barbare ». Il est anachronique de chercher une forme de nationalisme dans le monde grec antique, et l'unité politique de tous les Grecs n'a jamais été un objectif, donc comprendre son absence comme un échec du monde grec comme cela a pu être le cas par le passé est trompeur[12].
Néanmoins on admet souvent qu'une forme d'identité grecque (certes souvent formulée du point de vue athénien) existe au moment des Guerres médiques, ou du moins à leur sortir, en réponse à la menace que font peser les Perses sur la liberté des Grecs, et à la suite de la victoire des cités grecques qui ont choisi de résister : c'est donc une définition par la négative, face à un ennemi. C'est dans ce contexte qu'un texte souvent cité d'Hérodote (VIII, 144), définit la « grécité » par une même ascendance, l'usage d'une même langue, malgré les différences dialectales, et des mêmes rites et usages[13]. Cependant lorsqu'un mot d'ordre d'unité des Grecs est mis en avant, c'est pour servir l'ambition hégémonique d'une puissance (Athènes, royaume de Macédoine) : c'est donc par bien des aspects une affaire de contexte politique et militaire[14].
Le panhellénisme de la fin de l'époque classique cherche à unir les Grecs dans une conquête de la Perse sous forme de revanche, puis la réalisation de cette conquête et la mise en place des royaumes hellénistiques entraîne une nouvelle reconfiguration de l'identité grecque, d'autant plus que l'hellénisation à géométrie variable de nombreuses régions pose de nouvelles questions quant à la caractérisation d'une culture grecque. Les limites entre le monde grec et le monde barbare sont souvent floues sous la plume des auteurs grecs antiques, de même que les notions participant de l'identité grecque[14]. Environ un siècle après Hérodote, Isocrate propose une définition des Grecs qui exclut la notion d'ascendance : [15]. Cela implique un certain degré d'ouverture, et explique pourquoi durant l'époque hellénistique des individus originaires de peuples dominés par les monarchies grecques aient pu se revendiquer comme Grecs en intégrant des cités grecques et en adhérant à leur culture (paideia). Au minimum cela implique la maîtrise de la langue grecque, les autres éléments culturels dépendant des circonstances (alimentation, vêtements, loisirs, noms grecs, pratiques intellectuelles et politiques, etc.), ce qui explique pourquoi l'« hellénisation » présente des profils aussi divers[16].
De plus le monde grec est en permanence marqué par bien des aspects par la fragmentation, les identités pouvant se décliner suivant différentes strates :
Les phases d'expansion du monde grec durant les époques archaïque et hellénistique, en constituant un très vaste « monde grec » formé de nombreuses communautés grecques autonomes avec leurs propres identités et différentes manifestations de l'hellénité, accentuent l'impression de fragmentation[21].
Plus tard avec l'octroi de la citoyenneté romaine aux cités grecques sous le Haut Empire romain, les Grecs deviennent aussi « Romains » (Rhomaioi), au moins sur le plan juridique. Mais ils préservent généralement leur identité à part, soutenue par un fort sentiment de supériorité culturelle sur les autres peuples de l'Empire, Romains compris. Puis avec l'effondrement de l'Empire romain d'Occident les populations de l'Empire oriental (byzantin), majoritairement de langue grecque, se définissent comme des « Romains » et entendent reprendre l'héritage politique de Rome, tout en préservant leur hellénisme et leur impression de supériorité sur le monde latin[22],[23].
Les sources servant à reconstituer l'histoire grecque antique sont variées, mêlant les champs de l'histoire, de l'archéologie et de l'histoire de l'art, que l'on a tendance à de plus en plus mêler sous le vocable de « sciences de l'Antiquité »[24].
Les sources littéraires sont traditionnellement le moyen d'accès privilégié à la civilisation grecque antique. Ce sont des sources secondaires, de nature variée (histoires, théâtre, poésie, philosophie, traités scientifiques)[25]. La majorité des écrits — politiques ou historiques — de cette période qui sont parvenus jusqu'à nous provient de la sphère athénienne. C'est notamment le cas pour des auteurs comme Thucydide, Xénophon, Démosthène, Platon, Aristote. C'est pourquoi l’histoire d’Athènes occulte partiellement celle d'autres cités comme Corinthe, Sparte ou Thèbes, souvent mal connue dans le détail. En outre, de nombreuses sources ont disparu ou ne nous sont parvenues que partiellement.
Les sources primaires écrites relèvent du domaine de l'épigraphie, la collecte et l'étude des inscriptions antiques, généralement gravées sur pierre, mais aussi les ostraca écrits à l'encre sur des tessons de céramiques. À la différence du précédent ce corpus est extensible, car de nouvelles inscriptions sont régulièrement découvertes. Les tablettes administratives mycéniennes en linéaire B relèvent également de cette catégorie. Les inscriptions intéressant l'histoire grecque sont surtout rédigées en langue grecque, mais des sources des régions voisines, écrites en alphabet araméen ou en hiéroglyphes égyptiens sont également mobilisées pour l'époque hellénistique[26]. La papyrologie porte spécifiquement sur l'étude des papyri mis au jour lors des fouilles archéologiques, surtout en Égypte (textes du Fayoum d'époque hellénistique) dont le climat sec permet mieux leur conservation ; mais il s'agissait du support d'écriture privilégié dans le monde antique durant la période qui va d'environ 400 av. J.-C. à 600 ap. J.-C.[27]. La numismatique, l'étude des monnaies, permet d'obtenir des données appréciables sur l'histoire économique et politique, puisqu'il s'agit parfois des seuls documents permettant de connaître l'existence de rois (en Bactriane hellénistique notamment)[28].
Les fouilles archéologiques, en dégageant une grande quantité de vestiges matériels et les rendant disponibles pour une vaste gamme d'études, offre une grande quantité d'information sur les sociétés antiques. Elle ne porte plus seulement sur les villes et leurs monuments, puisque l'archéologie rurale s'est développée, avec la pratique des prospections, de même que celle des sites funéraires. Les données issues des fouilles sont indispensables pour les historiens spécialistes de la Grèce antique et leur exploitation a ouvert de nouveaux champs d'études, par exemple en histoire économique avec l'étude de la diffusion des céramiques[29].
L'iconographie, l'étude des images, est un autre champ important des sciences de l'Antiquité, dépendant en principe de l'histoire de l'art. L'analyse des images fournit de nombreuses informations sur la manière de penser durant les périodes antiques, notamment dans le domaine de la religion[30].
La recherche moderne considère généralement que la langue grecque n'est pas née en Grèce, mais elle n'est pas arrivée à un consensus quant à la date d'arrivée des groupes parlant un « proto-grec », qui s'est produite durant des phases préhistoriques pour lesquelles il n'y a pas de texte indiquant quelles langues étaient parlées. Les premiers textes écrits en grec sont les tablettes en linéaire B de l'époque mycénienne, au XIVe siècle av. J.-C., ce qui indique que des personnes parlant un dialecte grec sont présentes en Grèce au plus tard durant cette période. La linguistique n'est pas en mesure de trancher[31], pas plus que l'archéologie[32]. Les moments d'arrivée des premiers locuteurs d'une langue grecque dans ce pays ont généralement été recherchés durant les phases de transition entre cultures préhistoriques, vus comme des phases de rupture qui pourraient être imputables à des mouvements de populations. Ont donc pu être proposés : la fin de l'âge du Bronze moyen (vers le milieu du IIe millénaire av. J.-C.), position longtemps dominante, désormais supplantée par la période située entre la fin du Bronze ancien et le début du Bronze moyen (autour de 2300-2100 av. J.-C.), mais d'autres penchent en faveur de la fin du Néolithique et le début du Bronze ancien (v. 3200 av. J.-C.), voire la période des migrations du début du Néolithique (v. 6500 av. J.-C.)[33].
Quoi qu'il en soit, rien n'indique que l'arrivée de ces « proto-Grecs » soit une problématique historique cruciale. En effet, elle ne semble pas avoir eu un impact significatif sur l'évolution de la région et la formation des civilisations égéennes, qui ne sont pas venues d'ailleurs mais se sont développées en Grèce même. Les étapes majeures sont d'abord le développement de la civilisation minoenne, et de la mycénienne à sa suite, puis de la Grèce antique à proprement parler, dont les traits caractéristiques se constituent pour la plupart après l'âge du Bronze (voir plus bas)[32].
Cela implique du reste qu'il y ait eu des groupes parlant d'autres langues que le grec implantés en Grèce aux hautes époques, qui ont alors joué un rôle majeur. De nombreux noms de lieux, rivières, plantes et animaux présents en grec ne s'expliquent pas par une origine grecque, ni par une origine extérieure identifiable. La tradition grecque évoque des Pélasges qui auraient vécu en Grèce avant les Grecs, mais il est impossible de savoir s'ils ont effectivement existé, et le cas échéant quelle langue ils parlaient (des langues anatoliennes ont souvent été évoquées, sans emporter la conviction). Les seules traces de langues pré-grecques sont à chercher en Crète, dans les inscriptions de l'âge du Bronze, en hiéroglyphes crétois et linéaire A, qui ne transcrivent manifestement pas du grec, mais une ou des langue(s) non identifiée(s), et les inscriptions alphabétiques du Ier millénaire av. J.-C. en étéocrétois, langue non-grecque[34].
On distingue trois aires culturelles dans le monde égéen de l'âge du Bronze (3200-1200 av. J.-C.) :
Au début du IIe millénaire av. J.-C., durant l'âge du bronze moyen, émerge la première culture complexe de la Grèce, la civilisation minoenne qui s'est développée à partir de la Crète. Elle doit son nom au mythique roi crétois Minos, connu par la tradition grecque postérieure, qui a peut-être conservé par ce biais un lointain souvenir de cette civilisation. C'est une civilisation souvent désignée comme « palatiale », la première période étant dite comme « protopalatiale » (v. 2000-1850 av. J.-C.), parce qu'elle voit l'apparition d'un ensemble de constructions considérées comme étant des palais (à Cnossos, Phaistos, Malia, Zakros), même si leur fonction exacte est débattue (ils semblent avoir une fonction rituelle importante). L'organisation politique de l'époque est inconnue, quoiqu'il soit manifeste qu'elle s'inspire de celles des cultures du Proche-Orient et d’Égypte de l'époque. Des centres urbains se constituent, l'artisanat se développe (céramique, métallurgie). C'est alors que l'écriture apparaît avec les hiéroglyphes crétois et le linéaire A, mais ces systèmes ne sont pas traduits. Les textes, souvent sur tablettes d'argile, sont de nature administrative. Le début de la période néopalatiale (v. 1700-1450 av. J.-C.) voit la destruction des palais, puis une reprise autour de Cnossos qui semble devenue hégémonique. La culture minoenne s'étend sur les îles voisines de l'Égée, et aussi vers le continent, où la culture helladique connaît un essor, avec l'apparition de tombeaux monumentaux au riche matériel funéraire (tombes « royales » de Mycènes)[35],[36],[37]. La culture cycladique, qui était particulièrement prospère au début de l'âge du bronze, développe au même moment des centres urbains, sous influence minoenne (Akrotiri sur Santorin), avant de connaître une phase de déprise après le milieu du IIe millénaire av. J.-C.[38]
Pendentif en or représentant deux abeilles transportant une goutte de miel. Malia, Musée archéologique d'Héraklion.
Masque funéraire mycénien en feuille d'or, dit .
Fresque de la procession nautique d’Akrotiri (Santorin), détail, v.  1650-1500 av. J.-C.
Maquette du palais de Cnossos dans son état final (mycénien), musée archéologique d'Héraklion.
La « porte des Lionnes » de Mycènes.
Après 1450 av. J.-C., l'aire helladique voit à son tour l'émergence d'une civilisation palatiale, la civilisation mycénienne, qui doit son nom à son site principal, Mycènes, mais il y a d'autres sites majeurs tels Pylos, Thèbes, Volos, etc. qui sont des candidats pour être les capitales de royaumes mycéniens, quoi que là encore l'organisation politique ne soit pas bien connue. La redécouverte de cette civilisation s'est faite en grande partie avec les références homériques en tête, cette période étant souvent vue comme correspondant à celle de la guerre de Troie, si tant est qu'elle ait effectivement eu lieu. Cela explique qu'on y ait cherché les royaumes mentionnés par Homère, les « Achéens » dominés par le roi de Mycènes, mais la documentation archéologique et épigraphique n'a jamais confirmé cela, bien que là encore les épopées homériques aient pu conserver un lointain souvenir de ces époques, même si elles renvoient essentiellement au contexte de la fin des âges obscurs. L'archéologie indique que la Crète connaît au même moment une nouvelle crise, qui se solde par la destruction de ses palais, à l'exception de celui de Cnossos, qui devient le principal site de l'île. La Crète est alors devenue de culture mycénienne (période « postpalatiale », v. 1450-1000 av. J.-C.), ce qui est généralement vu comme la conséquence d'une invasion depuis le continent, à laquelle on pourrait imputer les destructions précédentes. Une nouvelle écriture, le linéaire B, apparaît dans l'île puis sur le continent ; elle est comprise puisqu'elle transcrit du grec, la documentation étant là encore de nature administrative. Elle ne contient pas d'informations sur l'histoire politique, mais donne des indications sur le système politique et économique, organisé autour d'un roi et de son administration dirigeant des activités économiques sur leur territoire depuis le palais, et sur la religion, puisqu'elle enregistre des offrandes aux divinités, qui sont pour plusieurs d'entre elles des figures connues de la religion grecque postérieure (par exemple Zeus, Héra, Hermès). C'est donc une civilisation grecque, au profil certes bien différent de celle de l'Antiquité classique. La culture mycénienne connaît à son tour une expansion dans le monde égéen, puisqu'on retrouve ses traits matériels jusque sur la côte anatolienne. Elle est sans doute rentrée en contact avec le royaume dominant cette région, les Hittites, qui mentionnent la présence d'un royaume occidental appelé Ahhiyawa, dont le nom rappelle celui des Achéens homériques. La civilisation mycénienne s'effondre après 1200 av. J.-C., avec la destruction de ses palais et de son système palatial (son écriture disparaît), pour des raisons indéterminées : des invasions extérieures (« doriennes ») ont été invoquées par le passé, des facteurs internes semblent actuellement plus probables, mais plusieurs éléments peuvent s'être combinés[39],[40],[41].
Les caractères matériels de la civilisation mycénienne disparaissent progressivement dans le courant du XIIe siècle av. J.-C. (période « post-palatiale »), puis l'habitat et les activités économiques connaissent une contraction brutale. S'ouvre ensuite une période sans documentation écrite et avec une documentation archéologique bien moins abondante, essentiellement constituée de sépultures, sans trace d'organisation politique complexe, désignée comme des « âges (ou siècles) obscurs », qui porte surtout bien son nom pour la période qui va d'environ 1150 à 1000 av. J.-C. La période suivante, celle qui voit le développement de la tradition des céramiques géométriques, voit une reprise des échanges et une nouvelle phase de formation d'entités politiques, visible surtout en Crète, mais aussi sur le continent (Lefkandi sur Eubée, en Attique, en Argolide). Cette période est le début de l'âge du fer, le travail de ce métal se développant rapidement. Les contacts avec le Proche-Orient sont renoués. Les Xe – IXe siècle av. J.-C. sont donc de plus en plus vus comme une phase de reprise, préparant la formation du monde des cités grecques. C'est peut-être de cette période que parlent avant tout les épopées homériques : un âge où les communautés sont dirigées par des chefs ayant un rôle avant tout militaire, et une autorité limitée. Il ne reste en fin de compte plus grand-chose des traditions mycéniennes, dont l'architecture et l'écriture ont disparu, et les accomplissements sont largement oubliés : les Grecs sont passés à autre chose, développant une nouvelle civilisation[42],[43].
Au VIIIe siècle av. J.-C., le rythme des changements s'accélère, et certains n'hésitent pas à parler de renaissance du monde grec, même si la tendance récente est à réévaluer les avancées accomplies durant la seconde partie des « âges obscurs ». C'est dans le courant de cette période que se situe le début de l'époque archaïque, souvent placé de façon symbolique en 776 av. J.-C., date supposée des premiers jeux olympiques[44]. Sa fin se produit au plus tard lors de la seconde guerre médique, en 480-479 av. J.-C.
Le principal signal de la mise en place d'un monde nouveau à la fin des âges obscurs est l'adoption de l'écriture, cette fois-ci à partir de l'alphabet phénicien, autour de 800 av. J.-C., ce qui conduit à l'émergence de l'alphabet grec. Sur le plan sociopolitique, cela s'accompagne de la formation d'un nouveau type d'entité, la « cité-État », polis, qui est adoptée progressivement dans la partie méridionale du monde égéen, entre la Grèce centrale et la côte occidentale d'Asie Mineure, en passant par les îles, dont la Crète qui joue encore un rôle pionnier durant les premiers siècles archaïques, avant de s'effacer. Le pays est alors divisé en une multitude de petites communautés indépendantes, situation imposée par la géographie grecque, où chaque île, vallée ou plaine est totalement coupée de ses voisins par la mer ou les montagnes. Le phénomène de formation des cités s'accompagne rapidement d'un processus d'expansion en dehors du monde égéen, la colonisation grecque, après 750 av. J.-C., qui voit la fondation de cités sur les rives de la Méditerranée, notamment en Sicile et sur la péninsule italienne, où se forme la « Grande Grèce », et sur la mer Noire. Les cités mettent progressivement en place des lois et des institutions qui leur sont propres, ce qui débouche sur le développement d'une vie politique originale, très dynamique, impliquant une grande partie du corps social, adulte et masculin, les citoyens. Cette particularité grecque est souvent vue comme l'origine des nombreux changements culturels qui se produisent et aboutissent à la mise en place de la civilisation grecque antique : c'est en effet à cette période que se développent l'art et l'architecture grecques, se dégageant vite des modèles orientaux et égyptiens, avant tout destinés dans un premier temps à satisfaire les demandes de la vie religieuse qui se réorganise dans le cadre civique, qu'émerge la littérature poétique grecque à partir des récits épiques d'Homère et d'Hésiode (qui auraient vécu dans la seconde moitié du VIIIe siècle av. J.-C.) puis la poésie lyrique, et les premiers philosophes et scientifiques grecs en Ionie et en Grande Grèce (Thalès, Pythagore, Héraclite, Parménide, etc.), qui créent une façon de voir le monde rompant avec les pratiques antérieures. Une autre innovation de la période est l’apparition de la monnaie frappée, depuis l'Asie mineure, qui se diffuse rapidement dans le monde grec. La civilisation des cités grecques reprend certes de nombreux éléments techniques et intellectuels des civilisations antiques du Proche-Orient et d’Égypte, mais elle se les réapproprie et les repense pour mettre en place une civilisation originale et novatrice[45],[46].
L'histoire politique et militaire de la période ne peut être reconstituée faute de sources explicites. L'art de la guerre grec se met en place lors de la « révolution hoplitique », qui voit l'apparition d'armées de citoyens organisés en phalanges de fantassins. Quelques guerres sont mentionnées pour cette période, la plus ancienne étant la guerre lélantine (autour de 700 av. J.-C.), entre Chalcis et Érétrie, qui aurait déjà impliqué par le jeu des alliances un grand nombre de cités du monde grec, y compris des colonies. Les cités connaissent divers défis, notamment la gestion d'une croissance démographique qui semble causer un manque de terres, qui serait en bonne partie à l'origine de la fondation des colonies, et des tensions sociales qui peuvent dégénérer en conflits civils. C'est dans ce contexte qu'émergent des tyrans, personnages qui profitent des troubles pour accaparer le pouvoir dans de nombreuses cités (Argos, Corinthe, puis Athènes, etc.). La cité de Sparte, dirigée par une oligarchie et organisée autour d'une armée redoutable, connaît une phase d'expansion territoriale qui en fait une des plus puissantes cités grecques. Athènes est également plus étendue et puissante que les autres, mettant en place son système démocratique après avoir mis fin à la tyrannie à la fin du VIe siècle av. J.-C.[47].
À la fin de la période archaïque, le monde grec fait face à des périls extérieurs : d'abord la Lydie qui domine la plupart des cités d'Asie Mineure durant la première moitié du VIe siècle av. J.-C., puis l'empire perse des Achéménides qui l'absorbe, et met en place une politique d'expansion militaire vers le monde égéen et les Balkans. Ils rencontrent la résistance de plusieurs cités grecques, conduites par Athènes, ce qui aboutit au déclenchement de la première guerre médique en 492, qui s'achève par le triomphe athénien lors de la bataille de Marathon, en 490. La seconde guerre médique, organisée une dizaine d'années plus tard et plus méthodiquement par les Perses, impliquant une armée d'une grande dimension, entraîne la soumission de nombreuses cités grecques et d'importantes destructions. La résistance grecque, menée par Athènes et Sparte, l'emporte cependant à nouveau et éloigne la menace perse plus durablement[48].
L'époque classique va de la fin des guerres médiques en 480/479 av. J.-C. jusqu'à la mort d'Alexandre le Grand en 323 av. J.-C. Elle couvre donc en gros le Ve et le IVe siècle av. J.-C. C'est traditionnellement la période vue comme l'« âge d'or » des cités grecques (au moins dans les domaines politique et militaire), et une référence du point de vue culturel.
La fin des guerres médiques laisse la Grèce partagée entre les deux vainqueurs, Athènes et Sparte, qui établissent chacune leur sphère de domination, la première prenant la domination des mers, la seconde celle des terres. Pendant un demi-siècle elles s'affrontent surtout de manière indirecte. Athènes connaît une forte expansion, constituant une sorte d'« empire » organisé autour de la ligue de Délos, alliance dont elle a la direction qui doit en principe servir à combattre l'influence des Perses, mais devient progressivement un instrument servant sa domination, surtout après la conclusion de la paix de Callias avec la puissance iranienne en 450 av. J.-C. En Occident, les riches cités grecques de Sicile et d'Italie connaissent leur propre lutte contre une puissance extérieure, Carthage[49].
L'époque classique est également dominée par Athènes sur le plan des sources, ce qui explique que cette cité ait focalisé l'attention. Son système politique démocratique est abondamment documenté, au risque de masquer la diversité des cités qui n'apparaissent qu'épisodiquement dans nos sources, à l'exception de sa rivale spartiate (à vrai dire surtout connue grâce à des témoignages athéniens). Du point de vue culturel Athènes devient le principal centre du monde grec, et produit la majeure partie des œuvres littéraires connues pour la période, est le lieu principal des débats philosophiques (Socrate, Platon, Aristote), la seule cité dont on connaisse les œuvres théâtrales, et son art (surtout la sculpture) et son architecture sont couramment présentés comme la quintessence des réalisations grecques, illustrées par les réalisations du chantier de l'Acropole commandité par Périclès, le principal homme politique de la période précédant le conflit contre Sparte. Le cité est également un pôle économique majeur du monde grec, et attire vers elles des individus de tout cet espace. Cela explique le tropisme athénien des études sur la Grèce antique, qui ne doit pas faire oublier la diversité d'un monde qui va de l'Espagne jusqu'à l'Asie Mineure et la mer Noire, qui ne comprend du reste pas que des cités puisqu'on trouve des monarchies en Grèce du Nord, notamment la Macédoine[50].
La rivalité entre Sparte et Athènes éclate en conflit ouvert en 446 av. J.-C., date qui marque le début de la très destructrice guerre du Péloponnèse. Les deux puissances se tiennent en respect en misant sur leurs domaines respectifs, la terre pour l'une et la mer pour l'autre, et s'infligent à plusieurs reprises des défaites cinglantes, qui ne modifient cependant pas l'équilibre. La dernière phase du conflit est marqué par une expédition calamiteuse des Athéniens en Sicile, et l'alliance entre Sparte et les Perses, qui leur donne accès à des moyens financiers leur permettant de constituer une flotte et de prendre progressivement le dessus sur les Athéniens sur leur domaine privilégié. En 404 Athènes capitule, mais elle est épargnée par son vainqueur[49].
Ce n'est pas pour autant que l'hégémonie spartiate peut s'imposer au monde grec. Le IVe siècle av. J.-C. est en effet marqué par des tentatives successives de domination qui tournent court malgré des succès importants : Sparte se voit de plus en plus concurrencée par Thèbes, tandis qu'Athènes reconstitue progressivement sa puissance. Les troupes terrestres spartiates voient leur suprématie s'achever après leur défaite à Leuctres en 371, face aux troupes thébaines menées par Épaminondas. Les trois rivaux s'affaiblissent chacun à son tour, et c'est le royaume de Macédoine dirigé par Philippe II qui parvient finalement à s'imposer en quelques années comme la principale puissance grecque. La bataille de Chéronée en 338 av. J.-C. met Thèbes et Athènes hors jeu, tandis que Sparte a déjà entamé son éclipse[51],[52].
Philippe II meurt assassiné en 336 av. J.-C., alors qu'il nourrit le projet de partir à l'assaut de l'empire perse. Son projet est mis à exécution par son fils Alexandre à partir de 334. En une dizaine d'années, il parvient à la suite d'une série de victoires d'ampleur à détruire l'empire achéménide et à dominer un territoire allant de l’Égypte jusqu'au monde indien, bouleversant l'ordre politique de cette partie du monde. Il entend en conserver le contrôle, et y fonde de nombreuses cités, ouvrant la voie à une nouvelle phase d'expansion du monde grec, cette fois-ci en direction des terres orientales. Sa mort prématurée en 323 av. J.-C. laisse aux Gréco-Macédoniens la suprématie sur les contrées des plus anciennes civilisations antiques[53],[54].
À la mort d'Alexandre le Grand en 323 av. J.-C., date qui marque le début de l'époque hellénistique, son immense empire est partagé entre ses principaux généraux, les Diadoques. Ptolémée, Séleucos et Antigone fondent des dynasties qui règnent sur de vastes royaumes. La prépondérance des rois et des royaumes dans le jeu politique est un changement de taille, reléguant les cités au rang de pions dans l'échiquier politique qui se met en place, généralement dans l'incapacité de s'émanciper. Les rois légitiment leur pouvoir par la victoire et mettent en place un culte royal[55]. Les Lagides (ou Ptolémées) dominent l'Égypte, disposant d'un royaume cohérent territorialement, où ils sont peu inquiétés. Ils étendent également leur influence vers Chypre et le monde égéen. Les Séleucides dominent au départ les pays asiatiques allant du Proche-Orient jusqu'à l'Asie centrale, soit la majeure partie de l'ancien empire perse, mais ils perdent rapidement les territoires des confins et leur empire se concentre surtout sur la Syrie et la Mésopotamie. Des royaumes grecs se constituent sur une partie des anciennes provinces séleucides, en Bactriane (royaume gréco-bactrien) et dans la vallée de l'Indus (royaumes indo-grecs). Après avoir été disputée entre plusieurs prétendants, la Macédoine passe sous le contrôle des Antigonides et redevient une puissance essentiellement européenne, et doit composer avec sa voisine, l'Épire. L'Anatolie est divisée en plusieurs royaumes plus ou moins hellénisés, le plus puissant étant celui de Pergame dirigé par les Attalides. En Grèce et en Asie Mineure certaines cités préservent leur autonomie, mais elles ne sont alors plus en mesure de jouer un rôle politique notable, à l'exception de Rhodes qui dispose d'une flotte puissante. La Grèce continentale voit l'émergence d’États fédéraux (Arcadie, Achaïe, Étolie) qui sont en mesure de tenir tête aux royaumes[56],[54]. La période est marquée par un état de guerre quasi-permanent, entre grands royaumes, cités, fédérations, mais également des puissances extérieures (Parthes, Rome)[57].
La conquête d'Alexandre a donc marqué une rupture en élargissant considérablement le monde grec, et en plaçant de vastes et riches régions sous domination gréco-macédonienne (Égypte, Syrie, Mésopotamie, Perse). De nombreuses cités grecques sont fondées, avant tout en Asie, même si la plus vaste d'entre elles est la capitale des Lagides, Alexandrie d’Égypte. Le nombre de cités grecques s'accroît donc considérablement. La cité reste donc le cadre de vie privilégié des Grecs et Grecques, quel que soit leur pays de résidence. Les Grecs constituent parfois une part importante de la population des pays dominés (Fayoum, Syrie du Nord, peut-être Bactriane), mais ils restent en général minoritaires. Les autochtones peuvent adhérer plus ou moins à la culture grecque, ce qui a été désigné comme une hellénisation, phénomène qui marque plus certaines régions (Phénicie, Anatolie centrale), que d'autres où les résistances sont fortes (Égypte, Judée). Quoi qu'il en soit la culture grecque se diffuse dans tous les pays dominés, certes à des degrés divers, et est amenée à exercer une influence durable sur beaucoup de ces régions, et des foyers de culture grecque se constituent hors de Grèce, en premier lieu Alexandrie qui est le principal lieu de création littéraire et artistique de la période[58].
Dans le courant de la seconde moitié du IIIe siècle av. J.-C., Rome commence à s'étendre en direction du monde grec, à commencer par les cités de Grande Grèce, puis prend pied sur la rive orientale de l'Adriatique dès 229. Le royaume de Macédoine subit une série de défaites qui se soldent par des pertes territoriales puis son annexion et sa division en 168. Entre-temps les Séleucides ont été amputés de plusieurs de leurs territoires, mais ce sont surtout leurs revers à l'est face aux Parthes qui précipitent leur déclin en leur faisant perdre la Mésopotamie. En Grèce, la confédération achaïenne et un ensemble de cités sont à leur tour défaites par Rome, qui prend le contrôle de la région après la destruction de Corinthe en 146 av. J.-C.[54]. Les Romains acquièrent progressivement les territoires hellénistiques par conquête, mais aussi par héritage (Pergame, Bithynie, Cyrénaïque). Une nouvelle tentative de secouer le joug romain, sous la direction du roi Mithridate VI du Pont en 88 av. J.-C. se conclut par un nouvel échec et le saccage d'Athènes. Puis les régions grecques subissent les guerres civiles de la fin de la République romaine, qui entraînent de nouvelles destructions et trouvent leur conclusion durant la bataille d'Actium en 31 av. J.-C., quand Octave défait Marc-Antoine et Cléopâtre, la dernière des souverains hellénistiques, ce qui débouche sur l'annexion de l’Égypte lagide[59].
Après les temps difficiles de la basse époque hellénistique, la mise en place de l'empire romain par Octave devenu Auguste ouvre une période de stabilité et de prospérité pour le monde grec, qui dure jusqu'au IIIe siècle. La cité grecque est l'unité de base sur laquelle se repose l'administration impériale, et les Romains en créent de nouvelles, ainsi que des colonies romaines, et ce jusqu'au IVe siècle, ce qui se traduit par un nouvel accroissement du nombre de cités, et continue de faire de celles-ci le cadre de vie fondamental des Grecs. Elles préservent largement leur autonomie pour leurs affaires internes[60]. Les éléments culturels romains se diffusent mal dans le monde grec : le culte des empereurs est certes adopté ainsi que certains traits de la vie romaine (jeux du cirque), mais le latin est peu pratiqué en dehors de l'administration impériale, alors que le grec continue à se diffuser, notamment en Asie Mineure, et est plus que jamais la langue de culture des élites du monde hellénisé. Les grands centres culturels hellénistiques restent très actifs (Alexandrie, Athènes, Rhodes, Antioche)[61]. Surtout la culture grecque a largement été adoptée par les élites romaines, ce qui octroie une position à part aux Grecs dans l'empire, parce qu'ils sont certes dominés politiquement mais sont dans une position de force culturellement. Les notables hellénisés sont de plus en plus intégrés dans l'édifice politique romain, en obtenant par exemple des positions au Sénat, et les Grecs sont généralement enclins à la loyauté envers le pouvoir romain, quand bien même cela soit parfois présenté comme de la résignation[62],[63].
Après la crise qui affecte le monde romain durant le IIIe siècle, le pouvoir impérial connaît de profonds changements dans son organisation, dont la fondation d'une nouvelle capitale en Orient n'est pas le moindre : Constantinople, inaugurée en 330. Celle-ci devient progressivement la principale ville du monde grec, son centre politique et également culturel[64]. La division de l'Empire romain entre Orient et Occident se constitue progressivement, et le monde grec devient alors autonome, assurant la continuité de l'empire romain après la fin de sa moitié occidentale. Cet empire survivant, que l'on connaît surtout sous le nom de « byzantin », est en fait romain de nom mais grec de culture, et aussi d'administration puisque le latin y perd rapidement son statut de langue officielle. La christianisation progresse, et devient un phénomène de première importance au IVe siècle, évinçant progressivement les cultes polythéistes. Le monde grec romain semble voir sa prospérité se poursuivre jusqu'au VIe siècle au plus tard. La peste justinienne qui sévit à partir de 541, les invasions slaves qui ravagent une grande partie de la Grèce à partir de 582, puis le terrible conflit entre l'Empire romain d'Orient et les Perses sassanides au début du VIIe siècle qui cause d'importantes destructions en Asie marquent un renversement de la tendance et voient ces régions s'enfoncer progressivement dans une période de difficultés profondes. La période des conquêtes arabo-musulmanes fait définitivement rentrer le monde hellénisé d'Asie et d'Afrique dans une nouvelle ère[65],[66]. Dans ce chaos, les villes déclinent et les institutions pluriséculaires des cités grecques disparaissent[67]. Le monde byzantin assure dès lors la continuité de l'hellénisme, certes sous un nouveau profil très marqué par le christianisme, mais conservant une partie des traditions intellectuelles antiques[68].
Les entités politiques des époques minoenne et mycénienne sont définies comme des États, ce qui implique qu'il s'agit d'une forme d'organisation sociale et politique « complexe », dirigée par une élite disposant du pouvoir et des richesses, apparemment organisée chez les Mycéniens autour d'un « roi » (wanax), encadrant le reste de la population (avec une « bureaucratie » attestée par les documents en linéaires A et B), qui est organisée suivant des statuts hiérarchiques et divers métiers spécialisés. Cela concerne les entités organisées autour de palais, le reste de la Grèce étant organisée autour de structures sociopolitiques plus « simples » (chefferies)[69]. Après la disparition des États mycéniens, les âges obscurs sont caractérisés par des entités politiques très peu hiérarchisées malgré la présence de rois (basileus), dont le pouvoir semble être d'origine guerrière[70], avant le retour de structures plus complexes entre le IXe siècle av. J.-C. et le VIIIe siècle, période qui voit un nouveau mouvement d'affirmation du pouvoir et de formation étatique[71]. La trajectoire sociopolitique grecque prend alors une tournure spécifique, avec la formation des cités-État, qui deviennent un élément cardinal du monde grec antique, mais jamais la seule forme d'organisation politique puisque d'autres coexistent avec elles.
Le terme grec polis peut se traduire par « cité-État » dans un contexte politique, mais il peut aussi désigner une « ville »[72],[73]. C'est la forme la plus répandue d'organisation politique durant l'Antiquité dans la Grèce centrale et méridionale ainsi que l'Asie mineure grecque et les colonies grecques. Elle se constitue progressivement durant l'époque archaïque suivant des modalités qui restent difficiles à tracer avec exactitude. On peut le suivre dans le développement progressif de sociétés urbaines avec un urbanisme et une architecture caractéristiques des villes et bien distincts des villages et bourgs, avec une stratification sociale et une spécialisation professionnelles plus affirmées, l'usage croissant de l'écriture, le développement des échanges. La fondation de nouvelles cités, les colonies, à partir du milieu du VIIe siècle av. J.-C., est la manifestation la plus évidente du phénomène. Mais ce n'est pas avant le VIe siècle av. J.-C. que les caractéristiques des cités achèvent de se constituer[74]. Quant à savoir pourquoi cela se produit à cette période, et suivant ces modalités et pas une autre (le modèle de la cité-État n'est certes pas spécifique à la Grèce mais il la distingue des pays orientaux alors dominés par des monarchies territoriales), cela est également en débat[75].
La cité-État[77] se définit avant tout comme une communauté humaine : dans les discours officiels, la cité est désignée par ses habitants, ou plutôt le corps de ses citoyens (politai), donc des « Athéniens », « Spartiates », « Milésiens », etc. Ce sont ceux qui prennent effectivement part au processus de décision politique, qui y accèdent suivant des principes variés, mais sont dans tous les cas des hommes adultes, ce qui revient à exclure du groupe une grande partie de la population qui vit dans la cité. C'est donc sa communauté citoyenne qui est la condition fondamentale pour son existence, plus que le territoire sur lequel elle exerce sa souveraineté, ce qui fait la spécificité du modèle de la cité grecque. Elle dispose d'un chef-lieu, en général une ville (mais pas toujours), où siègent ses institutions qui sont en gros similaires d'une cité à l'autre, avec une assemblée, un conseil, des tribunaux, et des magistrats désignés selon des modalités diverses. À compter du VIIe siècle av. J.-C. des lois écrites régissent l'organisation des cités. La population de la cité (au-delà des citoyens) manifeste sa cohésion et son identité par des cultes civiques, notamment ceux de la divinité tutélaire de la cité (la divinité « poliade ») et des héros et héroïnes locaux, dont les sanctuaires participent à la structuration de son territoire. Elle est plus largement soudée par un ensemble de coutumes ancestrales. En fin de compte, les foyers, les groupes de parenté, l'économie comme la religion et la culture sont subordonnés à la cité, qui les oriente dans un sens « politique ». La cité s'impose comme la référence principale d'un individu grec antique, au-delà des autres références (parentés, ethnè), et l'implication du corps des citoyens dans la prise de décision politique entraîne de nombreux débats et réflexions, l'émergence d'une culture politique qui affecte tous les aspects de la vie de la cité[78],[79],[80]. Ainsi,  (P. Brulé)[81]. Cela se manifeste en particulier durant l'époque classique. On trouve alors une myriade de cités de tailles et de population très diverses, constituant des microcosmes peu ouverts aux autres[82]. Écrivant au moment où le processus est achevé et consolidé, Aristote a ainsi pu définir l'homme grec comme un « animal politique », qui ne peut s'épanouir que dans une polis[83]. Cela explique qu'on mette souvent au crédit de la polis d'avoir stimulé les innovations intellectuelles de la Grèce antique.
La fin de l'époque classique est marquée par l'échec politique des cités, et la perte de leur autonomie face aux royaumes, puis face à l'expansion romaine. Mais cela ne signifie pas la fin des cités grecques, ou même leur déclin. Elles restent la forme courante d'organisation des communautés grecques, et ce de plus en plus puisque de nombreuses fondations ont lieu durant cette période, à l'initiative des rois, assurant la diffusion du modèle. Dans les pays conquis par les royaumes hellénistiques en Asie et (dans une moindre mesure) en Égypte, cela se traduit par la constitution d'enclaves grecques dominant un arrière-pays autochtone. La citoyenneté et l'éducation intellectuelle et physique qui l'accompagne sont alors fondamentales dans l'identité grecque : la polis est aussi un modèle culturel. Cela explique sa survie sous l'empire romain[84].
Les cités grecques sont en effet la structure de base de l'organisation politique de la moitié orientale de l'Empire romain, l'hellénisme trouvant là un vecteur d'épanouissement dans le monde « gréco-romain »[85]. Des colonies romaines sont fondées dans le monde grec (au sens large la partie orientale de l'Empire), parfois dans des cités grecques[86]. Du point de vue juridique la domination romaine s'accompagne de l'octroi progressif de la citoyenneté romaine aux habitants du monde grec, et elle se surimpose à la citoyenneté grecque d'origine. Elle est généralisée par l'édit de Caracalla en 212[87].
Durant l'Antiquité tardive, les cités grecques restent prospères, et poursuivent leur existence au moins jusqu'au VIe siècle. Elles disparaissent ensuite, dans la crise qui touche l'Empire romain d'Orient au VIIe siècle et porte un coup sévère aux villes, alors que le pouvoir byzantin constitue un système provincial qui prend en charge les affaires locales à leur place[67].
Si la polis est à l'origine du terme et du concept modernes de « politique », c'est parce qu'elle a donné lieu à des expériences d'organisation et de vie sociale diverses et novatrices. Cela a suscité des débats et affrontements très vigoureux et éveillé les réflexions des penseurs, qui se sont interrogés sur la vie dans la cité.  (P. Brulé)[81].
Le processus de formation des cités grecques s'accompagne du développement de lois (nomoi), couchées par écrit à partir du VIIe siècle av. J.-C. Souvent, elles sont attribuées postérieurement à des figures enveloppées d'une aura légendaire, tels Zaleucos à Locres Épizéphyrienne, Dracon et Solon à Athènes et Lycurgue à Sparte. Elles sont présentées comme l'expression de la volonté collective du corps citoyen, ce qui leur confère leur légitimité, et leur inscription sur pierre (elles sont souvent connues par ce biais, par exemple le « code de Gortyne »), métal ou bois les rend obligatoires. Elles concernent potentiellement tous les domaines de la vie privée et publique. En pratique la décision des lois prend différentes modalités : par l'assemblée de citoyens, par les personnes exerçant de façon collégiale la magistrature de juge. Ceux-ci siègent dans un tribunal civique, rendent leur verdict après des débats qui ont donné lieu à Athènes à l'apparition de personnes spécialisées dans la rédaction de plaidoyers, les logographes[89].
Les lois régissent suivant des modalités diverses le fonctionnement des institutions civiques, qui reposent en général autour d'une assemblée, d'un conseil, d'un tribunal, et de magistratures civiles et/ou militaires (archonte, agoranome, stratège, polémarque, etc.) exercées pour des durées plus ou moins longues selon le régime politique. Pour leur fonctionnement, les cités ne développent pas de bureaucratie, elles impliquent autant que faire se peut leurs citoyens dans la direction de ses affaires et la prise de décision. Le système athénien est de loin le mieux connu pour l'époque classique : les tâches administratives comme celles, financières, des poletai, sont réparties entre des citoyens, par groupes de dix exerçant une charge pendant un an, sous la supervision du conseil (la Boulè). On se soucie plus de leur probité que de leurs compétences[90]. La fiscalité prend des formes variées : il existe des taxes sur les transactions, un impôt de répartition exceptionnel (eisphora). Leur prélèvement est affermé. Les plus riches sont spécifiquement mis à contribution par le biais des liturgies, servant à financer le fonctionnement des gymnases, des pièces de théâtres, des banquets, des navires de guerre, etc. Tout cela ne suffit pas à financer les importantes dépenses militaires de la cité, qui recourt durant sa période impérialiste à un tribut prélevé sur ses alliés[91],[92].
La vie politique, et plus précisément le processus de prise de décision, surtout connu par les exemples athénien et spartiate pour les époques archaïque et classique, se déroulent suivant un processus ritualisé, placé sous les auspices divins, suivant une procédure fixée. Une fois la décision prise, elle doit s'appliquer aux citoyens. La discussion occupe une place centrale, afin de débattre et de parvenir à une prise de décision, lors d'assemblées auxquels tous les citoyens peuvent en principe participer. En pratique les chefs politiques qui s'y expriment sont généralement issus du milieu des familles de l'élite (comme les Alcméonides d'Athènes : Clisthène, Périclès, Alcibiade), mais à Athènes émerge le phénomène des « démagogues », d'extraction moins élevée, accusés de flatter le peuple pour assurer leur ascension politique. Avec le développement de la rhétorique, les discours politiques athéniens, surtout connus pour le IVe siècle av. J.-C., sont très savamment construits. Il n'y a pas de partis politiques à proprement parler, même s'il existe des formes de factions. Des phases de dissensions (stasis), ayant souvent pour origine des tensions sociales (accès à la terre, inégalités de richesse), brisent le consensus afin de changer l'ordre des choses. Elles peuvent dégénérer en conflits internes, et en changements de régimes politiques. Elles sont vues comme une forme de maladie de la communauté civique, devant laquelle les institutions sont impuissantes[93]. Ce bouillonnement politique s'accompagne rapidement d'un effort de théorisation débouchant sur l'émergence d'une science politique, visible avant tout dans les travaux de philosophes d'Athènes. Platon réfléchit à une cité « idéale », de même qu'Aristote, qui en plus se penche sur les différentes formes d'organisation politique (y compris la monarchie perse, vue comme « despotique »)[94].
Parmi les régimes politiques se développant durant les époques archaïque et classique, et qui sont décrits et théorisés par les historiens et penseurs antiques, trois se détachent par leur importance historique :
Le dynamisme de la vie politique des cités grecques implique des évolutions dans ces systèmes de gouvernement, les régimes ayant des durées variables, étant notamment impactés par les conflits militaires qui génèrent de fortes tensions internes, dans lesquelles les puissances rivales ne manquent pas de s'impliquer. Athènes expérimente ainsi la tyrannie, avant une évolution vers un régime démocratique, pensé comme une façon d'éviter le retour de ce régime (ce qu'indique notamment la procédure d'ostracisme, qui exile ceux soupçonnés de briguer le pouvoir pour eux-mêmes), mais l'opposition oligarchique y reste forte et triomphe à plusieurs reprises. Le régime démocratique y est très débattu : ses partisans le voient comme un gouvernement par la loi, reposant sur les principes de liberté (ce qui comprend la liberté de parole) et d'égalité (devant la loi et dans l'accès à la vie politique), alors que pour ses opposants (partisans d'un régime oligarchique, philosophes) c'est le gouvernement des gens du commun, voire des pauvres citadins, contre les paysans et grands propriétaires, et la liberté une illusion qui détourne les individus du véritable sens de l'existence par l'expression d'une pluralité d'opinions[102]. Syracuse connaît aussi ces trois types de régimes, une vie politique intérieure manifestement mouvementée, mais bien moins documentée[103].
Pour ce qui concerne les relations entre cités (et plus largement entre États), dans le vocabulaire des affaires extérieures la notion de « liberté » (eleutheria) indique qu'une cité ne subit pas de contrainte extérieure, tandis que celle d'« autonomie » (autonomia) renvoie au fait qu'une cité est passée sous la coupe d'une puissance dominante mais conserve une indépendance plus ou moins limitée et peut s'auto-administrer[104]. Il existe des alliances militaires (symmachies) de durée limitée ou indéfinie. Les secondes sont désignées comme des « ligues » ou « confédérations » dans la littérature moderne, car elles peuvent aboutir à l'unification de la politique extérieure des membres[105]. Les amphictyonies, groupements d’États servant pour l'administration de lieux de cultes majeurs (Delphes avant tout) ont également un rôle politique[106]. Ces groupements sont placés à l'époque classique sous l'égide d'une puissance dominante : Sparte pour la ligue du Péloponnèse, Athènes pour la ligue de Délos, Thèbes pour la confédération béotienne, le royaume de Macédoine pour l'amphictyonie de Delphes et la ligue de Corinthe. L'époque classique est en effet marquée par des tentatives d'imposer une hégémonie de la part d'une de ces grandes puissances, quand bien même elles promettent liberté et autonomie, ce qu'on désigne généralement comme un « impérialisme » (ou « empire athénien » pour la ligue de Délos), sans pour autant que cela n'accouche sur la constitution d'un État territorial[107].
Durant l'époque hellénistique, les cités grecques sont certes pour la plupart dominées par des royaumes (certaines restent indépendantes, comme Rhodes, Sparte, aussi Athènes par périodes) mais elles préservent leurs institutions et la gestion de leurs affaires internes. Les rois continuent régulièrement de leur promettre liberté et autonomie, mais elles doivent désormais composer et négocier avec eux (et leur versent souvent un tribut) et n'ont généralement pas d'armée (ce qui ne les empêche pas de conduire des affaires diplomatiques avec d'autres cités). Elles ont pour la plupart adopté des institutions de type démocratique[108]. Leur financement repose de plus en plus sur les contributions de leurs riches citoyens, qui deviennent des bienfaiteurs réguliers finançant toutes sortes de constructions et prestations (financement de spectacles, d'écoles, distributions alimentaires). C'est le phénomène qui a été désigné comme l'« évergétisme » (néologisme dérivé du grec eu ergein, « bien agir »). Les rois hellénistiques sont au départ d'importants bienfaiteurs, mais quand leur pouvoir s'affaiblit ils laissent la place aux élites locales[109],[110],[111].
Le pouvoir romain fait aussi des cités la base de son administration, et après leur avoir à son tour promis la liberté il les place sous le contrôle de gouverneurs provinciaux. Le gouvernement des cités évolue vers l'oligarchie, favorisée par les conquérants. Les conseils des cités ne sont plus renouvelés, ils sont accaparés par les citoyens les plus riches, qui exercent également la plupart des magistratures importantes, d'autant plus qu'il faut que leur détenteur les finance en bonne partie sur ses propres deniers. L'évergétisme prend alors une place plus importante que par le passé, devenant un mode de financement normal de la cité, avec de plus en plus un caractère contraignant pour le bienfaiteur. Beaucoup cherchent d'ailleurs à se dégager de cette charge, par une autorisation impériale[112].
Durant l'Antiquité tardive il est progressivement remplacé par une obligation pour les détenteurs de charges civiques de financer la cité, et les institutions charitables chrétiennes (hôpitaux, hospices), qui ne réservent plus leurs services aux seuls citoyens mais les ouvrent en principe à tous, en priorité aux pauvres. L'élite locale foncière conserve le pouvoir, elle peut participer au financement des églises et organisations caritatives. Les institutions ecclésiastiques, en particulier les évêques (choisis après élection par des clercs et des laïcs), prennent un rôle croissant dans la vie locale. Cela est renforcé quand les institutions civiques disparaissent avec la crise urbaine des VIe – VIIe siècles, le système provincial impérial prenant le relais pour la direction des affaires locales[113]. Les institutions civiques sont formellement abolies par Léon VI le Sage (886-912) mais certains de leurs éléments sont préservés dans l'administration des villes byzantines[114].
Alors que la cité se met en place durant l'époque archaïque, une autre forme d'organisation politique domine la Grèce du Nord et de l'Ouest, l’ethnos (ethnè au pluriel). Il se trouve par exemple en Haute Macédoine, Épire, Thessalie, Phocide, aussi en Achaïe et Messénie dans le Péloponnèse. Le monde grec ne peut donc être réduit au monde des cités grecques, les deux modèles coexistant, et se superposant dans plusieurs cas (Achaïe, Béotie, Acarnanie). L’ethnos doit être vu comme une autre forme étatique évoluant aux côtés de la cité-État, et non comme une forme primitive de celle-ci, moins bien organisée. Il regroupe également une communauté d'habitants, a une dimension territoriale, et on peut y trouver des villes, bien que les régions concernées soient en général de peuplement plus clairsemé que celles où la cité domine. Les ethnè sont également de tailles très diverses, certains regroupent une population très importante. Ils semblent en général avoir été plus ouverts que les cités quant à l'admission de nouveaux membres, ce qui explique sans doute comment plusieurs d'entre eux ont pu connaître une croissance importante aux IVe – IIIe siècles alors que les cités sont souvent en butte à un problème d'érosion de leur corps de citoyens. Leurs institutions sont régies par des lois et organisées selon différents niveaux territoriaux (village, districts, puis ethnos), disposent d'assemblées, elles peuvent être dirigées par des rois, dont le pouvoir est contrebalancé par celui des magistrats et des coutumes qui limitent leur champ d'action[115].
Les « États fédéraux » ou « ligues » sont des formes d'organisation regroupant un ensemble d'entités politiques, cités ou ethnè. Ils sont appelés koinon dans l'Antiquité, terme qui désigne « ce qui est commun » (aux membres du groupement). Cette forme d'organisation s'affirme surtout à la fin de l'époque classique et durant l'époque hellénistique, avec l'émergence d'entités puissantes telles que la Béotie (autour de Thèbes), l'Étolie, l'Épire et l'Achaïe. Ils se distinguent des autres formes de groupement de cités et ethnè (alliances militaires, groupements religieux) par le fait qu'il existe une citoyenneté propre au koinon, qui se surimpose à celle des cités et ethnè qui le composent, suivant un principe d'organisation pyramidal. Ses membres font généralement partie d'un même ensemble culturel, un peuple (ethnos dans le sens non-politique du terme) parlant un même dialecte, unifié autour de cultes communs. Ils mettent en commun leurs moyens militaires et diplomatiques, peuvent frapper une monnaie. À leur tête se trouve soit un exécutif exercé par un roi (en Épire) ou des magistrats militaires, par exemple un collège de stratèges en Acarnanie, et un conseil représentant les communautés fédérées. Une assemblée regroupe périodiquement tous ses membres (une fois par an en Étolie, au printemps). Les institutions fédérales disposent d'un pouvoir plus ou moins étendu selon le koinon. Les fédérations sont vaincues par Rome, elles peuvent ensuite survivre mais elles sont dépouillées de leurs pouvoirs politiques et militaires[116],[117].
Les textes d'Homère et d'Hésiode évoquent des « rois » ou « chefs » (basileus ; (w)anax, le terme employé pour désigner le roi dans la civilisation mycénienne, se retrouve encore dans des textes archaïques), ce qui est souvent pris comme un indice du fait que la royauté est répandue au début de l'époque archaïque, mais sous une forme tempérée puisque ces personnages n'ont pas un pouvoir très fort ou stable. Cette institution semble s'effacer avec l'affirmation de la cité. On peut certes retrouver des « rois » dans des cités, mais il s'agit en général de simples magistrats annuels, parfois de personnages se succédant suivant un principe héréditaire comme dans le cas de la double royauté spartiate, dont les attributions sont surtout militaires. La royauté héréditaire avec un aspect proprement monarchique se retrouve dans les ethnè du Nord, notamment l'Épire et la Macédoine, où le roi est avant tout chef de guerre et chef religieux, au pouvoir limité par l'aristocratie et le peuple : c'est une royauté nationale dans laquelle le pouvoir royal est exercé en accord avec le peuple, suivant un principe contractuel. Philippe II et Alexandre renforcent le pouvoir du roi en Macédoine, mais ils font face à des résistances répétées de l'élite du royaume[118],[119].
La conquête d'Alexandre puis la constitution des royaumes hellénistiques accouchent en revanche de l'apparition de royautés personnelles, des monarchies fortes, dans lesquelles le pouvoir royal est de type patrimonial (l’État est vu comme la propriété personnelle du roi), peu restreint, tend vers une forme d'absolutisme, sans toutefois l'atteindre. C'est une conséquence du caractère conquérant de la figure royale qui s'instaure à la suite des conquêtes, justifiant la domination de ses sujets, qui sont un ensemble hétérogène. Il doit aussi faire preuve de qualités dans l'exercice de la justice, et de générosité envers ses sujets par ses bienfaits (l'évergétisme royal). Des cultes dynastiques sont mis en place chez les Séleucides et les Lagides. Le roi dirige avec son entourage proche, ses « Amis » (philoi), sur les fidélités acquises auprès des élites gréco-macédoniennes (et dans une moindre mesure autochtone), ce qui rend son pouvoir personnel fragile, d'autant plus qu'aucune règle successorale n'a été précisée, au-delà d'un principe héréditaire, ce qui explique les nombreux coups d’État et conflits pour le pouvoir. La royauté macédonienne garde en revanche un caractère plus « national » (moins de distance entre élite et dominés, pas de culte dynastique)[120],[121].
L'antiroyalisme romain fait qu'on ne parle plus de « rois » mais d'« empereurs » (autokrator en grec ; Sebastos traduit « Auguste ») pour désigner le monarque durant le Haut Empire. Le terme grec basileus « roi » fait son retour dans la littérature non officielle en grec à partir du règne de Constantin Ier au début du IVe siècle, et s'impose progressivement comme la forme la plus courante de désigner un empereur, puis la façon officielle au moins à partir d'Héraclius vers 629[122].
La population de la Grèce antique ne peut être reconstituée de façon assurée, car aucun document de dénombrement de population complet n'a été préservé, et que les recensements étaient effectués (pour des raisons fiscales ou militaires) uniquement au niveau local et pas à un niveau central, même à l'échelle d'une cité. Les chiffres donnés par les auteurs antiques sont la plupart du temps vagues et exagérés. Les estimations modernes peuvent s'appuyer sur l'identification du nombre de sites habités et de leur surface, mais cela reste imprécis[123],[124].
Il est généralement estimé que la Grèce connaît une croissance démographique à partir du IXe siècle av. J.-C., dans les régions qui voient l'émergence des cités, et que la colonisation qui se produit à compter du VIIIe siècle av. J.-C. est en bonne partie liée à cette augmentation de la population. Le pic de population est atteint en Grèce sur les Ve – IIIe siècle av. J.-C.[125]. L'amélioration des techniques agricoles et l'extension de l'exploitation des campagnes dans les régions du centre-est et du sud y entraînent d'importantes concentrations de population, là où se constituent les plus importantes agglomérations urbaines : Attique, Béotie, région de Corinthe, Messénie, Laconie, Eubée notamment. On y trouve autour de 2 millions d'habitants au Ve siècle av. J.-C., des densités de 25 à 45 habitants au km² voire plus de 100 à Athènes et Corinthe. En revanche les parties centrales et septentrionales de la Grèce, montagneuses et pastorales, sont moins densément peuplées. Les campagnes de Thessalie, d'Épire et de Macédoine connaissent cependant une importante croissance au IVe siècle av. J.-C., sur laquelle s'appuie leur essor politique et militaire ; elles ont peut-être une population de 1,6 millions d'habitants durant l'époque hellénistique[126]. En dehors de Grèce, les cités de Sicile et du reste de l'Italie semblent avoir été particulièrement prospères et peuplées durant l'époque classique, la population de Syracuse devant excéder celle des autres cités grecques[127].
En gros les populations de la Grèce classique et hellénistique sont bien nourries et plutôt en bonne santé par rapport aux standards antiques, en tout cas elles vivent plus longtemps que celles de la Grèce de l'âge du Bronze, avec des âges moyens des décès autour de 45 ans pour les hommes et 36 ans pour les femmes. En revanche l'espérance de vie à la naissance est plus faible en raison d'une mortalité infantile élevée, à situer autour de 27 ans. On suppose donc que les taux de fertilité devaient être élevés, avec pour principal déterminant l'âge du (premier) mariage, à situer pour les femmes autour de 17 ans, mais il serait plus tardif pour les hommes (autour de 30 ans ?). La population croît vite en conditions normales, la limitation des naissances pourrait être pratiquée par contraception, avortement ou infanticide, et l'émigration permet de pallier la surpopulation[128],[125].
La tendance se retourne en Grèce centrale et méridionale à la fin de l'époque hellénistique et au début de la domination romaine, autour du IIe siècle av. J.-C., qui connaissent apparemment une baisse démographique liée à une baisse de la natalité, du moins si on se fie à certains auteurs, dont Polybe. Mais il pourrait s'agir d'un topos littéraire, aussi l'ampleur et la nature de ce phénomène sont discutés[129],[130],[131],[132]. La tendance se retourne durant l'époque impériale romaine, avec une croissance qui connaît son pic durant l'Antiquité tardive, au VIe siècle[133], les prospections indiquant que la Béotie et l'Attique retrouveraient leurs niveaux de peuplement de l'époque classique[134]. La série de crises qui frappe l'Empire romain d'Orient à compter du VIe siècle entraîne en revanche en Grèce le dépeuplement de nombreux sites et des mouvements importants de population[135].
Les anciens Grecs et Grecques ont dans leur environnement six zones écologiques : les plaines cultivables, les collines en roches tendres propres à la culture, les collines en roches dures impropres à la culture, les montagnes élevées, les marais, les côtes et la mer. Le paysage de la Grèce classique est sans doute plus boisé que celui de la Grèce moderne, mais le pays a tout de même connu une déforestation importante depuis les débuts de l'agriculture au Néolithique. Les forêts denses sont surtout préservées dans les zones hautes (pins, cèdres, etc.), la végétation des autres espaces étant de type garrigue et maquis, aussi savane. Parmi les aménagements de l'espace par les hommes, les terrasses de cultures existent dès cette époque, mais les espaces marécageux sont généralement préservés, et appréciés comme lieux de pâture. Les territoires des cités étant généralement peu étendus, la plupart d'entre elles ne disposent pas d'une grande variété de zones écologiques et de ressources ; et même la très vaste Athènes doit compter sur l'extérieur pour obtenir assez de bois de construction et de céréales pour combler ses besoins. L'habitat humain est présent un peu partout sous différentes formes, qu'il s'agisse de villes, de bourgs, de villages, de hameaux, de fermes ou de bâtisses rurales occupées de façon saisonnière[136].
La Grèce comprend de longs littoraux aux côtés souvent découpées, dont de nombreuses îles, ce qui explique que la mer joue un rôle important dans son identité et son imaginaire dès l'Antiquité. De nombreuses cités ont un accès direct à la mer, les Grecs exploitent ses produits (poissons, éponges, sel, algues), ils comptent parmi eux des navigateurs capables et aventureux comme le reflètent leurs implantations coloniales et explorations lointaines. Ils recourent souvent aux bateaux pour le déplacement de personnes ou le transport de marchandises, malgré les dangers que la navigation en mer peut représenter, alors que le relief accidenté gêne en bien des endroits les communications terrestres[137].
Le site de Philippes, Macédoine, vu depuis l'acropole.
Le site de Cassopè, dans les régions hautes d’Épire en Grèce du nord-ouest.
Le temple d'Apollon de Delphes, Phocide.
Le cap Sounion (Attique), avec son temple de Poséidon, le golfe Saronique et l'île de Patroklos.
Ruines de l'agora d'Argos, Péloponnèse.
Ruines du théâtre de Sparte, Péloponnèse.
Ruines de l'agora du site urbain de Cos.
Ruines de Gortyne, Crète.
Le théâtre romain de Milos.
L'occupation du territoire de la Grèce des cités est marquée par l'opposition entre un lieu d'habitat central ou chef-lieu, désigné par convention comme une ville, asty, et le territoire civique, chôra, qui s'étend jusqu'aux confins du territoire civique, eschatiai[138]. Cet espace est à dominante rurale, occupé par un ensemble de petits sites que l'archéologie est en mesure de repérer, qui connaissent une expansion entre le VIe siècle av. J.-C. et le IIIe siècle av. J.-C. avant de rentrer dans une phase de rétraction, mais se pose souvent la question de savoir s'ils sont des lieux d'habitat permanents ou temporaires, ce qui rend difficile l'estimation de la répartition de la population sur un territoire civique à un moment donné. Le poids des centres urbains paraît dans bien des cas très important. Approximativement 70 % de la population de la Béotie du IVe siècle av. J.-C. vivait dans des villes ou villages[139]. Mais la présence d'une ville n'est pas systématique, même dans une cité : Sparte est ainsi constituée de villages non murés[140].
L'habitat rural peut être concentré ou dispersé. Des fermes isolées ont été repérées lors de prospections et parfois fouillées ; elles peuvent être fortifiées, comme à Thasos. Les villages (kômai, kômopolis), situés dans la dépendance d'un centre urbain, sont plus difficile à définir et à identifier, les archéologues s'en remettant généralement à des questions de taille. Les formes de peuplement des territoires civiques sont variées : les villages sont nombreux dans certaines régions comme l'Attique (les centres d'un dème), l'Arcadie, la Béotie ou la Thessalie, mais il s'en repère moins en Phocide ou sur l'île de Kéos, d'autres encore sont plutôt dans une situation intermédiaire. La définition et l'identification des villes pose également des questions de critères : en général on s'en remet à la taille et à la présence de fortifications, mais cela ne saurait être systématique car une agglomération importante peut ne pas être murée, tandis qu'une de petite taille peut l'être[141],[142].
Une ville est quoi qu'il en soit bien plus qu'une concentration de personnes. Elle est aussi définie par une organisation topographique et un paysage spécifiques, un certain degré de sophistication culturelle et un sentiment d'appartenance à une communauté. Ce n'est pas forcément le résultat de l'accroissement d'un village par l'agglomération naturelle de personnes, car il y a de nombreux cas de formation volontariste de villes à partir du déplacement et du regroupement de la population de plusieurs villages d'un même territoire (synœcisme), pour des raisons politiques voire économiques, avec le développement d'une ville organisée au moins en partie de façon planifiée. Les villes grecques apparaissent au sortir des âges obscurs et durant l'âge classique, l'urbanisation étant liée à l'émergence du cadre politique de la cité, qui lui est souvent lié, et à l'indépendance politique. Le modèle de la ville grecque se construit également par la fondation de nouvelles cités lors de la colonisation, où la planification est parfois visible dès les premiers temps (Mégara Hyblaea). Par la suite l'aspect normé de l'urbanisme se voit dans la diffusion du plan hippodamien, en damier, là où le relief le permet. Certains sites sont dominés par une acropole. La ville donne un cadre autour duquel s'organise une communauté civique, et par lequel elle peut affirmer son identité, avec son agencement spatial et ses édifices publics, notamment autour de l'agora qui est le cœur de la vie civique. De ce fait même quand des sites secondaires atteignent une grande taille, comme les dèmes athéniens, ils ne sont pas dotés des équipements caractéristiques du centre urbain de la cité. Ce modèle urbain se diffuse à partir de la dernière partie de l'époque classique dans les pays du nord de la Grèce, la Macédoine, la Thessalie et l’Épire, puis à nouveau hors de Grèce avec l'expansion de l'époque hellénistique (Alexandrie et les autres fondations d'Alexandre le Grand, celles des Séleucides en Asie, Pergame)[143].
L'unité fondamentale de la société grecque antique est l’oikos, la « maisonnée » ou « foyer ». Elle est formée autour des personnes résident sous un même toit, autour d'une famille nucléaire constituée d'un couple marié et de leurs enfants. Elle peut prendre la forme d'une famille élargie, notamment si on y trouve des parents du couple, ou bien des femmes non mariées apparentées (sœurs, tantes, cousines, etc.). Le mari du couple central joue le rôle de chef de famille (kyrios), qui dispose des biens de la maisonnée et se charge de ses relations avec les autres unités sociales (autre oikos, tribu, institutions civiques). La maisonnée peut aussi comprendre des dépendants non apparentés, libres ou non-libres. Les esclaves se trouvent surtout dans les foyers les plus riches, qui peuvent en comprendre des dizaines. L’oikos ne se limite pas à des personnes, puisque cette notion recouvre aussi les propriétés et tous les moyens d'exploitation économiques (le « patrimoine »), ce qui en fait l'unité de base de l'économie grecque, et en son sein se déroulent différents cultes domestiques et des rites de passage liés à la naissance, au mariage, à la mort[144].
La famille grecque est généralement monogame. Elle est formée par un mariage entre deux personnes de condition libre, les esclaves ne pouvant fonder une famille, le maître pouvant séparer des parents et enfants non libres s'il le souhaite. Le mariage est en général une affaire privée, négociée entre l'époux et le père de la future épouse, qui sont la plupart du temps issus d'un même milieu social. C'est donc un mariage de convenance qui n'implique pas de sentiments dans la majorité des cas. Il donne lieu au versement d'une dot. La cérémonie de mariage (gamos) s'accompagne d'un sacrifice et d'un banquet, le plus souvent dans la maison de la famille de la mariée, laquelle ensuite va vivre chez son mari (patrilocalité). La possibilité de divorce, et surtout la fréquence des décès, notamment ceux des femmes en couches, font que les veuvages et remariages sont fréquents. Le père de famille a l'autorité au sein du foyer, et les individus ont un patronyme les rattachant à leur lignée paternelle[145],[146].
Les enfants sont assez peu mentionnés dans les sources antiques. Leur statut doit varier suivant les lieux, les conditions d'accès à la citoyenneté étant variables ; ainsi à Athènes seuls les enfants d'un homme et d'une femme athéniens peuvent devenir citoyens. Ils sont éduqués au sein de la famille durant la petite enfance, l'enseignement scolaire se développant à partir de l'Athènes classique, et surtout durant l'époque hellénistique avec l'institution du gymnase. Il existe aussi des formes d'éducation (et de rite de passage) dans un cadre civique pour les jeunes hommes atteignant l'âge adulte, comme l'éphébie à Athènes et l'agogé à Sparte. Les fils héritent d'une part du patrimoine de la maisonnée lors du décès de leur père, les filles aussi dans certaines cités, sinon leur dot correspond à leur portion du patrimoine. Comme elles sont destinées à être mariées dans une autre famille, elles ne sont pas chargées de la continuité de la maisonnée paternelle, ce qui explique qu'elles n’interviennent pas dans les héritages, ou par défaut quand il n'y a pas d'héritier masculin[147].
Les résidences privées de ces foyers suivent en gros les mêmes principes dans le monde classique et hellénistique, après une phase formative durant l'époque archaïque. La maison grecque est souvent de forme quadrangulaire, notamment dans les villes où la planification de l'habitat a prévu des îlots constructibles standardisés. Ses murs sont généralement composés de briques crues reposant sur une base en pierre. Elles devaient souvent avoir un étage. La maison est généralement organisée autour d'une cour principale, qui est le lieu autour duquel se déroule la vie domestique. Les pièces de séjour et de réception en sont séparées par des pièces à portique, appelées pastas ou prostas selon leur forme. Les pièces réservées aux hommes, les salles de banquet appelés andron, se repèrent lors des fouilles, en revanche les gynécées destinés aux femmes sont plus difficiles à identifier, et se trouvaient peut-être à l'étage. En revanche aucune pièce destinée aux esclaves ne se repère. Il y a des cuisines, mais les foyers pouvaient être mobiles et disposés dans la cour à ciel ouvert. Des pièces d'eau se repèrent aussi, des réservoirs servant à stocker l'eau de pluie, mais il fallait généralement aller en chercher dans les puits publics. Le rôle économique du foyer se repère par la présence d'espaces artisanaux, de boutiques donnant sur la rue, ou d'une plus grande cour pour les animaux dans les fermes. Les maisons riches, qui se développent surtout à l'époque hellénistique, suivent ce modèle de base, mais sont plus grandes, parfois avec deux cours, une décoration plus fournie ; le mobilier, souvent onéreux, devait aussi servir d'élément distinctif. Mais il semble que dans le cadre civique on ait souvent cherché à atténuer les possibilités d'affirmer sa richesse par une grande maison[148],[149].
Grande demeure d'Abdère d'époque hellénistique et romaine, avec sa cour centrale pavée.
Plan schématique d'une maison de Délos, IIe siècle av. J.-C.
Au-delà de la maisonnée, les relations des individus sont aussi déterminées par des relations de parenté plus larges, notamment à Athènes les « tribus » (phylai), aussi leurs subdivisions les phratries, qui en principe regroupe les descendants d'un même ancêtre. Elles ont souvent une assise territoriale, mais peuvent perdurer par le maintien de liens entre descendants masculins d'une même lignée, et jouent un rôle dans la vie de la cité puisque les rites d'admissions des citoyens lors du passage à l'âge adulte leur sont souvent confiés, les troupes militaires peuvent aussi être organisées en fonction de ce type de relation. Les déplacements de personnes rendent ces groupes de parenté moins pertinents au fil du temps[150].
Les Grecs pratiquent aussi des formes de solidarité qui lient des gens non apparentés, les relations d'amitié, qui impliquent qu'on se rende divers types de services, même si les rapports sont dans ce cas-là définis de façon moins rigide qu'entre parents[151]. La relation d'« hospitalité » (xenia), pratiquée surtout au sein des élites afin d'accueillir un étranger dans une cité, est en revanche plus ritualisée et implique plus d'obligations réciproques[152]. Il existe aussi des formes d'associations, avant tout à but religieux, ou des sortes de guildes de marchands, artisans et artistes, qui créent des solidarités sans forcément qu'il y ait des relations de parenté entre les membres[153].
Les statuts sociaux et légaux sont nombreux dans la Grèce antique, ne serait-ce qu'en raison de la coexistence de multiplicités de cités-États ayant chacune leur manière de classer les gens. Il y a néanmoins des grandes lignes de fracture récurrentes : entre libres et non-libres, entre citoyens et non-citoyens, entre hommes et femmes, entre jeunes et vieux[154].
La citoyenneté (politeia) est le statut légal de base dans tout le monde grec. Il concerne les hommes adultes libres. Les conditions précises d'accès à la citoyenneté, et de déchéance de celle-ci, ou encore la distinction entre plusieurs classes de citoyens varient selon les cités. Le droit de naissance est le plus important : on accède à la citoyenneté parce qu'on est fils de citoyen. À Sparte ou dans des cités crétoises, l'éducation publique est un passage obligé pour devenir citoyen. La citoyenneté spartiate peut se perdre si on ne contribue pas aux banquets collectifs, et celle de Thèbes si on exerce un métier artisanal ou commercial. La citoyenneté donne accès au droit à la propriété d'une portion du sol de la cité, et à des revenus exceptionnels (les distributions de grains notamment), entraîne des obligations de service militaire, et elle est évidemment indispensable si on souhaite exercer une magistrature, ou participer aux délibérations publiques. Les femmes ne sont pas politiquement citoyennes, mais elles sont quand même intégrées dans la vie de la cité (lors de fêtes notamment) et sont indispensables pour transmettre la citoyenneté à leurs enfants[155],[154]. Les rapports spécifiques entre les classes d'âge de citoyens sont aussi un élément structurant fort de la vie de la cité : un jeune homme doit recevoir une éducation publique à Athènes (éphébie) comme à Sparte (agogé) et accomplir des rites de passage pour devenir citoyen, mais il doit attendre plusieurs années avant de pouvoir exercer une magistrature (jusqu'à 50 ans à Chalcis). Passé la soixantaine, le citoyen n'est plus astreint au service militaire, et dans les cités aristocratiques il peut intégrer un conseil des Anciens[156].
Les élites sociales des cités grecques archaïques et classiques sont couramment désignées par les historiens comme une « aristocratie » (au sens de groupe social ; le terme désigne uniquement un type de régime politique dans les textes antiques), mot qui dérive du grec aristoi, les « meilleurs ». Ce groupe aux contours flous se définit entre autres : par ses origines illustres, réelles ou fantasmées (il y a des familles aristocratiques attestées sur plusieurs générations), par ses mérites et ses qualités morales (la « vertu », arété) et intellectuelles, plus largement leur essence supérieure (ils sont « beaux et bons », kalokagathoi). L'estime sociale accordée par la communauté semble primer sur les autres facteurs quand il s'agit des statuts sociaux, ce qui explique que la recherche de prestige soit primordiale pour ceux qui souhaitent occuper les positions les plus éminentes. La richesse n'est pas un élément suffisant, même si en pratique elle arrange beaucoup de choses, car elle permet de se constituer une clientèle et de prendre part à la culture élitiste, donc de consolider ses relations sociales. Le rang des élites se voit lors des banquets, des fêtes religieuses qu'ils patronnent, etc. qui sont des moments de compétition entre élites pour manifester leur prestige aux yeux de tous. De ce fait ces aristocrates sont souvent vus comme les meneurs naturels des cités, même si tous les régimes politiques ne leur ont pas accordé la même place, et que d'une manière générale la position de cette élite semble moins forte dans la Grèce archaïque et classique que dans les autres civilisations antiques[157],[158],[159].
Les différences économiques jouent donc un rôle important dans la vie des cités, puisqu'il s'y retrouve souvent une opposition entre riches et pauvres, et même s'il est difficile de les assimiler à des luttes de classes[160]. Une attitude de méfiance envers l'accumulation des richesses par certains citoyens est visible dans bien des cas, notamment en raison des dérives qu'elle peut entraîner (orgueil, extravagance, mollesse). Des mesures ont pu être mises en place pour limiter de tels phénomènes : séparation des patrimoines de façon égalitaire lors des héritages, lois somptuaires visant à limiter les excès, notamment lors des fêtes et des funérailles. Il est surtout attendu des plus riches qu'ils contribuent de leur poche au financement de rituels, de fêtes, de spectacles, de constructions, de distributions alimentaires, d'écoles, etc. dans le cadre de la liturgie puis celui de l'« évergétisme ». C'est souvent vu comme une obligation, quoiqu'il puisse être volontaire, et le bienfaiteur peut en tirer des honneurs[161]. Ce phénomène s'affirme surtout à partir de l'époque hellénistique et durant l'époque romaine, qui voit l'émergence d'un groupe de « notables » locaux caractérisés par leur grande richesse (dont l'exemple le plus extrême est le « milliardaire antique »[162] et prolifique évergète Hérode Atticus, 101-177) et l'exercice courant si ce n'est systématique des magistratures les plus importantes de leur cité. Ils prodiguent à leurs concitoyens des bienfaits plus dispendieux que jamais, qui entraînent en retour des honneurs importants. Leur prestige est donc considérable, en faisant plus que jamais les personnages les plus importants des communautés locales[163].
Les étrangers libres résidant dans une cité, qui sont Grecs ou non-Grecs et non citoyens, ont un statut de métèque, surtout attesté pour Athènes. Ils sont enregistrés comme résidents, doivent avoir un citoyen pour les patronner (un proxène), paient une taxe spéciale, sont soumis à un tribunal spécifique dirigé par des citoyens. Ils n'ont pas accès à la propriété du sol, mais peuvent participer à la plupart des fêtes. Ils peuvent se voir accorder la citoyenneté sur vote de l'Assemblée, mais cela est très rare[164],[165].
Il existe aussi une catégorie de libres ne jouissant pas de la citoyenneté, parce qu'ils en ont été exclus pour diverses raisons, sans pour autant être des étrangers. Ils semblent nombreux dans les régimes politiques aristocratiques. Ils n'ont pas accès à la vie politique ni à la propriété, mais ils bénéficient quand même d'une protection de la part de la cité[166].
L’esclavage est répandu dans les cités grecques, à des degrés divers. Les statuts des non libres (douloi) sont variés, puisqu'il concerne aussi bien l'esclavage de masse athénien, qui a pu être présenté comme une « mort sociale », que les hilotes spartiates, qui sont des sortes de serfs attachés à la terre mis en servitude de manière collective, ou encore ceux tombés en esclavage pour dette (interdit à Athènes par Solon), dont le statut est en principe temporaire. Il existe aussi des catégories intermédiaires qui troublent la limite entre libres et non-libres. Quoi qu'il en soit, même s'il ne faut pas forcément considérer que la société grecque repose sur l'esclavage, celui-ci est omniprésent dans le monde grec, et il est légitimé idéologiquement à plusieurs reprises, notamment par Aristote. En principe on ne peut réduire un Grec en esclavage, mais en pratique cela s'est produit à toutes les époques antiques. Quant au statut, un esclave appartient à un maître, qui l'a le plus souvent acheté, n'a pas droit à des possessions ni à fonder une famille ou intenter des procès. Mais selon le bon vouloir de leur maître il peut disposer d'une certaine autonomie, notamment s'il est spécialisé dans une activité artisanale, commerciale ou intellectuelle, ou bien appartient à la cité et travaille pour elle (notamment à des fonctions de police à Athènes). Ceux qui travaillent dans des grands ateliers ou des mines disposent de conditions de vie bien plus dures. Les affranchissements peuvent se faire par rachat ou par testament du maître[167],[168].
Comme dans toutes les sociétés, la condition de la femme dans la Grèce antique diffère selon les époques et même les cités[169]. Elle dépend du reste avant tout de son appartenance familiale et sociale : [170].
La position de la femme est inférieure à celle de l'homme. Hésiode formule le fondement mythologique de cette condition, la première femme, Pandore, étant créée pour exercer la vengeance divine sur les hommes, après le vol du feu par Prométhée, manifestation de leur volonté d'égaler les dieux[171]. Mais par-delà la pléthore de poncifs misogynes des auteurs antiques, la littérature présente d'autres modèles de femmes, par exemple Pénélope l'épouse d'Ulysse, qui sous la plume d'Homère devient l'archétype de la femme fidèle, travailleuse et ingénieuse[172],[173].
La condition de la femme ou fille de citoyen (de « citoyenne »[174]?) dans la Grèce antique a donc beaucoup été discutée : on l'a vue comme enfermée dans la maison, dans le gynécée, dans un statut d'éternelle mineure. D'autres ont interprété les images de femmes fortes présentes dans les comédies athéniennes comme la preuve qu'elles pouvaient échapper à la tutelle masculine. Il reste au moins certain que la femme de citoyen est exclue de la vie politique de la cité, qui attend avant tout d'elle qu'elle donne naissance à de futurs citoyens ; son intégration dans la communauté se voit dans les fêtes religieuses[175]. Il est également manifeste qu'elle est en position d'infériorité sur le plan patrimonial : elle ne succède pas à son père, hérite moins que ses frères[176].
Les conditions varient cependant selon les cités. Le modèle athénien, encore une fois le mieux connu et le plus étudié, ne doit pas être généralisé, même s'il y a des fondements communs à tout le monde grec. Par exemple, au regard de la propriété féminine : à Athènes une femme mariée possède sa dot, même si son mari en dispose, et en cas de divorce elle repart avec dans sa famille d'origine ; à Gortyne et à Sparte elle peut être pleinement propriétaire de biens[177]. La condition des femmes spartiates semble meilleure que dans la plupart des cités : elles y ont plus de liberté, pratiquent des activités physiques, participent à des concours, ce qui implique de s'exposer aux yeux de tous, et passent également pour avoir eu plus de répondant que les autres, autant de choses qui ont suscité la désapprobation des observateurs athéniens[178].
La situation diffère aussi selon les foyers et les milieux sociaux : dans les familles moins aisées les femmes doivent probablement travailler à l'extérieur, par nécessité économique ; dans les maisonnées riches l'enfermement est sans doute courant, mais même là la maîtresse de maison a un rôle d'intendance pour son époux. Il faut ajouter à ce tableau le rôle des sœurs, des filles et des servantes qui ont chacune des tâches différentes dans l’oikos[179]. Les filles des familles de la bonne société apprennent à filer et tisser, à participer aux rites familiaux et civiques, à danser et jouer de la musique, à compter et écrire : on les éduque donc pour permettre à la famille de tenir son rang et qu'elles puissent remplir leur rôle futur de maîtresse de maison et de femme de citoyen[180].
L'époque hellénistique marque une évolution notable pour les femmes dans le monde grec même si c'est surtout le cas dans les royaumes hellénistiques, comme en Égypte lagide où les femmes obtiennent des droits juridiques et financiers[181]. Certaines reines lagides ont eu une grande importance politique, telles Bérénice II et Cléopâtre VII, situation renforcée à plusieurs reprises par la pratique de mariages entre frères et sœurs au sein de la famille royale[182], et plusieurs d'entre elles bénéficièrent d'un culte, en premier lieu Arsinoé II qui fut très populaire en Égypte[183]. En Grèce, elles restent exclues de la communauté civique. Mais certaines femmes issues du milieu des élites peuvent hériter de grandes fortunes, par le hasard des décès de leurs parents masculins, et devenir de grandes évergètes obtenant en retour des honneurs exceptionnels, à l'image d'Archippè de Kymé (IIe siècle av. J.-C.)[184].
Pentadrachme en or à l'effigie de la reine Bérénice II, v. 244/3-221 av. J.-C.
La capacité d'action des femmes dans la cité se voit avant tout dans le culte. Elles sont non seulement présentes dans les rituels civiques, mais elles y jouent un rôle actif. On trouve des prêtresses dans le culte des dieux masculins, et plus largement des femmes interviennent à différents moments de rituels. La Pythie de Delphes, chargée de délivrer les oracles du dieu Apollon, est la figure féminine la plus importante des cultes du monde grec. Certaines fêtes telles les Thesmophories, les Halôa et les Adonies disposent de rituels destinés exclusivement aux femmes, certains ayant un caractère sexuel prononcé, probablement pensé sous le mode humoristique avec un aspect subversif[185],[186].
On trouve aussi des femmes dans les milieux savants, dans un rôle d'étudiante, d'enseignante ou d'écrivaine. Les Pythagoriciens passent ainsi pour avoir admis les femmes dès le début, et plusieurs d'entre elles auraient écrit des œuvres, aujourd'hui perdues. Platon et Épicure ont également eu des femmes parmi leurs élèves. La femme philosophe la plus connue de nos jours appartient à l'Antiquité tardive : Hypatie d'Alexandrie, directrice de l'école néo-platonicienne de cette cité, morte en 415 lynchée par une foule de Chrétiens[187].
L'économie de la Grèce antique[188] repose sur un ensemble de maisonnées privées, oikos, constituées autour d'une famille nucléaire et parfois des dépendants libres ou non libres, qui tend à l'auto-suffisance autant que possible, suivant l'idéal d'autarcie. Il s'agit en général de familles de paysans, disposant d'un lopin de terre sur lequel est pratiquées une agriculture de subsistance, reposant avant tout sur la culture des céréales, qui peut-être complétée par la culture de l'olivier, de la vigne, de figuiers et d'autres arbres fruitiers, et l'élevage de petit bétail (moutons, chèvres). La pêche et un artisanat domestique basique (textile, céramique, alimentaire) complètent ce panel d'activités. Pour ce que la maisonnée ne peut produire, il convient de recourir à des échanges, sur des marchés locaux, par troc ou échange monétaire, donc en échange de ses surplus ou profits, le tout sous le contrôle de la cité. Il est également crucial d'avoir un accès à la main d’œuvre, ressource essentielle, marquée dans le monde antique par la présence courante d'esclaves. La finalité de l'économie domestique est alors de dégager un surplus, pour les besoins patrimoniaux, sans but plus précis, ce qui implique aussi une importante thésaurisation, qui limite donc la circulation des richesses. Cela constitue un modèle « normal » pour les communautés grecques, mais il existe d'autres économies, ayant des profils plus archaïques, comme Sparte, alors qu'à l'opposé du spectre économique Athènes est aux époques classiques et hellénistique une économie marquée par l'importance des échanges à longue distance, de la circulation monétaire, des formes d'« entreprises » servant notamment pour l'exploitation minière[189],[190]. Certains estiment qu'une croissance économique peut se déceler durant l'Antiquité, notamment aux époques classique et hellénistique et sous le Haut Empire romain, voyant une amélioration sur le long terme du niveau de vie des Grecs[191]. Mais cela et plus largement le caractère « moderne » de cette économie est débattu. Des historiens estiment que l'économie grecque antique peut s'étudier avec les mêmes instruments d'analyse que pour l'économie moderne et qu'on y trouve de nombreux traits relevant de l'économie de marché, voire des aspects « capitalistes ». D'autres sont plus sceptiques sur ce point et privilégient une approche plus contextualisée, reposant sur des concepts qu'ils estiment plus adaptés au monde antique[192],[193].
L'agriculture est l'activité la plus pratiquée comme dans toutes les économies pré-modernes. Le relief accidenté, les sols généralement peu fertiles et le climat chaud et sec dans la plupart de la Grèce ne constituent pas des conditions particulièrement favorables au développement agricole, mais le pays comprend aussi des plaines fertiles, dont certaines sont étendues (Thessalie et Béotie), et plusieurs régions où se sont implantées des cités grecques lors de la phase de colonisation sont devenues des greniers à blés en mesure d'approvisionner le pays d'origine (Sicile, Cyrénaïque, Thrace, mer Noire). On privilégie donc les plantes plus adaptées au climat sec et aux sols minces, à savoir l'orge, l'olivier et la vigne. Les petites exploitations familiales disposent d'une petite portion de terre, exploitée en jachère, travaillée à l'araire, les rendements céréaliers sont généralement faibles. Dans la plupart des cas la petite paysannerie pratiquant la polyculture est vulnérable aux aléas climatiques ou autres, subit souvent un lourd endettement. Il existe quelques grands domaines appartenant à des familles puissantes, et quelques-uns de ces riches propriétaires peuvent se consacrer à des cultures spéculatives, en se spécialisant dans la production d'huile et de vin. Dans le cas spartiate, la terre est la propriété de la communauté civique, et elle est exploitée par des dépendants, les hilotes. De même à Thèbes les citoyens ne doivent pas travailler eux-mêmes la terre car cela est vu comme dégradant, et ils la font exploiter par leurs esclaves[194]. Dans le nord de la Grèce, au climat plus continental et moins aride, les conditions sont moins propices au développement des cultures de l'olivier et de la vigne, mais en revanche la présence de nombreux pâturages permet une pratique d'un élevage à grande échelle, souvent transhumant, y compris celui des bovins et des chevaux. L'élevage ne peut cependant être la seule ressource des communautés de ces régions, qui pratiquent une agriculture vivrière, et aussi la chasse et la cueillette ; selon Héraclide, en Athamanie ce sont les femmes qui pratiquent ces activités, alors que les hommes se consacrent au pastoralisme transhumant[195]. La pêche est également une activité importante pour l'alimentation des communautés grecques, surtout celles vivant au bord de la mer, et donne lieu à un commerce lucratif[196].
Les artisans spécialisés travaillent pour la plupart dans des petits ateliers, employant au mieux 5 à 6 personnes, qui réalisent des productions qui ne peuvent être faites dans le cadre domestique : métallurgie, orfèvrerie, céramique peinte, mobilier de qualité, etc. Certains ateliers sont plus vastes, disposant de dizaines d'artisans. Les plus grandes concentrations d'artisans sont peu nombreuses. Elles sont surtout documentées pour le cas des mines du Laurion situées sur le territoire athénien, qui emploient des milliers d'esclaves, dans le cadre de petites concessions minières disposant chacune de quelques dizaines d'esclaves. Les chantiers navals et les grands chantiers de constructions concentrent aussi beaucoup d'artisans[197]. L'esclavage est courant dans le domaine artisanal, beaucoup de personnes de condition servile ayant une spécialisation dans un métier. Mais il y a aussi de nombreux artisans libres, des personnes des deux conditions pouvant travailler ensemble dans un même atelier, sans forcément qu'il n'y ait d'écarts dans leur rémunération, généralement payée à la pièce[198].
Les échanges sont réalisés avant tout au niveau local, dans des marchés où se pratique un commerce de détail, généralement par le producteur (paysan ou artisan) lui-même, qui en profite pour se procurer en retour ce dont il a besoin. Il existe aussi des intermédiaires achetant et revendant la production d'autres. Des foires plus importantes ont lieu lors des fêtes religieuses[199]. Le commerce à longue distance sert à satisfaire une demande plus spécifique, en particulier dans le domaine artisanal, ou bien une demande que la production locale ne peut satisfaire que partiellement, en particulier dans le domaine alimentaire. Il peut être effectué par terre, mais en raison de la géographie de la Grèce la voie maritime occupe une place prépondérante, d'autant plus qu'elle permet de transporter plus aisément de grandes cargaisons ; Athènes s'approvisionne ainsi à l'époque classique en blé de l'Eubée puis de la mer Noire, de Sicile et d'Égypte[200]. Le commerce maritime se développe surtout durant l'époque de la colonisation grecque, puis au début de l'époque classique avec l'essor de la puissance navale athénienne. Des itinéraires habituels sont progressivement mis en place, permettant l'apparition de routes maritimes. Mais les conditions de navigation sont souvent difficiles, et la piraterie endémique durant certaines époques. La présence d'une puissance navale, comme Athènes, Rhodes ou l’Égypte lagide, permet de juguler le problème. Les navires commerciaux sont à voile, de forme arrondie, peuvent atteindre une capacité de 150 tonnes. Les voyages commerciaux sont financés par un armateur (emporos) et dirigés par un propriétaire de navire (nauclère), qui sont associés pour l'expédition et se partagent les profits à son issue[201].
L'amphore est un contenant d'une grande importance dans le commerce maritime. Comme son nom, amphiphoreus « que l'on peut porter des deux côtés », l'indique il s'agit d'un vase à deux anses, mais elle se décline suivant une grande variété de formes (identifiées par leur lieu de production) et de tailles (la capacité la plus courante aux époques classique et hellénistique est de 20-25 litres), peut être peinte (auquel cas elle peut être échangée pour elle-même) ou non (auquel cas sa valeur est liée à son contenu). De nombreuses amphores sont estampillées, ce qui permet de connaître leur lieu d'origine et leur contenu, aussi leur date, ce qui est appréciable pour les études des circuits d'échanges. Ce contenant prend son essor au VIIIe siècle av. J.-C., puis est produit en masse durant le reste de l'Antiquité. Il sert surtout à transporter des liquides (huile et vin), des saumures et salaisons (notamment le garum)[202],[203].
Amphore à vin de Thasos, Ve siècle av. J.-C. British Museum.
Amphore à huile de Corinthe, IVe siècle av. J.-C. Château Saint-Pierre, Bodrum.
Amphore rhodienne, fin IIIe - début IIe siècle av. J.-C. MET
Fragment d'anse d'amphore rhodienne timbrée, v. 180 av. J.-C. MET
Amphore à vin de Chios, époque romaine, Narbonne. Narbo Via.
L'apparition de la monnaie frappée en Asie Mineure à l'époque archaïque, dans la seconde moitié du VIIe siècle av. J.-C., constitue une innovation majeure dans la vie économique grecque, qui se diffuse rapidement. En général les monnaies sont frappées en bronze et en argent, émises lors de circonstances particulières, et circulent sur un territoire limité. Mais les monnaies de certaines cités, en premier lieu Athènes, voient leur valeur être reconnue sur un vaste espace, et se diffusent beaucoup[204]. Un système bancaire se développe autour de l'activité de change de monnaie. Il prend surtout son essor à partir de l'époque hellénistique. On sait alors pratiquer des paiements à distance sans déplacement de pièces de monnaie, grâce à des banques disposant de bureaux dans plusieurs endroits, mais cela reste rare. Le prêt à la grosse aventure permet de financer des expéditions commerciales[205].
Statère d'électrum, Ionie, vers 600-580 av. J.-C. Cabinet des médailles de la Bibliothèque nationale de France.
Tétradrachme athénien, avec la tête de la déesse Athéna au droit et la chouette la symbolisant au revers. Après 449 av. J.-C.
Monnaie frappée à Olbia du Pont vers la fin du Ve siècle av. J.-C.
Tétradrachme à l'effigie de Zeus frappé sous Philippe II de Macédoine, v. 323-315 av. J.-C.
Tétradrachme d'argent de Rhodes à l'effigie du dieu Hélios, v. 316-305 av. J.-C.
Tétradrachme à l'effigie du roi séleucide Antiochos IV Épiphane, v. 175-164 av. J.-C.
Hémiobole à l'effigie de Cléopâtre VII, frappée à Patras v. 32/1 av. J.-C.
Le régime alimentaire des Grecs antiques se fonde sur la « triade méditerranéenne » : blé, huile d'olive et vin. Les céréales (blé dur, épeautre et orge) constituent la base de l'alimentation grecque, notamment le plat le plus courant la maza, bouillie faite avec de la farine d'orge précuite à laquelle on a ajouté du liquide (eau, huile, lait, miel). Du pain est aussi fabriqué, sous forme de galettes, et les textes mentionnent des sortes de gâteaux. Le terme opson désigne ce qui accompagne les céréales dans le repas. Les légumineuses (pois chiches, lentilles, fèves, vesces) sont un autre élément important du régime alimentaire, dans des potages, avec des condiments et légumes (ail, oignon, poireaux, navets, choux, concombres, etc.). Les fruits sont également consommés couramment (figues, raisins, pommes, poires). Le poisson, frais, séché ou en saumure, est un mets répandu. La viande n'est consommée qu'à l'occasion des fêtes ou des sacrifices pour le commun des Grecs, issue d'animaux d'élevage et de gibier. L'huile d'olive est la matière grasse la plus employée pour la cuisson. Le lait sert à faire du fromage, il n'est pas beaucoup bu ni transformé en beurre, pratiques qui sont vues comme typiques des barbares, de même que la consommation de bière. Les Grecs boivent plutôt du vin[206],[207].
L'approvisionnement alimentaire est une préoccupation constante pour les cités grecques, qui produisent rarement plus que ce qu'elles consomment, et sont peu à être autarciques, malgré des efforts constants pour augmenter la production agricole, surtout celle des céréales. De ce fait tout ce qui peut perturber la céréaliculture est potentiellement source de calamité, en premier lieu un mauvais climat et la guerre, qui peuvent entraîner régulièrement des disettes, voire des famines. Les plus pauvres sont souvent victimes de malnutrition. Les pénuries de céréales génèrent des tensions sociales, et les institutions civiques ont rarement des greniers collectifs pouvant les combler leurs actions étant généralement plutôt des contrôles des prix sur les marchés, des incitations au développement du commerce de céréales et à leur distribution, puis à partir du IVe siècle av. J.-C. elles pratiquent de plus en plus l'achat de céréales sur les deniers publics en cas de pénurie et hausse des prix, sans parvenir à complètement combler les manques. Les plus riches citoyens, aussi les rois puis empereurs, fournissent des grains à la cité à titre de bienfaits (évergétisme) et en retirent du prestige[208],[209].
La cuisine est faite pour l'essentiel par les femmes dans le cadre domestique. Pour les sacrifices et banquets collectifs il y a des cuisiniers publics, des boulangers vendent aussi des produits. La qualité des repas est un élément de distinction sociale, par le biais de produits variés et de qualité, notamment ceux ayant une origine renommée, en particulier les condiments, sauces, vins et huiles, dont la combinaison faisait la qualité du repas. Une forme de haute cuisine se développe au sein des élites durant l'Antiquité[210].
La préparation des plats sacrificiels est ritualisée, la méthode de préparation et l'ordre étant codifiés[210]. Après l'immolation de l'animal, les viandes sont distribuées à ceux qui y ont droit, ce qui va jusqu'à impliquer toute la cité lors des grandes fêtes poliades. Des salles de banquets sont souvent aménagées dans des sanctuaires pour accueillir les convives. Ces banquets civiques sont un moment fort de la vie de la communauté, participant à l'identité collective des citoyens. Les plus riches participent de plus en plus à leur financement avec le temps, renforçant ainsi leur poids dans la cité[211]. D'autres formes de banquets sont organisées dans un cadre civique, entre gens d'une même classe d'âge et/ou d'un même groupe social (syssities à Sparte, andries en Crète).
Des textes et images donnent des descriptions des repas et banquets des élites, réservés aux hommes, qui se déroulent dans les résidences privées, dans la pièce appelée andron : ils commencent par le deipnon, le souper, suivi du symposion, le banquet à proprement parler, immortalisé par Platon, durant lequel on boit du vin, accompagné de fruits et mets sucrés tout en devisant. Les vases employés sont choisis avec soin, l'ordre de service est codifié, des divertissements organisés (avec des musiciens, des hétaïres). Ce type de banquet constitue un élément majeur de la sociabilité des élites grecques des époques archaïque et classique[212],[213].
Femme pétrissant de la pâte à pain, début Ve siècle av. J.-C. Musée national archéologique d'Athènes.
Braséro servant pour la cuisson des aliments. VIe – IVe siècle av. J.-C. Musée de l'Agora antique d'Athènes.
Un jeune homme prépare une tête de cochon après le sacrifice, cratère à figures rouges, v. 360-340 av. J.-C. Musée archéologique national de Madrid.
Homme banquetant diverti par un musicien, coupe à figures rouges, v. 480 av. J.-C. Staatliche Antikensammlungen
Les Grecs donnent à la guerre (polemos) une place primordiale, comme le montre le nombre d'œuvres littéraires, philosophiques ou artistiques ayant pour thème le fait militaire. La guerre mobilise la politique, l'économie et la religion à toutes les époques de l'histoire grecque. Certains édifices défensifs ont laissé des vestiges encore visibles de nos jours, montrant la volonté qu'ont les cités de faire face aux agressions extérieures[214].
Les Minoéens ne semblent pas avoir connu l'état de guerre en Crète mais ils sont parvenus à fonder un empire maritime (ou thalassocratie) en mer Égée. Les Mycéniens ont érigé dans la seconde moitié du IIe millénaire avant notre ère d'imposantes murailles que les Anciens appellent murs cyclopéens. Les guerriers mycéniens portent de lourdes armures de bronze et utilisent des poignards ou des épées, alors que le char sert essentiellement pour le transport vers le champ de bataille.
L'Iliade, qui dépeint la guerre de Troie, sert de modèle éthique aux conflits dans les siècles ultérieurs : Alexandre le Grand se voit par exemple comme un nouvel Achille. Même si son historicité reste débattue, l'œuvre d'Homère dépeint de manière précise les mœurs guerrières à l'époque mycénienne tout en empruntant à l'époque archaïque. Ainsi, la bataille donne lieu à des duels entre héros, car le corps à corps s'avère pour les Grecs plus noble que l'usage d'armes de trait. Des formations de combat rappelant la phalange sont également décrites. Homère montre ce que la guerre est aussi pour les Grecs : raids de pirates, captures d'esclaves ou de bétail, échanges contre rançon, etc.[215].
Le premier conflit attesté entre cités rivales à l'époque archaïque est la guerre lélantine entre Chalcis et Érétrie (710 à 650 av J.-C.). C'est à cette époque que serait apparue la phalange hoplitique qui domine la scène militaire grecque pendant trois siècles. Les armées sont aussi dotées de troupes légères, les peltastes, tandis que la cavalerie ne joue qu'un rôle d'appoint. Au IVe siècle av. J.-C., les Grecs commencent à construire des navires de combat très perfectionnés, les trières. Durant l'époque archaïque la guerre se limite à des conflits entre cités voisines. Les tentatives d'invasion perse lors des deux guerres médiques donnent une nouvelle dimension à la guerre, que cela soit sur terre (batailles de Marathon et de Platées) ou sur mer (bataille de Salamine). La guerre du Péloponnèse (431-404), le plus grand conflit entre cités grecques, met aux prises Athènes et Sparte à travers deux coalitions, l'une maritime, l'autre terrestre. Elle peut s'apparenter à une guerre civile entre Grecs et sonne le déclin des cités-État[215].
Au début du IVe siècle av. J.-C., les Thébains mettent au point l'ordre oblique qui vise à renforcer une des ailes pour tenter d'offrir la victoire. Puis les rois de Macédoine, Philippe II et son fils Alexandre le Grand, révolutionnent l'art militaire en s'appuyant sur une nouvelle organisation de la phalange de piquiers, porteurs de longues sarisses, et en intégrant de nombreuses unités de cavalerie lourde. Ils développent également les techniques de siège (ou poliorcétique). L'armée macédonienne devient l'instrument de la conquête de l'Asie. Après les conquêtes d'Alexandre, la guerre change d'échelle, mettant en jeu de grandes puissances territoriales à l'époque hellénistique : Égypte lagide, Asie séleucide et Macédoine antigonide. Ces trois entités sont finalement vaincues par les Romains[215].
Traditionnellement, les guerriers sont des citoyens accomplissant un service militaire et s'équipant en fonction de leurs revenus. Les esclaves ne sont pas autorisés à prendre part à la guerre en tant que combattants, à part exception comme les hilotes à Sparte. La guerre du Péloponnèse accélère l'emploi de soldats professionnels. À l'époque hellénistique, des mercenaires grecs se retrouvent embauchés dans toutes les armées, ce qui peut aboutir à des trahisons ou à des troubles.
Le monde grec étant très marqué par la présence de la mer, les flottes de guerre se développent tôt et prennent une grande importance. Que la « thalassocratie minoenne » évoquée par Thucydide ait existé ou non, il y a manifestement déjà des flottes durant l'âge du Bronze égéen. L'époque archaïque est l'âge de l'expansion maritime des Grecs et du développement de leur art de la guerre navale. Samos constitue une flotte de guerre sous son tyran Polycrate, mais c'est surtout Athènes qui développe une grande flotte, qui permet aux Grecs de défaire les Perses sur mer à plusieurs reprises. Le navire de guerre athénien est la trière. Elle est mue à l'aide de voiles ou de rames et possède une proue pointue pour éperonner les bateaux ennemis. Elle dispose d'un équipage d'environ 200 hommes, la plupart des rameurs. Leur coordination et leur capacité à manœuvrer leur bateau efficacement est la clef des batailles, permettant aux flottes de procéder des encerclements ou enfoncer les lignes ennemies. La suprématie athénienne sur mer dure durant la majeure partie de la guerre du Péloponnèse, avant que Sparte ne constitue sa propre flotte avec le soutien financier perse. Mais Athènes reprend l'avantage au IVe siècle av. J.-C., avant d'être éclipsée par le développement des grandes flottes de l'époque hellénistique : la Macédoine, l’Égypte lagide, et la cité de Rhodes[216].
La guerre est très codifiée chez les Grecs. Elle est en effet régie par le droit qui se fonde sur les « lois communes des Grecs ». Ainsi, les belligérants se doivent de respecter les traités et la parole donnée sous serment. Ils ne peuvent pas s'attaquer aux ambassades ou aux théores en mission. Il est interdit de mettre à mort les prisonniers qui se sont rendus, leur sort étant d'être réduit en esclavage[217]. Les Grecs suivent également des règles, liées aux conditions d'approvisionnement des troupes, comme de ne pas mener campagne pendant la saison hivernale. Il existe aussi des exemples de belligérants s'interdisant de couper les sources en eau des places assiégées[215]. Le non-respect de ces règles entraînent la désapprobation générale. Avant la bataille, des libations et des sacrifices sont offerts aux dieux, des devins cherchant à déchiffrer leur volonté. Les chants de guerre (ou péans) ont une connotation religieuse. Après la bataille, l'armée victorieuse dresse des trophées avec les armements pris à l'ennemi. Les grandes victoires sont l'objet de dédicaces dans les sanctuaires panhelléniques. Les vaincus sont autorisés à recueillir les dépouilles des soldats tués qui sont généralement incinérés ou ensevelis sur place, même si les usages diffèrent entre les cités[217]. Tous ces usages perdurent jusqu'à l'époque hellénistique[218].
En contrepartie il y a peu de limitations aux actions destructrices : les pillages et saccages de récoltes et propriétés de l'ennemi sont courants, il n'est pas inhabituel qu'un sanctuaire soit mis à sac au mépris de son caractère sacré, et les populations vaincues sont à la merci des vainqueurs qui peuvent les massacrer ou les asservir. L'époque hellénistique semble voir une baisse des violences par rapport à l'époque classique, mais les conquêtes romaines renversent la tendance et entraînent d'importantes destructions dans le monde grec[219],[220]. De ce fait les discours sur la guerre dans la Grèce antique ne sont guère enjoliveurs. Depuis Homère on glorifie certes les hauts faits de guerriers, mais pas la guerre en elle-même, qui est vue comme une calamité. Les descriptions des malheurs de la guerre sont courantes dans la littérature postérieure, notamment chez les historiens[221].
La sexualité des Grecs et Grecques antiques est difficile à catégoriser au regard des notions modernes telles que l'hétérosexualité ou l'homosexualité, tant elle répond à des logiques qui sont propres à cette civilisation (et se retrouvent en bonne partie dans la Rome antique). En effet, il n'y a pas de concept d'orientation sexuelle, car  (M. Sartre)[223]. Les rapports sexuels sont pratiqués entre personnes de sexe différent comme personnes de même sexe, mais il n'est que très rarement question de préférence ou d'exclusivisme pour l'un ou l'autre. Le sexe est essentiellement conceptualisé du point de vue masculin, autour des notions de pénétration et de plaisir phallique, peu importe le sexe anatomique du ou de la partenaire, avec l'idée courante que la personne active domine la personne passive, surtout si elle est inférieure socialement (au regard de son statut, son âge ou son genre) et que l'acte est potentiellement infamant pour la personne pénétrée[224]. Ce qui est plus réprouvé est une conduite allant contre les normes attribuées en fonction d'un genre, à savoir qu'un homme a un rôle sexuel insertif et une femme un rôle réceptif. Il est généralement admis qu'un homme peut aussi bien éprouver un désir érotique pour un beau jeune homme qu'une belle femme, les conduites condamnées étant certains excès comme l'adultère avec une citoyenne, la corruption de jeunes garçons, la dépense de sommes élevées pour une courtisane[225].
Le contrôle de la sexualité des femmes libres est fort, notamment leur virginité avant le mariage, puisque leur rôle principal est d'enfanter et qu'on souhaite alors s'assurer qu'elles donnent naissance aux enfants de leur mari, pour la pérennité de la famille et la stabilité sociale. De ce fait l'adultère n'est considéré que dans le cas où une citoyenne est impliquée. Les hommes libres peuvent de leur côté avoir recours aux services de prostitué(e)s, et pour les plus riches à ceux d'hétaïres, des sortes de courtisanes dont les services ne se cantonnent pas aux rapports sexuels. Leurs esclaves, femmes comme hommes, leur servent également de partenaires sexuels[226].
La pédérastie désigne dans la Grèce antique la poursuite par des hommes plus âgés de jeunes garçons, sans doute souvent prépubères, car il est considéré qu'un garçon imberbe a un attrait érotique supérieur à celui d'un homme mûr, quand bien même il est considéré comme beau. On a proposé d'y voir une forme d'initiation, une éducation et un enseignement moral, mais il est difficile d'en exclure les finalités sexuelles, même si elles ne sont pas systématiques. Cette pratique assignant des rôles en fonction de l'âge, qui a une visibilité et un prestige social importants, où on distingue le sujet du désir, l'éraste, de son objet, l'éromène, est surtout connue pour l'Athènes classique[227].
La religion grecque antique[228] présente un profil très différent des religions de l'époque moderne en cela qu'elle n'a ni textes sacrés, ni dogme, ni Église. Ses croyances ne reposent pas sur une révélation sacrée, elles sont le produit d'une expérience cumulative qui laisse la place à une diversité d'opinions qu'aucune « orthodoxie » ne prétend éteindre, s'intéressent plus aux affaires des vivants qu'à ce qui se passe après la mort. Elle est polythéiste et accorde une grande importance aux rites et moins à la dévotion personnelle. La piété envers les dieux et la pureté rituelle sont primordiales, et l'éthique secondaire. C'est une religion dont on a pu écrire qu'elle est « encastrée » dans la société, organisée principalement dans le du cadre de la cité, même si les rituels domestiques sont importants et que se développent des approches plus personnelles avec les cultes à mystères[229],[230].
Les Grecs anciens sont polythéistes, ils vénèrent une foule de dieux (theoi), qui ont pour trait principal leur immortalité et leur puissance, qui garantissent leur supériorité par rapport aux humains, et les diverses attentions de ces derniers à leur égard[231]. Ils sont représentés sous forme humaine (anthropomorphisme), et ont un comportement proche de celui des hommes, suivant la vision répandue par Homère et Hésiode[232],[233]. Du point de vue antique,  (P. Veyne)[234].
Les dieux ont chacun un domaine de compétence déterminé, parfois plusieurs, à la fois distinct et complémentaire de ceux des autres[235]. On retrouve un même ensemble de divinités principales dans le monde grec, qui se déclinent en plusieurs variantes distinguées par un deuxième nom (épiclèse), souvent construit à partir de leur lieu de culte précis (Héra de Samos, Héra d'Argos, etc.). Dans d'autres cas une même divinité peut être identifiée sous un de ses aspects précis, qui renvoie à une compétence spécifique (Apollon Agyiée pour les rues). Cela renvoie à la complexité du système polythéiste : d'un certain point de vue chaque dieu d'un lieu est spécifique, mais il y a manifestement la conscience d'une unité derrière cette multiplicité, autour d'un noyau similaire reliant ces variantes entre elles, à commencer par leur nom. L'Artémis de l'un n'est pas exactement l'Artémis de l'autre, mais les deux reconnaissent derrière une même déesse commune aux Grecs[232],[236].
Il existe un ensemble de panthéons locaux, réunissant les principales divinités d'une cité et participant à son identité. Les divinités sont certes souvent prises parmi le groupe des figures majeures (panhelléniques), mais sous une variante locale et en les associant d'une manière spécifique, suivant des logiques de complémentarité et d'oppositions. Cependant, à l'exemple des civilisations antiques orientales, et comme cela a été formulé dans les poèmes homériques, il existe un concept de société divine, dans laquelle les dieux principaux entretiennent des liens familiaux et ont chacun un rôle précis. Le groupe principal est celui des « olympiennes », dominé par Zeus, souvent réduit dans la littérature moderne à un groupe de douze divinités majeures auxquelles on attribue des fonctions principales[237],[232] :
Cette liste peut comprendre des légères variantes, et inclure Hadès, le dieu des Enfers, ou Hestia, la déesse des foyers. Il existe d'autres panthéons, notamment celui des cultes orphiques[232]. Dans la littérature moderne on distingue aussi couramment un groupe de divinités « chthoniennes », liées au monde infernal (Hadès, Perséphone), distinction qui n'est apparemment pas présente dans l'esprit des Anciens[238].
Iris, Héra et Zeus, frise des Panathénées, v. 447-433 av. J.-C. British Museum.
Tétradrachme en argent (revers) représentant Poséidon, règne d'Antimaque Ier de Bactriane (v. 174-165 av. J.-C.). Cabinet des Médailles.
Apollon citharède versant une libation, coupe attique à fond blanc, v. 460. Av. J.-C. Musée archéologique de Delphes.
Diane de Versailles, représentation d'Artémis chasseresse, copie romaine d'un original grec du IVe siècle av. J.-C. Musée du Louvre.
Hermès agoraios, stèle de Thasos, v. 480 av. J.-C. Musée du Louvre.
Zeus s'interpose entre Athéna et Arès, cratère à volutes de Nicosthénès, v. 540-510 av. J.-C. British Museum.
Déméter de Cnide, sculpture en marbre hellénistique, v. 350 ans av. J.-C. British Museum.
Aphrodite sur son cygne, kylix à fond blanc de Rhodes, v. 460 av. J.-C. British Museum.
Au-delà de ce groupe, se trouve une foule de « divinités mineures », ayant souvent un ancrage local, auxquelles les Grecs accordent plus ou moins d'importance selon la situation, l'époque de l'année, le lieu, etc. Aux marges du monde divin, les Grecs reconnaissent aussi l'existence d'êtres surnaturels, les nymphes, et des personnages à la charnière du monde des dieux et de celui des hommes, les héros, dont les cultes ont en général des aspects locaux très prononcés[232]. Cet univers divin évolue durant les époques hellénistique et romaine, avec le développement de nouveaux cultes comme celui de Tyché, la déesse de la Fortune, ceux des divinités aux origines asiatiques ou égyptiennes, tels Isis et Sarapis, qui comprennent en fait de nombreux éléments grecs[232], également celui des souverains divinisés[239].
Les philosophes développent leur propre vision des dieux, qui s'oppose parfois frontalement à celle la plus couramment admise, élaborant progressivement l'image d'un Dieu unique métaphysique, et moralement irréprochable[234]. Xénophane est le premier à proposer l'existence d'un Dieu unique, une entité suprême qui n'a pas l'apparence d'un homme[240]. Les réflexions sur un Dieu créateur unique et transcendant se font en particulier dans le platonisme, jusqu'au néoplatonisme[241], et le stoïcisme qui envisage un Dieu créateur bienveillant et rationnel[242].
Le terme mythe vient du grec mythos, qui signifie « mot », « parole », « message ». Dans son acception moderne, il peut être défini comme un « récit traditionnel avec une référence secondaire, partielle, à quelque chose qui a une importance collective » (W. Burkert)[243],[244]. Les mythes grecs[245] nous sont parvenus essentiellement sous forme écrite, notamment dans la Théogonie d'Hésiode, les poèmes homériques, les poésies de Pindare, les tragédies d'Eschyle, Sophocle et Euripide, puis les compilations des mythographes de l'époque hellénistique, notamment la Bibliothèque du Pseudo-Apollodore[246]. Ils circulaient manifestement sous forme orale avant leur mise par écrit à partir de l'époque archaïque, ils ont des traits qui semblent les faire remonter à un fonds mythologique très ancien, « indo-européen », présentent des emprunts à des civilisations voisines (notamment l'Anatolie de l'époque hittite), et circulent sous plusieurs variantes, connaissent des remaniements plus ou moins importants qui font évoluer leur contenu et leur sens[244].
Les mythes ont avant tout pour protagonistes des personnages divins et traitent de leurs rapports avec le monde des humains : Zeus devient le roi du monde divin, le maître de l'Olympe, Prométhée vole le feu pour le donner aux hommes, et en punition la première femme Pandore est créée, etc. Les dieux sont souvent présentés sous des jours peu flatteurs : les nombreux adultères de Zeus, la cruauté d'Héra, Hermès le voleur, Dionysos et ses orgies. On rattache aussi à la mythologie plusieurs cycles de récits ayant des protagonistes humains ou héroïques, souvent avec un ancrage dans une des grandes cités grecques : les exploits d'Héraclès, les histoires des familles royales de Mycènes, les Atrides (Agamemnon, Oreste), et de Thèbes, les Labdacides (Œdipe), le cycle de la guerre de Troie d'où découlent les récits homériques sur Achille et Ulysse, l'épopée des Argonautes conduits par Jason, les aventures de Thésée, Persée, etc.[247]
La recherche du sens des mythes occupe depuis longtemps les chercheurs. En raison de leur histoire complexe, ces récits se prêtent à des interprétations plurielles. Le mythe est souvent une histoire qui vise à divertir son audience, en même temps il peut servir à faire comprendre le rapport des hommes à leur monde et en particulier aux dieux, à expliquer leur quotidien et les événements, et il peut servir à expliquer l'origine d'un rite religieux ou d'une norme sociale. Certains chercheurs ont cherché à dégager des structures mentales derrière les mythes qui serviraient à expliquer les sociétés qui les ont formulés, à les relier à un contexte historique. De nombreux mythes présentent des aspects primitifs qui renvoient à des rites sociaux d'initiation, d'institution, de passage. Certains sont liés à des phases historiques, notamment la colonisation qui donne lieu à des mythes fondateurs, et à laquelle on peut aussi relier les cycles de Troie et des Argonautes. Les mythes de l'époque archaïque renvoient plus aux histoires de dynasties royales et d'exploits héroïques, à l'époque classique, en particulier à Athènes, les mêmes histoires servent plutôt à parler des relations familiales, ou des rapports entre un individu et sa cité[248],[249].
La religion grecque antique est profondément encastrée dans la société, et il n'y a pas vraiment de séparation entre profane et sacré[230]. Le terme grec qui renvoie le plus à cette dernière notion, hieros, fait référence à quelque chose consacré à un dieu, et hiera à ce qui est connecté au culte, donc aux rituels et matériaux religieux (y compris ce qui est sacrifié). Deux autres notions fondamentales sont hosios, qui désigne une tradition voire une loi religieuse, à laquelle il faut se conformer, ce qu'il est approprié de faire envers les dieux, et eusebeia, qui renvoie à la piété, au fait de témoigner d'un respect approprié envers les dieux. À l'opposé, la conduite incorrecte du point de vue de la loi divine et plus largement une infraction à la morale traditionnelle, anosion, est condamnée[250]. Les considérations de pureté et de pollution sont aussi au cœur des pratiques rituelles. Les dieux grecs s'intéressent plus à la piété de leurs ouailles qu'à leur éthique, ils sont avec eux dans une relation reposant sur la « faveur » ou « grâce », charis, trouvant de la réjouissance dans les sacrifices, offrandes et autres actes pieux faits par les hommes (chants, danses), et leur octroyant des bienfaits en échange (santé, fertilité, prospérité, sécurité). C'est donc une relation entre personnes de puissance différente (à l'image de celle entre un roi et un sujet), qui se veut mutuellement bénéfique[251]. La religion grecque ne met pas les préoccupations individuelles en avant, le culte étant généralement de nature communautaire : il se déroule au niveau de la cité, de l’ethnos, de la fédération, du royaume, de la tribu, de la communauté locale, de la famille, aussi des associations cultuelles d'amis, qui disposent de leur propre calendrier rituel marqué par des fêtes majeures qui sont un élément marquant de leur identité commune. Ce culte est pris en charge par les représentants de ces groupes, donc les magistrats au niveau de la cité, et il n'y a pas de statut spécial pour le clergé des sanctuaires, à la différence de bien des civilisations antiques[230],[252].
L'acte central du culte est le sacrifice sanglant d'un animal aux divinités, qui occupe la place majeure dans la plupart des rituels ; dans d'autres cas l'animal sacrificiel est consumé par le feu (holocauste). Le sacrifice sanglant sert à la fois à marquer la relation entre les humains et les dieux, par un acte d'offrande, que la relation entre membres de la communauté, par le partage collectif du repas sacrificiel constitué par les restes de l'animal immolé. Ce rite peut être effectué de manière quotidienne, mais il existe des occasions plus importantes, les fêtes religieuses, qui sont généralement publiques et organisées suivant la séquence procession, sacrifice, banquet puis concours, peuvent réunir toute la cité comme les Panathénées et les Grandes Dionysies d'Athènes, voire le monde grec avec les fêtes panhelléniques qui ont lieu tous les quatre ans dans des lieux de culte majeurs (à Olympie, Corinthe, Delphes et Némée)[230],[253]. D'autres cultes ont un aspect plus secret, les mystères, réservés à des initiés et renvoyant à des préoccupations sur le devenir de l'individu après la mort[230].
Les sacrifices sanglants et les autres offrandes qui peuvent être faits aux dieux pour attirer les faveurs divines sont accompagnées de prières, déclamées à voix haute, courtes quand il s'agit de prières personnelles, mais plus longues dans un contexte public, et très développées chez les poètes. Dans leur formulation la plus complète, elles débutent par une invocation de la divinité, puis se poursuit en un argument qui rappelle la piété de l'orant ou fait les louanges de la divinité, puis la prière à proprement parler ou pétition qui formule le vœu. Celui-ci peut être formulé pour une personne ou pour toute la communauté organisant le rite, appeler un bienfait (prospérité, paix, santé) ou demander qu'une malédiction frappe un ennemi[254]. Si le dieu ou la déesse accède à la demande, on le remercie et l'honore pour sa miséricorde, mais s'il ou elle ne le fait pas, certains ne manquent pas de manifester leur incompréhension voire de lui reprocher son ingratitude dans des invocations[255].
Les sanctuaires grecs sont désignés par le terme hieron, ou encore temenos qui concerne plus précisément l'espace sacré délimité. Ils se développent entre le IXe siècle av. J.-C. et le VIIIe siècle av. J.-C., en lien avec l'émergence des cités. Ils sont d'importance variée : beaucoup servent un culte local, certains ont une importance à l'échelle d'une cité, d'autres à l'échelle régionale voire nationale ou supranationale, les sanctuaires « panhelléniques » (Olympie, Némée, Delphes). Ils peuvent être localisés dans une ville, dans un village, ou dans l'espace rural, notamment aux limites des cités. Ils sont accessibles à tous, du moins sur leur plus grande partie, et peuvent servir pour des rassemblements importants lors des grandes fêtes. Leur taille varie beaucoup, et pas forcément en fonction de leur importance. Au minimum la présence d'un autel sacrificiel est suffisante, notamment dans les bosquets sacrés. Le temple avec la statue de culte destinataire des sacrifices ne sont pas des éléments indispensables, pas plus qu'un mur d'enceinte délimitant clairement le temenos. Selon les configurations, un sanctuaire peut aussi comprendre des entrées monumentales, des temples secondaires, des salles de banquet, des lieux de réunion, des hébergements pour les prêtres ou les pèlerins, des ateliers, et d'autres aménagements liés à leur fonction spécifique, comme les bains et les espaces d'incubation dans les sanctuaires des dieux guérisseurs, les théâtres, stades et gymnases des sanctuaires organisant des concours sportifs, théâtraux ou musicaux. Le temple a des propriétés sacrées, issues des offrandes qui y sont consacrées, qui peuvent garnir un trésor très riche, les plus importantes (statues, monuments) n'étant pas abritées. Les cultes chthoniens ou à mystères se déroulent dans des pièces spécifiques, non exposées aux regards[256],[257].
Au quotidien, les individus peuvent présenter leurs hommages aux divinités lorsqu'ils passent devant une de leurs chapelles, faire un petit sacrifice personnel de nourriture ou d'encens (non sanglant), exposer une image divine. Les prières et offrandes votives sont des actes manifestement courants. Les consultations oraculaires sont un autre moyen de communiquer avec le divin employé par des individus pour des préoccupations quotidiennes[230].
Les oracles sont une forme de divination qui joue un rôle très important dans le monde grec, dès l'époque archaïque. Ce terme désigne une réponse qu'un dieu ou un héros donne à un fidèle qui l'a interrogé dans un de ses lieux de culte précis. Les plus connus sont les oracles rendus par Apollon à Delphes, mais il s'en trouve dans tout le monde grec. Certains ont une importance locale, d'autres en revanche sont sollicités dans tout le monde grec, voire au-delà, tels ceux d'Apollon à Didymes et Claros en Asie Mineure ou Cumes en Italie, celui de Zeus à Dodone, ou ceux où le message est dispensé par un héros (Amphiaraos à Oropos, Trophonios à Lébadée). Les Grecs sollicitent aussi des oracles d'autres pays, comme celui d'Amon (assimilé à Zeus par les Grecs) à Siwa en Égypte. Les oracles des sanctuaires d'Apollon sont prononcés par une prophétesse, la Sibylle (Pythie à Delphes). Les oracles des temples d'Asclépios se produisent par des rêves suscités (incubation) et concernent les questions de santé. Celui de Dodone est documenté par des tablettes en plomb comportant les questions posées au dieu, mais on ne sait pas de quelle manière se déroule la procédure, qui cherche le message divin dans le frémissement des feuilles d'un chêne sacré. Ces sources offrent un aperçu des préoccupations des fidèles : enfantement, opportunité de mariage, de carrière, santé, et plus largement la manière d'obtenir la faveur divine. Les questions posées par les États, les moins nombreuses mais celles qui ont le plus suscité l'attention, interroge surtout à Dodone le dieu sur les pratiques cultuelles qu'il approuve. Les questions des cités posées à Delphes à l'époque archaïque et classique concernent les affaires politiques et militaires, et sont connues pour leur formulation cryptique qui peut s'interpréter de différentes manières, ou encore la fondation de cités à l'époque de la colonisation, mais cela disparaît par la suite[258],[259].
D'autres formes de divination sont attestées dans le monde grec, cette pratique de communication avec le divin étant manifestement très répandue pour des préoccupations rituelles ou quotidiennes. L’Anabase de Xénophon mentionne ainsi, aux côtés d'un oracle delphique, la divination par les rêves (oniromancie), par le vol des oiseaux (ornithomancie), par le sacrifice d'un animal, également par un éternuement, ce qui renvoie plus largement à considérer que toute chose survenant durant la journée peut renfermer un message divin. Il existe des devins spécialisés dans tel ou tel type de divination, qui sont souvent affublés d'une réputation de charlatanisme, ce qui n'empêche pas d'y recourir, et plus largement l'efficacité de la divination est très débattue[260]. L'astrologie se diffuse dans le monde grec à l'époque hellénistique, à partir des pratiques babyloniennes, avant tout sous la forme des horoscopes[261].
Les anciens Grecs croient que les morts se rendent aux Enfers, monde souterrain (dont la description la plus influente pour les représentations gréco-romaines se trouve dans le livre XI de l’Odyssée), où l'existence est une errance sans but et sans fin, pathétique et morose. Les actions, qu'elles soient bonnes ou mauvaises, n'ont de conséquence que pour les vivants, et la plupart des Grecs et Grecques ne semblent pas avoir espéré grand-chose de ce qui se passerait après sa mort. D'un autre côté il existe aussi des histoires de fantômes, des offrandes alimentaires sont faites aux morts, manifestement pour leur bien-être, et la mythologie évoque le fait que quelques-uns des défunts ont un traitement différent aux Enfers : ceux qui ont commis de grandes fautes envers les dieux sont torturés éternellement dans le Tartare, alors que les plus vertueux ont droit à l'allégresse aux Champs Élysées. De façon marginale, des croyances divergentes en des formes de vie après la mort se trouvent chez des philosophes (la réincarnation chez Platon), ou en la possibilité d'une amélioration de son sort dans l'au-delà dans l'orphisme et certains cultes à mystères. Les croyances sur la mort sont donc diverses et parfois contradictoires[262],[263].
Les rites funéraires visent à accompagner le défunt vers l'au-delà. Il est primordial de rendre les honneurs à un défunt et de pouvoir enterre ses restes, l'impossibilité de le faire étant vue comme un scandale de premier ordre[264]. Le mort est lavé, enveloppé dans un linceul, puis exposé durant une journée. Un cortège funèbre l'emporte ensuite vers son tombeau. Le corps peut être inhumé dans un cercueil, ou incinéré, les cendres étant alors disposées dans un vase qui est enterré. Les deux pratiques sont attestées sans que l'on ne sache la raison présidant au choix de l'un ou de l'autre[265],[263]. Dans l'Athènes classique il est courant de faire des offrandes de gâteaux et des libations d'eau sur les tombes lors d'une fête annuelle dédiée aux défunts[263].
L'archéologie indique que les formes de traitement des morts varient selon les lieux et les époques dans le monde grec. Ainsi à Athènes elles évoluent au fil du temps : plutôt la crémation dans des urnes durant les âges obscurs, puis des inhumations dans des tombes en fosse durant la période 750-700 av. J.-C., avant un retour à la crémation dans la tombe jusqu'au milieu du VIe siècle av. J.-C. et ensuite la prédominance de l'inhumation, dans des tombes en fosse, ou bien couvertes de tuiles, ou des sarcophages. Alors qu'on trouve des tombes riches durant l'époque archaïque, les sépultures semblent plus simples à partir du VIe siècle av. J.-C., ce qui pourrait s'expliquer par des lois somptuaires visant à rendre moins visibles les inégalités. La majeure partie de la Grèce classique pratique l'inhumation dans des fosses, mais en Grèce occidentale on inhume souvent les adultes dans des grandes jarres[266]. La tombe peut être individuelle, familiale ou collective, comprend aussi des offrandes funéraires (vases, armes et autres objets), et peut être signalée par une stèle inscrite[265],[263]. Les sculptures funéraires florissent à Athènes à la fin de l'époque classique. Puis durant l'époque hellénistique les vastes tombes à voûte des plus riches se développent, à la suite de la Macédoine où les rois ont montré l'exemple (tombeau de Philippe II à Vergina), à l'exception d'Athènes. Les communautés grecques d’Égypte et du Moyen-Orient adoptent en général les pratiques de leur région d'implantation[267].
Le christianisme se constitue à partir du judaïsme, mais cela prend place dans un monde où la culture grecque domine, ce qui explique pourquoi les textes du Nouveau Testament sont écrits en grec. La prédication de Paul de Tarse, déterminante pour le devenir de cette religion, se dirige vers les cités grecques, d'Asie Mineure et de Grèce continentale. Des communautés chrétiennes s'y développent aux IIe – IIIe siècles, elles subissent les persécutions organisées par le gouvernement romain, dont l'ampleur exacte reste incertaine, mais qui donnent naissance à de nombreux cultes de martyrs qui participent à la consolidation des groupes chrétiens. Avec la conversion des empereurs à partir de celle de Constantin, le christianisme connaît un essor marqué au IVe siècle. Les conciles fixent la doctrine, des lieux de culte chrétiens sont érigés un peu partout[268],[269].
Les traditions polythéistes restent longtemps vivaces malgré l'essor du christianisme. Le polythéisme a alors connu diverses évolutions qui sont en partie dues à la concurrence du christianisme : le sacrifice sanglant est supplanté par le sacrifice d'encens, essor des pratiques privées, etc. Mais au milieu du Ve siècle le christianisme est probablement devenu majoritaire et le polythéisme disparaît progressivement[270]. Par bien des aspects l'essor de cette nouvelle religion peut être vu comme marquant la fin de la civilisation grecque antique[10] : parmi les éléments qu'elle expurge se trouvent les cultes, les mythes et les images des dieux polythéistes, de nombreux divertissements traditionnels tels que le théâtre, la célébration de la boisson et de la sexualité, qu'elle remplace par un mode de vie plus simple et austère, un détachement par rapport au corps et aux sens, un engagement émotionnel au sein d'une communauté soudée par la foi en un Dieu unique, les promesses de pardon des pêchés et de la vie après la mort[271]. Les cultes des divinités grecques sont encore attestés au IVe siècle et survivent au suivant, mais ils perdent leur caractère public, alors que de nombreux temples sont détruits ou transformés en églises[272].
Les empereurs chrétiens mettent en place une législation qui vise ceux que l'on dénomme alors les « Hellènes » (synonyme de « païen » ; on parle plutôt de « gentils » à cette période), dont la répétition indique les limites, mais ces mesures sonnent le glas de nombreux aspects caractéristiques de la culture grecque antique : la proscription des sacrifices sanglants et cultes païens, le dernier oracle de Delphes et les derniers jeux olympiques (antiques) dateraient de 393, Justinien ordonne l'obligation de baptême en 529, ainsi que la fermeture des écoles de philosophie. Vers cette époque le polythéisme disparaît progressivement[273]. Cela est certes pour partie le résultat des mesures répressives et aussi de persécutions, mais également de la christianisation de rites polythéistes (par exemple la transformation de cultes de dieux guérisseurs en cultes des saints chrétiens), qui accompagne un mouvement plus profond de christianisation de la société qui la modifie profondément, et se voit dans le cadre public comme dans le cadre privé (fêtes et cycle liturgique, rites de naissance, de mariage et de mort, formes de dévotion et de charité, etc.)[274]. Malgré tout des pratiques « païennes » sont encore dénoncées dans des fêtes, pratiques magiques et divinatoires durant les siècles suivants, notamment dans les campagnes. Il peut certes s'agir de survivances, mais l'accusation de paganisme (ou d'« hellénisme », terme qui renvoie alors à l'héritage intellectuel grec polythéiste) est aussi dans le monde byzantin une manière de dénoncer ceux qui s'intéressent beaucoup à la culture grecque antique, toujours suspectée en raison de son caractère païen de les détourner du christianisme[275].
Au sortir des siècles obscurs, la culture grecque connaît des bouleversements majeurs : l'adoption de l'écriture alphabétique donne naissance à une littérature, à commencer par les épopées homériques qui restent durant toute l'histoire grecque antique des textes fondateurs, et à une floraison intellectuelle qui voit la naissance de la philosophie puis celle d'autres disciplines (histoire, rhétorique, théâtre, etc.) alors qu'émergent de nouvelles formes artistiques et architecturales, qui évoluent rapidement. Tout cela s'appuie sur l'intégration et l'appropriation de divers éléments culturels venus des civilisations orientales (en premier lieu l'alphabet), et se produit dans le contexte de la mise en place de la cité grecque, institution et cadre social derrière laquelle on voit souvent une cause majeure des changements culturels à l'origine de la civilisation grecque antique. Ce bouillonnement culmine à Athènes durant l'époque « classique », qui est vue dès les siècles qui lui succèdent comme une référence. Bien qu'on ne puisse résumer les accomplissements de la culture grecque antique à ceux de cette cité et cette période, tant s'en faut, elle se caractérise incontestablement par une remarquable créativité, qui a exercé une fascination sur les civilisations qui lui ont succédé et l'ont érigée en modèle, quand bien même cela relève souvent de l'idéalisation et renvoie à des considérations qui en disent plus sur le récepteur que sur l'origine[276]. Le monde grec des époques hellénistique et romaine ne s'est pas pour autant contenté de dupliquer ou de s'inspirer de ce modèle, puisqu'il a connu des accomplissements culturels et intellectuels de premier ordre. Durant l'Antiquité tardive les évolutions sont encore importantes, en bonne partie liés à la christianisation, qui entraîne certes la fin de nombreux aspects caractéristiques de la culture grecque antique, mais une partie significative de son héritage est préservée à l'époque médiévale dans la culture byzantine.
Le grec est parlé durant l'Antiquité en Grèce continentale, dans les îles égéennes, Chypre, et les cités grecques d'Asie, d'Afrique et d'Italie. Cette langue est d'abord attestée dans les tablettes mycéniennes, autour de 1450-1400 av. J.-C., et surtout par l'alphabet grec après environ 800 av. J.-C. Les origines des premiers locuteurs de langue grecque (le « proto-grec ») arrivés en Grèce sont débattues, de même que leur date d'arrivée. Cette langue n'a pas pu être rangée dans un sous-groupe des langues indo-européennes, ce qui complexifie l'étude de ses origines. Les premières formes de langue grecque attestées, en gros jusqu'en 300 de notre ère, sont rangées dans la catégorie du grec ancien, auquel succède une période byzantine ou médiévale. Elles comprennent une grande quantité de dialectes. Le grec des tablettes mycéniennes n'est qu'un des dialectes de cette période, et quand le grec est à nouveau documenté à l'époque archaïque, coexistent un ensemble de dialectes régionaux, ou même locaux (au niveau de la cité), qui sont rangés par les linguistes dans plusieurs groupes au regard de leurs similitudes : ionien-attique, arcadochypriote, éolien, dorien, grec du nord-ouest. Le grec ancien évoqué dans les textes modernes et étudié est généralement celui de l'Attique, mais il n'est pas la forme standard du grec durant les époques archaïque et classique, les inscriptions révélant la coexistence de plusieurs dialectes. À partir du IVe siècle av. J.-C. il prend l'ascendant, et c'est à partir de lui qu'est formé le grec standard de l'époque hellénistique, koinè. C'est la langue qui est employée par les Grecs des cités d'Asie et d’Égypte à l'époque hellénistique. À la fin du IIe siècle av. J.-C. la plupart des inscriptions en grec se font dans ce dialecte[277]. C'est par la suite la langue la plus courante de l'Empire romain oriental, la première langue du christianisme, puis la langue officielle de l'Empire byzantin à compter du VIe siècle. Ce premier grec « médiéval » s'est alors s'est bien éloigné du grec ancien et tend à ressembler plus au grec moderne[278].
Le lexique du grec ancien repose sur des bases indo-européennes, aussi des emprunts à des populations pré-grecques du monde égéen, et aux langues sémitiques, et ce dès l'époque mycénienne. Le vocabulaire de base est très proche d'un dialecte à l'autre. Durant l'époque classique les emprunts se raréfient, la langue grecque « s'insularise », les nouveaux mots étant surtout des créations[279]. Elles sont faites notamment par composition (deux mots accolés) ou suffixation (-ikos, -ismos, -ma, etc.). Le vocabulaire grec a servi de modèle ou de source où puiser du vocabulaire pour les autres langues européennes, en particulier le latin[280].
Le grec littéraire reflète au départ la coexistence des dialectes, y compris dans un même texte puisque les auteurs jouent volontiers sur les variations dialectales : béotien (éolien) pour Hésiode et Pindare, mélange dialectal mais une base ionienne pour Homère, dialecte de Lesbos chez Sappho et Alcée, les tragédies athéniennes sont en attique littéraire mais témoignent d'influences ioniennes et homériques, également doriennes (pour les chœurs). La langue épique est très marquée par l'empreinte des textes homériques ; elle sert aussi de base pour les oracles delphiques, des poésies lyriques. En raison des mélanges et emprunts à divers dialectes, les textes littéraires ont développé une langue empreinte d'artificialité, même si les mélanges divers évitent sa fossilisation, du moins jusqu'à l'époque hellénistique. La poésie alexandrine (Théocrite) est en mesure de mêler plusieurs dialectes, puis s'impose une imitation de la langue attique de l'époque classique (atticisme)[281]. Ce grec archaïsant, forgé notamment par la seconde sophistique (IIe – IIIe siècles), est encore le modèle du grec littéraire des savants byzantins, bien différent de la langue parlée[282].
L'écriture apparaît dans le monde égéen durant le IIe millénaire av. J.-C., sous l'influence de l'Anatolie et de l’Égypte mais sous des formes originales. Les deux premières formes d'écriture apparaissent en Crète à l'époque minoenne (v. 1600 av. J.-C.) : les hiéroglyphes crétois, et le linéaire A, dans des inscriptions sur des tablettes d'argile, des poteries, des sceaux, quelques graffitis. Elles ne sont pas comprises, mais il semble qu'elles reposent, à l'image des écritures orientales, sur une combinaison de signes syllabiques (un signe = un son, une syllabe) et logographiques (un signe = une chose, un mot), avec des signes numériques. Le disque de Phaistos, d'époque minoenne, est un document isolé comprenant des signes dans une écriture non comprise. Le linéaire B, développé à l'époque mycénienne vers 1400 av. J.-C., suit les mêmes principes que les écritures minoennes dont il s'inspire, mais il est compris car il transcrit une langue grecque. Il est essentiellement écrit sur des tablettes d'argile qui ont une fonction administrative, parfois sur des jarres. Chypre voit vers la même époque le développement du syllabaire chypro-minoen (v. 1600/1500 av. J.-C.) et son dérivé de l'âge du fer le syllabaire chypriote (v. 1100 av. J.-C.), qui présentent des affinités avec les écritures égéennes, mais ils ne sont pas traduits[283].
Pendentif en argile inscrit avec des hiéroglyphes crétois. Musée archéologique d'Héraklion.
Tablette en linéaire A. Aghia Triada, Musée archéologique d'Héraklion.
Face A du disque de Phaistos. Musée archéologique d'Héraklion.
Tablette inscrite en linéaire B, XIIIe siècle av. J.-C., provenant de Mycènes, Musée national archéologique d'Athènes.
Le linéaire B disparaît vers 1200 av. J.-C. et l'écriture disparaît du monde égéen durant les siècles obscurs. Vers 800 av. J.-C. apparaissent les premières inscriptions en alphabet grec, développé sur le modèle de l'alphabet phénicien. C'est une innovation fondamentale puisqu'elle donne aux Grecs la possibilité d'enregistrer des informations, des savoirs et des histoires, à commencer par les poèmes homériques, permet le foisonnement intellectuel des périodes suivantes : en ce sens l'invention de l'alphabet grec crée une césure entre les « âges obscurs », préhistoriques, et la Grèce archaïque et classique, historiques[284]. L'alphabet est présent à l'époque archaïque sous différentes variantes régionales (dites « épichoriques »), qui font évoluer le principe de l'alphabet tel qu'il a été mis au point dans le Levant sémitique, en y adaptant dès le début des signes pour transcrire des voyelles indépendantes (Α Ε Ι Ο, plus l'invention Υ ; et Ω qui apparaît en Ionie), et d'autres pour transcrire des double-consonnes (Φ Χ Ψ). La forme est-ionienne de l'alphabet grec, adoptée à Athènes, devient dans les premières décennies du IVe siècle av. J.-C. la plus courante dans le monde grec, pour s'imposer comme la forme classique de l'écriture grecque antique. Les principales évolutions qui surviennent concernent la forme des signes, notamment en raison du développement de lettres cursives pour écrire à l'encre entre la fin du IVe siècle av. J.-C. et le IIIe siècle av. J.-C.[285].
Un alphabet grec archaïque sur une coupe attique. Musée national archéologique d'Athènes.
Ostracon portant le nom de Thémistocle. Musée de l'Agora antique d'Athènes
Inscription oraculaire sur lamelle de plomb, Dodone, fin du VIe siècle av. J.-C. Musée archéologique de Ioannina.
Dédicace de monuments en l'honneur de Ptolémée VI d’Égypte, milieu du IIe siècle av. J.-C.. Musée du Louvre.
Papyrus d’Égypte, lettre privée du début du IIIe siècle. Metropolitan Museum of Art.
Ostracon scolaire provenant de Thèbes (Égypte) comportant les premiers vers de l’Iliade, v. 580-640. Metropolitan Museum of Art.
Les anciens Grecs écrivent sur différents supports : des inscriptions sur pierre et lamelles de métal, des inscriptions gravées ou peintes sur des vases ou sur des tessons de céramique (ostracon), sur des tablettes de cire sur un support en bois, sur du parchemin et surtout sur du papyrus. Ce support se diffuse à compter du VIIe siècle av. J.-C., peut-être depuis la Phénicie (le terme grec byblos, « livre », dérive du nom grec de la ville phénicienne Gubla, Byblos). Les livres grecs antiques se présentent sous la forme de rouleaux (volumen). Dès la fin de l'époque archaïque et le début de l'époque classique il s'en trouve à Athènes dans les bibliothèques des puissants, dans les lieux d'enseignement, et un commerce du livre commence à émerger, pour vraiment se développer au IVe siècle av. J.-C. Durant l'époque hellénistique l'usage de l'écriture sur papyrus continue à se répandre, Alexandrie devient le principal centre intellectuel, autour de son Mouseion et de sa vaste bibliothèque, où les savants procèdent à une forme de standardisation du grec écrit. Durant l'époque romaine, au début de notre ère, le codex fait de pages reliées (la forme classique du livre en Occident) commence à se répandre, et il devient la forme dominante à partir du Ve siècle[286].
La majeure partie des Grecs ne sait pas lire, mais les nécessités de la vie politique (notamment dans un cadre démocratique) et les progrès de l'éducation durant l'époque classique et surtout hellénistique font qu'une part croissante de la population est alphabétisée, au moins de façon rudimentaire (les estimations hautes allant jusqu'à 20-30 % pour la population des cités hellénistiques). Cela concerne surtout des hommes des milieux aisés, mais la maîtrise de l'écriture n'est pas un élément de distinction et d'ascension sociale si on en juge par le fait que de nombreux esclaves servent de scribes pour leurs maîtres, notamment dans le monde gréco-romain[287]. La lecture des livres se fait principalement à voix haute, en public, plutôt que de façon silencieuse[288]. L'oralité grade une place importante même après la diffusion de l'écriture, jusque dans le milieu de la haute culture où sont appréciées les qualités d'orateurs, pour les discours, la poésie[289].
L'éducation (paideia) revêt un caractère important chez les Grecs, même s'ils n'ont pas mis en œuvre une éducation universelle ou mené une politique publique de l'éducation. Aux époques classique et hellénistique, la plupart des futurs citoyens de milieux aisés sont éduqués aux lettres (grammata), à la poésie, à la musique et à l'athlétisme (pratiqué dans la palestre). Les écoles s'adressent, à de rares exceptions, uniquement aux garçons. En l'absence de toutes subventions, les paysans et les esclaves n'ont pas accès aux écoles, même si l'écriture peut leur être enseignée[290]. Les écoles destinées aux enfants ne doivent pas être confondues avec les gymnases, un équipement sportif réservé aux éphèbes et aux citoyens adultes, à la fois centre social, athlétique et parfois intellectuel[291]. L'éphébie, institutionnalisée à Athènes au IVe siècle av. J.-C., concernent les jeunes hommes de 18 à 20 ans qui reçoivent un enseignement militaire et civique[292].
L'alphabet grec a probablement été créé au début du VIIIe siècle av. J.-C. et s'est diffusé vers 650 av. J-C. dans la plupart des régions de Grèce[293]. L'écriture est enseignée à cette époque de manière informelle, au sein de la famille ou des chœurs religieux. Les premières mentions d'écoles, faites par Hérodote et Pausanias, renvoient au début du Ve siècle av. J.-C.[293]. Platon mentionne qu'Alcibiade a appris à l'école les lettres, la lutte et à jouer de la lyre[294].
L'école, privée et payante le plus souvent, commence vers l'âge de 7 ans. Les écoliers athéniens sont accompagnés d'un esclave (le paidagogos) dédié à les aider dans leurs tâches[293]. Les maîtres sophistes, à partir du milieu du Ve siècle av. J.-C., font payer cher leurs services pour enseigner la rhétorique à des jeunes gens d'au moins 17 ans ; le plus connu de ces sophistes est Protagoras. L'Iliade et l'Odyssée forment la base des études littéraires, la mémorisation de la poésie paraissant faire office de méthode pédagogique. La géométrie est également enseignée. Il a probablement existé des maîtres qui enseignent les bases de l'écriture et de la lecture et d'autres qui enseignent une connaissance plus poussée de la langue grecque. Apparemment, des maîtres sont aussi spécialisés en musique et d'autres en athlétisme[293]. À Sparte, l'enseignement scolaire semble lui avoir été délaissé au profit d'un enseignement athlétique et militaire à travers l'agôgè. Sparte se distingue néanmoins des autres cités en développant l'éducation des filles[295]. Dans Les Lois, Platon promeut une scolarisation universelle, même pour les filles[296] ; Aristote se montre lui plus réservé sur ce sujet[297].
À l'époque hellénistique, l'enseignement se généralise pour atteindre son apogée au IIe siècle av. J.-C. Des cités, à travers des bienfaiteurs (ou évergètes) comme les Attalides de Pergame, mettent en place un enseignement généralisé pour les jeunes garçons ; des filles peuvent parfois être admises. L'occupation romaine parait avoir mis un terme à ce développement de l'éducation. L'enseignent évolue peu durant cette période, même si l'éducation musicale semble moins présente. Après Homère, les auteurs les plus étudiés sont alors Euripide et Ménandre. La Grammaire grecque de Denys le Thrace est un classique de l'époque. Les exercices scolaires se font sur des ostraca, des tablettes de bois ou des papyrus. La plupart des cités possèdent alors une école, même si l'équipement et le salaire des professeurs sont modestes[291].
L'enseignement des savoirs élémentaires est relativement connu pour l'époque hellénistique[298], mais l'enseignement supérieur l'est beaucoup moins. L'éphébie, sur le modèle athénien, s'étend aux autres cités grecques, du moins dans les milieux aisés, pour devenir une sorte de « club »[291]. Aux exercices physiques et militaires s'ajoutent désormais l'étude des lettres. Les écoles de philosophie et de rhétorique, héritières des sophistes, s'adressent à une élite[291]. L'éducation, paideia, devient alors un élément fondamental pour les Grecs, la culture qu'elle dispense, également désignée par le mot paideia, étant vue comme nécessaire pour pouvoir se revendiquer comme un « Hellène », l'identité grecque étant alors surtout culturelle[299].
L'éducation « supérieure » se développe à Athènes vers la fin du Ve siècle av. J.-C., autour d'un maître renommé qui réunissent des élèves issus de la bonne société, avec les Sophistes, l'école de rhétorique d'Isocrate, l'Académie de Platon, puis le Lycée d'Aristote, peut-être la médecine à Cos. Durant l'époque hellénistique se développent les études de niveau supérieur dans l'enseignement civique, avec des cours de littérature, rhétorique et philosophie, et la constitution de bibliothèques. Surtout émergent plusieurs centres d'apprentissage spécialisés dans un domaine du savoir : philosophie et rhétorique à Athènes, Rhodes, Pergame ; médecine à Cos, Éphèse, Pergame ; tout à Alexandrie, avec son Mouseion et sa bibliothèque qui vise à réunir les savoirs du monde entier[300].
Ce système éducatif survit dans le monde romain, mais il décline dans l'Antiquité tardive. La christianisation lui nuit manifestement, puisque son contenu repose grandement sur les écrits païens, malgré la tentative de certains auteurs chrétiens de christianiser la paideia, ainsi Basile de Césarée qui conseille la lecture des textes classiques, mais en ignorant autant que faire se peut les passages évoquant clairement le polythéisme[271]. Les difficultés économiques de la fin de la période sont manifestement une autre cause de son délitement[301]. Le déclin de l'élite des cités entraîne un déclin culturel, mais les bases de l'enseignement de la grammaire et de la rhétorique survivent et sont revivifiés à l'époque byzantine[302].
Les premiers textes littéraires grecs[303] sont peut-être couchés par écrit dès le début de l'époque archaïque, vers 750-700 av. J.-C. De cette époque sont datées les épopées homériques, l’Iliade et l’Odyssée, issues d'une tradition orale plus ancienne, et dont la date de première mise par écrit est discutée[304]. De la même période datent les poèmes d'Hésiode, qui abordent des thèmes mythologiques et aussi moraux et pratiques. Après ces textes fondateurs, le principal genre qui se développe durant la seconde partie de l'époque archaïque est la poésie lyrique (Alcman, Archiloque, Alcée, Sappho, Pindare), dont il ne subsiste en général que des fragments. Le genre décline au début de l'époque classique. Celle-ci voit le développement d'une autre forme de poésie, celle des tragédies et des comédies (Eschyle, Sophocle, Euripide, Aristophane), et d'une littérature en prose très diverse, qui comprend les textes de philosophes (Platon et Aristote), d'historiens (Hérodote, Thucydide, Xénophon), et aussi la rhétorique avec la mise par écrit des discours de grands orateurs (Lysias, Isocrate, Isée, Démosthène, etc.)[305].
L'époque hellénistique est notamment marquée par le théâtre comique de Ménandre, la poésie alexandrine, dominée par Théocrite et Callimaque, aussi Apollonios de Rhodes dans le genre épique[306]. L'époque romaine est très productive pour la littérature en grec : histoire avec Appien, Arrien, Dion Cassius, Flavius Josèphe ; philosophie avec Épictète, Marc Aurèle, puis Plotin, Jamblique, Proclus durant l'époque tardive ; poésie et épopée avec Nonnos de Panopolis ; rhétorique avec Libanios et l'école de Gaza ; littérature de voyage avec Pausanias le Périégète ; développement du genre du roman, avec notamment Longin et Héliodore d'Émèse. Plutarque et Lucien de Samosate s'illustrent dans plusieurs genres[307],[308]. Plusieurs de ces auteurs sont issus de la frange hellénisée de peuples non grecs (Lucien de Samosate, Flavius Josèphe, Philon d'Alexandrie, Héliodore d'Émèse, etc.). Puis la littérature chrétienne se développe, en particulier après le IVe siècle avec les Pères cappadociens qui sont des écrivains accomplis (Basile de Césarée, Grégoire de Nysse, Grégoire de Nazianze), Jean Chrysostome, l'histoire ecclésiastique avec Eusèbe de Césarée, aussi le développement des hagiographies. Les lettres (et plus largement la vie intellectuelle) connaissent un déclin durant la crise de la fin du VIe siècle et durant le VIIe siècle[309].
La poésie se situe à la charnière de l'oral et de l'écrit : elle est souvent chantée, accompagnée de musique, dans un contexte cérémoniel ; elle nous est connue par des écrits, et c'est la plus ancienne forme de littérature grecque connue[310]. Elle repose sur un rythme syllabique, alternances de syllabes longues et brèves suivant des schémas définis, pour former divers types de vers, les plus courants étant le iambique, l'héroïque et le lyrique[311].
Les poèmes épiques attribués à Homère entament l'histoire littéraire grecque. L’Iliade, relatant un épisode de la dernière partie de la légendaire guerre de Troie centré sur le héros Achille, sa colère après la mort de Patrocle et sa vengeance destructrice, et l’Odyssée qui relate la longue errance d'Ulysse après la fin de ce conflit pour retrouver son pays, Ithaque. Ces deux chefs-d’œuvre sont restés un modèle durant toute l'Antiquité grecque, et au-delà. Cette littérature trouve son origine dans les poèmes déclamés par les aèdes lors des banquets aristocratiques des débuts de l'époque archaïque, qui comprenait un ensemble de récits transmis sous forme orale, mis par écrit après l'adoption de l'alphabet, et tous disparus à l'exception de ces deux exemples. Ces poèmes suscitent des émules dès l'époque classique, prenant également souvent pour base le cycle troyen, et qui sont eux aussi perdus[313]. Hésiode, contemporain de Homère, est un cas à part par la diversité de son œuvre et le fait qu'elle a également eu une grande influence : la Théogonie qui est la base de la connaissance des mythes grecs, et Les Travaux et les Jours qui mêle description pratique de la vie d'un domaine agricole et aspects religieux[314]. Les philosophes présocratiques s'expriment également souvent sous forme poétique, alors que ceux de la période suivante écrivent en prose[311].
La poésie lyrique s'épanouit à partir de la fin du VIIe siècle av. J.-C. et florit au siècle suivant, récitée lors des banquets et donnant lieu à des concours. La plupart des œuvres de l'époque sont perdues, les plus chanceuses ayant été préservées par des fragments. Parmi les poètes célébrés de cette époque se trouvent Alcman, Tyrtée, Archiloque et les Lesbiens Alcée et Sappho, dont les œuvres sont un peu mieux préservées. Le mieux connu et le plus reconnu dans l'Antiquité est le béotien Pindare, dont les poèmes célébrant les vainqueurs des concours sportifs ont été préservés. La poésie lyrique décline durant l'époque classique, où l'art poétique se retrouve dans les tragédies et comédies athéniennes[315].
La poésie hellénistique connue provient avant tout des milieux lettrés d'Alexandrie, dont les grands noms sont Théocrite, Callimaque, Apollonios de Rhodes (le récit épique les Argonautiques). S'y développent de nouvelles formes de poésie érudite et raffinée, avec des thèmes bucoliques, pastoraux, amoureux. Une forme plus réaliste de poésie se trouve dans les Mimes d'Hérondas. L'épigramme sous forme poétique, pièce courte sur des sujets variés, connaît une grande vogue à cette période et reste populaire à l'époque romaine impériale[316].
Pour l'Antiquité tardive, la principale œuvre poétique connue est l'épopée de Nonnos de Panopolis, les Dionysiaques. La poésie chrétienne en langue grecque s'est développée, à des fins liturgiques et missionnaires. Grégoire de Nazianze s'illustre dans une poésie plus érudite[317].
L'art de bien parler et de captiver son auditoire a acquis une grande importance dans le monde grec à l'époque classique, notamment pour convaincre les citoyens lors d'argumentations politiques, notamment dans les systèmes démocratiques (Athènes et Syracuse). Certes certains des spécialistes de rhétorique, les Sophistes, ont pu développer des techniques afin de tromper leurs auditeurs en les convainquant de tout et son contraire par une simple maîtrise de leur argumentation, mais cet art a souvent supposé de développer des arguments et une logiques très élaborés[318]. Les discours sont prononcés pour des décisions politiques, des litiges juridiques, des oraisons funèbres, les circonstances guidant la façon dont devait être élaboré le discours. Cet art implique de bien écrire et de bien déclamer. Les premiers théoriciens de la rhétorique (Gorgias, Isocrate, Aristote) ont produit d'importantes réflexions posant les bases de cet art, qu'il s'agisse de ses finalités ou de ses techniques. Sont définies les différentes phases de la rhétorique (invention, disposition, élocution, action et mémoire), les divisions d'un discours, les figures de style, des genres (épidictique, délibératif, judiciaire). Les discours des grands orateurs (les orateurs attiques tels que Démosthène et Lysias) ont été transmis afin de servir de modèles. Leur style est imité par les rhéteurs de la seconde sophistique, à l'époque romaine, durant laquelle l'art de bien discourir est très valorisé, reposant désormais plus sur la virtuosité et le style que la persuasion. La rhétorique grecque dispose encore d'illustres représentants durant l'Antiquité tardive (Libanios, l'école de Gaza). Entre-temps, elle a donné naissance à un art oratoire en latin (Cicéron, Quintilien) reposant sur les bases posées à l'époque grecque classique[319],[320].
La Grèce antique voit l'apparition d'une forme de pensée originale, la philosophie. Si on la résume généralement à un ensemble de doctrines formulées par des penseurs, c'est plus largement , donc , une discipline qui a une  (A. Motte)[321].
En dépit des différences entre les pensées philosophiques, se dégage une manière d'aborder le monde en mettant l'homme au centre de ses réflexions en le faisant acteur de sa propre destinée, qui est une des singularités de la Grèce antique par rapport aux civilisations antiques qui l'ont précédées, et aussi une de ses principales influences sur les civilisations postérieures[322]. L'apparition de ces « amis de la sagesse » (c'est le sens du mot philosophos) est donc traditionnellement vue comme un élément marquant du « miracle grec », et les causes derrière ce phénomène ont fait l'objet de nombreux débats. Une explication courante est la coïncidence avec l'émergence de la cité, qui établit une égalité des citoyens devant la loi et leur permet de s'exprimer dans des débats publics contradictoires, libérant ainsi les réflexions et la parole[323]. À la suite de Karl Jaspers, il a pu être tenté de relier ce phénomène à d'autres se produisant au même moment ailleurs (Israël, Inde et Chine) qui présenteraient une même approche mettant l'homme au centre de leurs préoccupations, formant un « âge axial », dont la réalité est débattue[324].
Il est généralement considéré que le premier philosophe est Thalès de Milet, qui a vécu dans les premières décennies du VIe siècle av. J.-C.. S'ouvre une première phase de l'histoire de la philosophie, dite « présocratique ». Le premier développement de la philosophie se fait en Ionie, puisqu'il est suivi par ses compatriotes milésiens Anaximandre et Anaximène, puis plus tard l'éphésien Héraclite et Anaxagore de Clazomènes. Cette région est un des principaux points de contact entre le monde grec et les civilisations orientales, et peut se nourrir de ces influences intellectuelles (notamment scientifiques) tout en les repensant. Il apparaît qu'ils sont en fait plus que des philosophes au sens moderne du terme, puisqu'ils font aussi évoluer les sciences (voir plus bas). Ces penseurs développent une philosophie de la « nature » (physis) s'interrogeant notamment sur les origines de l'univers en se détachant des explications traditionnelles, essentiellement surnaturelles. Il ne reste néanmoins quasiment rien de leurs écrits, en dehors de citations. Autour de 500 av. J.-C. se développe un nouveau pôle de la pensée, en Grande Grèce (Crotone, Élée, Agrigente), notamment à la suite de la venue dans cette région d'un des principaux penseurs antiques, Pythagore, originaire de Samos, qui développe le concept de cosmos, et fonde un courant de pensée qui porte son nom[325]. Un de ses disciples, Parménide, introduit une importante évolution dans la pensée grecque en développant une approche moniste (il n'y a qu'un seul principe formant le cosmos), et les philosophes suivants se positionnent face à sa proposition : il est suivi par son disciple Zénon, mais Empédocle d'Agrigente et Démocrite d'Abdère ont une approche pluraliste[326],[327],[328].
Athènes devient le centre de la philosophie à partir du milieu du Ve siècle av. J.-C., avec l'essor de son régime démocratique qui donne un élan aux débats et réflexions. La philosophie athénienne se dégage des préoccupations présocratiques sur la nature, pour se consacrer à la réflexion  (C. Mossé) [329]. La pensée est d'abord stimulée par la venue de Sophistes (Gorgias, Protagoras) qui se spécialisent dans l'art rhétorique, l'éducation et ont une approche morale relativiste, puis par un penseur athénien, Socrate, qui introduit une rupture majeure dans la philosophie, ceux qui lui succédant comme l'indique la césure entre philosophes « présocratiques » et « socratiques ». Sa pensée est surtout connue par les écrits de son disciple Platon. Il raisonne par le dialogue, considère que la vertu est dans le savoir, sa maxime étant le fameux  qui enjoint à l'homme de prendre conscience de sa propre mesure[330]. Platon (v. 427-347 av. J.-C.) et son disciple Aristote (v. 384-322 av. J.-C.) sont les deux philosophes grecs antiques qui ont le plus marqué la philosophie occidentale. Leur œuvre, prolifique, est connue par quelques dizaines de textes en prose, une grande partie étant perdue. Ils interrogent sur la place de l'homme dans la cité, donc la politique, le recherche de la perfection morale et de la vérité, l'éducation. Platon a porté à un nouveau stade de développement l'art du dialogue, avec la dialectique qu'il érige en méthode majeure du raisonnement philosophique. Aristote est aussi à l'origine du raisonnement scientifique par sa capacité de systématisation et son intérêt pour à peu près tous les domaines du savoir de son temps[331],[332].
Ces deux philosophes ont chacun fondé un lieu d'enseignement, l'Académie de Platon et le Lycée d'Aristote, qui recueillent leur héritage et structure les écoles de pensée qui se revendiquent d'eux. Mais la philosophie hellénistique voit le développement d'autres courants opposés. L'époque n'est plus vraiment à la réflexion sur la place dans la cité, mais plus sur la posture et le perfectionnement moral. Le cynisme (Diogène de Sinope) refuse ainsi l'implication politique. Le scepticisme met plus l'emphase sur le savoir et la vertu, tout comme l'épicurisme, qui doit son nom à Épicure (341-270), enseignant dans le « Jardin », qui recherche le bonheur par la satisfaction des seuls désirs basiques. Le stoïcisme, développé par Zénon de Kition (336-262), généralement considéré comme son opposé, qui professe la compréhension et l'acceptation du monde naturel sans laisser ses sentiments l'emporter[333].
Durant l'époque romaine, l'épicurisme et surtout le stoïcisme s'imposent comme des courants majeurs auprès des élites romaines[334], même si les Aristotéliciens et Platoniciens poursuivent leurs réflexions en adoptant diverses tendances. Le dernier courant philosophique important de l'Antiquité grecque est le néoplatonisme, apparu dans le courant du IIIe siècle à la suite des réflexions de Plotin (un Grec d'Égypte), qui donnent un tournant encore plus métaphysique au platonisme. Les autres philosophes majeurs de ce courant sont Porphyre de Tyr et Jamblique. Cette école comme les autres courants philosophiques déclinent face à l'essor du christianisme qui apprécie très peu leurs réflexions « païennes », leur fin symbolique dans le monde grec étant la fermeture des écoles athéniennes par décision de Justinien en 529, même si Alexandrie reste un centre de philosophie pendant un bon siècle. Le néoplatonisme conserve une influence notable à l'époque byzantine[335],[336].
Le mot histoire vient du nom grec de l'ouvrage d'Hérodote (v. 480-425 av. J.-C.), Historiai, « enquêtes », ce qui vaut à cet auteur le surnom de « père de l'Histoire ». Il a peut-être pris pour modèle les œuvres d'Hécatée de Milet (v. 550-480), dont il ne reste plus rien. Il s'agit alors de travaux réunissant un vaste ensemble d'information traitant d'événements historiques, d'anecdotes édifiantes et extraordinaires (souvent peu crédibles), aussi des descriptions de peuples et de leurs pays. Hérodote édifie son œuvre autour des guerres médiques, cherchant à expliquer le triomphe des Grecs face aux Perses[337].
Après lui la littérature historique est plus spécialisée sur un sujet ou registre, notamment les histoires de conflits et de peuples ou personnages, et connaît constamment des évolutions[338]. Thucydide (v. 460-400/395) fait ainsi le récit de la guerre du Péloponnèse, voulant en tirer des leçons pour la postérité, avec des réflexions plus rationnelles que celles de son prédécesseur. Xénophon (v. 430-355) entend poursuivre son œuvre, mais on lui reconnaît généralement moins de talent. D'autres développent ensuite des récits avec un cadre plus large (Éphore de Cumes, Théopompe), mais ils ne sont connus que par des fragments. Polybe (v. 208-126) remet en avant les réflexions sur les évolutions historiques, dans son histoire de l'ascension de la puissance romaine[339]. Pour les époques hellénistique et romaine, près d’un millier d’auteurs d’ouvrages historiques sont identifiés, signe de la popularité du genre, mais leurs travaux ne sont pour la plupart connus que par des citations ou paraphrases dans d’autres œuvres. Ceux dont au moins une partie des œuvres a survécu sont Diodore de Sicile, Denys d'Halicarnasse, Appien, Arrien, Dion Cassius, Hérodien et Plutarque. Parmi les historiens de langue grecque issus de populations hellénisées se trouvent le judéen Flavius Josèphe et l’égyptien Manéthon[340]. Durant l’Antiquité tardive des historiens tels que Zosime et Procope de Césarée assurent la continuation de cette tradition[341], et Eusèbe de Césarée la christianise en posant les bases de l’histoire ecclésiastique[342].
Le mot geographia d'où provient géographie est quant à lui forgé à l'époque hellénistique par Ératosthène (v. 276-198 av. J.-C.), pour désigner une étude de la Terre. Cela concerne aussi bien le corps céleste que sa surface, et un géographe antique peut aussi bien être l'auteur d'un traité que d'une carte[343]. L'origine de la cartographie est attribuée à Anaximandre, tandis que la géographie descriptive au sens moderne dérive comme l'histoire des travaux d'Hécatée et d'Hérodote, qui comprennent aussi des éléments d'ethnographie et anthropologie, rendant difficile leur catégorisation suivant des critères modernes. Cela se retrouve aussi dans des récits de voyages (des « périples ») qui font progresser la connaissance des régions du monde. Les recherches cartographiques et descriptions géographiques progressent par la suite, comme l'illustre la monumentale Géographie de Strabon (v. 60 av. J.-C.-20 ap. J.-C.). Ératosthène introduit les mathématiques dans la discipline, par sa tentative de mesure de la terre. Plus tard Marin de Tyr et Ptolémée précisent encore la connaissance du monde[344].
Le mot théâtre vient du grec theatron « lieu d'où l'on regarde », et désigne donc dans l'Antiquité la structure comprenant les lieux de représentation, à savoir l’orchestra (où jouent le chœur, les acteurs et les musiciens) et la skenè (à la fois coulisses et second espace de jeu pour les acteurs), et l'auditorium (koilon), espace d'accueil du public, à flanc de colline sans aménagement et parfois avec des gradins de bois, puis des gradins en dur (pierre, brique) à la fin de l'époque classique. Le théâtre en tant que bâtiment devient un élément caractéristique des cités grecques à partir de l'époque hellénistique. Le théâtre en tant que spectacle est une désignation moderne, puisque les Anciens distinguent trois genres : la tragédie, la comédie, et le drame satyrique. Il est surtout connu pour Athènes, lieu d'origine des principaux auteurs de pièces antiques. L'activité théâtrale est liée comme bien d'autres aux festivités religieuses, à savoir celles placées sous les auspices de Dionysos, les Dionysies et Grandes Dionysies, qui donnent lieu à des concours théâtraux. Durant l'époque hellénistique les concours théâtraux sont de plus en plus dédiés à d'autres divinités (Apollon, les Muses, Asclépios). Le théâtre est une manifestation politique, propre à une cité durant ses origines, en particulier à Athènes où les œuvres renvoient souvent à la vie politique, il a également des aspects sociaux puisque toute la communauté se retrouve pour les grandes représentations (certains théâtres peuvent accueillir des milliers de spectateurs), et économique en raison des coûts engagés pour les constructions et la mise en scène (financement par la chorégie à Athènes, sur les deniers d'un riche bienfaiteur)[345].
Les grands noms de la tragédie athénienne du Ve siècle av. J.-C. sont Eschyle, Sophocle et Euripide, tandis que la comédie est à la même époque marquée par l’œuvre d'Aristophane. La production du siècle suivant n'a quasiment pas été conservée, la seule exception étant Ménandre, auteur comique majeur de la fin du IVe siècle av. J.-C.[346]. Si la création théâtrale en grec se poursuit après, elle n'a manifestement pas marqué les esprits, tandis que les grands auteurs athéniens sont devenus des classiques, dont les pièces sont rejouées et, mises par écrit, constituent un des éléments fondamentaux de la paideia[347]. Dans les cités grecques d'Italie méridionale, se développe au IVe siècle av. J.-C. une sorte de farce, le jeu de Phlyax, et qui semble se diffuser à Alexandrie[348].
Pour ce qui est des pièces en elle-même, elles sont depuis le milieu du Ve siècle av. J.-C. jouées par trois acteurs, tous des hommes, tenant plusieurs rôles identifiés pas le masque qu'ils portent, accompagnés d'un chœur qui comprenait de douze à quinze personnes, et de musiciens. Les pièces athéniennes renvoient à la vie de la cité : les comédies recourent plutôt aux personnages réels (politiciens, magistrats, philosophes, artisans, etc.), puisent dans un répertoire de personnages stéréotypés (vieillard, jeune homme, jeune fille, courtisane, soldat, etc.) avec des intrigues imaginaires, alors que les tragédies se tournent surtout vers le répertoire mythologique (parfois des événements réels, militaires), reposant plus sur l'organisation de l'intrigue que la psychologie des personnages, avec des réflexions qui renvoient souvent au moment de leur rédaction. Les tragédies suivent un schéma similaire, traitent en général d'un épisode unique sur un temps court, même si des trilogies permettent parfois de développer une histoire sur un temps plus long (l'Orestie d'Eschyle)[349].
Très populaires durant l'époque hellénistique (notamment les comédies de Ménandre), quand le théâtre est devenu un élément caractéristique des cités grecques, encore courantes (mais mal documentées) durant l'époque romaine, les représentations théâtrales semblent se faire de moins en moins fréquentes entre le IIIe siècle et le VIe siècle[350]. Les spectacles de mimes et pantomimes connaissent en revanche un essor à l'époque romaine et sont mêmes intégrés aux concours à partir du IIe siècle[351]. Au IVe siècle les représentations théâtrales semblent encore courantes dans une ville comme Antioche. Avec la christianisation elles font l'objet des condamnations des penseurs chrétiens, qui les voient comme des spectacles obscènes et décadents[352].
Relief en marbre, représentant Dionysos (à droite) et des acteurs tenant des masques (à gauche), voué au dieu après une représentation. Le Pirée, v. 400 av. J.-C. Musée national archéologique d'Athènes.
Le théâtre de Pergame, IIIe siècle av. J.-C.
Les gradins du théâtre d'Argos.
Plan du complexe du théâtre de Babylone, époque hellénistique.
Figurine en bronze d'un acteur tenant un masque, v. 150-100 av. J.-C. Walters Art Museum.
Figurine en terre cuite d'un masque de théâtre représentant Dionysos. Myrina, IIe – Ier siècle av. J.-C. Musée du Louvre.
Scène de pièce de phlyax : maître (à droite) et esclave (à gauche), cratère en calice à figures rouges de Sicile, v. 350-340 av. J.-C. Musée du Louvre.
Le sport est en Grèce une affaire individuelle, découlant d'un esprit de compétition, visant à l'importer sur les rivaux. La pratique sportive récréative a peu sa place dans ce contexte, même si elle a pu exister. On parle alors d'« athlétisme », notion qui renvoie justement à la lutte et la compétition, et cela inclut plus que l'athlétisme moderne, puisqu'on trouve lors des concours sportifs : des sports de course (distingués par distance : stade, double stade, douze tours de stade ; aussi la course en armes), des sports de combat (pugilat, lutte et pancrace), le pentathlon qui combine course (stade), lutte, lancers du disque et du javelot, et saut en longueur. Les compétitions équestres y rentrent également. L'athlétisme constitue une part importante de l'éducation grecque, et les équipements sportifs destinés aux citoyens, le gymnase et la palestre, sont des lieux caractéristiques des cités grecques. Les athlètes professionnels suivent un entraînement plus poussé, avec un entraîneur (généralement leur père et/ou des anciens vainqueurs de concours), qui implique également une alimentation adéquate. L'athlétisme se pratique généralement nu, avec le corps couvert d'huile pour éviter la poussière. Le physique des athlètes suscite souvent l'admiration, notamment pour son attrait érotique. Cette activité est essentiellement masculine, mais les femmes n'en sont pas exclues, et elles peuvent participer à une compétition, la course, lors des concours d'Olympie (Héraia)[353],[354].
L'« athlétisme » est la forme privilégiée de concours et compétitions (agones) qui sont caractéristiques de l'esprit de la Grèce des cités, mais qui concernent aussi la musique, la poésie, le théâtre. Les concours panhelléniques, qui se produisent lors de fêtes religieuses, sont ainsi des moments majeurs du monde grec, à commencer par ceux d'Olympie, les « Jeux olympiques », fondés selon la tradition en 776 av. J.-C., auxquels s'ajoutent au fil du temps les « Jeux pythiques » de Delphes, les concours de l'Ishtme de Corinthe, et ceux de Némée. Formés durant l'époque archaïque, ils constituent un « circuit » (periodos), se déroulent dans des stades grandioses spécialement aménagés pour eux, attirent une foule importante, et les vainqueurs de ces compétitions en tirent un immense prestige. Les concours athlétiques des Grandes Panathénées d'Athènes sont courus, mais pas autant. Durant l'époque hellénistique et l'époque romaine de nombreux concours athlétiques sont constitués, d'importance généralement locale ou régionale, devenant une caractéristique de l'hellénisme tardif, pleinement adoptée par les Romains. La christianisation contribue au déclin de la plupart de ces concours et spectacles, qui disparaissent au VIe siècle[353],[355].
L'époque romaine voit également le développement dans le monde grec des spectacles de sport et combat sous de nouvelles formes venues de Rome : les courses de char, les chasses, combats d'animaux et de gladiateurs. Les premières y ont plus de succès que les autres, et sont encore très prisées durant l'époque byzantine, alors que la christianisation a mis fin aux spectacles violents[351].
Coureurs participant au concours des Panathénées, vase, v. 530 av. J.-C. Staatliche Antikensammlungen.
Statue d'une jeune coureuse, victorieuse à un concours. Copie romaine d'un original du Ve siècle av. J.-C.. Musées du Vatican.
Discobole Lancelotti, copie romaine du Discobole de Myron (Ve siècle av. J.-C.) Palais Massimo alle Terme.
Pugilistes en garde, coupe; v. 470 av. J.-C., musée du Louvre.
Athlètes au gymnase, Cratère attique, fin du VIe siècle av. J.-C. Musée de Berlin.
Ruines du gymnase de Cos.
Le stade de Delphes, servant pour les jeux pythiques.
La sculpture des âges obscurs est peu documentée, peut-être parce qu'elle était réalisée essentiellement sur bois, matériau périssable qui a disparu. Un groupe de statues de divinités de Dréros (Crète) en bronze plaqué sur bois, date d'environ 750 av. J.-C. La sculpture crétoise se développe au siècle suivant avec le style dédalique, aux formes angulaires, d'inspiration orientale. Dans les Cyclades la sculpture de la même période cherche plutôt son inspiration du côté de l’Égypte, adaptant la statuaire masculine égyptienne pour créer les statues en marbre de jeunes hommes, kouroi (singulier kouros), nus et sur pied, caractéristiques de l'art grec archaïque. Ce style se diffuse rapidement à l'est et sur le continent. On crée aussi des statues de jeunes filles, korè, puis les formes dédaliques sont abandonnées au profit d'une recherche de réalisme. La sculpture de style archaïque (v. 600-480 av. J.-C.) de jeunes hommes témoigne d'une volonté de transcrire sur pierre les idéaux de beauté physique, qui varient selon les préférences des écoles, qui se trouvent dans les îles (Naxos, Samos) et sur le continent (Béotie, Athènes). Du côté des jeunes femmes, habillées, les sculpteurs se concentrent sur les visages et les formes des habits, surtout à partir du moment où sont représentés les vêtements amples (chiton et himation) offrant des possibilités de jouer sur les drapés. Les commandes à cette époque sont se font essentiellement pour des finalités religieuses, accompagnant le développement des sanctuaires. Pour le décor des temples, se développe une sculpture architecturale sur calcaire et marbre, en bas ou haut-relief, représentant des scènes et créatures mythologiques, en développant les façons de représenter des scènes frappantes et dramatiques en jouant sur les postures des personnages. Pour la sculpture en pierre, le marbre est de plus en plus employé. Se développe aussi une statuaire en métal, moins bien conservée car les métaux ont généralement été refondus. À la fin de l'époque archaïque, la créativité explose, chaque école cherchant à innover et à expérimenter, les styles changeant vite y compris dans un même atelier[356],[357].
« Triade de Dréros », VIIIe siècle av. J.-C. Musée archéologique d'Héraklion.
La « Dame d'Auxerre », Crète, style dédalique, v. 640-630 av. J.-C. Musée du Louvre.
Statue de Zeus représenté avec des éclairs dans les mains. Glyptothèque de Munich.
Cléobis et Biton. Vers 580 av. J.-C. Musée archéologique de Delphes.
Le Moschophore (« porteur d'agneau »), statue datée de v. 570-550 av. J.-C. Musée de l'Acropole d'Athènes.
Phrasikleïa, korè polychrome, v. 550 av. J.-C. Musée national archéologique d'Athènes.
Frise du trésor de Siphnos représentant une gigantomachie, Delphes, v. 525 av. J.-C. Musée archéologique de Delphes.
La sculpture de l'époque classique (v. 480-330 av. J.-C.), généralement tenue pour l'apogée de l'art grec, s'oriente vers des rendus plus réalistes, naturalistes, aussi une grande attention pour la narration, et aussi une quête d'intériorité, mêlant sagesse et modération (la sophrosynè), en lien avec les réflexions sur l'homme qui se produisent au même moment. Les matériaux travaillés sont encore le marbre et le bronze, la plupart des œuvres originales ont disparu, mais le prestige qu'elles ont rapidement acquis fait qu'elles ont été copiées par la suite (y compris les statues en bronze copiées sur pierre), ce qui permet de les connaître. Parmi les principaux sculpteurs du Ve siècle av. J.-C. (qui comprend la période du « classicisme » au sens strict, surtout v. 450-420), Polyclète d'Argos (actif v. 470-420) s'illustre dans ses représentations de nus masculins pour lesquels il élabore un nouveau canon ; son contemporain Phidias d'Athènes, maître d’œuvre du chantier de l'Acropole, est célébré pour ses représentations du divin, notamment sa statue chryséléphantine d'Athéna qui trône dans le Parthénon. Ses élèves poursuivent la décoration des temples athéniens durant les temps difficiles de la guerre du Péloponnèse (période d'activité de Callimaque). Après la guerre la sculpture athénienne concerne essentiellement les stèles funéraires. Les grands sculpteurs du IVe siècle av. J.-C. (notamment le « second classicisme », après 370) sont Praxitèle, connu pour avoir développé le nu féminin avec sa statue d'Aphrodite, Scopas, qui dirige le programme sculptural du Mausolée d'Halicarnasse, puis Lysippe (actif v. 370-310) qui révolutionne la représentation du nu masculin, et s'illustre par ses portraits détaillés (notamment d'Alexandre le Grand)[358],[359].
Frise des Panathénées, Parthénon d'Athènes, v. 445‑435 av. J.-C. British Museum.
Dieu du cap Artémision, bronze, vers 460 av. J.-C.. Musée national archéologique d'Athènes.
Aurige de Delphes, vers 470 av. J.-C., bronze. Musée archéologique de Delphes.
Dionysos allongé, atelier de Phidias, fronton du Parthénon, v. 447–433 av. J.-C. British Museum.
Stèle funéraire attique d'Hègèsô, attribuée à Callimaque. Musée national archéologique d'Athènes.
Monument des Néréides, reconstitution de la façade. British Museum.
Aphrodite Braschi, du type de l'Aphrodite de Cnide, attribuée à Praxitèle. IVe siècle av. J.-C., glyptothèque de Munich.
L'« éphèbe de Marathon », v. 330/325 av. J.-C., attribué à l'école de Praxitèle. Musée national archéologique d'Athènes.
Tête d'Hygie, parfois attribuée à Scopas, provenant du temple d'Athéna-Aléa à Tégée. Musée national archéologique d'Athènes.
Jeune hoplite, stèle funéraire athénienne, v. 350–325 av. J.-C. Musée national archéologique d'Athènes.
Hermès d'Atalante, copie romaine d'une œuvre attribuée à Lysippe. Musée national archéologique d'Athènes.
L'expansion du monde grec durant l'époque hellénistique offre de nouvelles opportunités aux sculpteurs, qui puisent leur inspiration dans les maîtres des époques précédentes, développant pour certains des styles éclectiques, empruntant à l'un et à un autre, tandis que d'autres s'en tiennent à un classicisme plus prudent restant plus proche d'un modèle. À Alexandrie les portraits royaux dégagent une impression de sérénité supra-humaine, alors que les stèles funéraires sont de type attique ; à Pergame se développe un art réaliste, autour d'Épigone qui réalise des œuvres célébrant les exploits guerriers du royaume[360]. La sculpture hellénistique se retrouve jusqu'aux cités des confins du monde indien et de l'Asie centrale, notamment sur le site afghan d'Aï Khanoum, où elle devait par la suite donner naissance à l'art gréco-bouddhique. Parmi les œuvres célèbres de la période on compte la Victoire de Samothrace (v. 190 av. J.-C.), de style baroque qui témoigne d'une volonté de mise en scène dramatique. Le Faune Barberini (v. 230-200) illustre l'émergence d'une statuaire plus fantaisiste, prisée par les élites dans un cadre privé. La Vénus de Milo (v. 100 av. J.-C.) est un nu féminin de style néoclassique, à la manière de Praxitèle[361].
Statue d'une reine lagide (Arsinoé III ?). Altes Museum de Berlin.
Statuette d'Héraclès en bronze, provenant du temple principal d'Aï Khanoum (Afghanistan), IIe siècle av. J.-C.
La Victoire de Samothrace, musée du Louvre.
Faune Barberini, Glyptothèque de Munich.
La Vénus de Milo, musée du Louvre.
Le groupe du Laocoon, original hellénistique ou copie romaine à partir d'un modèle hellénistique. Musée Pio-Clementino.
Statue de bronze d'un cheval et de son jeune jockey, épave du Cap Artémision, v. 150 av. J.-C. Musée national archéologique d'Athènes.
Les guerres de conquête romaines sont par bien des aspects dévastateurs pour la sculpture grecque : des centaines de statues sont prises et emportées en Italie où elles sont au goût des élites. Cela s'accompagne d'une demande croissante pour des copies d’œuvres classiques, qui garnissent les carnets de commande des ateliers du monde grec. Ce marché ne se tarissant pas avec le temps, c'est par ce biais que la plupart des œuvres des grands sculpteurs grecs sont connues. Durant cette basse époque hellénistique est aussi réalisé le grand autel de Pergame, dont les frises aux accents baroques représentent un gigantomachie et la fondation mythique de la cité. La sculpture privée est attestée dans les riches demeures de Délos. Après les guerres mithridatiques, beaucoup de sculpteurs grecs s'installent en Italie, où ils créent les dernières écoles hellénistiques[362].
Haut-relief du grand autel de Pergame : Athéna et Nikè contre Alcyon. Pergamon Museum.
Gaulois blessé de Délos (école de Pergame). Musée national archéologique d'Athènes.
La sculpture d'époque romaine est par bien des aspects ancrée dans le passé, marquée par la copie ou l'imitation de modèles anciens, suivant les styles classiques et hellénistiques. Les ateliers sont implantés en Grèce (notamment Athènes) et en Asie Mineure, mais les artisans peuvent se déplacer dans tout l'empire, où se retrouve leur travail. Parmi les grands chantiers de l'époque comprenant un programme ambitieux de sculptures se trouve par exemple le Sébasteion d'Aphrodisias en Carie. Les ateliers grecs sont également d'importants centres de production de sarcophages sculptés produits à destination des élites romaines[363].
Statuette d'Athéna, copie du Ier siècle de l'Athéna Parthénos de Phidias. Musée national archéologique d'Athènes.
Statue de type « Grande Herculanaise », copie du IIe siècle d'après un modèle du IVe siècle av. J.-C. Musée archéologique d'Héraklion.
Anchise et Aphrodite, relief du Sebasteion d'Aphrodisias, Ier siècle. Musée archéologique d'Aphrodisias.
Sarcophage d'Hercule, Pergé, v. 200. Détail : le héros affronte le lion de Némée et l'hydre de Lerne. Musée archéologique d'Antalya.
Sarcophage attique représentant la chasse du sanglier de Calydon. Musée archéologique d'Éleusis.
La coroplathie, la production de figurines moulées en terre cuite, représente un versant plus populaire de la sculpture. Elle a connu un développement important dans le monde grec, en particulier à l'époque hellénistique. Le principal centre de production connu est Tanagra en Béotie, et on désigne souvent ces figurines comme des « tanagras », mais il n'était pas le seul, loin de là, puisque des centres de production importants ont été identifiés en Asie Mineure (Myrina, Smyrne, Tarse). Ces figurines ont avant tout pour but d'être offertes à des divinités ou des défunts, mais elles peuvent avoir une fonction décorative. Elles représentent souvent des jeunes filles, des éphèbes, des enfants, des divinités (Éros, Aphrodite, la Victoire), s'inspirant couramment du style de sculpteurs renommés[364].
Jeune femme drapée appuyée contre un pilier, production de Tanagra, fin IIIe siècle av. J.-C. ou début IIe siècle av. J.-C.. Musée du Louvre.
Femme assise, production de Tanagra ou Corinthe, v. 325-150 av. J.-C. Collection des Antiquités de Berlin.
Apollon ou Adonis, figurine de Smyrne, Ier siècle av. J.-C.. Musée national de Varsovie.
Aphrodite, production de Myrina (?), IIe siècle av. J.-C. Altes Museum de Berlin.
Éros drapé, production de Myrina, seconde moitié du Ier siècle av. J.-C. Musée du Louvre.
Brûle encens représentant Attis, production de Tarse, IIe siècle av. J.-C. ou Ier siècle av. J.-C. Musée du Louvre.
La céramique peinte est une caractéristique du monde égéen dès les époques minoenne et mycénienne, mais les formes et styles sont bouleversés durant les âges obscurs. Après une phase « submycénienne » encore très marquée par les traditions antérieures, le proto-géométrique et le géométrique (v. 1050-700 av. J.-C.) sont caractérisés par des décors peints en noir sur fond beige faits de bandes, pouvant couvrir tout le vase, et des scènes figurées se développent au VIIIe siècle av. J.-C. (scènes funéraires, processions de chars, batailles), des styles locaux ressortent (notamment l'Attique où les nécropoles ont fourni un matériel céramique abondant, aussi Argos) et des « mains » d'artistes (le « Maître du Dipylon ») commencent à se déceler. Le style orientalisant (v. 700-600 av. J.-C.), développé autour d'ateliers corinthiens, attiques et dans la partie orientale du monde grec, intègre des éléments proche-orientaux (motifs floraux et animaux) qui supplantent les motifs géométriques et développe la polychromie. Les représentations figurées deviennent plus complexes. Après 600 se développe le style à figures noires, comme son nom l'indique caractérisé par des scènes peintes en noir, parfois rehaussées d'autres couleurs ou incisées pour souligner les détails, et représenté sur des vases de formes diverses (notamment des coupes et cratères) ; c'est pour cette période que les premiers noms d'artistes sont connus (Sophilos, Exékias), et que se développe l'habitude d'inscrire les noms des potiers, peintres et/ou chefs d'atelier sur les vases. Une poterie à figures noires de qualité est également produite en Laconie à cette période. Vers 525 se développe le style attique à figures rouges (avec le « Peintre d'Andokidès » et Psiax), qui inverse le schéma chromatique précédent puisque cette fois-ci le fond et les détails sont peints en noir et les personnages laissés de la couleur rouge de l'argile du vase, d'autres couleurs étant de plus en plus utilisées pour les détails (surtout le blanc). Des artistes tels qu'Euphronios développent des représentations anatomiques plus détaillées, permises par la nouvelle technique qui permet un rendu plus précis des détails de la musculature. La peinture attique a notamment un grand succès en Italie, où elle est importée puis imitée. La céramique du début de l'époque classique poursuit sur ces bases, développe les scènes intimes (notamment de gynécées), puis la céramique peinte sur fond blanc (notamment pour les lécythes), et la polychromie fait son retour. Alors qu'Athènes décline après la guerre du Péloponnèse, les ateliers de Grande Grèce prennent l'ascendant au début du IVe siècle av. J.-C. en produisant une céramique à figures rouges aux scènes riches. La céramique à figures rouges est abandonnée à la fin du IVe siècle av. J.-C., et l'époque hellénistique voit d'une manière générale le déclin de la céramique peinte de qualité. Les représentations sont généralement limitées à des frises et motifs floraux, mais on réalise aussi des décors en relief moulés sur des vases[365],[366]. Après émerge la céramique à vernis rouge classique de l'époque romaine, la sigilée, à décor fait de motifs imprimés, dont les ateliers de céramistes du monde grec (en particulier l'Asie Mineure) participent à la production en masse[367].
Amphore de style proto-géométrique, v. 950-900 av. J.-C. British Museum.
Cratère de style géométrique, début VIIIe siècle av. J.-C. Metropolitan Museum of Art.
Alabastre de style orientalisant, v. 620-590. Musée archéologique de Corinthe.
Prométhée et Atlas, kylix à figures noires du Peintre d'Arcésilas, Laconie, v. 560-550 av. J.-C. Musée grégorien étrusque.
Achille et Ajax jouant, vase à figures noires d'Exékias, v.540–530 BC, Musées du Vatican.
Héraclès au repos, amphore du Peintre d'Andokidès, v. 520 av. J.-C. Staatliche Antikensammlungen de Munich.
Une Muse jouant de la cithare, lécythe attique à fond blanc. « Peintre d'Achille », v. 440-430 av. J.-C. Staatliche Antikensammlungen
Canthare à figures de femme et de satyre attribué à Aison. Spina (Italie), v. 420 av. J.-C. Metropolitan Museum of Art
En plus des céramiques, la peinture se retrouvait également sur les sculptures antiques, bien qu'elle en ait généralement disparu. La grande peinture sur murs ou panneaux de bois est surtout connue par des descriptions antiques (Pline l'Ancien, Pausanias) et les copies qui en ont été faites sur mosaïques. Des trouvailles archéologiques, surtout dans un contexte funéraire telles celles des tombes macédoniennes de Vergina et Agios Athanasios, ont depuis précisé la connaissance sur cet art majeur. Les peintres et leur art jouissent en effet d'un statut important dans l'Antiquité. Les textes antiques en ont préservé des grands noms. Cimon de Cléones aurait été un des pionniers à la fin du VIe siècle av. J.-C., puis Polygnote de Thasos et Micon développent cet art à Athènes au début de l'époque classique, représentant avant tout des thèmes mythologiques. Apollodore d'Athènes s'illustre à la fin du Ve siècle av. J.-C. par son travail sur l'ombre et la lumière, puis se développent l'école ionienne (Zeuxis d'Héraclée, Parrhasios d'Ephèse) et surtout l'école de Sicyone, qui s'illustre dans les portraits individuels et l'expression des sentiments (la peinture devant rapporter les traits physiques et moraux du sujet), notamment avec Apelle de Cos qui travaille pour Alexandre. Les peintures de Vergina, réalisées vers la même période, représentent des thèmes mythologiques (rapt de Perséphone par Hadès) et de chasse. Durant l'époque hellénistique la peinture murale est employée dans un cadre privé, représentant des thèmes floraux, des scènes de genre, des représentations architecturales, parfois des thèmes grotesques ou érotiques[368],[369].
Les élites romaines commanditent des copies des peintures grecques, comme cela se voit à Pompéi et Herculanum, y compris sur mosaïque puisqu'il est estimé que la fameuse « mosaïque d'Alexandre » de Pompéi est une copie d'une peinture hellénistique renommée. Les Romains font aussi venir des peintres grecs en Italie (Métrodore d'Athènes ou encore Iaia de Cyzique, une des rares femmes peintres dont le nom soit connu) pour satisfaire leur demande. De ce fait les styles des débuts de l'époque impériale romaine dérivent de ceux de la peinture hellénistique tardive. La peinture du Haut Empire est mal connue ; pour le monde grec elle est surtout attestée par les nombreux portraits funéraires du Fayoum[370].
Hadès enlevant Perséphone, peinture de Vergina (Aigai), v. 340-330 av. J.-C.
Scène de banquet, tombe d'Agios Athanasios (près de Thessalonique), v. 325-300 av. J.-C.
La « mosaïque d'Alexandre », Pompéi, copie d'une peinture hellénistique. Musée archéologique national de Naples.
L'art de la mosaïque apparaît en Grèce durant l'époque classique, peut-être à partir de modèles anatoliens, avec des mosaïques de galets. Il est attesté à la fin du Ve siècle av. J.-C. à Olynthe et Corinthe, reproduisant des motifs géométriques et scènes figurées, cherchant sans doute à imiter les motifs des tapis qui ornaient les riches demeurent. Les mosaïques des sols des maisons riches de Pella, dans la seconde moitié du IVe siècle av. J.-C., témoignent du développement de cet art, avec une extension du répertoire chromatique et iconographique, aboutissant à la réalisation de scènes de grande qualité, s'inspirant sans doute des peintures. La période hellénistique voit le développement des mosaïques en tesselles, petites pièces de marbre ou autre pierre, peut-être originaire de Sicile, où elle est attestée en premier. Cela conduit à un perfectionnement de l'art de la mosaïque, qui rivalise avec la peinture par l'inventivité de ses compositions, constituées d'un panneau central encadré par des motifs végétaux ou géométriques. Il s'en trouve sur les sols des maisons riches, et également des bâtiments publics, notamment à Alexandrie, Pergame, Délos. Au-delà de leur aspect décoratif, elles manifestent le statut social du commanditaire, et sont surtout employées dans un contexte privé pour les salles de banquet. Le technique se diffuse dès cette époque dans le monde romain, où elle devient murale, et sert comme vu plus haut pour des reproductions de tableaux célèbres dans les demeures de Pompéi. Pour le monde grec romain, l'art de la mosaïque est bien attesté en Syrie dans la région d'Antioche, à Zeugma et Apamée, et ce jusqu'au début de l'époque byzantine. Il est également adopté au Levant pour des synagogues et des églises chrétiennes[371],[372].
Mosaïque représentant Bellérophon, Olynthe, fin du Ve siècle av. J.-C.
Mosaïque représentant une chasse au daim, Pella, fin du IVe siècle av. J.-C.
Mosaïque représentant la Méduse, Palais des grands maîtres de Rhodes, IIe siècle av. J.-C.
Mosaïque d'une panthère, Délos, v. 100 av. J.-C. Musée archéologique de Délos.
L'épiphanie de Dionysos, mosaïque de Dion, IIe siècle. Musée archéologique de Dion.
Le rapt d'Europe, mosaïque de Zeugma, IIe siècle-début IIIe siècle. Musée de Zeugma, Gaziantep.
« Mosaïque du Phénix », provenant d'une résidence de Daphné (faubourg d'Antioche), fin du Ve siècle. Musée du Louvre.
L'art de l'Antiquité tardive se caractérise en effet par ses mosaïques de grande qualité destinées aux églises, dont l'exemple le plus célèbre sont les mosaïques de la basilique Saint-Vital de Ravenne commanditées par Justinien. Les thématiques et motifs chrétiens prennent de plus en plus d'importance dans l'imagerie de l'époque (Christ, anges, croix). La sculpture se poursuit sur les bases gréco-romaines, avec une grande importance accordée aux portraits des empereurs et des élites, avant d'évoluer vers des formes plus stylisées et expressives. La peinture trouve de nouveaux supports d'expression avec les enluminures sur manuscrit et les icônes sur bois[373]. D'un autre côté la christianisation fait que représentations mythologiques comme les nus s'effacent progressivement, ne survivant pour un temps que dans des arts « mineurs » luxueux et discrets (orfèvrerie, ivoire, tissus brodés)[374].
Après la fin de l'époque mycénienne, l'architecture grecque retourne à un stade rudimentaire en matériaux peu pérennes, les constructions monumentales étant très rares (Lefkandi, Thermos). Le retour d'une architecture monumentale se produit dans les sanctuaires, au début de l'époque archaïque, avec des structures absidiales puis rectangulaires, posant les bases de la forme classique du temple grec. Au VIIe siècle av. J.-C. les premiers édifices en pierre apparaissent. Des techniques d'extraction et de taille de la pierre sont introduites depuis l'Égypte vers la même époque, et la construction en pierre se développe, avec le premier développement des ordres architecturaux, le dorique et le ionien. Ils sont clairement visibles dans des temples du début du VIe siècle av. J.-C., quand sont fixées les bases du temple grec type, rectangulaire, entouré de colonnes (périptère), avec un porche d'entrée (pronaos), une pièce centrale rectangulaire où se trouve la statue divine (naos) et une sorte de faux porche à l'arrière (opisthodome) ; dans certains cas, notamment à l'ouest, se trouve une salle à l'arrière du naos, l'adyton. Dans les Cyclades le marbre est employé, plutôt le calcaire ailleurs, mais au VIe siècle av. J.-C. Athènes emploie aussi le marbre grâce à ses carrières du Pentélique. La pierre est peinte, le décor jouant sur le contraste entre le clair et le sombre, de plus en plus chargé de bas en haut[375],[376].
Les ordres architecturaux grecs sont l'. Leur classification nous vient des écrits de l'historien de l'architecture romain Vitruve (Ier siècle av. J.-C.), qui l'a reprise de traités grecs. L'ordre dorique, plus ancien (v. 550 av. J.-C.), se caractérise par son aspect géométrique, ses colonnes sans base d'une vingtaine de cannelures coiffées de chapiteaux à échine profilée supportant un abaque rectangulaire. Les plans des temples sont marqués par la symétrie et l'axialité. L'ordre ionique est caractérisé par son chapiteau à volutes horizontales, supporté par des colonnes élancées, avec une base moulurée. L'ordre corinthien (apparemment créé à Athènes) est en fait une variation du précédent, caractérisée par ses chapiteaux décorés par des feuilles d'acanthe. En pratique, les règles sont suivies à peu près de la fin du VIe siècle av. J.-C. jusqu'à la première moitié du IVe siècle av. J.-C., mais après l'éclectisme domine et brouille les distinctions[377].
Plan et représentation du naos du temple d'Apollon à Bassae.
Les types de colonnes antiques (Grèce et autres régions) et la description des parties d'une colonne.
Colonne dorique du temple de Zeus à Olympie.
Chapiteau ionique du temple d'Artémis de Sardes. Metropolitan Museum of Art.
Chapiteaux et colonnes corinthiens de la Bibliothèque d'Hadrien à Athènes.
Les temples de l'époque archaïque sont mal conservés. De cette époque datent des constructions monumentales en Ionie (temple d'Artémis d’Éphèse, temple d'Héra de Samos) et dans les cités de Sicile[378]. Le Ve siècle av. J.-C. est dominé par le chantier de l'Acropole d'Athènes, autour du Parthénon, le temple de la déesse Athéna, et des bâtiments voisins (Propylées, Érechthéion, temple d'Athéna Nikè), traditionnellement tenus pour être la quintessence de l'architecture grecque classique. Mais des temples de style similaire et pas moins remarquables sont érigés partout dans le monde grec, en particulier les temples siciliens qui sont parmi les mieux conservés (Sélinonte, Poseidonia, Agrigente, Ségeste)[379]. Les grands sanctuaires panhelléniques et fédéraux (Olympie, Delphes, Délos) rassemblent également d’impressionnants groupes monumentaux[380].
Le Parthénon, temple d'Athéna à Athènes.
L'Érechthéion de l'Acropole d'Athènes.
Ruines du temple d'Apollon de Corinthe.
Péristyle du temple d'Apollon à Bassae.
Le temple de la Concorde à Agrigente.
Maquette du sanctuaire de Delphes. Musée archéologique de Delphes.
L'architecture des temples influence celle des autres bâtiments, dans une certaine mesure. On trouve ainsi des colonnades pour entourer des cours, ou bien soutenir des portiques (stoa), ayant diverses fonctions. Pour les constructions domestiques le principe de la cour centrale est adopté et bien établi au VIe siècle av. J.-C., et sert de point focal pour l'édifice. On les retrouve dans des bâtiments publics. On privilégie les formes simples, rectangulaires, pour les bâtiments et les cours. Les bâtiments curvilignes sont rares. Les bâtiments au plan complexe existent, consistant généralement en la juxtaposition d'unités rectangulaires (Propylées, Érechthéion à Athènes). La construction de théâtres avec des gradins en pierre débute durant la seconde moitié du Ve siècle av. J.-C. à Athènes[381]. À partir du IVe siècle av. J.-C., l'enrichissement des nantis leur permet de construire des demeures plus grandes et luxueuses, puis l'architecture palatiale fait son retour en Grèce avec la montée en puissance du royaume macédonien[376]. De cette même période et ce même royaume proviennent les tombes royales à voûte de Vergina, avec des façades peintes, dont l'aspect architectural imite celui des temples[375]. Autre tombeau monumental, le Mausolée d'Halicarnasse mêle influences des mondes grec et perse[382].
Le théâtre d'Épidaure, IVe siècle av. J.-C. ou début du IIIe siècle av. J.-C.
Façade de la tombe de Philippe II de Macédoine à Vergina (Aigai).
Maquette du mausolée d'Halicarnasse au musée d'archéologie sous-marine du château Saint-Pierre à Bodrum.
L'urbanisme « hippodamien », en plan en damier, s'est imposé depuis l'époque des colonies archaïques, et est employé pour les villes nouvelles de la fin de l'époque classique et l'ère hellénistique. Il est notamment attesté sur les sites d'Olynthe, Pella, Messène. Toute cité grecque qui se respecte se doit alors de posséder un ensemble distinctif de bâtiments administratifs (salle de conseil, tribunal), un gymnase, un théâtre, une agora et ses monuments (temple, autels, portiques), un stade[376].
Plan schématique de Pella.
Ruines d'Olynthe, laissant apparaître le plan hippodamien.
Ruines d'une stoa sur l'agora de Milet.
Ruines de l'agora de Ségeste.
Le Bouleutérion (salle de conseil) d'Apollonia d'Illyrie.
Le stade de Messène.
Durant l'époque hellénistique, les principes architecturaux définis durant les époques antérieures trouvent une application plus large. La brique crue et le bois restent les matériaux de constructions les plus courants, mais l'usage de la pierre se répand en dehors des temples, avec l'emploi de colonnes, et les bâtiments à cours se répandent et leurs arrangements se complexifient. Des variations par rapport aux techniques et styles traditionnels se développent, par exemple les chapiteaux en forme de feuilles de palmier à Pergame. Les chapiteaux corinthiens du monde séleucide ont des motifs ornementaux repris des traditions locales antérieures[381]. Le groupe monumental de Pergame, construit sur un relief escarpé, avec son grand autel, est l'exemple le mieux conservé d'ensemble architectural de la période[383]. La plus grande cité de l'époque, Alexandrie, est surtout connue par des descriptions, notamment celle de son vaste secteur palatial, et de son phare[384]. L'urbanisme et l'architecture hellénisants se retrouvent jusqu'en Afghanistan, sur le site d'Ai Khanoum[385].
Plan d'Alexandrie à l'époque hellénistique.
Proposition de reconstitution du phare d'Alexandrie.
Maquette de Pergame. Dans la ville haute : agora, palais, arsenal, bibliothèque, théâtre, temples, Grand Autel. Pergamon Museum.
Le grand autel de Pergame, reconstitué au musée de Pergame, Musées d'État de Berlin.
Le grand escalier de la cour du temple d'Apollon de Didymes.
Le théâtre du sanctuaire de Dodone, Épire.
Péristyle au sol décoré de mosaïque de la « Maison des dauphins », demeure cossue de Délos durant la basse époque hellénistique.
Chapiteau corinthien. Aï Khanoum (Afghanistan). Musée National d'Afghanistan, Kaboul.
L'époque romaine ne conduit pas à une évolution marquée dans l'architecture. Durant le Haut Empire les constructions sont nombreuses, la prospérité permet à des cités jusqu'alors peu dotées d'ériger des groupes monumentaux similaires à ceux des grandes villes. L'usage de la pierre est dominant, le béton est peu employé, peu de bâtiments de style romain sont attestés, en dehors des thermes[386], ou des temples spécifiquement romains, sur podium. Les bâtiments monumentaux du monde grec romain partent des styles hellénistiques ou classiques[381].
Section de l'aqueduc de Nicopolis d'Épire.
L'Odéon d'Hérode Atticus, Athènes, IIe siècle.
La fontaine Pirène de Corinthe.
Façade de la bibliothèque de Celsus à Éphèse.
Ruines du temple de Zeus Lepsynos à Euromos, IIe siècle.
Le Sebasteion d'Aphrodisias, temple dédié à Auguste.
L'amphithéâtre de Thasos.
Ruines des thermes d'Heraclea Lyncestis.
Les cités de l'Antiquité tardive disposent du même type de bâtiments que celles de l'époque du Haut Empire romain, qui sont construits et restaurés jusqu'à leur crise après la fin du VIe siècle[67]. La principale évolution est comme souvent pour cette période liée à la christianisation : les temples polythéistes laissent la place aux églises, dont les formes architecturales s'inspirent plutôt des bâtiments civils romains, la basilique rectangulaire et l'édifice circulaire ou polygonal à plan centré, parfois couverts d'une coupole, dont l'exemple le plus spectaculaire est Sainte-Sophie de Constantinople[387].
Basilique d'Aliki, Thasos.
Le baptistère du complexe épiscopal de Philippes, VIe siècle.
La science grecque antique se développe durant l'époque archaïque, autour des « physiciens » d'Ionie (Thalès, Anaximène, Anaximandre), qui, comme vu plus haut, savants qui peuvent aussi bien être définis comme des philosophes que comme des scientifiques. Ils reprennent les savoirs scientifiques des civilisations mésopotamienne et égyptienne, et les repensent dans un nouveau cadre conceptuel marqué par l'étude de la nature et une recherche d'explications en dehors des mythes (mais pas pour autant « rationnelles » du point de vue scientifique moderne)[388],[389]. Puis en Grande Grèce les spéculations quasi-mystiques des Pythagoriciens les orientent vers des considérations mathématiques, contribuant au développement de ce savoir. On attribue également à des penseurs de cette époque des traités de botanique, zoologie, astronomie qui sont perdus[390]. Là encore l'essor scientifique grec est relié au contexte de l'essor des cités : le débat y occupe une place prépondérante, comme dans la vie politique[391]. Le Ve siècle av. J.-C. est marqué par une spécialisation scientifique : plutôt que de chercher une explication globale aux phénomènes affectant le cosmos, les savants s'orientent vers des questionnements plus spécialisés dans un domaine, ce qui donne lieu à un développement de spéculations abstraites, avant tout mathématiques, et à un développement de l'observation et de l'empirisme, visible notamment dans les cercles hippocratiques qui donnent son essor à la médecine grecque, et dans les traités de botanique de l'école aristotélicienne[391]. Les savants grecs se sont couramment reposés sur la pratique de tests consciencieusement élaborés, des formes anciennes d'expériences scientifiques, afin de tester la véracité de théories, quand c'était possible (ce qui renvoie plus largement aux considérations sur l'utilisation des preuves) ; cela est notamment visible en optique, en zoologie, en anatomie[392]. Les sciences grecques connaissent une période florissante durant l'époque hellénistique, en particulier en astronomie et géométrie[393], et continuent leur développement durant l'époque romaine avec des savants de premier ordre tels que Claude Ptolémée et Galien.
Les mathématiques grecques se développent sur des bases proche-orientales, mais la discipline appelée mathematikè en elle-même constitue une rupture avec cet héritage se focalisant avant tout sur la pratique, en cela qu'elle repose sur la méthode déductive, ce qui semble caractéristique de l'esprit grec, avec son goût pour le débat et la persuasion, et pose les bases de la méthode scientifique moderne. Les anciens Grecs traçaient l'origine des mathématiques chez les penseurs archaïques, en particulier Thalès et Pythagore (prestige qui explique qu'on leur ait attribué leurs fameux « théorèmes » alors qu'ils étaient connus bien avant en Babylonie), mais il ne reste rien de leurs travaux dans ce domaine. On peut considérer que cela résulte du souci de ces penseurs d'intégrer les mathématiques à leurs réflexions philosophiques, les conduisant vers l'abstraction, notamment les Pythagoriciens qui cherchent à expliquer le cosmos par les nombres. Quoi qu'il en soit le premier grand développement des mathématiques grecs se fait durant l'époque hellénistique, là encore avec un arrière-plan très marqué par la recherche d'un ordre logique par les nombres, mais la discipline mathématique s'est clairement distinguée de la philosophie. Les traités des mathématiciens sont pensés comme des discours de persuasion, ils cherchent à élaborer des théorèmes voire des axiomes à partir de problèmes, comme cela est visible dans les travaux des grands mathématiciens de l'époque, Euclide (actif v. 300 av. J.-C.) et Archimède (v. 287-212), ou encore Apollonios de Perga (v. 240-190). Les recherches mathématiques des Grecs se concentrent sur les figures planes figurées dans un diagramme, accompagnées de raisonnements abstraits, et la recherche des proportions, dégageant notamment des ratios incommensurables et infinis. Les mathématiques « pures » existent certes chez certains auteurs, avec une focalisation sur la géométrie ou l'arithmétique, mais elles sont souvent employées pour d'autres formes de savoirs (astronomie et mécanique en particulier, optique). L'approche philosophique se retrouve dans les travaux de l'époque romaine, notamment chez Héron (qui emploie les mathématiques pour la mécanique) et Ptolémée (application à l'astronomie). Les mathématiques progressent durant les périodes tardives avec Diophante, Pappus et Théon d'Alexandrie[394],[395].
L'astronomie scientifique grecque se développe à partir de connaissances « traditionnelles » sur les astres (servant notamment pour élaborer les calendriers et le cycle agricole) et des spéculations cosmologiques des physiciens de l'époque archaïque, aussi l'introduction d'éléments astronomiques babyloniens (le Zodiaque, les constellations, le cycle métonique). Au IVe siècle av. J.-C. elle prend son essor avec les premières tentatives d'expliquer les mouvements des objets célestes par des modèles géométriques reposant sur des mouvements circulaires, qui se retrouve en particulier chez Eudoxe de Cnide. Parmi les développements suivants, Aristarque de Samos (actif v. 280 av. J.-C.) propose le premier système géocentrique connu. Hipparque (v. 145-125 av. J.-C.) bouleverse l'astronomie grecque en introduisant le principe des modèles astronomiques prédictifs, repris des astronomes babyloniens, mais en employant plus la géométrie que l'arithmétique à la différence de ce que faisaient ces derniers. Les travaux des trois siècles suivants sont moins bien connus, mais ils ont poursuivi le développement des modèles mathématiques (surtout arithmétiques), servant notamment à des fins astrologiques (pour les horoscopes, autre pratique importée de Babylonie). Au milieu du IIe siècle, Claude Ptolémée (v. 100-168) rédige son Almageste, qui devient l'ouvrage astronomique de référence jusqu'à l'époque moderne. Il part des travaux antérieurs, exclut les modèles arithmétiques pour privilégier une approche géométrique rigoureuse, et décrit les mouvements du soleil, de la lune, des planètes proches et des étoiles fixes, le tout accompagné de tables de calcul des mouvements et divers phénomènes astronomiques observables, dont la précision est remarquable pour l'époque, en dépit d'erreurs et approximations (notamment sur la théorie solaire)[396].
Dans le domaine médical, les savoirs traditionnels mêlent l'usage de remèdes pharmaceutiques, d'incantations et amulettes magiques, et durant l'époque archaïque il n'y a pas de médecin spécialisé, mais un ensemble de prestataires de services médicaux allant du vendeur d'herbes médicales à la sage-femme. Ces pratiques ne cessent jamais durant l'Antiquité, en dépit de l'essor de la médecine scientifique spécialisée, qui en présente une vision très négative. Les temples des divinités guérisseuses, en particulier Asclépios, qui accueillent des patients, sont un autre élément majeur pour les pratiques curatives. À l'époque classique des médecins itinérants se rendent auprès de patients, parfois embauchés par les cités. C'est de cette époque que date le corpus hippocratique (une soixantaine de textes), dominé par la figure d'Hippocrate de Cos, mais qui implique manifestement d'autres médecins, et n'est véritablement stabilisé qu'à l'époque impériale romaine. La médecine grecque reprend peut-être des savoirs égyptiens et mésopotamiens, et elle intègre également les réflexions des physiciens-philosophes, visibles dans la théorie des humeurs qui est fondamentale pour la médecine antique et la recherche d'une origine naturelle aux maladies, en concevant le corps comme un tout. Les traitements reposent beaucoup sur la diététique, et les remèdes pharmaceutiques. La chirurgie et les manipulations physiques (en particulier pour les traumatismes) sont également traitées dans le corpus hippocratique. Cela s'accompagne d'une réflexion sur l'exercice de la médecine et le rôle et la déontologie du médecin, qui doit  (traité Épidémies), aspect de l'art médical dont le fameux « serment d'Hippocrate » est vu comme le fondement. La médecine grecque se développe sur ces bases durant l'époque hellénistique (Hérophile, Érasistrate à Alexandrie), avec des progrès dans les connaissances de l'anatomie et de la physiologie, des réflexions sur la diététique et l'hygiène pour préserver la santé. L'état de la médecine gréco-romaine d'époque impériale est synthétisé en latin par Celse, qui présente des écoles rivales (dogmatiques, empiriques, méthodiques). Galien (v. 129-199) domine ensuite la médecine antique et celle des époques postérieures, par l'ampleur de ses travaux (170 traités), qui synthétisent et repensent les travaux hippocratiques et post-hippocratiques, et font de l'étude anatomique et physiologique la base de la pratique médicale[397],[398].
La botanique se développe là encore à partir de savoirs traditionnels mêlant observations et croyances sur les plantes, notamment pour leurs usages magiques et pharmaceutiques (il en est question dans le corpus hippocratique), et des réflexions philosophiques telle celle d'Aristote qui s'interroge sur leur place parmi les autres êtres vivants. Son élève Théophraste (v. 370-285 av. J.-C.) écrit plusieurs traités sur les plantes, reposant sur une observation poussée de différentes leurs parties, qui lui permet ensuite de distinguer les plantes suivant leurs formes, leur croissance (ce qui lui permet par exemple de distinguer entre monocotylédones et dicotylédones), de constater leur répartition et leurs différences géographiques. Il aborde aussi leurs méthodes de culture et leurs usages. Les travaux de botanique se prolongent à l'époque hellénistique, entreprenant notamment l'analyse des plantes du nouveau monde grec constitué en Orient, mais ils ne sont connus que par des citations, notamment chez Dioscoride et Pline l'Ancien (en latin) dont les travaux référencent des centaines de plantes, ou encore Galien pour l'approche médicale. Les développements de la botanique se poursuivent, notamment avec des études locales, et la discipline s'appuie à la fin de l'Antiquité sur un corpus de connaissances étoffé[399].
Les savants grecs développent également des réflexions dans le domaine de l'optique, ici entendue comme une théorie de la vision. Là encore leur origine est à chercher chez les présocratiques, puis les philosophes, avant de se développer avec une approche mathématique. Plusieurs propositions concurrentes sont émises sur la manière dont l’œil perçoit l'image : pour les intromissionnistes c'est l’œil qui reçoit des émanations des objets ; pour les extramissionnistes l’œil émet une sorte de rayon qui lui permet de percevoir l'objet. Cette seconde approche tend à dominer, et est reprise par Euclide qui développe une approche géométrique de l'optique, proposant des théorèmes qui sont plus tard commentés par un autre mathématicien, Pappus d'Alexandrie (début du IVe siècle). Claude Ptolémée produit également un traité d'optique, connu uniquement par des fragments. D'autres développent la catroptique, étude de la vision par la réflexion dans les miroirs[400].
Dans le domaine technologique, il est traditionnellement considéré que le monde grec, ou plus largement gréco-romain, a certes produit des inventions, mais a échoué à leur trouver une application, blocage qui est attribué à des facteurs sociaux et religieux. De plus on a longtemps opposé l'esprit théorique des Grecs, qui auraient un profil d'inventeurs, à celui plus pratique des Romains, qui seraient des applicateurs. Cette manière de penser marquée par l'idée de progrès continu s'est révélée être une impasse pour penser la technologie de l'Antiquité gréco-romaine, et les techniques ont été repensées en articulation avec la société et l'économie antiques[401],[402].
À partir d'un socle technologique issu du Néolithique et de l'âge du Bronze, en plus d'apports pris depuis l'étranger, la technologie reste stable dans bien des domaines (métallurgie, activités extractives, céramique, architecture, transport, agriculture). Mais il y a des changements d'échelle : dans la métallurgie les savoirs sont repris de l’Égypte, du Levant et de la Mésopotamie, en revanche ils sont appliqués pour une production plus importante et intensive ; il en va de même pour l'extraction de minerai dans les mines du Laurion, qui varie au fil du temps en termes d'échelle (expansion et rétraction) plus que de technique. Cette capacité à développer les techniques anciennes se voit en particulier dans les nombreuses constructions en marbre réalisées dans l'Antiquité gréco-romaine, les milliers de pièces de monnaie frappées, la diffusion d'outils agricoles et d'ustensiles domestiques de qualité, à un niveau qui n'allait être égalé qu'après l'époque médiévale. Il n'empêche que des innovations significatives se sont produites durant l'Antiquité classique, telles que le moulin à eau, des pompes à eau, le soufflage du verre, la presse à vis. Ces inventions ont pu trouver à s'appliquer dans les activités économiques (agriculture, mines). Les innovations technologiques sont souvent utiles à l’État, par exemple la monnaie frappée ou les machines élévatrices servant dans la construction. L'élaboration et la diffusion des techniques de soufflage de verre (mise au point au Ier siècle av. J.-C., peut-être à partir de la Syrie) et de glaçure au plomb des céramiques (à la même période, attestée sur des sites d'Anatolie) indiquent que des innovations peuvent se propager rapidement et très loin dans le monde antique. Il n'y a certes pas eu de révolution industrielle dans l'Antiquité, mais la prolifération, l'intensification et la diffusion de techniques et méthodes de productions, anciennes comme nouvelles, a manifestement appuyé une croissance économique sur le long terme, un développement des productions et des échanges[403].
Durant le Néolithique et l'âge du Bronze, la Grèce suit les mêmes développements que les régions voisines du Moyen-Orient (Anatolie, Levant, Mésopotamie) et l’Égypte, à la suite de ces dernières : adoption du mode de vie néolithique, puis les progrès agricoles qui se produisent par la suite, émergence de sociétés plus « complexes » et hiérarchisées, et ensuite le développement de l'urbanisme, des systèmes palatiaux des époques minoenne et mycénienne, avec une pratique de l'écriture et une administration inspirées de celles des civilisations orientales. Cette dynamique se rompt après l'effondrement de l'âge du Bronze récent, durant les âges obscurs et le début de l'époque archaïque, quand la Grèce constitue une civilisation très différente de celles du Proche-Orient et d’Égypte[404].
Les influences orientales sont cependant visibles durant cette période, qui est marquée par une réouverture du monde grec aux contacts avec les régions voisines (et en particulier les Phéniciens[405]) : adoption de l'alphabet à partir du modèle phénicien, développement d'un art « orientalisant » empruntant beaucoup de ses motifs et techniques aux arts des régions orientales. Des influences ont aussi été décelées dans la religion et la mythologie. Des marchands et mercenaires grecs sont présents en Égypte et au Levant, les communautés grecques de Chypre et de Ionie jouent un rôle de passeurs culturels, peut-être aussi des « Orientaux » venus en Grèce[406],[407],[46].
Mais même si la réouverture aux pays orientaux joue un rôle crucial à cette période, son impact doit être nuancé, et pas seulement au regard des nombreux traits originaux de la culture grecque archaïque, mais aussi parce que les modèles sont rapidement réappropriés et repensés et que la culture grecque s'en éloigne rapidement. La relation ne peut donc être pensée comme une dépendance, car ce sont avant tout les dynamiques propres aux cités grecques qui expliquent ce qui est adopté et de quelle manière il l'est[408].
Aussitôt qu'ils affirment une forme d'identité collective, les auteurs grecs créent la figure du « Barbare », celui qui n'est pas grec, dont le nom dérive du fait que sa langue est incompréhensible pour un Grec. Cette figure s'affirme surtout au sortir des guerres médiques qui ont vu la résistance des Grecs face aux Perses, puis sont suivies de l'hégémonie athénienne dans la ligue de Délos qui repose en principe sur la défense du monde grec face aux ambitions perses. La littérature athénienne de la période définit le Barbare comme celui qui ignore la liberté, ne sait pas raisonner et débattre comme un bon Grec, ne sait pas contrôler ses pulsions relatives au sexe, à la nourriture, et à la violence. Isocrate et Aristote poussent la distinction jusqu'à affirmer que le Barbare est esclave par nature. Ce portrait de l'« Autre » dessine en filigrane le Grec idéal, qui est libre, moralement responsable, vertueux et mesuré, et il doit servir à renforcer la cohésion grecque. On dénigre un Grec en disant qu'il a des comportements « barbares ». Les Barbares sont exclus des grands événements affirmant l'unité du monde grec que sont les fêtes et concours panhelléniques. À partir de l'époque classique, les Grecs adoptent peu d'éléments culturels « barbares », tout en étant ouverts à l'intégration à l'hellénisme d'individus non-Grecs qui embrassent leur culture et leur mode de vie et délaissent donc le mode de vie « barbare ». L'image du Barbare se poursuit dans la littérature grecque postérieure, mais la distinction évolue durant l'époque hellénistique et s'atténue chez certains penseurs (notamment les Stoïciens). D'une manière générale son importance ne doit pas être surévaluée, les Grecs n'ayant jamais théorisé de façon poussée leur identité et celle des autres[409],[410]. Il est cependant souvent considéré que cette notion a mis en place les stéréotypes véhiculés en Occident sur l'« Orient » depuis l'Antiquité[411].
Une expansion de groupes grecs a lieu dès les âges obscurs, en premier lieu à Chypre où la présence grecque devient très importante au XIe siècle av. J.-C.[412]. Dans le monde égéen également le phénomène se repère, avec notamment l'implantation grecque en Asie mineure, mais on ne parle pas de colonisation en l'absence d'autorité centrale organisant cette migration[413].
La « colonisation » grecque concerne l'époque archaïque, entre 750 et 580 av. J.-C., sous la forme de projets d'implantation organisés depuis certaines cités. Un groupe quitte une cité sous la direction d'un meneur, et fonde une nouvelle cité dans une région de la Méditerranée (Sicile et Italie du sud, la « Grande Grèce », aussi Adriatique, Cyrénaïque et jusqu'en France avec Massalia et l'Espagne à Emporion) ou de la mer Noire (le Pont Euxin des Anciens). La nouvelle fondation est progressivement dotée des caractéristiques physiques et institutionnelles d'une cité grecque, ce phénomène participant manifestement à l'émergence de la polis. Les liens entre la cité d'origine, la métropole (un nombre limité de cités : Eubée, Corinthe, Ionie surtout), sont préservés, et parfois réactivés par l'envoi de nouveaux migrants. Les relations avec les populations autochtones sont parfois houleuses voire brutales, mais des échanges et mélanges se produisent. Les motifs de cette expansion sont discutés : les textes anciens parlent d'un manque de terres en Grèce, donc les implantations sont souvent vues comme des colonies agricoles, et de fait nombre d'entre elles deviennent prospères par leurs cultures ; mais on suppose aussi des buts commerciaux, pour l'approvisionnement en matières premières, voire la recherche de débouchés. Cette expansion présente de nombreux points communs avec celle des Phéniciens, qui se produit au même moment[414],[415]. De fait, ce phénomène participe à un ensemble de changements affectant plus largement le monde méditerranéen, voyant un essor démographique, la formation d’États et une augmentation des connexions entre les régions de cet espace[416]. La colonisation grecque est d'une grande ampleur, et elle aboutit à la constitution d'un monde grec allant au-delà de la Grèce même, puisqu'on estime qu'en gros 40 % des Grecs de l'époque classique vivent dans ces cités coloniales[21].
La Méditerranée au VIe siècle av. J.-C. En jaune : les cités phéniciennes. En rouge : les cités grecques. En gris : les autres cités.
Carte des cités grecques et des dialectes en Grande-Grèce
Chronologie des fondations en Sicile
Carte des principales cités grecques autour du Pont Euxin (mer Noire), « grenier à blé » de la Grèce antique.
La seconde phase d'expansion des Grecs et des cités grecques se produit durant l'époque hellénistique, cette fois-ci en direction de l'est et du sud, et dans le cadre de projets impérialistes mis en place par les monarques gréco-macédoniens pour consolider leur emprise sur des territoires conquis. Alexandre le Grand lance le mouvement, en fondant de nombreuses cités sur les différents territoires qu'il conquiert. Ce sont au départ essentiellement des colonies de soldats vétérans, sans doute peu développées, même s'il existe des colonies de peuplement, en premier lieu en Égypte avec Alexandrie qui est pensée d'emblée pour devenir une métropole. Les Séleucides poursuivent le mouvement de fondation des cités en Orient, notamment par la fondation de capitales en Syrie (Antioche, Apamée) et Mésopotamie (Séleucie du Tigre). Il s'en trouve jusqu'en Bactriane (actuel Afghanistan) et dans le golfe Persique (Failaka au Koweït actuel). Des villes existant depuis bien avant la conquête grecque reçoivent aussi le statut de cité (Babylone, Suse). Ces fondations reçoivent un nom grec, sont accompagnées par l'implantation de colons Grecs et/ou Macédoniens, et un urbanisme présentant de nombreux caractéristiques grecques[417].
La première expansion grecque durant l'époque archaïque se traduit par une influence culturelle sur les communautés rencontrées ; par exemple les Étrusques d'Italie centrale sont en contact dès le milieu du VIIIe siècle av. J.-C. avec des marchands grecs, venus d'Eubée, et ils importent des biens de luxe de facture grecque et adoptent l'alphabet[418]. Mais la grande période d'expansion de la culture grecque est la période hellénistique, qui voit la création de nombreuses cités grecques depuis l’Égypte et l'Asie Mineure jusqu'aux limites du monde indien. Ces villes comprennent les bâtiments caractéristiques de la culture grecque inconnus jusqu'alors dans ces régions (théâtres, gymnases, agoras, temples grecs), leurs citoyens sont versés dans la culture grecque (rhétorique, philosophie, sciences, arts), et des centres de culture grecque de première importance se constituent dans ces régions (Alexandrie, Pergame, Antioche). Ces communautés fonctionnent en quelque sorte comme des vitrines de l'hellénisme, dans des pays sous domination gréco-macédonienne, ce qui incite une partie des autochtones à adopter à leur tour la culture grecque, même s'il n'y a pas de politique délibérée de la part des royaumes hellénistiques d'acculturer leurs sujets non-Grecs. Cela se perçoit dans la littérature, l'art, l'architecture, la religion. Cette influence est définie comme une « hellénisation », la culture de la période étant défini comme l'« hellénisme » par les historiens modernes à la suite de J. G. Droysen. Elle est présente à des degrés divers selon les régions dominées, en fonction des dynamiques propres aux sociétés indigènes, plus ou moins réceptives aux aspects culturels grecs, certaines manifestant une résistance forte à l'influence grecque (Égypte, Judée), alors que dans d'autres les élites sont plus marquées par la culture grecque (Anatolie intérieure, Phénicie). L'hellénisation laisse une trace durable dans de nombreux endroits même après la fin de la domination grecque, jusqu'au nord du sous-continent indien où se développe l'art « gréco-bouddhiste ». Le syncrétisme concerne aussi les Grecs, comme l'illustre l'essor des « cultes orientaux » (à Isis, Sarapis)[419],[420],[421].
L'influence culturelle grecque se ressent assez tôt à Rome, mais elle prend vraiment de l'ampleur au IIIe siècle av. J.-C. et au suivant, après la conquête de pays grecs, d'abord en Italie du sud et Sicile, puis en Grèce continentale et dans le reste du monde hellénistique. De nombreux membres de l'élite romaine prennent la culture grecque comme référence, phénomène désigné sous le terme de « philhellénisme », en quelque sorte une variante de l'hellénisation. Cette posture suscite des critiques, mais elle se poursuit durant toute l'époque de conquête des pays hellénisés, à côté des pillages et destructions, souvent organisés par des philhellènes qui d'un autre côté se font bienfaiteurs pour les sanctuaires grecs et certaines cités. Des artistes, savants, œuvres d'art du monde grec sont apportées à Rome à la suite des pillages, des copies d’œuvres grecques sont commanditées en Italie. Durant le dernier siècle de la République romaine ce phénomène a triomphé et la culture grecque a été absorbée par l'aristocratie romaine, qui parle aussi bien grec que latin, débat de philosophie grecque, cite de la poésie en langue grecque, etc. constituant une culture « gréco-romaine » qui est dominante dans l'Empire romain. C'est un phénomène d'une importance capitale pour l'histoire et la réception de la civilisation grecque antique, puisque ce sont les goûts des Romains qui ont déterminé ce qui devait être préservé parmi ses accomplissements, et qui ont achevé de faire de la culture grecque une référence, en particulier pour l'Occident[422],[423],[424].
Dans ce contexte, le monde hellénisé de l'Orient romain ne connaît pas de « romanisation » culturelle importante, le latin reste secondaire face au grec, sauf si on souhaite embrasser une carrière dans l'administration impériale. En revanche du point de vue légal les Grecs deviennent citoyens romains, phénomène achevé par l'édit de Caracalla de 212 qui rend la citoyenneté romaine quasi-universelle dans l'Empire[425]. Les Grecs préservent un sentiment de supériorité culturelle, ont souvent considéré les Romains comme des « Barbares », avant de leur concéder une place à part, mais la fascination qu'exerce la culture grecque sur les Romains renforce la vision des Grecs[22].
Après la séparation de l'empire entre sa moitié occidentale et sa moitié orientale, ou plus exactement entre sa partie latine et sa partie grecque, puis la chute de Rome et celle de l'Empire romain d'Occident, les populations de langue et culture grecque dominant l'empire byzantin en viennent à se définir avant tout comme des « Romains ». Mais cela ne veut pas dire que les Grecs ont abandonné leur identité : il s'agit plutôt , notamment en récupérant l'idée d'Empire venue de Rome pour l'implanter durablement dans le monde grec autour de la « Nouvelle Rome » qu'est Constantinople, et ainsi prolonger durant l'époque médiévale le sentiment de supériorité du monde grec sur le monde latin[23].
La culture grecque antique est érigée en référence dès l'Antiquité. Dès l'époque hellénistique les œuvres littéraires (notamment Homère) et artistiques (la sculpture athénienne) des époques antérieures ont un statut de modèle, dans la culture grecque, la paideia, partagée par les régions hellénisées, des collections de ces œuvres sont constituées, et on commence à les classer et hiérarchiser. Les Romains, qui portent comme vu plus haut une grande considération à la culture grecque, poursuivent cette tendance, et dépouillent la Grèce de nombre de leurs œuvres d'art, ou en commanditent des copies. La Grèce devient une sorte de musée, où se rendent des voyageurs (des « touristes » antiques). Au IIe siècle Pausanias laisse une description des principaux sites grecs qui devait servir de référence aux futurs explorateurs et fouilleurs du pays[426].
En fin de compte, ce qui est transmis par la suite de la Grèce antique est largement dû à ce que les Romains (ou du moins les Grecs de l'Empire romain) ont mis en valeur et préservé, donc une Grèce antique quelque peu « tronquée ». Ainsi bien des œuvres poétiques et théâtrales cessent d'être copiées et de circuler durant la période impériale parce qu'elles ne trouvent plus de lecteurs, par exemple les pièces de Ménandre qui sont connues par des papyri d'époque hellénistique (période durant laquelle cet auteur est très populaire) mis au jour lors de fouilles modernes. Cette transmission se poursuit durant l'Antiquité tardive, avec le triomphe du christianisme, et la recomposition de la paideia dans le moule chrétien, qui conduit à un nouveau processus de sélection, même si la déférence des érudits chrétiens envers les grands classiques antiques fait qu'ils sont préservés malgré leur coloration polythéiste. Puis le changement d'écriture, avec le passage de l'onciale à la minuscule, qui se produit dans le monde byzantin aux IXe – Xe siècles entraîne la perte de ce qui n'est pas copié à cette période, les œuvres d'auteurs païens étant encore plutôt délaissées au profit des textes chrétiens. L'essentiel des manuscrits grecs antiques transmis par la suite est issu de ce travail de copie, auquel s'ajoutent les traductions en arabe d'ouvrages de philosophes et scientifiques grecs antiques. En revanche la civilisation byzantine se soucie très peu d'art antique, et l'art grec est largement oublié[427]. Les pays d'Europe occidentale, de tradition savante latine, ont alors largement oublié la culture grecque antique. Le contact est rétabli au XIVe siècle avec la naissance de l'humanisme en Italie, qui s'accompagne de l'arrivée et de la copie de nombreux manuscrits grecs depuis l'Empire byzantin, et des savants byzantins viennent enseigner le grec en Occident[428],[68],[429].
L'époque de la Renaissance est donc cruciale pour la transmission et la redéfinition de la culture grecque antique, alors que l'Empire byzantin disparaît et que la culture savante grecque entre dans une phase de déclin. Les humanistes redécouvrent alors de larges pans de la littérature et de la philosophie grecque, éditent et traduisent des textes grecs, prennent pour modèles l'art, l'architecture et l'urbanisme antiques, « gréco-romains », largement issus du moule culturel grec. La science grecque est également redécouverte et étudiée. La tradition antique a alors acquis dans ce milieu un statut prestigieux, de « classique », elle est devenue une source d'inspiration majeure et un modèle dans la culture d'Europe occidentale. L'époque suivante est celle du classicisme, qui est également marquée par de nombreuses inspirations antiques, ou revendiquées comme telles. Cela se retrouve dans une moindre mesure chez les Lumières, puis dans les différents mouvements culturels du XIXe siècle. En revanche à partir du XXe siècle les classiques antiques perdent en importance dans la culture savante des pays occidentaux, comme l'illustre l'érosion de la connaissance du grec ancien et du latin[428],[430].
L'exploration des ruines des cités grecques antiques est initiée depuis les pays d'Europe occidentale à l'époque moderne, et permet progressivement d'étoffer peu à peu le corpus documentaire permettant l'étude de la Grèce antique[431].
L'Italien Cyriaque di Pizzicoli est le premier à porter un intérêt poussé aux ruines antiques de Grèce, lors de voyages commerciaux effectués dans le pays entre 1434 et 1448, copiant des inscriptions et dessinant maladroitement des monuments, mais il n'est jamais publié. Les Humanistes de la Renaissance ne voyagent pas dans ces régions, coupées du monde chrétien depuis la prise de Constantinople par les Ottomans en 1453. C'est au XVIIe siècle que débutent les voyages en Grèce effectués pour le compte d'aristocrates ou de rois désirant obtenir des œuvres antiques. Cela s'accompagne aussi de descriptions et de copies. Des marchands, collectionneurs, érudits et artistes (les « antiquaires ») posent alors les bases de l'archéologie. Le médecin lyonnais Jacob Spon (1647-1685), qui voyage en Italie, Grèce et Asie Mineure et y étudie les traces de l'Antiquité, le récit de son voyage rencontrant un grand succès[432].
La mode des antiquités se développe au XVIIIe siècle en Europe, les descriptions et illustrations de monuments et œuvres se répandent, servant notamment pour les architectes imaginant les bâtiments « néoclassiques ». Les voyages sur les sites antiques deviennent un élément distinctif de l'élite, et les premières fouilles archéologiques sur des sites d'Italie. Le comte de Caylus (1692-1765) propose des classements et typologies d'objets antiques, au regard de considérations techniques plutôt qu'esthétiques, ce qui marque un tournant dans le raisonnement scientifique archéologique. Johann Joachim Winckelmann (1717-1768) procède quant à lui à la première tentative d'histoire de l'art grec, qui était jusqu'alors confondu avec l'art romain et rangé avec lui dans les « antiquités », et son travail est d'une importance cruciale pour le développement de la discipline et la marque durablement, notamment par son approche chronologique entre balbutiement/apogée/décadence. Cette période voit aussi l'apogée du pillage des sites grecs antiques par les élites des pays d'Europe occidentale, marquée par le transfert du décor du Parthénon à Londres par Lord Elgin en 1811, ou l'achat de la Vénus de Milo par les Français (1821)[433],[434].
Le XIXe siècle voit le développement de l'archéologie scientifique. L'indépendance de la Grèce à partir de 1827 crée de nouvelles conditions favorables aux fouilles, tout en stoppant le pillage des sites antiques grecs. Les pays occidentaux n'arrêtent pas pour autant l'exploration du pays, et ils se lancent dans une compétition pour obtenir le droit de fouiller les sites grecs, soutenus par les autorités politiques. L'expédition française de Morée (1829-1831) explore ainsi le Péloponnèse, localisant le temple de Zeus à Olympie. Des institutions spécialisées dans l'étude du passé grec antique sont créées, comme l’École française d'Athènes (1846), l'Institut archéologique allemand d'Athènes (1873), le britannique (1885), etc. Des chaires sont créées pour étudier l'Antiquité grecque, ainsi que des revues, les publications scientifiques se développant, par exemple les corpus d'inscriptions publiés en Allemagne. Sur le terrain, les équipes se répartissent les chantiers de fouilles : Délos, Delphes, Thasos et Argos pour les Français, Olympie, Samos et le Céramique d'Athènes pour les Allemands, Corinthe pour les Américains, etc. alors que les archéologues Grecs procèdent à de nombreuses fouilles, dont l'Acropole d'Athènes. L'étude de la plus haute Antiquité grecque est initiée, notamment à la suite des découvertes d'Heinrich Schliemann à Mycènes et Troie, et d'Arthur Evans à Cnossos. Les cultures de l'âge du Bronze ancien et du Néolithique commencent également à être redécouvertes, permettant de préciser la chronologie de l'Antiquité grecque sur la très longue durée[435]. Le XXe siècle voit se poursuivre les découvertes, et le développement de l'archéologie grecque, avec la modernisation des méthodes et des analyses, grâce à la mise en place d'équipes pluridisciplinaires. De nouvelles régions du monde grec antique sont explorées, avec la fouille de colonies d'époque archaïque et hellénistique[436],[437].
La postérité de la Grèce antique est souvent abordée en termes d'« héritage » : la Grèce antique a développé de nombreuses nouveautés, qu'elle a transmis ou légué aux civilisations qui lui ont succédé, qui sont donc ses héritières voire ses débitrices. Cela est couramment désigné à la suite d'Ernest Renan comme un « miracle grec », lequel le cantonne au Ve siècle av. J.-C. athénien[438] ou encore par la notion de « classique » / « classicisme » qui désigne, surtout en art et en littérature, ce qui est considéré comme un apogée, un modèle de qualité, si ce n'est de perfection, et un exemple à imiter, en référence là aussi plutôt à l'« âge d'or » athénien du Ve siècle av. J.-C.[439].
La Grèce ancienne est généralement considérée comme étant à l'origine de la civilisation occidentale. Par exemple Jacqueline de Romilly a insisté sur la survivance de valeurs et principes issus du monde grec, et écrit que [440]. Ainsi a pu être mis au crédit de la Grèce antique un héritage considérable légué aux civilisations postérieures, surtout celles d'Europe et du pourtour méditerranéen[441] :
Une autre manière d'aborder la postérité de la Grèce antique, développée en premier lieu pour l'étude de sa littérature, consiste à faire une histoire de la « réception » des œuvres antiques[442]. Si les œuvres des écrivains et artistes grecs antiques sont vues comme la « tradition » et des « classiques », c'est le résultat d'une construction historique : les civilisations postérieures se sont tournées vers la Grèce antique, dans une dynamique de dialogue avec leur passé, et ont repris et réinterprété certains des aspects de sa culture (on parle parfois aussi de réappropriation). Elles ont donc par bien des aspects sélectionné leur héritage, en puisant parmi ce qui était (restait) à leur disposition. En Occident en particulier, la littérature et l'art grecs ont été élevés au rang de « classiques », donc de modèles, et ont été intégrés dans la tradition de la civilisation occidentale, et ce dès l'Antiquité. Cette civilisation a donc constitué durant toute l'histoire postérieure de l'Occident une référence incontournable, une source inépuisable de modèles, idéalisée ou critiquée, sans cesse réinterprétée et discutée. Ainsi dans l'art :  (Holtzmann et Pasquier)[443]. Dans le domaine politique, le système démocratique athénien a été largement ignoré jusqu'au XIXe siècle, avant d'être reconstitué par les historiens et de susciter l'intérêt dans un contexte marqué par l'essor des principes de gouvernement démocratique, mais ce système politique est alors envisagé sous sa forme représentative (ou parlementaire) et plus comme une démocratie d'assemblée (directe) telle qu'elle était envisagée dans l'Antiquité[444]. Les savants et esthètes de diverses époques ont donc déterminé ce qui a été préservé et mis en avant parmi les créations grecques antiques en sélectionnant les œuvres qu'ils estimaient dignes d'être admirées, au regard de leurs propres préférences et idéologies, qui ont évolué au fil des siècles.  (S. Schein)[445].
Vase Wedgwood imitant la céramique grecque à figures rouges, v. 1815. Birmingham Museum and Art Gallery.
Phidias faisant visiter le chantier du Parthénon à ses amis, dont Périclès et Aspasie. Peinture de Lawrence Alma-Tadema, 1868.
Démosthène pratiquant l'art oratoire par Jean-Jules-Antoine Lecomte du Nouÿ, 1870.
Le Parthénon de Nashville, réplique grandeur nature du Parthénon d'Athènes à Nashville (Tennessee, États-Unis), 1897.
Ainsi la Grèce antique a pu être analysée au XIXe siècle et après sous le prisme de l’État-nation, et on s'est interrogé sur son « échec » à atteindre l'unité nationale, vue comme un horizon logique. Durant la guerre froide la rivalité entre Sparte et Athènes a attiré l'attention, car elle était vue comme renvoyant à celle entre les deux blocs se disputant alors l'hégémonie mondiale. Plus récemment, l'essor des pensées féministe et post-coloniale a incité au développement des études sur l'histoire des femmes, la sexualité, le concept de « race », etc. Par bien des aspects, la manière dont les individus modernes étudient les anciens Grecs reflète les préoccupations du moment[446]. Ainsi il y a une forme d'opposition entre les points de vue consistant à défendre une supériorité de la « civilisation occidentale » sur les autres, qui a tendance à exalter et idéaliser le « miracle grec », en minimisant les apports extérieurs pour imputer un maximum de choses au « génie » grec, premier avatar de l'Occident, alors que les plus critiques vis-à-vis de ces approches et des présupposés racistes et colonialistes ont tendance à minimiser les spécificités grecques[447]. La réaction aux discours traditionnels a pu être de mettre en avant l'« étrangeté » des Grecs anciens, ou leurs aspects moins reluisants (esclavage, condition féminine)[448]. Ces nouveaux discours incitent donc à une approche pondérée :  (T. Harrisson)[449].
En Grèce même, sous l'influence des savants et voyageurs occidentaux, au début du XIXe siècle une partie de l'intelligentsia grecque commence à revendiquer son héritage grec antique, ce qui a pu être désigné comme une « vénération de l'Antiquité », arkaiolatreia. Cela se traduit par exemple dans le choix de prénoms antiques pour nommer des enfants, plutôt que ceux de saints chrétiens, au grand dam des Popes. Les plus radicaux proposent de purifier la langue grecque en retournant au langage d'Athènes du Ve siècle av. J.-C.[450]. Dans le discours national, la Grèce antique est traditionnellement la période plus valorisée et exerce une fascination importante, alors que la Grèce médiévale (byzantine), qui avait une image négative à l'ouest chez les auteurs des Lumières, est reléguée au second plan. L'approche de l'histoire grecque sur le long terme s'est rééquilibrée à partir du milieu du XIXe siècle pour y inclure l'Empire byzantin, vu comme la continuité de l'hellénisme antique, mais l'Antiquité grecque conserve une importance majeure[451]. Selon la « Grande Idée », qui a pour projet d'unifier toutes les régions de peuplement dominant grec au XIXe siècle et au début du XXe siècle, la Grèce doit être construite autour d'Athènes, à cette époque une ville modeste, vue comme le centre du monde grec classique, et de Constantinople, à cette époque capitale de l'empire ottoman, vue comme le centre du monde grec chrétien qui doit devenir la capitale d'un État réunissant tous les Grecs. Mais l'échec de conquête de la ville a coupé court à cette ambition et Athènes est devenue la seule capitale de la Grèce[452]. Durant la période suivant l'indépendance, l'exploration archéologique du pays est vue comme un moyen de démontrer la filiation entre les Grecs antiques et les Grecs du présent. Plusieurs des provinces de Grèce sont nommées en référence aux régions antiques, là encore afin de retrouver la gloire antique du pays[453]. À l'époque contemporaine, le débat autour du nom de la Macédoine renvoie aussi aux usages nationaux des civilisations antiques[454].
La culture grecque antique a aussi stimulé des réceptions dans des cultures non occidentales, avant tout dans les milieux intellectuels, par exemple dans le monde arabe, y compris après l'époque médiévale[455] et au Japon à compter de l'ère Meiji (1868-1912)[456].
Pour les articles homonymes, voir Apartheid (homonymie).
L’apartheid (mot afrikaans partiellement dérivé du français[Note 1], signifiant « séparation, mise à part »[1]) était une politique de  (afsonderlike ontwikkeling) affectant, selon des critères raciaux ou ethniques, les populations d'Afrique du Sud concernées dans des zones géographiques déterminées. Il fut conceptualisé et introduit à partir de 1948 en Afrique du Sud (Union d'Afrique du Sud, puis République d'Afrique du Sud) par le Parti national, puis aboli le 30 juin 1991.
La politique d'apartheid se voulait l'aboutissement institutionnel d'une politique et d'une pratique jusque-là empirique de ségrégation raciale (Pass-laws, baasskap et colour bar), élaborée en Afrique du Sud depuis la fondation par la Compagnie néerlandaise des Indes orientales de la colonie du Cap en 1652. Avec l'apartheid, le rattachement territorial (puis la nationalité) et le statut social dépendaient du statut racial de l'individu. L'apartheid a également été appliqué de 1959 à 1979 dans le Sud-Ouest africain (actuelle Namibie), alors administré par l'Afrique du Sud.
La politique d'apartheid fut le [2]. Les lois rigides qui en résultèrent,  en tant que nation distincte, furent ainsi le résultat d'une confrontation, sur une même aire géographique, d'une société sur-développée, intégrée au premier monde avec une société de subsistance, encore dans le tiers monde, manifestant le refus de l'intégration des premiers avec les seconds[3].
Après le massacre de Sharpeville en 1960 et dans le contexte de la décolonisation, les critiques internationales contre l'apartheid commencent à prendre de l'ampleur (exclusion de l'Organisation mondiale de la santé, du bureau international du travail puis du comité international olympique, retrait du mandat sur le Sud-Ouest africain). Mais ce n'est qu’après les émeutes de Soweto en 1976 que des sanctions internationales contraignantes (embargo sur les ventes d'armes) sont imposées par le Conseil de sécurité des Nations unies contre l'Afrique du Sud. Les réformes entamées sous les gouvernements de Pieter Botha (autorisation de syndicats non blancs puis mixtes, abolition des emplois réservés, nouvelle constitution ré-instaurant de droits politiques aux indiens et aux métis, abolition de la loi sur les laissez-passer et de celle interdisant les mariages mixtes, ouverture des lieux publics à toutes les communautés) ne suffisent pas à enrayer la multiplication des sanctions internationales bilatérales (restrictions diplomatiques ou commerciales, embargo sur les importations de charbon, refus d'exportation de technologies) tandis que les townships deviennent ingouvernables.
À la suite de l'arrivée au pouvoir en août 1989 du président Frederik de Klerk et à la libération, après vingt-sept années d'emprisonnement, le 11 février 1990, de Nelson Mandela, chef de file de la lutte contre l'apartheid, les dernières lois piliers de l'apartheid (notamment le group Areas Act et le Population Registration Act) sont abolies en juin 1991. Les négociations constitutionnelles (CODESA) menées entre le gouvernement, le congrès national africain, le parti national et les principaux partis politiques sud-africains aboutissent à l'élaboration d'une constitution intérimaire, aux premières élections parlementaires non raciales au suffrage universel (27 avril 1994) et à l'élection de Nelson Mandela comme premier président noir d'Afrique du Sud le 10 mai 1994.
Pour avoir pacifiquement mis fin à la politique d'apartheid et entamé des négociations politiques, Nelson Mandela et Frederik de Klerk reçoivent conjointement le Prix Nobel de la paix en 1993.
L'historien Hermann Giliomee rapporte que l'apartheid ne doit pas être considéré au départ comme un projet clairement défini dans sa conception, sa mise en œuvre est loin d'être immédiate ou globale et sa vision d'ensemble n'est ni cohérente ni uniforme[4].
Le concept de l’apartheid s’articule néanmoins autour de la division politique, sociale, économique et géographique du territoire sud-africain et de sa population répartie en quatre groupes raciaux hiérarchiquement distincts :
L'apartheid se distingue également en deux catégories. La première, le petit apartheid ou apartheid mesquin qui protège l'intimité des Blancs dans leur vie quotidienne en limitant leur rapport avec les non-blancs, et la deuxième, le grand apartheid concernant la division spatiale du pays imposant des zones de résidence géographiquement séparées et racialement déterminées. Ce grand apartheid a été accompagné de mesures de déplacements et de regroupement des populations noires dans des foyers nationaux appelés bantoustans.
L'apartheid est le produit de l'Histoire, des mythes et des singularités de l'Afrique du Sud. Cette singularité est marquée par le fait que d'anciens colons européens (néerlandais, allemands et français) ont pris souche dans cette partie de l'Afrique dès le XVIIe siècle, ont développé et revendiqué une identité nationale qui leur est propre. Leur expression s'est effectuée notamment par opposition à la métropole coloniale néerlandaise mais aussi par rapport aux colons britanniques arrivés au XIXe siècle et attachés à leur mère patrie. Elle aboutit finalement à un nationalisme afrikaner exacerbé par la religion, la souffrance et la guerre contre l'impérialisme britannique et dont la politique d'apartheid ne sera que l'une des manifestations les plus notoires.
Même si des interprétations littérales de la Genèse 9:27 (impliquant l'attribution des populations modernes à la descendance de Noé et la malédiction de Cham comme source d'inégalité entre elles) ont pu « justifier » différentes formes de ségrégation raciale au cours des temps, jusqu'à la période moderne pour l'esclavage aux États-Unis, voire pour les doctrines raciales de l'anthropologie du XIXe siècle, l'apartheid ne peut pas être considéré comme une forme flétrie du calvinisme primitif, ni comme un bastion arriéré du colonialisme et encore moins comme une variante tropicale du fascisme ou du nazisme européen[5]. Son idéologie a de multiples racines, à la fois dans la théologie et dans les justifications de la colonisation. C'est par l'interprétation propre aux Boers de la doctrine calviniste de la prédestination, puisque selon eux, Dieu a créé des élites pour diriger le monde et des « non-élus » pour obéir aux premiers, que les concepts ségrégationnistes ont d'abord été avalisés par les prédicateurs de l'église réformée hollandaise[6]. Les Boers, isolés dans le veld, s'étaient ainsi facilement identifiés au « peuple élu » et bon nombre d'entre eux ont cru jusqu'à la fin des années d'apartheid, que Dieu leur avait donné l'Afrique du Sud comme il avait donné le pays de Canaan aux Hébreux, les Noirs étant assimilés aux Cananéens.
C'est à la fois par idéalisme, par intérêt et par sécurité que les Afrikaners élaborent et maintiennent aussi longtemps le système d'apartheid, qui selon eux, est le seul moyen pour leur permettre non seulement de survivre en tant que groupe ethnique distinct mais aussi pour préserver leurs intérêts de classe au sein du groupe blanc[7]. L'apartheid est ainsi présenté comme un arsenal juridique destiné à assurer la survie du peuple afrikaner comme ethnie particulière mais aussi comme un « instrument de justice et d'égalité qui doit permettre à chacun des peuples qui constituent la société sud-africaine d'accomplir son destin et de s'épanouir en tant que nation distincte ». Ainsi, beaucoup de nationalistes afrikaners pensent sincèrement que l'apartheid ouvrira des carrières et laissera leurs chances aux Noirs, chances qu'ils n'auraient pu saisir s'ils avaient été obligés d'entrer en compétition avec les Blancs au sein d'une société intégrée[8]. Avec la volonté manifeste de revaloriser les différentes ethnies du pays, l'Afrique du Sud est alors l'un des très rares états centralisateurs à prêcher le droit au séparatisme[9]. Cependant, à aucun moment les propres aspirations des peuples noirs d'Afrique du Sud ne sont prises en considération. L'apartheid leur est imposé dans la plus pure tradition du baasskap.
À partir des années 1970, les Afrikaners n'ont plus la peur pathologique de perdre leur identité qui s'affirme d'ailleurs au travers de l'État sud-africain, un État militairement fort et économiquement puissant. La discrimination et la ségrégation raciale ne sont plus justifiées en termes idéologiques mais en termes économiques et politiques : la survie du capitalisme et la lutte contre le communisme. L'apartheid finit par représenter l'expression de désirs, d'angoisses et de complexes d'une population blanche, accrochée à une vision de l'histoire qui n'avait plus cours ailleurs depuis la fin de la Seconde Guerre mondiale[10].
L'Afrique du Sud est un territoire caractérisé par une grande diversité ethnique et culturelle, héritée d'un passé complexe[11]. Avant l'arrivée des Européens, les peuples africains qui vivent en Afrique australe sont divers par leur physique, leur langue et leur mode de vie. On distingue principalement les Bantous, les Khoïkhoïs et les Bochimans. Au XVIIe siècle, les Néerlandais fondent une colonie au Cap peuplée progressivement par des Européens originaires principalement des Pays-Bas, des États allemands, de France et nettement plus minoritairement de Scandinavie, de Suisse et de Grande Bretagne (recensement de 1807)[12],[13]. Nombre des descendants de ces premiers colons formeront une nouvelle communauté d’agriculteurs et d'éleveurs, désignés sous le terme de Boers (qui signifie littéralement « fermier »). Au 20ème siècle, ils seront englobés sous le vocable générique d'Afrikaners pour désigner l'ensemble de la communauté blanche de langue afrikaans et de religion calviniste. Ils forment le principal groupe blanc de l'Afrique du Sud et vivent de l'agriculture ou de l'élevage.
Pendant la période néerlandaise, la colonie importe également des milliers d'esclaves venant de Madagascar et d’Indonésie[14]. Les Coloureds[Note 2] sont issus du métissage entre les différents groupes de population. C'est à la fin du XVIIIe siècle que les Britanniques commencent à coloniser l'Afrique du Sud : entre 1815 et 1914, près de 900 000 Britanniques s’installent en Afrique du Sud[15].
En 1814, la colonie du Cap passe définitivement sous le contrôle du Royaume-Uni et l'anglais devient une langue officielle en 1822. L'esclavage est aboli en 1833 : c'est alors que débute le Grand Trek, l'émigration de milliers de Boers vers l'intérieur des terres.
Après la découverte des diamants et de l’or dans la seconde moitié du XIXe siècle, l'immigration européenne et africaine augmente. Des dizaines de milliers d'ouvriers indiens et chinois sont encouragés à venir travailler dans les mines et l'agriculture. Deux tiers des Indiens restent en Afrique du Sud après la fin de leur contrat[16].
L'apartheid tire ses origines idéologiques dans le mouvement identitaire afrikaner. Celui-ci tire ses racines de la colonisation commencée en 1652 avec Jan van Riebeeck avant de se développer progressivement au XIXe siècle et de déboucher sur la mise en place de l'apartheid au milieu du XXe siècle. Le développement d'un groupe homogène afrikaner s'est globalement appuyé sur la langue afrikaans, sur une souche néerlando-franco-allemande et sur une interprétation de la doctrine calviniste qui distinguait un peuple élu et les autres (voir calvinisme afrikaner). La désignation des Afrikaners comme peuple élu a constitué le paradigme central de l'histoire sud-africaine des Afrikaners. Cependant, ces facteurs culturels unificateurs qui les distinguaient des autres communautés du pays n'enlevaient pas les différences qu'il pouvait y avoir entre des fermiers du Transvaal et des hommes d'affaires afrikaners du Cap[17]. Ainsi, l'histoire des Afrikaners s'est continuellement référée à une représentation quasi religieuse, utilisant les comparaisons bibliques entre l'oppression des juifs dans l'Ancien Testament, notamment l'Exode hors d'Égypte, et l'exode des Afrikaners du Cap en 1835[18]. Le Grand Trek est ainsi devenu la racine historique du peuple afrikaner, l'évènement qui lui a donné son âme, le berceau de la nation[Note 3]. Il a instauré une trame commune du passé pour unir les Afrikaners, toutes classes sociales confondues, vers une même destinée. Le mouvement identitaire afrikaner va être conforté par d'autres historiens comme George McCall Theal, un Britannique natif du Canada. Influencé par le darwinisme social, Theal exaltait le colonialisme comme le triomphe du progrès sur les races moins avancées. Il est ainsi l'un des premiers historiens à avoir examiné l'Afrique du Sud comme une nation et non comme un ensemble hétérogène de colonies distinctes[19]. Il va également idéaliser l'épopée du Grand Trek en mettant l'accent sur la main de Dieu[20].
Le nationalisme afrikaner se développe d'abord contre la domination de l'Empire britannique quand, en 1815, le Royaume-Uni remplace définitivement la tutelle néerlandaise sur la colonie du Cap. Elle aboutit d'abord en 1835 à un vaste exode de 15 000 boers vers l'intérieur des terres d'Afrique du Sud. Cette épopée, connue sous le nom de Grand Trek fut magnifiée par l'historiographie sud-africaine au travers notamment du récit du massacre de Piet Retief et des familles boers à Blaauwkraus et Boesmanspruit par les Zoulous puis par le récit de la bataille de Blood River contre les guerriers Zoulous et le serment d'allégeance à Dieu. Cet exode aboutit à la fondation de plusieurs petites républiques boers dont certaines s'unifient pour devenir la République sud-africaine du Transvaal en 1852 et l'État libre d'Orange en 1854. En 1875, au Cap, des historiens et des pasteurs de l'Église réformée hollandaise comme Stephanus Jacobus du Toit forment un mouvement de revendication culturel, (l'« Association des vrais Afrikaners », dont l'objectif est de donner à l'afrikaans, la langue parlée par les boers, ses lettres de noblesse et d'en faire un véritable outil de communication écrite[21]. En publiant le premier journal en langue afrikaans et le premier livre d'histoire sur le peuple afrikaner dans cette langue (L'histoire de notre pays dans la langue de son peuple[Note 4]), Du Toit veut éveiller la conscience nationale des Afrikaners et les libérer de leur complexe d'infériorité culturelle face aux Britanniques. Dès lors, la défense de la langue se confond avec celle de l'identité afrikaans[22].
La Seconde Guerre des Boers (1899-1902) est le deuxième événement historique après celui du Grand Trek qui cristallise le sentiment national afrikaner. Le souvenir des camps de concentration britanniques où périrent plus de 26 000 civils boers, de la tactique de la terre brûlée par les troupes impériales et l'annexion des anciennes républiques boers, nourrissent leur rancœur contre les Britanniques. La ponction démographique provoquée par la guerre, puis l'afflux de milliers de travailleurs africains et asiatiques venus travailler dans le secteur minier alimentent l'idée d'un déclin afrikaner en Afrique du Sud. Les Boers se sentent menacés par les changements démographiques et politiques. L'imposition de l'anglais dans les anciennes républiques boers, l'interdiction de l'enseignement de l'afrikaans[Quand ?] et diverses mesures vexatoires[Lesquelles ?] vont avoir pour corollaire la création d'écoles privées gérées par les Afrikaners eux-mêmes qui fournissent alors un terrain propice à la création d'une identité commune fondée sur la langue afrikaans, la croyance calviniste et une interprétation quasi-religieuse de l'histoire[23].
En 1910 est créée l'Union d'Afrique du Sud qui rejoint les dominions de l'Empire britannique. C’est après la guerre des Boers que la puissance britannique s’étend sur les capitales des républiques boers. L’Afrique du Sud devient donc un dominion britannique et, en 1910, l’Union sud-africaine est créée et rassemble les colonies de l’État libre d’Orange, du Transvaal, du Natal et du Cap. L’administrateur Alfred Milner est un des personnages qui a joué un rôle important dans la création de l’Union avant son départ en 1905. Son argument principal pour cette union entre les 4 États était que « [l’union] éliminerait la compétition économique entre eux ». En 1914-1915, plusieurs anciens officiers boers tentent de s'opposer à la participation de l'Afrique du Sud à la Première Guerre mondiale, notamment quand le gouvernement lève un contingent pour envahir le Sud-Ouest africain[24]. Dans un manifeste, ils proclament le rétablissement des républiques boers. La mort d'un de ses chefs, le général Koos de la Rey alors que celui-ci tente de forcer un barrage de police, précipite une rébellion de près de 12 000 boers contre le gouvernement de Louis Botha. En quelques semaines la rébellion est écrasée. Politiquement, elle a suscité un réflexe nationaliste au sein des quatre provinces renforçant la position du tout jeune Parti national de James B. Hertzog. Cette ségrégation qui aura été soutenue par l’Union sud-africaine aura fait soulever un grand sentiment nationaliste auprès des Africains Noirs et chez les Afrikaners. Les Afrikaners, ne se sentant pas assez représentés, forment, dès 1914, le Parti national, qui a pour but principal d’émanciper l’Afrique du sud du Royaume-Uni. Aux élections de 1915, le parti national devient le troisième parti du pays derrière le Parti sud-africain de Botha et les unionistes. Pourtant, cela n’est pas suffisant pour regagner le contrôle de l’Afrique du Sud étant donné que, durant la Première Guerre mondiale, le dominion conquiert de nouveaux territoires. On parle notamment de la colonie du Sud-Ouest africain allemande.
Après la Première Guerre mondiale, les paysans afrikaners, chassés du platteland par une grave sécheresse et une crise économique, se retrouvent confrontés à un double phénomène d’urbanisation et d’acculturation, et entrent en compétition avec les ouvriers noirs au moindre coût. Les valeurs et l'ordre traditionnel des Afrikaners s'effondrant, ces derniers se sentent acculés face à la domination des Anglo-sud-africains, de leurs valeurs liées aux affaires et à l'argent et face au risque de submersion des Noirs qui affluent alors vers les villes[25]. Face à cette situation, les nationalistes afrikaners s'efforcent de réinventer des modèles culturels à partir de l'Afrikanerdom. Destiné à arracher les Afrikaners pauvres à leur condition misérable et à les aligner sur la petite bourgeoisie anglo-sud-africaine, le concept ressuscite les rêves d'indépendance et d'auto-suffisance des Boers. En 1918, l'Afrikaner Broederbond (Ligue des frères afrikaners) est fondé à Johannesburg avec pour objectif de défendre et promouvoir les Afrikaners. Rassemblant d'abord des pasteurs calvinistes, des employés des chemins de fer et des policiers, le Broederbond devient 6 ans plus tard une ligue calviniste secrète de type franc-maçonne, recrutant un nombre croissant d'instituteurs, de professeurs, d'universitaires et de politiciens. Le Bond va étendre son influence et son audience au sein de la communauté de langue afrikaans et définir l'identité de l'Afrikaner duquel il placera les intérêts au-dessus de toutes les autres communautés d'Afrique du Sud. Ainsi, le Broederbond repose sa doctrine sur le national-christianisme, inspiré du néocalvinisme, qui stipule que « les nations sont nées d'une volonté divine, que chacune d'elles est détentrice d'une spécificité et d'une mission à accomplir »[26]. La défense de l'identité afrikaner devient une mission sacrée dont le triomphe exige la mobilisation totale du peuple de langue afrikaans (le Volk). Le concept de l'apartheid va progressivement être élaboré sur cette base doctrinale.
En mars 1922, l’armée est envoyée pour faire cesser la grève insurrectionnelle des ouvriers afrikaners des mines d'or du Witwatersrand. Les mineurs afrikaners s'étaient mis en grève pour protester contre le recours accru aux travailleurs noirs, main-d’œuvre abondante et moins bien payée, par le patronat du secteur minier. Pour les mineurs afrikaners, le patronat remettait en cause le Colour Bar et les emplois réservés des mines[27],[28]. Le conflit avait commencé dans les mines de charbon, soutenu par les nationalistes de Tielman Roos et les communistes de Bill Andrews, puis s'était répandu à travers tout le bassin minier du Rand, regroupant 20 000 travailleurs blancs. La grève s'était ensuite transformée en insurrection, avec la proclamation de soviets alors que des affrontements violents ensanglantaient la région. Quelques jours après le déclenchement de la grève générale, le premier ministre Jan Smuts mena une sanglante répression (214 tués dont 76 grévistes, 78 soldats, 30 africains tués par les grévistes) tandis que 5 000 mineurs étaient emprisonnés. L'échec du mouvement ouvrier conduisit à une mobilisation insolite rassemblant travaillistes, socialistes, communistes, des mouvements politiques de couleurs[Note 5] derrière les nationalistes du parti national qui remportèrent en 1924 les élections législatives. En 1925, sous le nouveau gouvernement nationaliste de J.B. Hertzog, l'afrikaans se substitue au néerlandais et est reconnu comme langue officielle au côté de l’anglais en 1925[29]. En 1927, le pays se dote de son premier drapeau national et d'un hymne officiel « Die Stem van Suid-Afrika ». La plus ancienne église du pays, l'Église réformée hollandaise, véritable Église du peuple afrikaner (Volkskerk), diffuse et généralise, dans le cadre d'une éducation nationale-chrétienne l'idée d'une élection collective des Afrikaners et élabore des justifications théologiques à la ségrégation[29].
En 1934, le Parti national (au pouvoir depuis 10 ans) fusionna avec le Parti sud-africain de Jan Smuts pour former un parti d'inspiration libérale, le parti uni. Une minorité au sein du Parti National demeura au sein d'un parti radicalisé, prônant la supériorité du nationalisme afrikaner sur toutes les autres nations d'Afrique du Sud. En fait, Daniel François Malan et les députés qui l'avaient suivi pour maintenir en vie le Parti national étaient essentiellement des membres du Broederbond. Débarrassé des modérés, le Parti national devenait la vitrine et l'outil politique du Broederbond.
En 1938, les célébrations du centenaire de la bataille de Blood River unissent les Afrikaners autour du thème du Volkseenheid (l'unité du peuple afrikaans) avec la reconstitution du Grand Trek. Ainsi, le 8 août 1938, des centaines de chariots portant chacun le nom d'un des héros boers du Grand Trek ou célébrant la mémoire des femmes et des enfants partent du Cap en direction de Pretoria. À mesure que les convois progressent et traversent les communes et villages, une vague de patriotisme parcourt le pays. D'autres villes et villages organisent leur propre trek vers Pretoria. En chemin, les Afrikaners se mobilisent en masse : les routes et rues sont rebaptisées Voortrekker Straat ou Pretorius Straat, les hommes se laissent pousser leurs barbes comme leurs ancêtres, les femmes mettent leur bonnet traditionnel et des tabliers de paysannes, des jeunes fiancés font bénir leur union en costume de Voortrekker et des enfants baptisés le long des chars à bœufs et les feux de joie illuminent les soirées. À l'approche de la destination finale, les thèmes nationalistes et républicains se précisent alors que le pays est pavoisé aux couleurs sud-africaines et le 16 décembre 1938, plus de 100 000 afrikaners (1/10e de la population afrikaner) assistent à Pretoria à la pose de la première pierre du Voortrekker Monument, symbole phare du nationalisme boer en présence des descendantes d'Andries Pretorius, de Piet Retief et d'Hendrik Potgieter[30],[31]. Plusieurs mouvements extrémistes non parlementaires, certains influencés par le nazisme, tentent à l'époque d'exploiter ce nationalisme ambiant comme l'Ossewa Brandwag, l'Active Citizen Force, les Chemises grises de Louis Weichardt, le SA Gentile National Socialist Movement ou le Boernasie de Manie Maritz[32].
En 1946, le Parti travailliste sud-africain, qui fut de toutes les coalitions gouvernementales entre les deux guerres mondiales, adoptait une « politique non-raciale » innovante dans le contexte de l'époque en réclamant la « reconnaissance de certains droits humains fondamentaux, le droit au travail, à la libre éducation, à la sécurité sociale, à un logement convenable, et l’abolition du travail sous contrat individuel », en préconisant l’octroi de « plus de terres aux indigènes, l’amélioration de leurs procédés de culture, une aide de l’État égale à celle accordée aux Blancs » et en proposant le remplacement des quartiers réservés aux indigènes par « des cités bien ordonnées et administrées démocratiquement par ses habitants ». Le parti va encore plus loin en s'opposant à la loi ségrégationniste sur les mines et les chantiers et en appelant à la « reconnaissance des syndicats africains, le salaire égal pour un travail égal, la possibilité d’une formation professionnelle ». Le parti se prononçait enfin pour un État fédéral octroyant l’exercice de tous les droits de citoyen à tous les individus résidant sur leur territoire. En dépit ou à cause de ce programme novateur, le parti travailliste sud-africain n'allait pas survivre aux élections générales de 1948.
En 1948, la victoire du parti national purifié de Daniel François Malan allié au parti afrikaner de Nicolaas Havenga, consacre la victoire du peuple afrikaans face à l'acculturation anglo-sud-africaine. Le danger de domination anglo-sud-africaine est définitivement écarté et l'unité du peuple afrikaans réalisé. Le thème récurrent n'est plus dès lors la défense de l'identité afrikaans mais celle du peuple blanc d'Afrique du Sud (3 millions de personnes en 1954, 21,4 % de la population totale)[33]. Cependant, la cohésion raciale de celui-ci reste menacé par le « Swaartgevaar » (le péril noir), c'est-à-dire par la puissance de la démographie africaine (8 millions de personnes en 1950 soit 67 % de la population totale)[34],[Note 6]. L'apartheid est alors présenté comme un arsenal juridique destiné à assurer la survie du peuple afrikaner mais aussi comme un « instrument de justice et d'égalité qui doit permettre à chacun des peuples qui constitue la société sud-africaine d'accomplir son destin et de s'épanouir en tant que nation distincte ».
La ségrégation raciale était une réalité bien avant l'établissement de l'Union d'Afrique du Sud. Pratiquée globalement de façon moins dogmatique et moins légaliste dès le XVIIe siècle, le confinement spatial des non-Blancs dans les zones rurales du Cap ou du Natal tout comme dans les centres industriels résultaient de la double volonté de réduire la dangerosité sociale (vagabondage, vol…) des Africains que de contrôler la disponibilité de la main-d’œuvre[35]. Le caractère généralisé de la ségrégation ne relevait encore que de règlements empiriques comme les Pass laws (laissez-passer)[36], adoptées dès 1809 dans la colonie du Cap. En 1894, l'assemblée législative du Cap vote la loi Glen Grey qui définit notamment les modalités de l'allocation obligatoire de délimitations géographiques pour les populations noires de la colonie du Cap[37].
La ségrégation à grande échelle date de l'époque d'Alfred Milner, haut commissaire en Afrique du Sud après la Seconde Guerre des Boers quand il met en place une commission intercoloniale des affaires indigènes sud-africaines composée exclusivement de Britanniques et dont le but était d'élaborer un plan pour les futures relations raciales du dominion en tenant compte de la législation disparate des quatre colonies. Les théories du darwinisme social et du racisme scientifique imprègnent alors un grand nombre d'intellectuels de l'époque et, prétextant de la supériorité des Blancs, l'une des recommandations de la commission présidée par Sir Geofrey Lagden préconise la création de réserves indigènes à travers toute l'Afrique du Sud[38]. Ces réserves n'avaient à l'origine qu'une fonction économique, subordonnées à l'ensemble sud-africain pour servir de réservoir de main-d'œuvre[39]. Elles n'avaient pas vocation à devenir indépendantes. À partir de 1948, le gouvernement mené par le parti national, donna un contenu idéologique pour justifier et développer ces réserves qui, dans le cadre de la politique d'apartheid qui commença à être mis en place, prirent les noms successivement de bantoustans puis « homelands », « black states » et enfin « national states »[39].
Quant à l'origine du mot « apartheid », il est attribué au professeur P. van Biljoen qui, en 1935, propose à la ligue afrikaans pour les études raciales de définir ainsi les bases d'une nouvelle politique qui serait basée sur la séparation verticale entre les différents groupes de population et qui serait distincte de la ségrégation raciale mise en œuvre jusque-là[40].
En application de Loi sur l'Afrique du Sud, le suffrage électoral est celui qui était en vigueur dans les colonies constitutives de l'Union sud-africaine. Concrètement, le suffrage électoral est réservé aux hommes âgés de plus de 21 ans. Dans l’État libre d'Orange et le Transvaal, le suffrage électoral est limité aux seuls hommes blancs âgés de plus de 21 ans. Selon les modalités définies dans chaque province, des restrictions censitaires relatives aux revenus et aux biens subsistent cependant pour cette catégorie de population. Dans la province du Cap, un système de franchise électorale non raciale, hérité de la colonie du Cap et basé sur l'instruction, le salaire et la propriété, permet aux hommes de couleurs (coloured et noirs) de bénéficier du droit de vote et d'émarger sur les mêmes listes électorales que les blancs. Lors des premières élections générales sud-africaines de 1910, 22 784 personnes issues des communautés coloureds ou bantoues disposent du droit de vote dans la province du Cap sur un total de 152 221 électeurs. Un système similaire plus restrictif existe aussi au Natal mais seulement 200 non blancs sur un total de 22 786 électeurs bénéficient de cette franchise électorale[41].
Le tout premier gouvernement de l'Union d'Afrique du Sud, dirigé par le premier ministre afrikaner Louis Botha (1910-1919) et dominé par le parti sud-africain, appliqua et renforça les Pass-laws et les lois coloniales britanniques appliquées en fonction du code de couleur, le Colour Bar (« barrière de couleur »), qui réglementait les relations interraciales dans les anciennes colonies sud-africaines[42]. Ainsi, en 1911, le Native Labour Regulation Act réglemente le travail indigène en instaurant un laissez-passer au niveau national[43] alors que le Mines and work Acts institue les premières barrières raciales dans le travail[44]. En 1913, le Native Land Act (« loi sur la propriété foncière indigène ») interdit aux Africains d’être propriétaires de terres en dehors des « réserves » indigènes. Cette loi, pourtant présentée par un ministre des affaires indigènes considéré comme favorable aux africains, J.W. Sauer[45], et soutenu par le journaliste et militant John Tengo Jabavu, deviendra l'un des textes emblématiques préfigurant la politique d'apartheid. Cette loi dispose notamment les conditions d'achat, de location, de propriété et d'occupation des terres sur l'ensemble du territoire sud-africain par les populations noires et blanches, désignant  8,9 millions d'hectares (7 % de la superficie du territoire sud-africain[38]) à la propriété exclusive des populations noires (concrètement, les réserves indigènes et tribales). Les dispositions de cette loi, inapplicable cependant dans la province du Cap pour des raisons constitutionnelles, allaient avoir des répercussions importantes sur les conditions de travail des populations noires dans les zones rurales et urbaines. Elle impliquait concrètement qu'aucun blanc ne pouvait acquérir de terre ou de propriété foncière dans les zones désignées pour les populations noires mais également qu'aucun noir ne pouvait acheter ou louer des terres en dehors de ces zones[46]. Pour de nombreux députés, cette loi permet alors d'ancrer le principe que l'Afrique du Sud est le pays de l'homme blanc et que la ségrégation régit les relations avec la population noire[45]. En 1923, le Native Urban Areas Act (« loi sur les régions urbaines indigènes ») introduit la ségrégation résidentielle.
Cependant, ces lois ségrégationnistes ne s'inscrivent pas dans un projet d'ensemble cohérent mais plutôt comme une réponse à l'interpénétration croissante entre Blancs et Noirs[47]. Pour Smuts ou Botha, la ségrégation raciale n'est pas envisagée comme une solution à long terme. Pour Smuts, il s'agit d'un expédient temporaire[48].
En 1936, un Conseil représentatif autochtone, censé être un instrument national de médiation et de communication entre le gouvernement et les populations noires d'Afrique du Sud, est mis en place en échange de la suppression de la franchise électorale des électeurs noirs de la province du Cap (Representation of Natives Act ou loi sur la représentation des indigènes). En vertu de cette loi, les électeurs noirs de la province du Cap sont retirés des listes électorales communes pour être réinscrits sur des listes électorales séparées afin de désigner 3 députés (blancs) chargés de défendre leurs intérêts au parlement[49]. Trois candidats indépendants furent élus parmi lesquels Margaret Ballinger. Enfin, en 1942, les grèves des travailleurs noirs sont interdites[50].
Ces lois restent conformes à l'esprit pragmatique et conjoncturel du colour bar. La politique indigène et raciale des gouvernements d'alors n'est pas présentée comme immuable mais plutôt comme un expédient provisoire. Le premier ministre Jan Smuts d'ailleurs énonce le principe des « droits civils pour tous les peuples « devenus civilisés » sans distinction de race ». Cette expression est considérée comme le gage d'une ouverture à la citoyenneté pour tous les résidents de l'Union. Toutes les évolutions restaient donc alors possibles comme le prouve le rapport de la commission Fagan présenté au lendemain de la Seconde Guerre mondiale. Mandaté par le gouvernement de Jan Smuts, ce rapport préconisait une libéralisation du système racial en Afrique du Sud en commençant par l'abolition des réserves ethniques ainsi que la fin du contrôle rigoureux des travailleurs migrants. Le premier ministre Jan Smuts approuva les conclusions du rapport alors que de son côté, l'opposition représentée par le Parti National mandatait sa propre commission (la commission Sauer) dont les conclusions furent exactement inverses. En recommandant le durcissement des lois ségrégationnistes, le rapport de la commission Sauer se référa explicitement au nouveau concept d'apartheid. Celui-ci puisait principalement son inspiration dans les lois ségrégationnistes en vigueur dans le sud des États-Unis et dans les politiques de réserves mises en place pour les Indiens dans ce pays ainsi qu'au Canada et aussi dans les politiques indigènes menées alors en Australie à l'encontre des Aborigènes, en Nouvelle-Zélande pour les Maori.
En 1947, le Broederbond prend le contrôle du bureau sud-africain des affaires raciales (South African Bureau of Racial Affairs -SABRA). En son sein, le concept de ségrégation totale au travers du dogme de l'apartheid est finalisé.
La résistance à la ségrégation institutionnalisée et aux discriminations quotidiennes s'organise : Mohandas Karamchand Gandhi mène des actions de protestations non-violentes contre les vexations à l'égard de la classe moyenne indienne[51]. Plusieurs associations sont fondées : l’Organisation du peuple africain (African People’s Organisation, 1902), le Congrès national des Natifs sud-africains (South African Native National Congress, 1912) qui deviendra en 1923 le Congrès national africain (African National Congress ou ANC), la Youth League (1944). Le syndicat de l’Industrial Commercial Union, fondé en 1919, organise des grèves pour la défense des travailleurs noirs, relayé par le Parti communiste d'Afrique du Sud, créé en 1921. Durant la Seconde Guerre mondiale, par hostilité tout à la fois envers le capitalisme, l'impérialisme britannique et le colonialisme, des dirigeants noirs et indiens tels Yusuf Dadoo, un influent dirigeant du congrès indien du Transvaal et membre du parti communiste sud-africain, prononcent plusieurs virulents discours contre la guerre et le suivisme du gouvernement sud-africain ce qui lui vaudra plusieurs séjours en prison[52]. En conséquence de ces discours et par prévention des troubles, les grèves des travailleurs noirs sont déclarées illégales au titre de l'effort de guerre[53].
De son côté, le congrès national africain, qui peine alors à s'imposer dans la société civile noire sud-africaine, entreprend de se reconstruire sous la direction d'Alfred Xuma. Son but est de transformer l'organisation intellectuelle qu'est l'ANC en un véritable parti de masse. En 1943, il fait adopter une nouvelle charte constitutionnelle qui ouvre l'adhésion à l'ANC aux gens de toute race, élimine de l'organigramme la chambre des chefs tribaux et accorde aux femmes des droits égaux aux hommes au sein du mouvement[54]. En 1944, il facilite, au sein du monde étudiant, principalement à l'université de Fort Hare, la création de la ligue des jeunes de l'ANC par Nelson Mandela, Walter Sisulu et Oliver Tambo, dont l'objet est de former un renouvellement des idées et des cadres d'un parti vieillissant. Cette ligue de jeunesse se révèle vite plus radicale que son aînée dans son mode d'expression, partisan de manifestations de masse pour faire aboutir les revendications d'égalités raciale et politique de la majorité noire[55]. Elle conteste notamment le bilan de ses aînés, plaide pour une émancipation morale vis-à-vis du paternalisme blanc et pour l'affirmation d'un nationalisme sud-africain noir, débarrassé de ses oripeaux ethniques[56].
Le rebondissement des problèmes raciaux intervient au sortir de la Seconde Guerre mondiale, époque où la totalité de la population urbaine noire dépasse pour la première fois celle de la population urbaine blanche pour atteindre 1,5 million de personnes[57]. En 1947, Xuma formalise son alliance avec le Congrès indien du Natal et le Congrès indien du Transvaal du docteur Yusuf Dadoo, afin de présenter un front uni, dépassant les clivages raciaux, face à la classe politique blanche.
En juin 1948, à la surprise générale et bien que minoritaire en voix, l'alliance du Parti national de Daniel François Malan, et du Parti afrikaner (Afrikaner Party - AP) de Nicolaas Havenga, remporte la majorité des sièges aux élections de 1948[58] avec 42 % des voix et 52 % des sièges. Cette victoire du parti national consacre aussi celle du Broederbond, une société secrète fondée en 1918 et consacrée exclusivement à la promotion des Afrikaners dans la société civile. Le thème récurrent des gouvernements nationalistes successifs ne sera plus dès lors la défense traditionnelle de l’identité afrikaans face au danger de domination ou d’acculturation anglophone mais celui du peuple blanc d’Afrique du Sud (anglophones, afrikaners, lusophones) menacé par la puissance de la démographie africaine[59] et la crainte d'un soulèvement de millions de Noirs (population majoritaire dans le pays) qui balaieraient le peuple afrikaner, sa langue, sa culture, ses institutions et toute sa manière de vivre[60]. L'idée est aussi de mettre en place une politique permettant de satisfaire aux deux tendances constitutives du parti national, l'une portée sur la suprématie blanche garantissant la sécurité des blancs, l'autre mobilisée autour de la promotion et de la défense de la culture afrikaner enracinée dans l'histoire  (le volk)[61].
Avant 1948, la politique indigène des gouvernements de l'Union Sud-Africaine avait constamment été présentée comme un expédient provisoire en attendant que, devenues  aient accès à la citoyenneté. Après 1948, l’apartheid, ou développement séparé des races, vient rompre avec le pragmatisme de la Colour Bar et avec la discrimination conjoncturelle héritée de l’ère coloniale[62]. Présenté par la commission Sauer, l’apartheid fige les rapports entre races et groupes de populations, excluant dorénavant tout évolution simple et pacifique du système politique et économique. Il devient la pierre angulaire de la politique nationale sud-africaine en s'insérant dans une philosophie précise, fixe, permanente et immuable, bien éloignée de l'ancien concept du Colour Bar[62]. Pour nombre de chefs d’États étrangers où sévit déjà une séparation plus subtile voire coutumière entre les classes, les ethnies ou les religions, la ségrégation affichée et revendiquée de l’apartheid va leur permettre d'utiliser à leur profit la politique intérieure de l'Afrique du Sud et de faire de ce pays un bouc émissaire providentiel[62].
D.F. Malan ne s'était jamais passionné pour les questions relatives aux autochtones d'Afrique du Sud[63]. Pour lui, la priorité était de pallier la pauvreté des petits Blancs, qu'elle soit matérielle ou spirituelle[64]. Il considère notamment que l'équilibre racial en Afrique du Sud repose sur un accord tacite entre Noirs et Blancs fondé sur le respect et l'exemplarité que ceux-ci doivent inspirer. C'est pourquoi, régler le problème des Blancs pauvres doit aussi permettre, selon lui, de gérer la question autochtone[65]. Selon ses déclarations, l'objectif de l'apartheid est alors la division du pays en deux parties avec d'un côté les Noirs et d'un côté les Blancs, sans que les premiers ne perdurent à être les réservoirs de main d'œuvre des seconds[66].
En fait, le concept de l'apartheid n'est pas clairement défini. Certains n'envisagent que de renforcer les lois ségrégationnistes en gardant le même système économique de production, alors que d'autres estiment que ces lois sont insuffisantes et inutiles si on laisse perdurer l'intégration économique entre les Blancs et les autres populations de couleur, première étape d'une intégration politique qui conduira inéluctablement à la chute de la domination blanche en Afrique du Sud[67]. Pour Hendrik Verwoerd, le libéralisme basé sur la promotion de l'individu ne va pas accepter plus longtemps le maintien de la ségrégation traditionnelle jusque-là pratiquée dans plusieurs pays de la sphère occidentale. Il convient alors de trouver une alternative sans remettre en cause le principe de la domination blanche. Or, comme il écrit lui-même en 1950, les Blancs doivent comprendre que les populations de couleur ont leurs propres consciences nationales et des ambitions et, ainsi, qu'elles ne pourront éternellement continuer à accepter d'être leurs subordonnées en étant, de plus intégrées à leur vie quotidienne. Il convient dès lors, dans l'esprit de Verwoerd, que chaque peuple d'Afrique du Sud puisse exercer ses droits, séparément des autres, dans des cercles définis et évoluer à son rythme[68], en « bons voisins ». L'apartheid que conçoit Verwoerd, et qui va s'imposer, doit alors être un système politique fondé sur des principes stricts et clairs qui ne peuvent souffrir aucune exception, même mineure, au risque sinon de corrompre sa cohérence. Ainsi n'est-il pas possible d'accepter des diplomates noirs ou des maoris dans l'équipe de rugby de Nouvelle-Zélande[69]. Cette conception exclusive et fondamentaliste de l'apartheid s'oppose à d'autres conceptions plus souples du système, notamment celles du ministre Theophilus Donges pour qui l'apartheid n'est qu'un moyen provisoire visant à protéger les Afrikaners sur deux à trois générations avant le jour où ils n'auront plus le monopole du pouvoir[69]. Ou encore Ben Schoeman pour qui les différents groupes de population sont déjà trop imbriqués et qui estime irréaliste une partition territoriale ou l'interdiction de la région du Cap occidental aux travailleurs noirs[69]. Des dissidences philosophiques sur ce thème apparaissent aussi avec le professeur Wicus du Plessis et le Bureau des affaires raciales (South African Bureau of Racial Affairs - SABRA)[70].
Dans un premier temps, Malan confie le ministère des affaires indigènes à un pragmatique modéré, Ernest George Jansen, qui maintint la tradition libérale du Cap et se montre plus préoccupé par la réhabilitation des réserves ou la pénurie de logements dans les townships[63]. C'est sous la responsabilité des ministres Theophilus Dönges et Charles Swart que l'arsenal législatif de l’apartheid se met d'abord concrètement et rapidement en place, dans le but existentiel de préserver l’identité du « Volk » (le peuple afrikaner). Ainsi, le refus de toute mixité génétique et raciale est inscrite dès la loi de 1949 sur l’interdiction des mariages interraciaux (illégaux, ils sont passibles des tribunaux) et celle de 1950 sur l’interdiction des relations sexuelles interraciales. En 10 ans, près de 4 000 personnes seront condamnées en vertu de cette loi. Mise à part Margaret Ballinger, la représentante au parlement des populations de couleur du Cap, cette législation ne rencontre guère d'opposition et laisse indifférente la grande majorité de la population blanche[71].
Ce texte fondamental, pilier de toutes les autres lois d'apartheid, est la loi d'enregistrement de la population (Population Registration Act), adoptée en 1950 qui institutionnalise la classification raciale pour chaque habitant du pays âgé de plus de 16 ans en définissant 4 principaux groupes raciaux (Blancs, Coloureds, Noirs, asiatiques) et établissant des critères d'appartenance raciale. Ce texte de référence allait être le support de tous les autres textes législatifs et réglementaires relatifs à l'apartheid. Ainsi en découlent les textes sur l'habitat (Group Areas Act), l'enseignement (Bantu Education Act), les mouvements des personnes, l'emploi et tout ce qui concerne la vie sociale. La mise en place du « Groups Areas Act » renforce ainsi la ségrégation résidentielle au prix du déplacement de centaines de milliers de personnes dont quelques dizaines de milliers de Blancs. Il s'agit pour les nationalistes de freiner le processus d'urbanisation des Noirs et de contrôler les transactions immobilières et l'occupation des sols[72].
En 1951, sous la pression de la commission des affaires indigènes du parlement, Malan remplace Jansen par Hendrik Verwoerd au ministère des affaires indigènes[63]. Pendant les 8 années suivantes, Verwoerd allait jeter les bases du projet de grand apartheid centré autour de la création de bantoustan, procédant alors à un renversement de logique par rapport aux anciennes politiques gouvernementales : il ne s'agit plus de maintenir l'unité de la nation mais de sacrifier l'intégrité territoriale et les relations sociales au profit d'un ordre racial[73]. Dans le cadre du débat parlementaire relatif au projet de loi sur l'autonomie bantoue, qui prévoit la suppression de la représentation noire au Parlement (par des blancs) et le développement d'unités nationales bantoues autonomes, Verwoerd déclare [74]. Toute la politique du gouvernement vise dorénavant à recréer des états ethno-linguistiques homogènes à partir des réserves ethniques préexistantes. Il lui faut dans le même temps distinguer et définir les différentes communautés ethno-linguistiques du pays. Cependant, le « grand apartheid » qui se met alors en œuvre repose sur des données et des projections démographiques complètement erronées[75].
Si seuls les Blancs bénéficient alors des garanties d'un régime démocratique, puis dans une moindre mesure à partir de 1984, les Indiens et les métis, le professeur américain Lyn Boyd Judson note que les sud-africains noirs se considèrent pour leur part, durant la période s'étalant de 1940 à 1990, comme des , terme que les noirs sud-africains contemporains utilisent pour évoquer cette période[76].
Le ministre des affaires indigènes était de facto le chef des africains du pays. Tous les commissaires et tous les chefs de tribus, en zone rurale, étaient responsables devant lui. Son pouvoir était aussi total sur les africains urbanisés. En 1950, Hendrik Verwoerd est appelé par le docteur Malan pour diriger ce ministère et organiser l'apartheid géographique. Considéré comme un visionnaire, et non comme le grand architecte de l'apartheid, Verwoerd donna une impulsion, une direction et un calendrier pour la mise en place du grand apartheid. Il nomma Max Eiselen, professeur d'anthropologie sociale à l'université de Stellenbosch avec qui il partageait les mêmes points de vue sur le tribalisme et le nationalisme noir, pour mettre en forme cette politique. L'obsession que les Afrikaners avaient pour définir leur propre identité est transposée dans les analyses faites pour définir ethno-linguistiquement les différents peuples bantous à travers leur histoire, leur langue, leur culture qui font l'objet d'une relecture et d'un réexamen pour coïncider avec la nouvelle politique raciale, quitte à exacerber les particularismes afin de créer des entités artificielles, proposées voire imposées à des peuples qui n'en veulent pas[77].
Des États ethniques, les Bantoustans (ou Homeland), sont ainsi créés à partir des 263 réserves indigènes déjà constituées sous l'ancienne législation. Économiquement peu viables, limités à seulement 13 % du territoire, ils enferment des populations entières sur des territoires dont la plupart sont privés de richesse naturelle et d'industrie, sans accès au commerce international. Si cette indépendance de façade satisfait parfois les potentats locaux, elle est essentiellement factice.
Durant les 8 années de son mandat au ministère des affaires indigènes, sous les gouvernements Malan et Strijdom, Verwoerd rénove son département ministériel qu'il scinde en deux en 1955 pour former le département des affaires bantu et celui de l'éducation bantu. La politique, alors menée pour organiser le grand apartheid, repose sur le renouveau des structures tribales, privilégiant l'aristocratie et les forces traditionalistes africaines, alors en perte de vitesse sur tout le continent africain. La hiérarchie tribale fut rehaussée afin de recréer de nouvelles structures pour l'administration locale qui devaient dorénavant davantage reposer sur les chefferies et non plus sur les commissaires du gouvernement[78]. Le Bantu authorities act en 1951, abolissant le conseil représentatif indigène, instaura à la place des autorités tribales, régionales et territoriales, fondées sur des critères géographiques et ethniques. Leurs compétences relevaient du domaine administratif et judiciaire alors que le droit tribal coutumier était revalorisé. En l'absence de chefs reconnus, une autorité communautaire était instituée avec des conseillers traditionnels reconnus. Toutes ces autorités tribales, dont les fonctions étaient entre autres de maintenir l'ordre, de percevoir les impôts et de s'occuper des registres d'état civil, étaient salariées du gouvernement central sud-africain. Cette loi fut complétée en 1959 par la Bantu Self Government Act élargissant les compétences de ces autorités. L'application de la loi fut néanmoins lente car elle imposait au gouvernement de persuader l'aristocratie locale d'accepter les nouvelles structures. En dépit de l'hostilité des africains urbanisés, l'aristocratie tribale en zone rurale fut séduite et dès 1953, les premières autorités tribales furent établies dans les réserves indigènes du Transvaal, du Cap et du Natal. Il y eut cependant de fortes résistances, notamment au Zululand au Sekhukhuneland et au Transkei, d'autant plus que le système mis ainsi en place favorisait les nobles les plus dociles au pouvoir. En accordant aux chefs tribaux les responsabilités des commissaires aux affaires bantous, la nouvelle structure organisationnelle des réserves allait aussi favoriser la corruption au plus haut niveau des autorités régionales.
Alors que Verwoerd et les anthropologues du ministère des affaires indigènes mettaient en place ces structures tribales, le gouvernement avait mandaté le professeur Tomlinson pour faire un rapport sur la viabilité économique de ces réserves. Les conclusions du rapport Tomlinson allaient notamment amener le gouvernement à envisager la séparation territoriale de ces territoires. Alors que jusqu'en 1948, l'Afrique du Sud réclamait l'incorporation du Bechuanaland, du Basutoland et du Swaziland à son territoire, les gouvernements successifs de la période d'apartheid allaient mener une politique inverse souhaitant, en raison de leur lien ethno-linguistique, l'incorporation du Bophuthatswana avec le Botswana et le QwaQwa avec le Lesotho. Répondant ainsi à l'Afrique décolonisée par le biais de la création de ces États ethniques, le gouvernement de Pretoria pensait ainsi pouvoir créer une république sud-africaine constituée majoritairement de blancs (les métis et les indiens n'étant pas concernés par la politique des bantoustans)[79]. Dès le départ, il est envisagé à terme d'accorder l'autonomie interne aux futurs bantoustans sans pour autant leur accorder la pleine souveraineté[80].
Dénonçant la misère économique des réserves, le groupe d'études de Tomlison faisait également un certain nombre de propositions pour assurer leur viabilité économique, notamment au niveau agricole, mais aussi envisageait une nouvelle carte géopolitique de toute la sous-région avec une redistribution spatiale des frontières à partir de critères ethno-linguistiques. En proposant un développement économique général de territoires africains, sur lesquels vivraient 10 millions de personnes et en critiquant certains des aspects de la politique des bantoustans, notamment leurs faibles ressources et superficies, le rapport Tomlison fixait aussi et surtout des objectifs qui étaient financièrement ambitieux et couteux, comme la création au sein de ces territoires non seulement d'industries mais aussi d'une centaine de villes équipées. À long terme, condamnant implicitement la politique des bantoustans, le rapport estimait que si toutes ses recommandations, notamment financières, étaient respectées, les bantoustans ne recevraient que 60 % de la population noire d'Afrique du Sud à l'horizon 1981, laissant encore les blancs en minorité en zone blanche[81]. Les estimations financières demandées dans le rapport Tomlison ne furent pas mises en œuvre. Néanmoins, la politique des bantoustans continua. D'autres critiques émanèrent cependant au sein même du parti national pour critiquer la politique de Verwoerd. Le plus emblématique d'entre eux fut le professeur L. J. du Plessis de l'université de Potschefstroom[82]. Cet ancien chef du Broederbond en vint à la conclusion que l'apartheid ne pouvait être justifié qu'avec l'accord des africains et dans le cadre du droit des peuples à l'autodétermination[82], concept que les Boers puis les Afrikaners avaient invoqué pour eux-mêmes dans leur lutte contre l'impérialisme britannique. Dans le contexte de la décolonisation alors en cours sur le continent africain, Wicus du Plessis argumenta pour que des discussions approfondies soient entamées avec les chefs des mouvements noirs tels que le congrès national africain[82]. Il se montra notamment favorable, à terme, à une égalité territoriale et politique entre blancs et noirs et à la création d'une Afrique du Sud unitaire comprenant de nouvelles provinces dirigées par les élites noires du pays[82]. Le débat interne aux cercles nationalistes se solda cependant par la victoire des partisans de Verwoerd, la mise à l'écart de Wicus du Plessis et la purge de ses partisans des organismes afrikaners[82].
En 1964, Verwoerd proclama officiellement que l'évolution naturelle des Bantoustans était la partition effective du territoire sud-africain et leur indépendance. De 1956 à 1977, une dizaine d'autorités territoriales tribales furent établies et 4 d'entre eux à commencer par le Transkei devinrent indépendants mais non reconnus par la communauté internationale.
De 1951 à 1956, le gouvernement Malan mène une véritable bataille constitutionnelle pour radier les coloureds des listes électorales communes et instituer des collèges électoraux séparés. Politiquement, la mesure permettrait de priver le parti uni et le parti travailliste de voix déterminantes dans plus de la moitié des 55 circonscriptions de la province du Cap[83]. En 1951, une loi est votée au terme de laquelle les Coloureds et métis du Cap et du Natal seraient désormais représentés au parlement par 4 députés blancs élus pour 5 ans sur des listes séparés. La loi est vivement attaquée par l'opposition parlementaire. Des manifestations sont organisées par l'association des vétérans de guerre, avec le soutien de la Springbok Legion. Partout dans le pays se forment des mouvements de soutien au maintien des métis sur les listes électorales communes. Celui des Torch commando dirigés par Louis Kane-Berman et Adolph Malan, héros de la bataille d'Angleterre, est le plus emblématique de ces mouvements. Le mouvement reçoit l'appui financier de Harry Oppenheimer et forme un front commun avec le parti uni et le parti travailliste. Finalement, la question de la suprématie législative du Parlement se retrouve placée au centre des débats après l'invalidation de la loi par la Cour suprême par référence au South Africa Act. La tentative de D.F. Malan de contourner la décision est également un échec[84]. Le succès des nationalistes aux élections générales sud-africaines de 1953 ne parvient pas à lui faire acquérir la majorité nécessaire pour modifier la constitution alors qu'au sein même du parti national, plusieurs de ses membres à l'instar de Nicolaas Havenga sont réticents à modifier le droit de vote reconnu constitutionnellement aux métis et Coloureds du Cap. La franchise est finalement supprimée en 1956 par le gouvernement Strijdom.
Le système génère des frustrations, bien sûr chez les Noirs et autres groupes désavantagés qui trouvent en l'ANC un de leurs principaux porte-paroles, mais aussi chez certains Blancs libéraux, représentés notamment d'abord par le Parti Progressiste. Le gouvernement réagit de manière souvent violente, au mépris des idéaux démocratiques qui sont censés le fonder. Les contestataires sont condamnés et emprisonnés.
À partir de 1953 et de l'entrée en vigueur de la loi concernant les accès aux services et aux lieux publics, les panneaux  (« Seulement pour Européens »)  (« Seulement pour Natifs ») et  (« Seulement pour Métis »)[33], devenus obligatoires, se multiplient dans tous les lieux publics du pays[85]. Les premières campagnes d'oppositions à la mise en place de l'apartheid remontent à 1952 (« campagne de défiance ». En juin 1955, 3 000 délégués de l'ANC et de divers autres groupes anti-apartheid comme le congrès indien, le Congrès des Démocrates ou la Fédération des femmes sud-africaines (Federation of South African Women - FSAW), se réunissent dans un congrès du peuple à Kliptown, un township de Johannesburg. Ces délégués adoptent la Charte de la liberté (Freedom Charter), énonçant les bases fondamentales des revendications des gens de couleur, appelant à l'égalité des droits quelle que soit la race, l'abrogation de toute discrimination raciale, l'instauration d'un régime démocratique pour tous et un programme politique oscillant entre inspiration communiste (nationalisations, réforme agraire) et socialiste (salaire minimum, semaine des 44 heures, couverture sociale)[86]. Un million de personnes signent le texte[87]. À la suite de l'adoption de ce texte, 156 membres de l'ANC et des organisations alliés sont arrêtés et accusés de haute trahison pour avoir prôné le renversement du gouvernement. Parmi les accusés se trouvent Albert Luthuli, Oliver Tambo, Walter Sisulu, Nelson Mandela, Ahmed Kathrada et Joe Slovo. L'affaire est très médiatisée. L’instruction judiciaire, connue sous le nom de « procès de la trahison », dure pendant quatre ans, période durant laquelle les charges tombent progressivement contre les inculpés. Finalement, le 29 mars 1961, la justice sud-africaine acquitte l'ensemble des inculpés, admettant dans le verdict que l'ANC ne pouvait être reconnu coupable d'avoir défendu une politique visant à renverser l'état par la violence[88].
Entretemps, en 1956, le nouveau premier ministre, Johannes Strijdom, est parvenu à supprimer la franchise électorale des personnes de couleur (coloureds) de la province du Cap malgré la campagne lancée par les Torch commando. D'ailleurs, durant toute la décennie des années 1950, les mouvements opposés à l'apartheid, issus des différentes communautés, peinent à s'unir et à organiser des manifestations inter-raciales. Malgré les appels de l'ANC, la communauté blanche échoue totalement à constituer un mouvement unique blanc anti-apartheid. Bien au contraire, l'opposition blanche à l'apartheid s'est morcelée en deux grandes familles (radicaux et libéraux), elles-mêmes divisées en sous-groupes divers. L'opposition libérale ignore également les appels de l'ANC à manifester ou à se rassembler (campagne de défiance, rassemblement de Kliptown), préférant privilégier les procédures légales. En fait, les motifs de mobilisation des blancs (centrés surtout sur le droit de vote des métis) ont été différents de ceux de l'ANC et, tant le parti uni que le parti libéral, ne sont pas favorables à l'extension d'un droit de suffrage sans restriction aux populations de couleurs. De ce fait, l'opposition libérale est définitivement discréditée aux yeux de l'ANC qui ne privilégiera que ses alliés radicaux[89],[Note 7]. Le mouvement anti-apartheid se divise davantage en 1959 quand les radicaux quittent l'ANC pour fonder le Congrès panafricain d'Azanie (PAC).
Après le massacre de Sharpeville, la destruction en place publique de leurs laissez-passer par plusieurs milliers de noirs sud-africains et la mise en place de l'état d'urgence décrété par le gouvernement le 8 avril 1960, l'ANC et le PAC sont à leur tour interdits en vertu de la loi de suppression du communisme. Sur le plan international, la situation de l'Afrique du Sud se détériore pour la première fois, ce qui se manifeste par son exclusion de l'Organisation mondiale de la santé, de l'organisation des Nations unies pour l'éducation, la science et la culture et du bureau international du travail.
En 1961, Umkhonto we Sizwe (MK), branche militaire de l'ANC est fondée et lance une campagne de sabotage. Les premières attaques visent des bâtiments officiels mais, rapidement, les chefs de MK sont arrêtés en juin 1963 à Rivonia et en juin 1964 seront condamnés à la réclusion à perpétuité. Parmi, eux, on trouve Nelson Mandela — lui-même arrêté en août 1962 —, qui est aussi condamné avec 8 des 9 autres prévenus à la réclusion à perpétuité. En fait, à partir des évènements de 1960 et 1961, le régime se replie dans un réflexe de camp défensif (le « laager » propre à l'imaginaire politique sud-africain)[90] destiné à empêcher l'intrusion du communisme dans le pays. Ce repli se manifeste d'abord lors de la proclamation de la république d'Afrique du Sud le 31 mai 1961 (approuvée lors du référendum de 1960 par 52 % des électeurs dont les trois quarts des électeurs du Tranvaal, de l'état libre d'Orange et du Cap), jour symbolique de la défaite des Boers en 1902. Ce repli s'accompagne d'une modernisation des concepts surtout après la mort d'Hendrik Verwoerd en 1966 et jusqu'au gouvernement de Pieter Willem Botha (fin des années 1980). L'idéologie de l'apartheid se modernise alors constamment[91].
Cette modernisation est complexe. D'une part, à la suite de la proclamation de la république, les discours de la classe dirigeante afrikaner ne se focalisent plus sur la défense de l'identité afrikaner mais sur les concepts de nation sud-africaine dont celle de la nation blanche d'Afrique du Sud regroupant Anglo-sud-africains, Afrikaners et lusophones, dans un effort d'incorporation des groupes européens du pays. Ceux-ci, au travers de leur expression politique qu'est le parti national, n'en revendiquent pas moins « le droit historique et le devoir de maintenir leur souveraineté sur l'Afrique du Sud », alors que les Noirs ne sont plus présentés comme inférieurs mais comme différents[92]. Elle s'accompagne d'une stratégie politique d'alliance intérieure progressive avec les groupes métis, indien et asiatique du pays qui se caractérise, sous Pieter Botha, par la création en 1984 de chambres parlementaires pour chacun de ces groupes définis racialement. D'autre part, le régime d'apartheid qui argumente sur le plan diplomatique sur la base de sa mission de défense des valeurs occidentales en Afrique et de bastion contre le communisme athée, engage une déstabilisation extérieure de ses voisins (avec le soutien du bloc occidental) pour contrer la progression des régimes marxistes en Afrique, soutenus par Cuba, la Chine ou l'Union soviétique (principalement l'Angola et le Mozambique à partir de 1975). En dépit de la théorie officielle selon laquelle les immigrés blancs pourraient devenir de « vrais Afrikaners », une partie d'entre-eux sont sujets à une certaine xénophobie. S'ils disposent de leurs propres domestiques noirs, les immigrés français s'adaptent difficilement à la communauté afrikaner, qui les rejette. Les anciens colons portugais du Mozambique et de l’Angola sont eux considérés par une partie des Afrikaners comme des « wit kaffer » (des « cafards blancs »)[93].
Si dans les années 1970, les Afrikaners n'ont plus la crainte pathologique de perdre leur identité nationale qui s'affirme d'ailleurs au travers de l'État sud-africain, militairement fort et économiquement puissant, ils sont néanmoins ébranlés dans les années 1980, par trois facteurs qui remettent en cause leur foi dans la suprématie naturelle des Afrikaans : les condamnations internationales dont l'Afrique du Sud fait l'objet pour sa politique d'apartheid (en 1973, une convention internationale votée par l'assemblée générale des Nations unies qualifie l'apartheid de crime contre l'humanité[94]) ; la montée de la contestation interne des Noirs à partir de 1976 ; l'opposition grandissante des pasteurs afrikaans, issus de l'église réformée hollandaise (qui condamne l'apartheid en 1986).
À partir de 1976 et des émeutes de Soweto, le pays est en proie à la montée de la violence politique et de la répression policière dans les townships. Au bout de plusieurs mois d'émeutes, le bilan est de 600 morts. Le mouvement de la Conscience noire, à l'origine des troubles de Soweto, est décapité avec la mort de son chef charismatique, Steve Biko. L'émotion causée par la mort de ce dernier conduit le conseil de sécurité de l'ONU à imposer pour la première fois des sanctions obligatoires contre l'Afrique du Sud en décrétant un embargo sur les ventes d'armes[95].
Après les émeutes de Soweto, Umkhonto we Sizwe, la branche militaire de l'ANC, recrute de nombreux militants et organise sa guérilla dans des camps d'entrainement en Angola, en Tanzanie ou en Zambie où des actes de torture ou des exécutions sont pratiqués contre les militants accusés d'espionnage.
À partir de 1977, elle organise des sabotages et des attentats au sein même de l'Afrique du Sud. Parfois, ces actions restent symboliques (attentat contre des postes de police des townships, contre la centrale de Koeberg) mais parfois ce sont de véritables attentats terroristes (attentat de church street à Pretoria en 1983, attentat d'Amanzimtoti en 1985, meurtres de fermiers blancs dans le nord et l'est du Transvaal ou de conseillers municipaux ou de policiers noirs accusés de collaboration dans les townships…). Des scientifiques sont chargés par les services secrets gouvernementaux de travailler sur un programme de diminution du taux de fertilité des femmes noires par le biais d'une substance répandue dans l'eau ou les produits de consommation courante[96]. D'autres procédés sont testés comme l'imprégnation de poison sur des T-shirts portés par des militants de l'ANC.
L'année 1986 est marquée par la poursuite de la répression, des milliers d'arrestations et des centaines de morts avec son cortège de bavures policières et de meurtres menés par de mystérieux  touchant à la fois des universitaires blancs de gauche ou des personnalités noires impliquées dans des organisations civiles anti-apartheid[97]. Au début de l'année, plus de 54 townships du pays sont ainsi en guerre ouverte contre le gouvernement et sa politique d'apartheid, deux millions d'étudiants sont en grève et plus de 2 millions de salariés font grève au début du mois de mai[98]. Le gouvernement est divisé entre faucons et réformistes. Une médiation est tentée par les pays du Commonwealth pour amorcer des pourparlers entre l'État et l'ANC : ils proposent qu'en échange de la libération de Nelson Mandela et de ses compagnons, l'ANC renonce à la lutte armée et accepte de négocier une nouvelle constitution sur le modèle des accords de Lancaster House pour la Rhodésie du Sud. Parallèlement, des représentants des plus grandes entreprises sud-africaines rencontrent des membres de l'ANC à Lusaka en Zambie[98]. Le 12 juin 1986, après avoir imposé graduellement des mesures d'urgence dans plusieurs districts administratifs, Botha proclame l'état d'urgence dans les townships[99]. Après avoir appelé à rendre les townships ingouvernables, l'objectif des militants anti-apartheid des townships est dorénavant de créer des contre-pouvoirs à travers la mise en place de comités de rues et de quartiers[100].
De 1946 à 1974, l'Afrique du Sud connait un taux de croissance de 5 % par an[98] qui place l'économie sud-africaine parmi les plus performantes au monde, du point de vue des taux de profit[98]. Cependant, à partir de 1975, elle enregistre une croissance économique relativement faible (2 % en moyenne) avec un tendance au ralentissement (moins de 1 % en 1990), alors que la croissance démographique globale dépasse 2,5 % par an (dont 3 % pour les noirs contre 0,8 % pour les blancs). En termes de revenu par habitant, l'Afrique du Sud se place alors au troisième rang en Afrique avec près de 2 500 dollars, mais le revenu d'un noir représente le quart de celui d'un blanc et le tiers de celui d'un asiatique. Si l'Afrique du Sud est la première puissance économique et militaire du continent africain, le gouvernement sud-africain est soumis progressivement à des embargos à partir des années 1970 en raison de sa politique d'apartheid. Cet embargo l’affecte particulièrement du fait que l'économie sud-africaine est très dépendante de la technologie et des capitaux étrangers. En 1991, le pays fait cependant encore partie des 20 plus grandes nations commerçantes au monde et son commerce extérieur compte pour plus de 60 % de son PIB[101].dégradation
Les États-Unis d'Amérique, l'un de ses principaux alliés, prennent leur distance avec l'Afrique du Sud à la fin des années 1970, sous l'administration de Jimmy Carter. Le pasteur Leon Sullivan obtient partiellement que les filiales et établissements des compagnies nord-américaines implantées en Afrique du Sud ne pratiquent aucune discrimination entre ses salariés sud-africains (principes de Sullivan)[102]. Au cours des années 1980, la dégradation de l'économie sud-africaine ne manque pas d'avoir un impact sur les pays de l'Afrique australe, très dépendants de l'Afrique du Sud et qui absorbent 10 % des exportations sud-africaines[101]. En 1985, la dette extérieure atteint 24 milliards $ tandis que le rand perd la moitié de sa valeur. L'exode des capitaux, débuté au milieu des années 1970, s'accélère non seulement à cause des campagnes anti-apartheid mais aussi en raison de la baisse de rentabilité des firmes étrangères implantées en Afrique du Sud[98]. Ainsi le secteur minier, qui représente 70 % des exportations, stagne et le secteur industriel, le plus vaste du continent, décline faisant perdre à l'Afrique du Sud son statut de pays nouvellement industrialisé[98].
Pour contrer les sanctions économiques et industrielles bilatérales, le gouvernement sud-africain développe durant cette période tout un système permettant de contourner ces sanctions en s'appuyant notamment sur l'internationalisation des grands groupes financiers ou industriels, d'investissements dits off shore et sur quelques États partenaires comme Israël et Taïwan[103]. Le capital afrikaans (majoritaire dans les groupes Sanlam, ABSA, la manufacture d'armes Armscor et le métallurgiste Iskor) contribue notamment à cette stratégie économique et politique destinée à préserver la domination politique de la communauté blanche, conduite par la realpolitik afrikaans[Note 8].
En juin 1986, le gouvernement proclame l'état d'urgence et déploie l'armée dans les townships. Les partenaires commerciaux de l'Afrique du Sud sont inquiets mais peu imposent de sanctions véritablement sévères. En fait, l'État sud-africain reprend les choses en main, parvient à renégocier un rééchelonnement de sa dette grâce à l'appui des banquiers européens, stabilise le rand et profite d'une nouvelle flambée du prix de l'or à partir de 1987. Une nouvelle récession frappe cependant le pays à partir de 1988.
En plus des pressions propres au marché intérieur sud-africain et sa difficulté à produire ce qu'elle consomme, l'impact propre des sanctions économiques et financières reste difficile à évaluer mais il est indéniable que des facteurs extra-économiques ont perturbé les échanges commerciaux et financiers de l'Afrique du Sud. Les premières sanctions ont été posées en 1962 sans être contraignantes. Avant 1984, seul un embargo sur les ventes de pétrole par les membres de l'OPEP et un embargo sur les ventes d'armes, proclamé par les Nations-Unies, avaient eu un minimum d'effets. À partir de 1984, alors que la situation intérieure se dégrade, quelques pays proclament et appliquent un embargo total sur le commerce avec l'Afrique du Sud (Suède, Danemark et Norvège) sans être suivi par les partenaires commerciaux traditionnels de l'Afrique du Sud. Les restrictions que ces derniers appliquent sont principalement diplomatiques ou commerciales comme l'embargo sur les importations de charbon, la fermeture de consulats ou le refus d'exportation de technologies. Seuls les États-Unis, premier partenaire commercial de l'Afrique du Sud en 1985, adoptent une position dure avec le comprehensive anti-apartheid act de 1986 : arrêt de nouveaux investissements, embargo sur plusieurs produits comme le charbon et l'acier, arrêt des liaisons aériennes[101]. En 1987, seulement 8 % des exportations sud-africaines ont été affectées alors que l'or et les métaux dits stratégiques n'ont été frappées d'aucun embargo. Si les exportations sud-africaines vers les États-Unis ont chuté de 44,4 %, cela résulte surtout de l'embargo sur le charbon et sur l'uranium. Le Japon a quant à lui remplacé les États-Unis comme premier partenaire commercial de l'Afrique du Sud en devenant le principal importateur de produits sud-africains suivi par l'Allemagne et l'Angleterre[101]. De 1981 à 1988, 40 % des multinationales opérant en Afrique du Sud quittent le pays (soit 445 firmes)[101] mais des études démontrent que le départ des firmes internationales s'est effectué de manière à maintenir des liens financiers et technologiques entre les multinationales et leurs ex-filiales sud-africaines. Ainsi, 53 % des groupes américains ayant désinvesti d'Afrique du Sud ont assuré la persistance d'un certain nombre d'accords de licence, de fabrication, d'accords de franchise ou d'échanges technologiques (IBM ou Ford par exemple)[101]. Fin 1989, il reste 180 entreprises britanniques, 160 firmes américaines, 140 sociétés allemandes et 24 entreprises françaises directement implantées dans le pays alors que Taïwan a augmenté ses parts de marché en profitant du départ des entreprises occidentales[104]. Si l'Afrique du Sud paye les frais de sa politique d'apartheid, les sanctions économiques et commerciales, plus ou moins contraignantes, ont été diversement appliquées : l'Allemagne et le Japon sont devenus ainsi les premier et deuxième partenaires commerciaux de l'Afrique du Sud.
L'exclusion des compétitions sportives porte un coup au moral de la population blanche et joue un rôle dans le recul de la politique d'apartheid.
L'isolement de l'Afrique du Sud a commencé au début des années 1960 après que l'Afrique du Sud s'est retirée du Commonwealth et s'est érigée en République. Plusieurs pays africains (Liberia, Éthiopie), l'Inde et les pays du bloc de l'Est font pression pour que l'Afrique du Sud soit exclue de plusieurs organismes internationaux.
En 1966, le conseil du sport en Afrique (une structure de l'Organisation de l'unité africaine) exige, auprès du comité international olympique, l'expulsion de l'Afrique du Sud et menace de boycotter les Jeux olympiques de 1968. Malgré un avis d'abord défavorable du CIO, et à la suite des menaces de boycott des athlètes afro-américains et pays du bloc de l'Est si l'Afrique du Sud était présente, le comité exécutif du CIO décide finalement d'exclure l'Afrique du Sud des jeux olympiques de Mexico[105]. En 1976, l'Afrique du Sud est indirectement la cause du boycott des jeux olympiques de Montréal. Vingt-six pays africains, l'Irak et le Guyana protestent contre les rencontres sportives de rugby entre la Nouvelle-Zélande et l'Afrique du Sud, alors que le rugby n'est pas un sport olympique[106]. En 1980, l'Afrique du Sud participe encore aux jeux paralympiques et n'est exclue du comité qu'en 1985[107].
En 1980, les Nations unies publient un registre mentionnant le nom des sportifs et des officiels participants à des manifestations sportives en Afrique du Sud afin d'exercer une pression morale sur les athlètes.
La tournée faite par les Springboks en 1981 en Nouvelle-Zélande est marquée par des mouvements de protestation contre l'apartheid[108],[109]. Plus aucun pays n'accepte d'organiser de rencontres avec l'équipe de rugby sud-africaine, emblématique de la population blanche sud-africaine. Pour tenter de briser son isolement et à défaut de pouvoir jouer contre les Pumas, l’Afrique du Sud dispute huit matchs en 1980, 1982 et 1984 contre une sélection appelée les Jaguars sud-américains. En 1984, l'équipe des Springboks aligne deux joueurs métis, Errol Tobias et Avril Williams, contre l'Angleterre mais en 1985, la tournée des All Blacks en Afrique du Sud est annulée. Une tournée non officielle est cependant effectuée par une équipe appelée les Cavaliers comprenant de nombreux All Blacks. Les joueurs sont suspendus à leur retour en Nouvelle-Zélande[110]. Les Springboks sont isolés et privés de rencontres sportives internationales. Le grand prix automobile de formule un organisé à Kyalami en 1985, constitue alors le dernier grand évènement sportif international organisé en Afrique du Sud (jusqu'en 1992).
Pendant l'apartheid, seuls les blancs sont soumis au service militaire. Le statut des objecteurs de conscience n'est ouvert qu'aux adeptes de certaines religions.
L’Assemblée générale des Nations unies a implicitement reconnu un type d’objection sélective dans sa résolution 33/165 du 20 décembre 1978, dans laquelle elle a demandé aux « États membres d’accorder l’asile ou le droit de transit vers un autre État […] aux personnes contraintes de quitter leur pays d’origine parce qu’elles refusent, par objection de conscience, de contribuer à l’application de l’apartheid en servant dans des forces militaire ou policières »[111].Le mouvement Halte à la conscription, lancé en 1963, organise, en juin 1985, à Johannesbourg, un Festival de la paix avec le soutien des principales Églises du pays, notamment l'Église catholique et le conseil des Églises d'Afrique du Sud[112]. L'écrivain André Brink refuse d'obéir à un amendement à la loi de Défense qui permet de rappeler sous les drapeaux les hommes jusqu'à l'âge de cinquante-cinq ans.
« J'ai informé les autorités militaires que je refuserais de subir tout entraînement ou de servir dans l'armée à quelque poste que ce soit. Quelle que soit la peine encourue, je ne suis pas prêt à vivre et à mourir pour ce régime et ce système ; ils ne représentent pas l'Afrique du Sud que j'aime et que je respecte, et ils ne servent pas les idéaux et les valeurs auxquels je suis attaché en tant qu'écrivain et en tant qu'homme. »
— André Brink[113], traduit de l'anglais par Jean GuiloineauEn 1988, 143 blancs annoncent leur refus d'accomplir leurs obligations militaires dans l'armée d'apartheid. Certains d'entre eux sont condamnés à six ans de prison[114]. Le gouvernement sud-africain interdit la Campagne pour la fin de la conscription (End Conscription Campaign)[115].
La conscription est abolie à la fin de l'apartheid.
Sous la pression de l'opinion occidentale (accompagnée de pressions économiques, notamment en provenance des États-Unis), de la contestation interne, de l'agitation de plus en plus incontrôlable dans les cités noires (les townships), et également de la pression démographique, l'apartheid est assoupli sous le gouvernement de Pieter Willem Botha.
Après avoir autorisé la formation de syndicats noirs (en 1979), Botha entame en 1984 la réduction graduelle des inégalités de salaires entre Blancs et Noirs dans les mines et en 1985, autorise la formation de syndicat inter-raciaux et de partis non exclusivement blancs, du moment qu'ils n'aient pas été interdits par le passé. Ainsi se constituent le Front démocratique uni (en fait vitrine interne de l'ANC interdite) et la COSATU qui en deux ans allaient confédérer 33 syndicats et revendiquer 220 000 adhérents.
Inspiré du modèle consociationnel, une nouvelle constitution entre en vigueur le 3 septembre 1984, établissant un régime présidentiel basé sur un parlement tricaméral. Cette constitution, adoptée en 1983 au cours d'un référendum par les électeurs blancs sud-africains, libéralise le régime institutionnel de l'apartheid. Approuvée par 65 % des électeurs, elle établit trois chambres parlementaires distinctes, élus par les trois groupes raciaux minoritaires du pays, les blancs, les indiens et les métis en proportion de leur population. Les noirs n'ont droit à aucune représentation nationale. Ceux résidant dans les zones urbaines se voient cependant octroyés de nouveau la citoyenneté sud-africaine et obtiennent le droit d'élire leurs propres conseillers municipaux[116]. Le cabinet ministériel s'ouvre pour la première fois aux non blancs avec la nomination de Amichand Rajbansi et Allan Hendrickse chargés des affaires indiennes et métis.
En 1985, la loi portant interdiction des mariages mixtes est abrogée.
En 1986, la loi sur les laissez-passer ('Pass Laws Act') de 1952 est abolie[117], permettant aux Noirs sud-africains pauvres de se déplacer librement en ville, voire de s'y installer. La pérennité de la présence des Noirs dans les frontières de l'Afrique du Sud blanche est reconnue au sommet de l’État[118].
Plus généralement, le gouvernement annonce la suppression des mesures vexatoires de discrimination dans les lieux publics (aussi appelé « petty apartheid ») provoquant de vives réactions dans les milieux conservateurs[119]. Après les élections municipales sud-africaines de 1988 favorables au parti conservateur, celui-ci fait réinstaller dans les municipalités qu'il dirige les panneaux de ségrégation.
En 1987, la loi de 1922 fixant la liste des emplois réservés aux Blancs est abrogée. Bien que des négociations secrètes directes entre l'ANC et des membres du parti national soient organisées depuis 1985 avec l'accord du gouvernement, Botha refuse d'aller plus loin dans ses réformes, soucieux de la scission de plus en plus importante entre afrikaners modérés et conservateurs. Ainsi, la répression policière l'emporte durant les dernières années de son mandat alors que le front démocratique uni est à son tour interdit.
L'arrivée au pouvoir en 1989 de Frederik de Klerk modifie la donne politique. C'est au bout de six mois de délibération au sein du parti national que cet Afrikaner, jusque-là réputé pour ses positions conservatrices, avait accédé au poste de dirigeant du parti avec pour mandat de réformer le système afin de répondre à l'aspiration du vote égalitaire des Noirs et de poser dans un cycle de négociations constitutionnelles, l'intangibilité de certains principes comme l'inviolabilité de la propriété privée et la mise en place de dates butoirs pour bloquer de possibles revendications foncières. Les partis politiques interdits sont légalisés et les négociations officiellement entamées dès mars 1990 entre l'ANC et le gouvernement. La majeure partie des lois d'apartheid sont abolies entre 1989 et juin 1991 et un forum constitutionnel est mis en place en avril 1992, à la suite du mandat spécifique pour négocier avec les partis comme l'ANC, accordé par près de 70 % des électeurs blancs au président sud-africain le 17 mars 1992 lors d'un référendum. Si certains des Afrikaners conservateurs se réfugient dans des utopies communautaristes (Volkstaat), d'autres, qui considèrent aussi qu'ils sont le cœur de la nation blanche d'Afrique du Sud, réinventent le slogan « s'adapter ou mourir » pour conduire l'ouverture politique envers la majorité noire du pays. Après 4 années de négociations constitutionnelles, les premières élections multiraciales se déroulent en avril 1994, débouchant sur l'élection de Nelson Mandela, premier président noir de la République d'Afrique du Sud.
De 1996 à 1998, une Commission de la vérité et de la réconciliation sillonne le pays pour récolter les témoignages des victimes et des oppresseurs, des partisans ou des opposants à l'apartheid[120], afin de recenser toutes les violations des droits de l'homme commis de 1960 à 1993 et d'éclaircir les crimes et exactions politiques commis au nom du gouvernement sud-africain mais également les crimes et exactions commis au nom des mouvements de libération nationales.
Le rapport final de cette commission a épinglé l'absence de remords ou d'explication de certains anciens responsables politiques du régime d'apartheid (Pieter Botha, Frederik de Klerk, Magnus Malan) mais aussi le comportement de certains chefs de l'ANC, notamment dans les camps d'entrainements d'Angola et de Tanzanie. Des poursuites judiciaires ou des amnisties ont été recommandées (quand les motivations d'auteurs de crimes ou de délits étaient essentiellement politiques), d'autres ont été refusées. La plupart de ceux qui avaient été inculpés devant les tribunaux comme Magnus Malan ou Wouter Basson ont été acquittés pour insuffisance de preuves, ou parce qu'ils avaient obéi aux ordres.
Lors d’un entretien avec un journaliste[121], Philippe Hugon[122] affirme que malgré l’apartheid, l’Afrique du Sud est devenue . En fait, même si l’on pouvait craindre une situation à l’algérienne, avec une forte opposition du pouvoir blanc sur le modèle de l’OAS et un rejet violent des Blancs par les populations discriminées, il faut reconnaître que « grâce à quelques grandes figures comme Frederik de Klerk et Nelson Mandela », le bain de sang entre les deux communautés a pu être évité. D’une part, l'African National Congress (ANC) a abandonné le concept de « clivage de race » pour celui de « clivage social » grâce notamment aux actions conjuguées de Mgr Desmond Tutu et de la Commission de la vérité et de la réconciliation. Malgré l’apartheid, l’Afrique du Sud a su se redresser assez vite. .
L'Affirmative Action (la discrimination positive) a permis l’accès de nombreux Noirs dans l'économie, la politique, la culture et l'enseignement. Pourtant en 2011, à la date de l’interview, le chômage des Noirs avoisinait les 40 %, la lutte contre le sida avait souffert d'un aveuglement politique et la réforme agraire, très attendue, se faisait très lentement puisque 80 % des terres appartenait toujours aux Blancs. Bien que très optimiste, Philippe Hugon notait que .  Chez les plus modestes, force est de constater que « l'identité ethnique joue un rôle central dans la manière de se penser et de penser l'autre ». En outre, il ne faut pas oublier qu’entre 1995 et 2009, 800 000 Blancs ont quitté le pays fuyant la violence, le chômage et la discrimination positive.
1948
1949
1950
1951
1952
1953
1954
1955
1956
1958
1959
1960
1961
1963-1964
1966
1968
1970
1971
1974
1976
1977
1978
1979
1981
1982
1983
1984
1985
1986
1988
1989
1990
1991
1992
1993
1994
1996-1998
Dans les médias, l'apartheid est représenté que ce soit dans les domaines littéraires, cinématographiques et musicaux.
Vous lisez un « bon article » labellisé en 2009.
Cet article concerne le pôle Nord géographique. Pour les autres définitions, voir Pôle Nord .
Le pôle nord géographique terrestre, ou simplement pôle Nord, est le point le plus septentrional de la planète Terre. Il est défini comme le point d’intersection de l'axe de rotation de la Terre avec la surface terrestre de l'hémisphère nord, où tous les méridiens et les fuseaux horaires se rencontrent. Ce point géographique n'est pas fixe par rapport à l'axe de rotation de la Terre, car elle oscille faiblement suivant une période d'environ quatorze mois. Néanmoins, on considère souvent que sa position est fixe par commodité.
Le pôle Nord géographique ne doit pas être confondu avec le pôle Nord magnétique, le point central du champ magnétique terrestre vers lequel toutes les boussoles pointent. Ce concept, appelé « vrai Nord », a été découvert, décrit et rapporté par le scientifique polymathe chinois Shen Kuo au XIe siècle.
Le pôle Nord se situe au milieu de l'océan Arctique, au-dessus de la plaine abyssale polaire (nommée aussi bassin d'Amundsen ou encore bassin du Fram) et à proximité de la dorsale de Lomonossov. À la verticale du pôle, l'océan atteint une profondeur de 4 261 mètres[1] et est couvert en permanence par la banquise arctique, contrairement au pôle Sud situé sur la masse continentale Antarctique. Hormis certains bancs de gravier non permanents, la terre émergée la plus proche est l'île Kaffeklubben, située à 707 kilomètres du pôle.La première exploration du pôle Nord, bien que contestée, est attribuée à l'Américain Frederick Cook qui aurait atteint le pôle le 21 avril 1908, mais il aurait maquillé son trajet réel[2]. Le Congrès des États-Unis a plutôt attribué la première exploration à l'Américain Robert Peary qui prétend avoir atteint le pôle Nord le 6 avril 1909, mais les historiens contestent ce fait depuis la découverte d'une copie du journal de Peary, qui se serait trompé dans ses estimations[3]. La première exploration confirmée du pôle Nord revient donc au Norvégien Roald Amundsen et à l'Italien Umberto Nobile, qui le survolent à bord d'un ballon dirigeable le 12 mai 1926.
La température du pôle Nord peut varier entre −43 °C et 0 °C, ce qui favorise la permanence de la glace de mer dont l'épaisseur varie entre deux et quatre mètres. La banquise est cependant menacée et l'océan Arctique pourrait être libre de glace dans le courant du XXIe siècle, en partie du fait du réchauffement climatique et de la diminution de l'effet albédo[4]. Cette situation nouvelle rendra plus facile l'accès aux ressources du sous-sol Arctique et une dispute territoriale est enclenchée entre les cinq pays limitrophes de l'Arctique : le Canada, la Russie, la Norvège, le Danemark et les États-Unis. Bien que le pôle Nord soit hors des zones économiques exclusives de ces pays, la découverte récente de la dorsale de Lomonossov relance le débat de la souveraineté territoriale de l'Arctique.
L'axe de rotation de la Terre est incliné de 23° 26′ 13" par rapport à l'écliptique, le plan de l'orbite héliocentrique de la Terre. Le pôle Nord définit les géodésiques de latitude 90° Nord (Φ = + π / 2), ainsi que la direction du vrai Nord, alors que sa longitude peut être définie comme n'importe quelle valeur (-π ≤ λ ≤ + π)[5]. L'axe de rotation — et donc la position du pôle Nord — fut longtemps considéré comme fixe par rapport à la surface de la Terre, mais au XVIIIe siècle, le mathématicien Leonhard Euler pressent que l'axe pourrait osciller[6]. Au début du XXe siècle, les astronomes ont remarqué une petite variation de la latitude telle que déterminée pour un point fixe sur la Terre à partir de l'observation des étoiles. D'une portée de quelques mètres pour les plus fortes oscillations, l'errance du pôle à la surface de la Terre, semblable au mouvement d'une toupie, a plusieurs composantes périodiques et irrégulières. La composante périodique de 433 jours, identifiée aux huit mois prévus par Euler, est désormais appelée oscillation de Chandler[7]. Le point exact de l'axe de la Terre, à un moment donné, est appelé le « pôle instantané », mais en raison de l'oscillation des pôles, on ne peut pas l'utiliser comme définition d'un pôle fixe lorsqu'une mesure précise à l'échelle du mètre est requise[8].
L'oscillation de l'axe de rotation terrestre serait due principalement aux variations saisonnières de la masse de la calotte glaciaire et de la répartition des précipitations neigeuses. En 2005, un changement de direction et de vitesse anormal du déplacement du pôle Nord géographique vers le Groenland a été observé. Ce changement est fortement corrélé à l'accélération de la fonte des glaces au Groenland et plus faiblement sur tout le globe[9].
Un système fixe de coordonnées terrestres (latitude, longitude et altitude, ou l'orographie) serait utile, mais compte tenu de la dérive des continents, de la hausse et de baisse de la Terre en raison des volcans, de la précession des équinoxes, de l'érosion, entre autres, il n'y a pas de système dans lequel toutes les caractéristiques géographiques puissent être fixées. L’International Earth Rotation and Reference Systems Service et l'Union astronomique internationale ont donc défini un cadre appelé le « Système international de référence terrestre ». Le pôle Nord de ce système définit à présent le Nord géographique pour le travail de précision, mais il ne coïncide pas exactement avec l'axe de rotation terrestre[10].
Comme il n'y a pas de présence humaine permanente au pôle Nord, il n'y a pas d'heure ni de fuseau horaire officiellement attribués à cette région du globe, mais en pratique le pôle comporte tous les fuseaux en un seul point[11]. Étant donné que toutes les lignes de longitude s'y rencontrent, on peut marcher à travers tous les fuseaux horaires en une seconde en tournant autour du pôle. De plus, les heures locales étant à peu près synchronisées sur la position du Soleil à son zénith, cette approximation ne fonctionne pas au pôle Nord, car il est continuellement dans le ciel durant six mois.
Les expéditions polaires peuvent utiliser un fuseau horaire qui leur convienne, généralement le fuseau horaire de leur base de ravitaillement, ou encore n'importe quelle unité du Temps moyen de Greenwich (GMT), du Temps universel coordonné (UTC), ou le fuseau horaire du pays de leur choix. Cependant, la convention veut que l'on utilise le fuseau UTC+0[12], contrairement au pôle Sud où l'on utilise le UTC+12 (fuseau horaire de la Nouvelle-Zélande durant l'été austral)[11].
Le pôle Nord est nettement plus chaud que le pôle Sud, car il se situe au niveau de la mer au milieu d'un océan qui agit comme un réservoir de chaleur, plutôt qu'en altitude sur une masse continentale. En hiver (janvier), la température au pôle Nord peut varier de −43 à −26 °C, pour une moyenne de −34 °C. La température moyenne d'été (juillet) se situe autour du point de congélation (0 °C)[13].
La glace de mer du pôle Nord est d'environ deux ou trois mètres d'épaisseur, mais il existe des variations considérables, et parfois la circulation des banquises expose la surface maritime. Une récente étude[Quand ?] financée par l'Union européenne et publiée dans la revue Geophysical Research Letters a montré que la moyenne de l'épaisseur de la glace a diminué au cours des dernières années[Depuis quand ?][14]. Selon les chercheurs du Centre for Polar Observation and Modelling (CPOM) de l'University College de Londres (UCL) au Royaume-Uni, la diminution de la banquise est attribuée au réchauffement climatique et à l'océan Arctique de plus en plus exposé au rayonnement solaire[14]. L'accélération drastique de la fonte des glaces de mer en Arctique n'avait pas été prévue par les modèles climatiques, car ils ne tenaient pas compte de l'effet albédo ; plus l'océan Arctique se libère de ses glaces, plus la chaleur des rayons solaires est absorbée par l'eau, accélérant la fonte de la calotte polaire[15],[16].
En 2008, des scientifiques prédisent que le pôle Nord pourrait devenir libre de glace durant l'été dès 2050[17].
De récentes découvertes[Quand ?] confirment que le pôle Nord a connu un climat subtropical, il y a 55 millions d'années[18]. La mission Arctic Coring Expedition (ACEX) a récolté des carottes de glaces sur la dorsale de Lomonossov en 2004[19], dont l'analyse révèle la présence de microfossiles de plantes et d'animaux typiques d'un environnement de mer chaude (environ 20 °C) et peu profonde subtropicale. Les fossiles trouvés à 400 mètres sous le plancher océanique datent du maximum thermique de la période du Paléocène-Éocène[18].
Au pôle Nord, le Soleil est en permanence au-dessus de l'horizon pendant les mois d'été et de façon permanente au-dessous de l'horizon pendant les mois d'hiver. Le lever du soleil a lieu juste avant l'équinoxe vernal (environ le 19 mars). Le Soleil prend ensuite trois mois pour atteindre son point culminant, soit environ 23,4368° d'élévation, au solstice d'été (environ le 21 juin). Ensuite, il commence à redescendre pour atteindre le coucher du soleil juste après l'équinoxe d'automne (environ le 24 septembre). Quand le Soleil est visible dans le ciel polaire, il semble se déplacer dans un cercle au-dessus de l'horizon. Ce cercle passe peu à peu de près de l'horizon, juste après l'équinoxe vernal à son maximum d'élévation (en degrés) au-dessus de l'horizon au solstice d'été et redescend vers l'horizon avant de passer à l'équinoxe d'automne[20],[21].
Au pôle le crépuscule civil dure environ deux semaines avant le lever et après le coucher du soleil, alors que le crépuscule astronomique dure environ sept semaines avant le lever et après le coucher du soleil[22].
Ces effets sont causés par une combinaison de l'inclinaison axiale de la Terre et de sa révolution autour du Soleil[23]. La direction de l'inclinaison axiale de la Terre, ainsi que son angle par rapport au plan orbital de la Terre autour du Soleil, restent à peu près constants au cours d'une année (les deux évoluent très lentement sur de longues périodes de temps). Au milieu de l'été, le pôle Nord est incliné vers le Soleil à son maximum. Au cours de l'année, la Terre se déplace autour du Soleil et le pôle Nord se détourne progressivement (par rapport au Soleil), pour finalement s'en éloigner au maximum. La même séquence s'observe au pôle Sud avec six mois de décalage.
Les animaux et les végétaux de l'Arctique sont, par leur physique et leur comportement, adaptés aux conditions particulières des régions au nord du cercle Arctique (66° 32′ Nord). La courte saison de croissance est certainement le facteur le plus contraignant pour la faune et la flore arctiques, limitées par la température, la lumière et la calotte glaciaire. La productivité marine au pôle est plus ou moins importante selon les années, les saisons et la proximité au pôle[25], ainsi la croissance de la biomasse ne dépasse pas 100 mgC/m2/jour au centre du bassin polaire, elle est de deux à cinq fois moins importante que dans la zone ouverte de l'océan Arctique. La présence de la vie sous cette partie de la banquise arctique n'est pas plus importante qu'en haute mer ; la production primaire mesurée en Méditerranée occidentale est équivalente, contrairement aux zones de très haute production comme les côtes froides du Pérou où la biomasse est cent fois plus productive[25].
Cette biomasse arctique est principalement constituée de zooplancton tel que les amphipodes benthiques se nourrissant du phytoplancton (dinoflagellés et diatomées) qui poussent dans les couches inférieures et sous la surface submergée de la glace flottante. Même durant l'hiver, certaines algues peuvent continuer leur processus de photosynthèse en profitant des très faibles lueurs de la nuit polaire. Cette production attire les poissons, les cétacés et les phoques durant l'été, parfois même à proximité du pôle[24]. Les récits témoignant d'une présence animale autour du pôle Nord demeurent tout de même anecdotiques, mais on observe une importante perturbation de la productivité causée par le réchauffement climatique[26]. En effet, on peut observer plus de 275 espèces de plantes et d'animaux se rapprochant du pôle durant l'été en raison du réchauffement[27].
La pêche halieutique autour de cette zone est favorisée tandis que la mégafaune est défavorisée. L'ours blanc se déplace rarement au-delà de 82° de latitude Nord, en raison de la rareté de la nourriture, bien que des traces soient parfois observées près du pôle Nord[28]. Une expédition en 2006 a signalé avoir observé un ours blanc à un peu plus d'un kilomètre du pôle[29]. Le phoque annelé a également été observé près du pôle, et un renard polaire a été vu à moins de 60 kilomètres, à 89° 40′ Nord[30].
Parmi les oiseaux observés près du pôle, plusieurs espèces ont été signalées : des bruants des neiges, des fulmars boréaux et des mouettes tridactyles, bien que certaines observations puissent être faussées par le fait que les oiseaux ont tendance à suivre les navires et les expéditions[28]. Des poissons ont été vus dans les eaux au pôle Nord, mais ils sont probablement peu nombreux[28]. Bien que certaines espèces puissent comporter un grand nombre d'individus, le froid polaire ralentit leur métabolisme et elles peuvent mettre jusqu'à deux ans en eau polaire avant d'atteindre leur maturité sexuelle[24].
La pollution des eaux arctiques a également un impact important sur la natalité via la chaîne alimentaire du cercle polaire. Certains métaux lourds tels que le zinc, le cadmium, le mercure et le sélénium sont concentrés dans l'océan Arctique par les courants marins provenant des océans Atlantique et Pacifique. Les polluants bioaccumulés dans le métabolisme d'un individu augmentent avec l'absorption des niveaux inférieurs du réseau alimentaire océanique[24]. Ainsi, des contaminants peuvent être présents en quantité infime dans le zooplancton, mais on observe des taux anormalement concentrés dans le métabolisme des espèces superprédatrices comme les oiseaux de mer, les phoques, les ours et même à l'extrémité de la chaîne, chez les humains. Des prélèvements de sang de cordon des nouveau-nés inuits révèlent un taux de polychlorobiphényles quatre fois plus grand et un taux de mercure quinze à vingt fois plus élevé que chez les bébés nés plus au sud. Ces polluants présents dans les métabolismes ont un impact sur le taux de natalité, et peuvent provoquer des déficits de neurotransmission et divers problèmes cognitifs[31].
L'océan Arctique est identifié depuis des décennies comme une région riche en pétrole et en gaz naturel. Dans une publication datant de juillet 2008, le United States Geological Survey estime que le sous-sol arctique renfermerait 90 milliards de barils de pétrole, 1 670 billions de pieds cubes de gaz naturel et 44 millions de barils de gaz naturel liquide[32]. Cette situation rend le pôle Nord et l'Arctique très convoités par les pays limitrophes[33].
En vertu du droit international, aucun pays ne possède actuellement le pôle Nord ou la région de l'océan Arctique qui l'entoure. Les cinq États limitrophes de l'Arctique, soit la Russie, le Canada, la Norvège, le Danemark (via le Groenland), et les États-Unis (via l'Alaska), sont limités à une zone économique exclusive de 200 milles marins (environ 370 kilomètres) autour de leurs côtes. Au-delà de ces ZEE, la zone restante, qui représente plus d'un million de kilomètres carrés, n'est attribuée à aucun pays et c'est l'Autorité internationale des fonds marins qui administre ce territoire[34].
Lors de la ratification de la Convention des Nations unies sur le droit de la mer, un pays a une période de dix ans pour faire valoir légalement sa zone de 200 milles marins[35]. Ainsi, la Norvège (qui a ratifié la convention en 1996[36]), la Russie (en 1997[37]), le Canada (en 2003[38]) et le Danemark (en 2004[36]) ont tous lancé des projets pour revendiquer certains secteurs de l'Arctique[39].
En 1948, une expédition russe fait la découverte de la dorsale de Lomonossov, une structure géologique s'étendant sur 1 800 kilomètres depuis les îles de Nouvelle-Sibérie jusqu'au large du Groenland[40],[41] et de l'île d'Ellesmere[42]. Ce n'est qu'au début des années 2000 que la dorsale océanique attire l'attention de la communauté internationale, à la suite de la requête officielle de la fédération de Russie auprès de la commission des Nations unies, portant sur la limite du plateau continental. Le document propose d'établir une nouvelle limite périphérique pour le plateau continental russe, en dehors de la zone des 200 milles marins. Cependant l'article 76 de la Convention des Nations unies sur le droit de la mer prévoit qu'une extension de 150 milles marins peut faire l'objet d'une requête, si le plateau continental se prolonge sous l'eau[4], ce qui exclut la revendication russe dont le plateau se trouve à plus de 1 000 milles marins du pôle.
Le Danemark, le Canada et la Russie se disputent la structure géologique et affirment que la dorsale est une extension de leurs plateaux continentaux respectifs[43]. À la fin juin 2007, quelque soixante-dix géologues russes auraient réussi à prouver que les dorsales médio-océaniques de Lomonossov seraient des extensions du continent eurasiatique, reliant ainsi le pôle Nord au territoire russe[44],[45]. C'est sur la base de cette affirmation que l'expédition russe Arktika 2007 envoya le brise-glace nucléaire Rossia, le navire de recherche Akademik Fédorov et deux mini-sous-marins, Mir 1 et Mir 2, pour explorer la région. Le 2 août 2007, des scientifiques russes ont plongé à 4 261 mètres sous la surface, et déposé une capsule de titane contenant le drapeau russe, en symbole de leur revendication sur la région[45],[46].
Les États-Unis, le Canada et le Danemark contestent la prise du pôle Nord par la Russie[47]. À ce sujet, le ministre canadien des Affaires étrangères, Peter MacKay a déclaré : 
La 9e conférence Artic Frontiers se tenait du 18 au 23 janvier 2015 à Tromsø en Norvège[49]. L’envoyé spécial du président de la Russie était Artur Chilingurov, à qui on attribue d’avoir planté un drapeau russe au fond de l’océan du pôle Nord en 2007. M. Chilingurov déclarait que les pays limitrophes de l’arctique avaient démontré sans exagération, qu’ils étaient un modèle de bon voisinage… « Dans l’Arctique, il n’existe pas de problèmes qui requièrent une solution militaire. Nous ne ressentons aucun des effets reliés aux tensions politiques mondiales. » Au mois de mars 2015, la Russie planifie l'ouverture d'un centre de recherche et la station arctique de Barneo[50].
Dès le XVIe siècle, de nombreux géographes estiment que le pôle Nord est situé dans une mer, qui est appelée une polynie au XIXe siècle, ou mer polaire ouverte[51]. Le géographe allemand August Petermann est le principal partisan de l'existence d'une mer polaire libre de glaces. Cette théorie se révèle plus tard erronée, mais elle justifie plusieurs expéditions entre 1853 et 1876. Ces explorateurs espèrent trouver une voie navigable libre de glaces vers le pôle à un moment favorable de l'année.
Le 16 juin 1596, une expédition néerlandaise menée par Willem Barentsz atteint la latitude 79° 49′ Nord, fixant le premier record homologué d'une présence humaine aussi septentrionale[52]. Barentsz meurt l'année suivante.
En 1893, Fridtjof Nansen et Hjalmar Johansen mènent une expédition avec le navire Fram, qui est à l'origine d'un nouveau type de navire, le brise-glace. Ils sont les premiers hommes à s'approcher d'aussi près du pôle Nord, atteignant la latitude 86° 14′ Nord le 8 avril 1895[53].
En 1908, l'Américain Frederick Cook organise une expédition très légère, et part accompagné de seulement deux Inuits. Il affirme avoir atteint le pôle le 21 avril 1908, mais ne rentre à sa base que le 18 avril 1909, après un long périple. Son témoignage est mis en doute par plusieurs historiens, qui avancent que Cook se serait trompé dans ses estimations. Certains ont également relevé un faisceau d'indices indiquant qu'il aurait pu maquiller son trajet réel et n'aurait en fait jamais tenté d'approcher le pôle Nord[2]. L'histoire de cette expédition est encore aujourd'hui controversée.
L'Américain Robert Peary prétend avoir atteint le pôle Nord le 6 avril 1909 accompagné de Matthew Henson et de quatre Inuits – Oatah, Egingwah, Seegloo et Ookeah[54]. Il obtient avec difficulté la reconnaissance de son exploit par le Congrès des États-Unis, qui finit par le désigner officiellement comme le premier homme arrivé au pôle Nord. Depuis la découverte d'une copie du journal de Peary, il semble aujourd'hui presque certain qu'il s'est trompé dans ses estimations, et qu'il ne se soit approché du pôle que d'une quarantaine de kilomètres. La « communauté exploratrice » de l'époque lui reprocha vivement de n'avoir fait aucun relevé de position au cours des derniers 200 kilomètres de son voyage vers le pôle. Robert M. Bryce remarque également que la vitesse moyenne que Peary affirme avoir soutenue lors de son retour est considérée comme humainement impossible, et n'a jamais été égalée depuis[3]. De plus Matthew Henson, un explorateur afro-américain, a devancé de 45 minutes l'expédition et est ainsi le premier de l'expédition à atteindre le pôle. Mais Henson était noir, et ce fut Peary qui obtint tous les honneurs[54],[55].
Les premiers à atteindre le pôle Nord avec certitude sont l’italien Umberto Nobile et le norvégien Roald Amundsen, qui le survolent à bord du dirigeable italien Norge parti depuis Rome le 29 mars 1926 et ayant transité par Oslo le 14 avril. le 12 mai 1926, les deux hommes survolent le Pôle Nord.
Le Soviétique Ivan Papanine s'y pose en avion le 21 mai 1937, et le Britannique Wally Herbert l'atteint en traîneau à chiens le 5 avril 1969.
En 1958, le sous-marin américain USS Nautilus est le premier sous-marin à atteindre le pôle lors de la traversée de l'Arctique[56]. Il est suivi par le USS Skate en 1959, qui est le premier à faire surface au pôle[56].
Le 17 août 1977, le brise-glace soviétique à propulsion nucléaire Arktika est le premier navire de surface à atteindre le pôle Nord, démontrant ainsi qu'il est possible de naviguer en été dans les eaux arctiques[57].
Le 29 avril 1978, l'explorateur japonais Naomi Uemura est le premier à atteindre le pôle Nord en solitaire, au terme d'un voyage de 800 kilomètres. Parti du Cap Columbia au nord de l'Île d'Ellesmere au Canada, il suit les traces de l'expédition de Peary, en traîneau tiré par dix-sept chiens. Il est régulièrement ravitaillé par avion qui lui parachute des vivres[58].
En 1982, Ranulph Fiennes et Charles R. Burton sont les premiers à traverser l'océan Arctique en une seule saison. Le 17 février 1982, ils quittent l'île Crozier au large de l'île d'Ellesmere, et arrivent au pôle Nord géographique le 11 avril 1982. Leur expédition s'est faite à pied et en motoneige. Depuis le pôle, ils regagnent le sud en direction du Svalbard, mais en raison de l'instabilité de la glace, ils mettent fin à leur traversée au niveau de la lisière des glaces, et dérivent vers le sud sur une banquise durant 99 jours. Le 4 août 1982, ils peuvent finalement rejoindre leur navire d'expédition le MV Benjamin Bowring, à la latitude 80° 31′ Nord. À la suite de ce voyage, durant trois ans, Fiennes et Burton participent à l'expédition Transglobe, et deviennent les premiers à faire un circumpolaire, c'est-à-dire le tour du monde par les deux pôles[59]. Cette réalisation demeure incontestée à ce jour, et le Livre Guinness des records décrit Ranulph Fiennes comme « le plus grand explorateur vivant[60] ».
Le 1er mai 1986, la Will Steger International Polar Expedition est la première expédition à atteindre le pôle Nord sans aucun ravitaillement. Les membres de l'équipe sont : Paul Schurke, Brent Boddy, Richard Weber, Geoff Carroll, Ann Bancroft et une équipe de vingt-et-un chiens. Brent Boddy et Richard Weber deviennent ainsi les premiers Canadiens à atteindre le pôle, et Ann Bancroft la première femme[61]. Quelques jours plus tard, le 11 mai, Jean-Louis Étienne est le premier à atteindre le pôle Nord à ski et en solitaire. Il est le premier Français à atteindre le pôle, après 63 jours de marche[62].
En 1988, l'expédition Polar Bridge est la première traversée de l'océan Arctique à ski. L'expédition est composée de neuf Russes et quatre Canadiens qui parcourent les 1 800 kilomètres séparant le nord de la Sibérie de l'île d'Ellesmere au Canada en passant par le pôle Nord. Richard Weber (Chef de l'équipe canadienne) devint le premier homme à atteindre le pôle Nord à partir des deux côtés de l'océan Arctique[63]. Puis en 1995, Weber en compagnie de Mikhail Malakhov, sont les premiers à atteindre le pôle Nord et en revenir sans aucune assistance, sans ravitaillement et en utilisant uniquement des ressources humaines[63].
En 2005, le sous-marin nucléaire de classe Los Angeles de la United States Navy, le USS Charlotte, fait surface au pôle Nord, en perçant 155 centimètres de glace. Les 137 membres d'équipage et les 17 officiers passent 18 heures sur la calotte glaciaire. Certains des hommes prennent des photos, tandis que d'autres jouent un match de football[64].
Le 23 mars 2006, Børge Ousland et Mike Horn sont les premières personnes à atteindre le pôle Nord durant la nuit polaire[65]. Puis, un mois plus tard, le 16 avril 2006, Albert II de Monaco est le premier monarque régnant à atteindre le pôle Nord[66].
En avril 2007, l'artiste néerlandais en art performance, Guido van der Werve, a réalisé une de ses performances au pôle Nord, en se tenant exactement sur le pôle durant 24 heures, et en tournant lentement dans le sens horaire (la Terre tourne dans le sens antihoraire). En suivant sa propre ombre, Van der Werve n'a pas tourné avec le monde pendant une journée. Cette performance est intitulée Nummer Negen[67], le jour où je n'ai pas tourné avec le monde. Van der Werve a réalisé un montage vidéo accéléré de ces 24 heures en 9 minutes[68].
En juillet 2007, le nageur d'endurance britannique Lewis Gordon Pugh a réalisé 1 kilomètre de nage au pôle Nord. Son exploit, entrepris afin de mettre en évidence les effets du changement climatique, a eu lieu en eau libre entre les banquises, durant 18 minutes et 50 secondes et à une température de −1,8 °C[69],[70]. En 2008, Pugh tente de rejoindre le pôle Nord en kayak à partir de la Norvège, mais doit abandonner après avoir été coincé par les glaces pendant trois jours[71].
Pour les Américains, la résidence du père Noël se trouve au pôle Nord géographique. En 1983, Postes Canada a attribué le code postal H0H 0H0 au pôle Nord en référence au rire caractéristique du père Noël : « Ho ! Ho ! Ho ! »[72][source insuffisante].
En 1866, Jules Verne décrit dans son roman Voyages et aventures du capitaine Hatteras le récit d'une expédition menée par un Anglais vers le pôle Nord. Ce roman est fondé sur la théorie erronée de la mer polaire ouverte. Cette quête d'un Éden arctique n'est pas sans rappeler une ancienne mythologie d'Hyperborée, qui situe au pôle Nord, l'étrange axe du monde, la demeure d'êtres surhumains[73]. L'idée populaire faisant du pôle Nord géographique la résidence du père Noël est archétypale d'une transcendance spirituelle faite de pureté[74]. Comme Henry Corbin l'a établi, le pôle Nord joue un rôle clé dans la culture ésotérique du soufisme et du mysticisme iranien : [75].
Le pôle est aussi identifié comme une mystérieuse montagne dans l'océan Arctique, le mont Qaf (cf. Rupes Nigra), dont l'ascension, comme l'escalade du mont du Purgatoire par Dante et Virgile, représente la progression du pèlerin au travers de différents états spirituels. Dans la théosophie iranienne, le pôle céleste, point focal de l'ascension spirituelle, agit comme un aimant pour attirer les êtres à son « palais avec d'immatérielles préoccupations ».
Pour les articles homonymes, voir Canada (homonymie).
Canadamodifier Le Canada (prononcé en français standard /kanada/[7] Écouter ; en anglais /ˈkænədə/[8] Écouter) est un pays d'Amérique septentrionale. Constitué de dix provinces et trois territoires, il s'étend dans les océans Atlantique à l'est, Arctique au nord et Pacifique à l'ouest ; cette façade maritime lui offre la huitième plus vaste zone économique exclusive du monde, avec comme particularité qu'elle soit moins vaste que ses eaux territoriales. Le pays a des frontières terrestres avec les États-Unis d'Amérique et le Danemark (Groenland). Couvrant une superficie de 9,98 millions de kilomètres carrés, il est le deuxième plus grand pays du monde en superficie, derrière la Russie. Sa frontière sud et ouest avec les États-Unis, qui s'étend sur 8 891 km, est la plus longue frontière terrestre binationale du monde tandis que sa frontière avec le Danemark, qui s'étend sur environ un kilomètre, est la deuxième plus petite au monde après celle entre le Botswana et la Zambie. Sa capitale est Ottawa et ses trois plus grandes régions métropolitaines sont Toronto, Montréal et Vancouver. Les langues officielles du pays sont l'anglais et le français.
Ce qui constitue aujourd'hui le territoire canadien a d'abord été habité par des peuples indigènes pendant plusieurs milliers d'années. À partir de la fin du XVe siècle, des expéditions britanniques et françaises explorent puis colonisent la côte atlantique. Après plusieurs guerres intercoloniales, la France cède la quasi-totalité de ses colonies d'Amérique septentrionale en 1763. À la suite de la guerre d'indépendance des États-Unis en 1783, de nombreux loyalistes américains sont expulsés et migrent vers l'Amérique du Nord britannique, correspondant à l'actuel Canada. Les colonies parviennent à repousser une tentative d'invasion américaine en 1812. Au cours du XIXe siècle, les Britanniques élargissent l'espace colonial en explorant puis en établissant des colonies sur la côte pacifique.
L'Amérique du Nord britannique commence à s'unifier le 1er juillet 1867 par le biais de la Confédération, avec l'union de trois colonies donnant naissance au Dominion du Canada. Au fil du temps, le dominion s'étend et obtient de plus en plus de souveraineté vis-à-vis du Royaume-Uni, grâce notamment au statut de Westminster de 1931, puis à la Loi de 1982 sur le Canada, qui rompt les derniers liens de dépendance juridique du Canada vis-à-vis du Parlement du Royaume-Uni. À noter que contrairement aux autres provinces, le Québec n'a pas signé la Loi de 1982. La relation du Canada avec les États-Unis comme voisin a eu un impact significatif sur son histoire, son économie et sa culture.
Le Canada est une monarchie constitutionnelle et une démocratie parlementaire s'inscrivant dans la tradition de Westminster. Le chef du gouvernement est le premier ministre et il est nommé par le gouverneur général, représentant le monarque, qui est le chef d'État. Le pays est un royaume du Commonwealth. Il figure en haut des classements internationaux en termes de transparence gouvernementale, de libertés civiles, de qualité de vie, de liberté économique, d’égalité des sexes et d'éducation. En 2021, la population canadienne est de près de 37 millions d'habitants. La majorité de la population est blanche, mais elle comporte également plusieurs minorités raciales, en raison d'une diversification de l'immigration depuis l'introduction du multiculturalisme en 1971.
En tant que pays développé, le Canada possède le 24e PIB nominal par habitant le plus élevé au monde et occupe le quinzième rang en termes d'indice de développement humain. Son économie est dixième au niveau mondial et est principalement basée sur d'abondantes ressources naturelles, ainsi que sur des réseaux commerciaux internationaux développés. Le Canada est membre de plusieurs grandes organisations internationales, dont l'ONU, l'OTAN, le G7, le G10, le G20, l'OCDE, l'OMC, le Commonwealth, le Conseil de l'Arctique, l'OIF, l'APEC et l'OEA.
Le nom du Canada provient du mot iroquois « kanata », le « k » se prononçant comme un « g » et le « t » comme un « d » (soit « ganada », ce qui signifie « village », « établissement » ou « terre »)[9],[10],[11]. En 1535, les habitants indigènes de l'actuelle ville et région de Québec utilisèrent ce mot pour guider le navigateur breton Jacques Cartier jusqu'au village de Stadaconé. Cartier utilisa par la suite le mot « Canada » non seulement pour désigner ce village, mais également pour l'ensemble des terres de Donnacona, chef de Stadaconé.
Le comte de Castelnau donne une autre étymologie du nom Canada dans le récit de son voyage dans l'Amérique septentrionale effectué de 1837 à 1841[12], en précisant bien qu'il est presque impossible d'en retrouver l'origine : « les Espagnols ont fait la première découverte du Canada : ayant mis pied à terre, ils n'y trouvèrent rien de considérable ; cette raison les obligea d'abandonner ce pays, qu'ils appelèrent il capo di nada, c'est-à-dire Cap de rien ; d'où est venu par corruption ce nom de Canada. »
La France du XVIe siècle adopta donc le mot « Canada » pour désigner ce qui correspondait alors à la vallée du fleuve Saint-Laurent, ceci se reflétant dans les écrits et sur les cartes produites. Certaines références permettent de décrire davantage le contexte dans lequel ceci se matérialisa[13],[14],[15],[16].
En juillet 1534, au terme de son premier voyage, Jacques Cartier convainquit deux jeunes Amérindiens dans la vingtaine, Domagaya et Taignoagny, fils du chef Donnacona, de l’accompagner en France[17]. Il s’agissait d’une décision stratégique, Cartier savait que ceux-ci apprendraient le français et deviendraient des guides très utiles lors du prochain voyage qu’il planifiait déjà. Les deux frères furent ainsi en contact avec des Français pendant un an et acquirent une certaine connaissance de la langue.
Mai 1535, deuxième départ de Cartier, accompagné de ses deux guides iroquoiens. En arrivant sur la côte nord-américaine, Cartier navigua les premiers jours dans des secteurs qu’il avait déjà vus l’année précédente. Puis arrive l’étape capitale de ce deuxième voyage et même un jalon important de l’exploration de l’Amérique du Nord. La flottille atteint la pointe ouest de l’île d’Anticosti que Cartier prend, comme tous les autres explorateurs européens avant lui, pour une péninsule.
Domagaya et Taignoagny, qui avaient l’expérience de faire de la pêche dans ce secteur, reconnaissent des lieux qui leur sont familiers et ne veulent pas que Cartier fasse demi-tour pour continuer vers le nord. Le 13 août 1535, Cartier apprend de la bouche de ses guides que cette terre est en fait une île et que, s’il continue de naviguer au sud-ouest de celle-ci, il fera alors face au chemyn de Canada, expression qu’ils ont utilisée et qui signifiait le fleuve qui mène au « lieu » appelé Canada. Donc, en cette journée, deux faits majeurs, le mot Canada était entendu pour la toute première fois par un Européen et Cartier venait d'être mis au courant de l'existence de cette immense voie d'entrée dans l'Amérique du Nord[18],[19].
Les deux images affichent l'écriture de Cartier du mot Canada. Première image : Chemyn de Canada ; deuxième image : extrait de la page 13 du journal de Cartier, le mot Canada apparaît sur les 2e et 3e lignes, il a exactement la même épellation qu'aujourd'hui.
La flottille commence à s’exécuter, elle va en zigzaguant, Cartier semble hésiter, mais ses deux guides insistent : selon eux, il s'agit bien du « chemyn » qui mène vers « Canada » et « Hochelaga », le cours d’eau ira en se rétrécissant et personne n’en n’aurait encore vu l’extrémité. Ces paroles font grandir l’intérêt du navigateur breton.
Jacques Cartier écrira le mot « Canada » à 22 reprises dans son journal de voyage totalisant 134 pages[20]. [Seconde navigation faicte par le commandemen et vouloir du tres chrestien Roy Francoys Premier de ce nom…]. Parfois, il l’utilise pour désigner le fleuve sous le nom « Rivière de Canada ». Il ne s’agissait donc pas d’une occurrence unique ou accidentelle, mais d’un mot significatif. Domagaya et Taignoagny se sont référés à ce terme à plus d’une reprise.
Cartier et ses guides se côtoient depuis maintenant un an, ils séjournent sur le même navire depuis 3 mois, ces derniers parlent un certain niveau de français et Cartier connait des mots iroquoiens. On peut soupçonner que Cartier et ses guides avaient développé entre eux une capacité de communiquer à un niveau intelligible.
Après avoir navigué une semaine sur le fleuve, le 7 septembre 1535, la flottille atteint l’île d’Orléans, Cartier déclare dans ses écrits que c’est « le commencement de la terre et prouvynce de Canada »[21],[22]. Cartier n’étant jamais venu à cet endroit précis auparavant, il n’aurait vraisemblablement pu faire une telle assertion sans l’entendre de la part de ses guides.
C’est ainsi que, dans les écrits et sur la carte de Cartier, le mot Canada prit une place prépondérante qui finit par désigner la terre de l’ensemble de la vallée du Saint-Laurent. Cartier n’en est pas moins conscient du sens premier du mot. Il crée en effet un lexique de 140 mots iroquoiens à la fin de son journal. Ainsi, il écrit : « Ilz appellent une ville – Canada »[23]. Le Canada de l’époque correspondait à l’établissement autour de Stadaconé, c’est-à-dire le site actuel de la ville de Québec, et dont le chef était Donnaconna, celui-ci étant identifié comme Sieur de Canada dans les écrits de Cartier.
Après son retour en France, les écrits de Cartier devinrent la référence dans le milieu de la Cour, le roi François Ier lui-même était enthousiaste du projet du Nouveau Monde. Ainsi, en 1538, un mémoire de planification élaboré lui fut présenté en vue de la préparation d’une expédition beaucoup plus ambitieuse : Mémoire des hommes et provisions nécessaires pour les Vaisseaux que le Roy voulait envoyer au Canada[24].
Jusqu’ici, la nomenclature de Cartier demeurait d’une diffusion limitée à la Cour de France. Le toponyme Canada en tant que pays de la vallée du Saint- Laurent et sa diffusion à travers l’Europe, sinon de par le monde, prit son envol sous l’impulsion de la réputée École de cartographie de Dieppe qui produisit successivement, à partir de 1541, une série de cartes et de mappemondes illustrant de façon explicite le nom et la position du Canada : en 1541 (Nicolas Desliens), en 1542 (dite Harléenne), en 1543 (anonyme), en 1547 (Nicolas Vallard), en 1550 (Pierre Desceliers) et d’autres par la suite[25],[26].
Mais simultanément, les cartographes espagnols furent bien renseignés sur les observations de Cartier. Ainsi, la Real Academia de la Historia de Madrid possède une carte (anonyme) datant de 1541[27]. Elle représente le fleuve Saint-Laurent et ses rives avec l’identification de « Canada » à l’endroit approximatif de l’établissement iroquoien de Stadaconé, un peu à côté de l’île d’Orléans, qui fut elle aussi nommée par Cartier. Cette carte rapporte même les nombreux Français de l’équipage de Cartier y étant morts à l’hiver 1535-1536.
Ces cartes s’appuyaient sur les écrits et sur la carte de Cartier produits lors de son deuxième voyage en Amérique (1535-1536). Ces cartes servirent de référence pour les décennies à venir à d’autres cartographes européens de calibre. Sous l’impulsion de la France, l’existence d’une terre dite « Canada » fut ainsi consacrée pour les siècles à venir. Ci-joint un extrait de la mappemonde dite Harléenne (vers 1542-1544), qui affiche trois fois le nom Canada, comme pour indiquer à la fois son sens territorial global et sa référence à l'établissement de Stadaconé.
En 1541, des documents et des cartes espagnoles commencent à désigner cette région par le mot « Canada »[28].
Grâce à la circulation des cartes, l’usage du terme Canada est ainsi devenu courant auprès de ceux qui faisaient commerce avec le nouveau continent : notaires, marchands, marins puis avec les habitants des régions côtières de la France[29]. Un témoignage pertinent vient du jésuite François Du Creux (1596-1666) qui a publié en 1664 : Historiae Canadensis. Il a relaté que : «…Il est clair cependant que c’est un mot ancien [Canada] car il était employé de façon usuelle quand j’étais un jeune garçon, il y a 60 ans. »[30].
Au XVIIe et au XVIIIe siècle, le mot « Canada » désigne l'une des colonies formant la Nouvelle-France, qui se situait le long du fleuve Saint-Laurent et de la rive nord des Grands Lacs. C'était néanmoins celle qui abritait, et de loin, le plus de colons français, mais presque qu'exclusivement le long des rives du fleuve. Pour l'administration royale française, elle porte le nom de  comme on peut le voir sur un document qu'elle a émis le 15 mai 1722 : [31]. Aux deux dernières lignes du bas de la première page, on peut lire .
Cependant, l’usage du mot Canada par les cartographes français illustre une réalité un peu différente. La consultation d’une quarantaine de cartes datant du Régime français[32] indique une variabilité dans les titres retenus : parfois « Nouvelle-France » seulement, parfois « Canada » seulement, mais le plus fréquemment, « Canada ou Nouvelle-France » ou « Nouvelle-France ou Canada » ou encore « Nouvelle-France dite Canada ».
Une carte qui représente à merveille cet état de chose est celle datant de 1656 de Nicolas Sanson d’Abbeville, géographe ordinaire du roi à Paris, et ayant pour titre « Canada ou Nouvelle-France »[33].
Donc, pour les cartographes, les termes « Canada » et « Nouvelle-France » sont devenus interchangeables, sinon des quasi-synonymes. Ceci se confirme d’ailleurs dans l’Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers de Denis Diderot (début des années 1750)[34]. On y trouve en effet sous la rubrique Canada : Canada ou Nouvelle France, (Géog.) pays fort vaste de l'Amérique septentrionale, borné à l'est par l'Océan, à l'ouest par le Mississipi, au sud par les colonies Angloises, & au nord par des pays deserts & inconnus… Donc, le Canada de l'époque de Jacques Cartier a gagné beaucoup de superficie en 200 ans.
Mais pour les colons français habitant les rives du fleuve,  devient beaucoup plus qu'une référence géographique ou administrative. Observant leurs particularités par rapport à leurs cousins restés en France, ils commencent à se nommer  ou  vers les environs de 1670, et Canada prend ainsi le sens de patrie. Comme les habitants de la colonie voisine de l'Acadie se sont identifiés Acadiens, ceux de Canada s'identifièrent Canadiens. La première mention en français (et non en latin) faisant référence à une identité canadienne[35]propre est bien connue, elle provient d'une lettre de Mère Marie de l'Incarnation datée du 16 octobre 1666. Elle y mentionne . Une autre formule distinctive à l'époque est .
Un personnage qui contribua à la diffusion du nom Canada fut nul autre que Voltaire. Celui-ci était en effet très critique de la colonisation du Canada par la France et il écrivit à plusieurs reprises sur le sujet. Sa répartie la plus célèbre demeure celle du philosophe Martin[36], énoncée dans son conte philosophique Candide (début 1759), au chapitre XXIII, relativement à la France et l’Angleterre :
« Vous savez que ces deux nations sont en guerre pour quelques arpents de neige vers le Canada, et qu’elles dépensent pour cette belle guerre beaucoup plus que tout le Canada ne vaut[37]. »
Une production littéraire et poétique vit le jour en Nouvelle-France et au Canada durant le régime français, ceci étant relativement peu connu. La consultation de diverses œuvres produites au lendemain de la conquête et du Traité de Paris jette une lumière quant à la perception identitaire de ces nouveaux « conquis ». Citons ici de très courts extraits de quelques-uns de ces écrits.
Quand Georges Trois, prit l’Canada [Anonyme, 1763][38] : Courage mes frères canadiens, Prenons notre sort en chrétiens; Sire Louis, Quinze du nom [Anonyme, 1763][39]: Cher Canadien parle hardiment…Comment les Anglais ont-ils pris Québec? Comment Français, Canadiens et Sauvages, Ont-ils manqué d’hardiesse et de courage? Chanson Nouvelle [Mr. Divertissant, 1765][40] : LE PaÏs du CANADA, A beaucoup fait de Fracas; En Canada est arrivé [Anonyme, 1775][41] : En Canada est arrivé, Une chose à remarquer, Les Canadiens vivaient tranquilles, Les Bastonnais ont décidé, De les soumettre à leur contrée.
Puis vint l’étape historique et déterminante de la signature du Traité de Paris (10 février 1763) qui fit un usage bien précis du terme « Canada » à sept reprises. Ce document fut de nature très officielle puisque signé par les plénipotentiaires des rois des trois plus grandes puissances occidentales de l’époque : la France, la Grande-Bretagne et l’Espagne[42]. Un traité qui confirme officiellement par son contenu l’existence d’un pays français appelé le Canada et traite aussi du sort spirituel de ses habitants catholiques (donc sous-entendu français). Paradoxalement, ce même document confirme aussi sa disparition en tant que territoire français.
L’article 4 porte principalement sur l’Amérique du Nord. Ainsi, il est dit : 
« Sa Majesté Trés Chretienne cede & garantit à Sa dite Majesté Britannique, en toute Proprieté, le Canada avec toutes ses Dependances… »
Un peu plus loin il est mentionné :
« De son Coté Sa Majesté Britannique convient d'accorder aux Habitans du Canada la Liberté de la Religion Catholique. »
Cette dernière concession eut un impact important quant à la survivance du français en Amérique du Nord.
Les autres articles traitent de territoires aux Antilles, en Floride, en Amérique du Sud, en Europe et aux Indes orientales. De façon surprenante, le terme « Nouvelle-France » n’est jamais mentionné. On a vu un peu plus haut que le mot Canada était déjà devenu plus ou moins son synonyme depuis des années.
Le mot désigna par la suite deux colonies britanniques, le Haut-Canada et le Bas-Canada, qui ont été réunifiées en 1841 en la Province du Canada, ou plus communément appelée le Canada-Uni[11].
En 1864, le projet de fédérer plusieurs colonies de l'Amérique du Nord britannique pour former un pays démarre. Lors de la Conférence de Londres, plusieurs noms sont proposés pour désigner le nouveau pays qui allait naître (par exemple Tuponia), et parmi ces diverses propositions, le nom « Canada » est suggéré en février 1867, approuvé et adopté par l'ensemble des délégués. Le 1er juillet 1867, trois colonies de l'Empire britannique se fédérèrent et le nouveau pays devient officiellement le Canada.
La désignation en forme longue est « Dominion du Canada » (en anglais, Dominion of Canada)[43]. Toutefois, le gouvernement fédéral utilise la plupart du temps la simple désignation en forme courte « Canada » sur les divers traités et documents d'État. Ainsi, le mot « dominion » n'apparaît pas dans la loi constitutionnelle de 1982, bien que n'étant pas officiellement supprimé[44]. Par la même occasion, le nom de la fête nationale, la « Fête du Dominion » (en anglais Dominion Day, littéralement le « Jour du Dominion »), est changé en « Fête du Canada » (en anglais Canada Day, le « Jour du Canada »)[44].
Alors que la Pangée n'est encore qu'un immense territoire, à l'ère paléozoïque, la formation et le développement de la chaîne de montagnes des Appalaches débute dès la période dévonienne, il y a 410 millions d'années. Après la division de la Pangée au début de l'ère mésozoïque, la partie septentrionale de celle-ci, la Laurasie - (Laur)entien + Eur(asie)[45], se divisa en deux lors de la période jurassique. La partie occidentale forma ce qui devint plus tard l'Amérique du Nord et dériva pendant plusieurs millions d'années jusqu'à son emplacement actuel. Puis, la formation des montagnes rocheuses commença il y a 138 millions d'années lors de la période crétacé. C'est à la fin de cette même période que la Terre assista à l'extinction massive des animaux, dont les dinosaures. Les réserves de pétrole commencèrent à se former par la suite surtout en Alberta pour devenir plus tard l'une des principales ressources économiques du Canada. Puis, il faut attendre la fin de la période tertiaire de l'ère cénozoïque il y a 5 millions d'années pour que commence la formation de la calotte polaire recouvrant le Nord du Canada. Vers la fin de cette période, le climat commença à se refroidir ouvrant ainsi la porte à une ère glaciaire et à une migration des mammifères entre continents. C'est pendant la période quaternaire il y a 1,6 million d'années que le climat devint excessivement froid et que les variations du niveau de la mer provoquèrent l'apparition des Grands Lacs, du lac Champlain, du fleuve Saint-Laurent, d'autres grands lacs et ce, en plus du Plateau laurentien[46]. Le développement et la migration de l'humain moderne commencèrent à l'époque holocène il y a 10 000 ans. À cette époque, le climat se réchauffa et devint plus sec, favorisant ainsi la fonte des glaces[47].
Alors que peu de documents attestent de leur présence, des fouilles archéologiques font remonter la présence des peuples autochtones sur le territoire du Canada à plus de 26 500 ans dans le nord du Yukon et à 9 500 ans dans le sud de l'Ontario. Ainsi, certaines régions du territoire actuel du Canada sont habitées par les peuples amérindiens et inuits depuis des temps immémoriaux. Ces peuples autochtones seraient arrivés en Amérique grâce à leur migration par l'isthme de la Béringie entre l'Alaska et la Sibérie orientale.
Tant les Premières Nations que les Inuits vivaient essentiellement de la chasse, de la pêche et du piégeage. Chaque peuple était organisé dans des régions spécifiques et possédait ses propres caractéristiques culturelles, toutes liées à la nature. L'animisme était cependant la religion principale de l'ensemble des peuples autochtones. Alors basés sur l'île de Terre-Neuve, les Béothuks sont la première nation autochtone à entrer en contact avec les Européens venus fouler le sol de l'Amérique. Ils sont à l'origine de l'expression Peaux rouges, laquelle sera par la suite généralisée aux autres nations amérindiennes de l'Amérique du Nord. De par leur isolement des autres peuples autochtones, les Inuits ne seront quant à eux davantage connus qu'à la fin du XIXe siècle et début du XXe siècle lorsque des explorateurs canadiens-français iront à leur rencontre dans le Grand Nord. Ils seront alors connus sous le nom d'Eskimos.
Attention : ces données ont été revues depuis que les études sur l'ADN mitochondrial ont démontré une parenté étroite entre certaines populations du Canada et celles de l'Europe du Nord (facteur X) qui est d'ailleurs absent des autres continents. Pour plus de détails sur le sujet, voir l'article portant sur les migrations humaines.
Les premières explorations européennes commencent quant à elles sur les côtes du Labrador et de l'île de Terre-Neuve, lesquelles sont visitées par les Vikings, les Normands et probablement les Basques[48],[49] depuis le 1er millénaire. L'explorateur islandais Leif Erikson serait en fait le premier Européen à visiter l'Est du territoire actuel du Canada (île de Terre-Neuve) aux alentours de l'année 990 après que le navigateur islandais Bjarni Herjólfsson l'a aperçu quelques années auparavant, selon la saga
d'Erik le Rouge[50],[51]. Les vestiges du village qu'il érige peuvent être retrouvés à L'Anse aux Meadows sur l'île de
Terre-Neuve[52] alors qu'il nomme trois contrées s'étendant de l'île de Baffin à l'île de Terre-Neuve en passant par le Labrador : Helluland, Markland et Vinland. Les contacts entre les Vikings et les Amérindiens qu'ils appellent Skrælings sont tout d'abord cordiaux avant de devenir hostiles. Les historiens avancent l'hypothèse que ces Skraelings étaient peut-être les Béothuks.
Au cours du Moyen Âge apparaissent sur les cartes marines, bien avant le voyage de Christophe Colomb, les noms de deux lieux situés en Amérique du Nord, Estotiland localisé selon les cartes sur l'actuel Labrador et Québec, et Drogeo situé plus au Sud, sur l'actuelle Nouvelle-Écosse, Nouveau-Brunswick et Nouvelle-Angleterre. Drogeo attire l'attention en raison de son étymologie qui s'apparente à celle de la terminologie micmac [-geo-] que l'on retrouve dans les différents territoires Micmacs. Dans la mythologie irlandaise, des navigateurs et des moines irlandais auraient abordé le continent américain au cours du XIIIe siècle suivant en cela le voyage de saint Brendan effectué au VIe siècle.
Ces voyages deviennent un mythe dans l'exploration de l'Amérique. Jean Cabot et Jacques Cartier chercheront en vain le territoire de Norembergue, lieu supposé des Hommes du Nord européens venus coloniser l'Amérique au cours de la période médiévale. Par la suite, il semble que les Basques viendront pêcher sur les grands bancs de morue au large de l'île de Terre-Neuve pendant près d'un demi-siècle. Ces derniers fondent entre autres, après la redécouverte par Christophe Colomb, la colonie de Plaisance qui s'étendra sur les îles Saint-Pierre-et-Miquelon et laquelle deviendra plus tard un refuge pour les pêcheurs français en plus d'une colonie satellite pour l'Acadie et le Canada[53].
Les explorateurs vénitiens Jean Cabot et son fils Sébastien débarquent à Bonavista (Terre-Neuve) en 1497 pour le compte du roi Henri VII d'Angleterre, probablement suivant les traces des frères Zeno qui y auraient débarqué au XIVe siècle[54]. L'explorateur portugais João Fernandes Lavrador longe quant à lui le littoral du Labrador actuel (nommé en son honneur) et le cartographie vers 1500 en compagnie de l'explorateur Pêro de Barcelos, mais sans prise de possession des terres[55]. Dans les faits, la carte dessinée est à l'origine du conflit frontalier du Labrador, lequel oppose encore aujourd'hui le Québec à Terre-Neuve-et-Labrador quant au droit de propriété du territoire actuel du Labrador. Alors que le Québec soutient que le territoire du Labrador ne repose que sur une bande d'une largeur d'un mille sur le littoral de la mer du Labrador, la province de Terre-Neuve-et-Labrador considère qu'il s'étend jusqu'à la ligne de partage des eaux (frontière actuelle délimitée par le Comité judiciaire du Conseil privé de Londres en 1927)[56].
À la recherche du Passage du Nord-Ouest, l'explorateur portugais Gaspar Corte-Real visite quant à lui l'île de Terre-Neuve en 1500, mais retourne au Portugal après avoir capturé des esclaves amérindiens[57]. Ces territoires ne seront cependant intégrés au Canada qu'en 1949 pour former la province actuelle de Terre-Neuve-et-Labrador alors que le territoire du Labrador est lui devenu possession française et partie intégrante du Canada dès le XVIIIe siècle.
De Saint-Malo sur les côtes armoricaines à bord de deux navires, Jacques Cartier et son équipage de 61 hommes se dirigent vers le Nouveau Monde où ils visitent Terre-Neuve, le golfe du Saint-Laurent, les Îles-de-la-Madeleine ainsi que l'Île-du-Prince-Édouard. Puis finalement, Cartier débarque en 1534 à Gaspé (surnommé le « Berceau du Canada français »), y plante une croix et prend possession de la terre au nom du roi de France, François Ier. Ainsi, Jacques Cartier devient le deuxième mandataire du roi de France à venir en Amérique à la suite du voyage de Giovanni da Verrazano en 1524, lequel longe le littoral s'étendant de la Floride à la Nouvelle-Écosse et qui devient le premier à utiliser la dénomination « Nouvelle-France ».
Lors de son second voyage, en 1535, à bord de la Grande Hermine (la Petite Hermine et l'Émérillon complètent ses vaisseaux), Cartier remonte le fleuve d'abord jusqu'à Stadaconé (Québec), où il revoit Donnacona, chef des Iroquoiens du Saint-Laurent (peuple aujourd'hui disparu ou assimilé, que l'on confond souvent à tort avec les Iroquois et les Hurons), qu'il avait déjà rencontré à Gaspé lors de son premier voyage. Il désigne le territoire autour de Stadaconé sous le nom de « pays de Canada » (en gros, Québec et sa région), dénomination qui sera généralisée par la suite à toute la vallée du Saint-Laurent, puis finalement à l'une des colonies de la Nouvelle-France. Puis il remonte le fleuve jusqu'à Hochelaga (aujourd'hui Montréal), arrêté par les rapides de Lachine. Lors de son troisième et dernier voyage en 1541, Jacques Cartier explore les terres du Canada en plus de ses régions avoisinantes, et y fonde Charlesbourg-Royal à l'embouchure de la rivière Cap-Rouge, à l'extrémité ouest du cap aux Diamants, le village iroquoien de Québec étant à l'extrémité est de ce Cap.
Bien qu'il ne soit pas encore prouvé que Giovanni Caboto ait débarqué au Canada et à Terra Nova (Terre-Neuve), plusieurs explorateurs français reviennent explorer le Nouveau Monde après le départ de Jacques Cartier, dont Jean-François de La Rocque de Roberval qui en 1542 explore le Royaume de Saguenay et qui fonde France-Roy en l'emplacement de Charlesbourg-Royal laissé vacant. En 1555, Nicolas Durand de Villegagnon tente d'établir une colonie en France Antarctique dans la baie de Rio de Janeiro, mais est rapidement délogé par les Portugais. Puis de 1562 à 1565, les Français huguenots Jean Ribault et René de Goulaine de Laudonnière tentent de coloniser ce qui est aujourd'hui la Caroline du Sud et la Floride, mais sont massacrés par les Espagnols. À la recherche du Passage du Nord-Ouest, Martin Frobisher découvre quant à lui la région arctique de l'île de Baffin, notamment la baie de Frobisher (Iqaluit) en 1576, au nom de l'Angleterre, qui devient plus tard un territoire du Canada. Dans le même dessein, l'Espagne emploie l'explorateur grec Ioannis Phokas en 1592, lequel navigue vers le nord à partir du Mexique actuel et atteint les eaux du détroit de Juan de Fuca (nommé en son honneur en 1788 selon sa dénomination espagnole) situé entre le sud de l'île de Vancouver et le nord-ouest de l'État actuel de Washington. Il arrête cependant son voyage avant d'atteindre le passage Intérieur dans les archipels et les fjords situés à l'ouest de la Colombie-Britannique actuelle.
Le « Canada » proprement dit se réfère quant à lui à l'origine à un peuplement français situé sur le territoire de la ville actuelle de Québec et, en tant que colonie française, constitue une des provinces de la Nouvelle-France. La colonie est fondée le long des berges du fleuve Saint-Laurent en 1534, lors de la découverte du Québec par Jacques Cartier et du développement de relations diplomatiques avec les Amérindiens de la région. Il faut attendre Tadoussac en 1600 pour le premier établissement d'un fort français permanent, origine du village actuel du même nom à l'embouchure de la rivière Saguenay. Les colons français qui peuplent le Canada proviennent principalement des anciennes provinces de France de Normandie, du Poitou et de la Saintonge[58], alors que les filles du Roy et quelques dignitaires arrivent de l'Île-de-France et de l'Orléanais[59].
Entre 1598 et 1603, Henri IV charge Troilus de La Roche de Mesgouez, à titre de lieutenant général des pays de Canada, Terre-Neuve, Labrador et Norembègue, d'établir un nouveau poste de colonisation avec quelques dizaines d'hommes et de femmes en Nouvelle-France. Cette deuxième tentative de colonisation s'effectue sur l'île de Sable, située au large de la Nouvelle-Écosse actuelle.
Après de nombreuses tentatives ratées (dont La Nouvelle-Angoulême à Long Island et Saint-Augustine en Floride), les Français établissent finalement leur premier comptoir commercial estival à Tadoussac (Québec) en 1600, grâce à un monopole accordé par le roi à Pierre Chauvin, sieur de Tonnetuit. En 1603, Henri IV nomme Pierre Dugua de Mons « Lieutenant général en Amérique septentrionale », et lui accorde le monopole de la traite des fourrures, pour compenser les frais d'établissement d'une colonie à cet endroit. En 1604, Dugua organise une expédition qu'il conduit en personne au sud-est du Canada, où il est accompagné de Samuel Champlain, qui y participe en tant que géographe et cartographe, et de Jean de Poutrincourt.
Pierre Dugua de Mons installe, en 1604, une première colonie en Acadie, sur l'île Sainte-Croix, dans le fond de la Baie française. Mais l'hiver terrible enduré par ces premiers colons le conduit, au mois d'août 1605, à la fondation de Port Royal, un lieu protégé des vents du nord-ouest et situé sur un lagon à l'est de la Baie française (aujourd'hui dans la vallée dite d'Annapolis Royal, en Nouvelle-Écosse, près de Digby), première capitale de l'Acadie. Marc Lescarbot, avocat au Parlement de Paris, fait un séjour en Acadie entre 1606 et 1607 et rédige la première Histoire de la Nouvelle-France, parue en 1609, où il découvre des  Amérindiens micmacs sous le prisme de l'humanisme et de la tolérance[60].
Samuel Champlain fonde Québec en 1608, mandaté par Pierre Dugua de Mons, « là où le fleuve se rétrécit », selon l'appellation algonquienne, et il en fait la capitale de la Nouvelle-France aussi dite le « Canada ». Québec est le premier lieu habité à l'année de façon continue par des Français et leurs descendants en Amérique du Nord. Champlain remonte aussi le fleuve en 1615 jusqu'au-delà du Sault Saint-Louis (rapides de Lachine), à la baie Georgienne (partie orientale du lac Huron) et navigue sur les eaux de la rivière Richelieu jusqu'au site de l'actuel lac Champlain. Tout au long de son périple en Nouvelle-France, il établit notamment avec les Innus-Montagnais, les Algonquins et les Hurons-Wendats d'excellentes relations diplomatiques et commerciales, et agit de facto (non en titre) comme premier gouverneur de la Nouvelle-France.
Cependant, les colons européens apportent de nombreuses maladies qui, par les routes commerciales, se propagent rapidement au sein des populations autochtones, faisant des ravages parmi celles-ci. Les colons français, arrivant souvent très malades dans des bateaux qui ne sont pas très sains, sont sauvés par les remèdes amérindiens. Ainsi, pour soigner le scorbut, les Iroquoiens du Saint-Laurent proposent à Cartier des décoctions d'écorce de cèdre blanc, appelé annedda. Après son retour de France en 1616, .
Les Récollets, premiers missionnaires catholiques en Nouvelle-France, arrivent en 1615 et se voient offrir une terre aux abords de la rivière Saint-Charles en 1620 pour y fonder un couvent. Bien que l'emplacement soit laissé vacant pendant quelques années, les Récollets reviennent en 1670 et se voient rétrocéder le site qu'ils dénommeront Notre-Dame-des-Anges. En 1692, Jean-Baptiste de la Croix de Chevrières, Monseigneur de Saint-Vallier alors évêque de Québec depuis 1685, fait l'acquisition du site et y fonde l'hôpital général de Québec l'année suivante. Aujourd'hui, l'hôpital forme une municipalité enclavée et séparée de Québec sous le nom de Notre-Dame-des-Anges. Dans un but d'évangélisation et d'éducation des Amérindiens, les Jésuites arrivent en Nouvelle-France en 1625. Ils fondent le Collège de Québec en 1635 pour y instruire les garçons français et les Hurons devenus chrétiens. Bien que leur principal but soit la conversion religieuse des tribus amérindiennes, le rôle des missionnaires est aussi de découvrir le territoire grâce à leurs relations avec les Hurons. Cependant, en 1648, les Iroquois, soutenus par les Anglais, attaquent les missions de Saint-Joseph et de Saint-Michel en Huronnie, et y massacrent les pères catholiques, dont Jean de Brébeuf, connus aujourd'hui sous la dénomination des Saints-Martyrs-Canadiens.
Afin de diriger la colonie, le roi Louis XIII place le gouverneur de la Nouvelle-France directement sous l'autorité du cardinal de Richelieu dès son entrée en fonction en tant que principal ministre d'État en 1624 et jusqu'à sa mort en 1642. Par la suite, la régente Anne d'Autriche nommera le Cardinal Mazarin en 1643 pour lui succéder jusqu'en 1661.
C'est en 1627 qu'est créé le régime seigneurial, principal mode d'administration des terres de la Nouvelle-France. Ce système est inspiré du régime féodal de la France sous lequel le censitaire (ou habitant) est dépendant du seigneur. Fondée par Armand Jean du Plessis, Cardinal de Richelieu, la Compagnie des Cent-Associés dont fait partie Samuel de Champlain, se voit octroyer les droits légaux et seigneuriaux et ce, en plus du droit de distribution des terres. Elle remplace la Compagnie de Montmorency, fondée en 1621, laquelle reprenait le rôle de la première compagnie de marchands de fourrure, la Compagnie des marchands de Rouen, fondée en 1613 par Samuel de Champlain[62] et qui ont toutes deux manqué à leurs obligations de colonisation. C'est ainsi que le territoire de la Nouvelle-France est divisé en seigneuries, chacune faisant face à un cours d'eau, remises aux colons les plus offrants afin d'en exploiter les richesses, lesquelles deviendront des entités économiques essentielles à leur survie. De plus, la Compagnie des Cent-Associés obtiendra le monopole du commerce de la fourrure dans les colonies françaises de l'Amérique du Nord. En 1645, ce monopole de la traite sera transféré à la Compagnie des Habitants (à l'exception de l'Acadie)[63]. Autre changement important pendant l'année 1627 : la Compagnie des Cent-Associés introduit la Coutume de Paris qui, en 1664, devient obligatoire en vertu de l'édit royal créant la Compagnie des Indes occidentales. Cet unique code de loi vient ainsi uniformiser les rapports entre les citoyens à la grandeur de la colonie, notamment dans les affaires commerciales et civiles[64].
À la première conquête de 1629, la Nouvelle-France passe sous domination britannique lorsque le marchand Sir David Kirke, en compagnie de ses frères, prend possession du fort et château Saint-Louis après l'assaut sur la ville de Québec où il somme Samuel de Champlain à la capitulation. Ce dernier est emmené de force en Grande-Bretagne pour négocier les termes de la cession des territoires français en Amérique. Cependant, après une période de tergiversation de trois ans, celui-ci est libéré, et l'Angleterre restitue la Nouvelle-France à la France en 1632 lors de la signature du traité de Saint-Germain-en-Laye. À son retour en 1633, Samuel de Champlain fait construire l'église Notre-Dame-de-Recouvrance (sur le site de Place-Royale dans la basse-ville de Québec) et la nomme ainsi pour souligner le fait que la France (…) [vient] de recouvrer sa colonie[65].
En 1634, la ville de Trois-Rivières est fondée par un certain Laviolette (dont nous ne savons rien d'autre, sinon qu'il est un émissaire de Samuel Champlain), sur la rive nord du fleuve au confluent des trois chenaux dessinés par la rivière Saint-Maurice, à mi-chemin entre Québec et le futur site de Montréal. Ce site était, depuis le début du siècle, un endroit stratégique pour la traite des fourrures, avec développement vers le nord-ouest. Et c'est en 1639 que les premières religieuses de la congrégation des Ursulines s'établissent en Nouvelle-France dans la région de Québec, pour y fonder la première école pour filles en Amérique du Nord. En 1697, elles s'établiront à Trois-Rivières et, avec l'aide de l'évêque de Québec, achèteront du gouverneur de Trois-Rivières Claude de Ramezay, une maison dans laquelle elles auront pour mission d'ouvrir une école et un hôpital.
Lors de l'élargissement des frontières en terrains vacants et du développement de relations diplomatiques avec les Algonquiens, les Français sont aux prises avec la menace des offensives britanniques et iroquoises. C'est donc dans le but de protéger les colons que Ville-Marie (Montréal), fondée en 1642 par Paul de Chomedey de Maisonneuve est érigée sur une île au pied du mont Royal. Et c'est en cette même année que Jeanne Mance fonde l'hôpital de Ville-Marie, premier Hôtel-Dieu. Bien que relevant de l'État laïc, Jeanne Mance est toujours secondée, à compter de 1659 jusqu'à sa mort en 1673, par les Religieuses hospitalières de Saint-Joseph. En 1653, Sieur de Maisonneuve invite Marguerite Bourgeoys à s'installer à Ville-Marie pour y devenir institutrice. Elle fait construire en cette même année la chapelle Notre-Dame-de-Bon-Secours (dans le Vieux-Montréal actuel) et fondera la congrégation Notre-Dame en 1659.
Après les massacres des Jésuites, Charles le Moyne de Longueuil et Pierre Picoté de Belestre entreprennent, à partir de 1648, une série d'expéditions diplomatiques en pays iroquois, lesquelles mèneront à l'expédition menée par le gouverneur Daniel de Rémy de Courcelles en 1666 et qui met définitivement fin à la menace iroquoise. Cependant, alors que les attaques iroquoises et anglaises s'intensifient et deviennent de plus en plus imminentes au cours de ces années, plusieurs Français se dévouent à la défense de la colonie et s'élèvent au rang des héros de la Nouvelle-France. Le plus connu sera sans doute Adam Dollard des Ormeaux, Sieur des Ormeaux et commandant de la garnison du Fort de Ville-Marie, qui se rend en 1660, lors de la bataille de Long Sault[66], avec une équipe de jeunes soldats au Fort du Sault de la Chaudière sur la rivière des Outaouais, afin de défendre la Nouvelle-France contre l'invasion iroquoise. Bien qu'il mourra au combat, il sera néanmoins reconnu pour avoir repoussé l'invasion. Son nom est encore aujourd'hui bien ancré dans l'imaginaire des francophones du Québec et de l'Ontario qui le célébraient naguère chaque année au lieu de la fête de la Reine au mois de mai. Cependant la fête de Dollard fut renommée la journée nationale des patriotes en 2002 à cause de documents historiques qui selon les historiens réduirait Dollard des Ormeaux au titre de pirate qui serait mort en tentant de voler des fourrures et marchandises. Ces fourrures et marchandise auraient été attendues en France pendant près de deux ans, mais leur envoi aurait été retardé à cause de la guerre avec les Iroquois. Dollard des Ormeaux aurait été mystifié par les autorités religieuses de la colonie qui étaient avides de héros et de martyrs propres à stimuler le sentiment patriotique et religieux[67] Puis, une jeune femme de 14 ans du nom de Marie-Madeleine Jarret de Verchères défend, en 1692 pendant huit jours, le fort de Verchères grâce à un mouvement de va-et-vient et des habits de soldats tout en faisant croire aux assaillants que le fort est rempli d'hommes alors qu'un seul soldat y veille.
Entre 1654 et 1656, le coureur des bois Médard Chouart des Groseilliers élargit les limites de la Nouvelle-France en explorant les territoires de ce qui est aujourd'hui le nord de l'Ontario en plus de ceux du pourtour de la baie d'Hudson et devient un des premiers Européens à atteindre le lac Supérieur. Il y retourne en 1659 avec Pierre-Esprit Radisson afin d'y exploiter le commerce de la fourrure. Cependant, à leur retour en 1660, ils sont réprimandés par le gouverneur Pierre de Voyer d'Argenson, vicomte de Mouzay pour commerce illégal.
Comme la très grande majorité des familles pionnières du Canada, lesquelles s'établissent notamment à l'île d'Orléans, Charles Aubert de La Chesnaye arrive en Nouvelle-France au courant des années 1650. À partir de ce moment, il développera l'activité économique de la colonie, notamment en devenant le premier homme d'affaires du Canada et ce, en créant plusieurs commerces et en acquérant les droits de propriété de compagnies de traite de fourrures, mais aussi en devenant l'un des plus grands seigneurs et propriétaires terriens du Canada. En 1682, il créera la Compagnie de la Baie du Nord, laquelle obtiendra le monopole du commerce de la fourrure dans la colonie de la Baie du Nord (Baie d'Hudson) jusqu'en 1700[68], année à laquelle elle sera dissoute et remplacée par la Compagnie de la Colonie[69].
Nommé vicaire apostolique de la Nouvelle-France et sacré évêque en 1658, Monseigneur François de Montmorency-Laval devient le premier évêque de Québec où il arrive le 16 juin 1659. Il fonde le Séminaire de Québec en 1663, à l'origine de la première université du Canada et la plus ancienne université francophone en Amérique, l'Université Laval. Il assume la fonction de gouverneur intérimaire de la Nouvelle-France à deux occasions, soit en 1663 et 1682. En 1658, on érige, sur la Côte-de-Beaupré en aval de Québec, une chapelle dédiée à Sainte-Anne. Elle fut reconstruite en 1661 en un site où se situe aujourd'hui la Basilique Sainte-Anne-de-Beaupré, un des cinq sanctuaires nationaux du Canada. L'année 1672 verra les débuts de la construction de la basilique Notre-Dame de Montréal grâce aux prêtres de Saint-Sulpice. Bien que le diocèse de Québec ne soit créé qu'en 1674, le vicariat apostolique de la Nouvelle-France est créé en 1658 afin d'étudier le terrain pour l'instauration officielle d'une administration catholique au Canada. L'Église catholique jouera un rôle politique important où l'évêque de Québec sera responsable au sein du gouvernement des domaines touchant à la foi religieuse, à l'éducation et à la santé jusqu'à la Révolution tranquille du Québec dans les années 1960. Au fil des siècles, le diocèse prendra de plus en plus d'importance au point tel où il couvrira la totalité du territoire de la Nouvelle-France au XVIIIe siècle. Bien qu'il sera par la suite divisé en d'autres diocèses, il conservera son importance politique parmi tous les autres. En effet, il acquerra le titre d'archidiocèse, de province ecclésiastique et finalement, de primatie de l'Église catholique du Canada.
En 1665, Jean Talon, surnommé , est nommé premier intendant de la Nouvelle-France par Jean-Baptiste Colbert, sous commission du roi Louis XIV. Lors de son arrivée, le roi fait aussi venir des troupes militaires afin de défendre la colonie contre les menaces iroquoises. C'est ainsi que le lieutenant-général Alexandre de Prouville, marquis de Tracy, fait construire trois forts le long de la rivière Richelieu : le Fort Richelieu à l'emplacement actuel de la ville de Sorel-Tracy, le Fort Sainte-Thérèse près de Carignan et le Fort Saint-Jean près de la ville actuelle de Saint-Jean-sur-Richelieu. Toujours dans l'esprit de sa mission de bâtir la colonie, Jean Talon propose en outre d'instaurer le Conseil souverain au sein d'un gouvernement royal et de créer des cours de justice dans les villes de Montréal, de Québec et de Trois-Rivières. En 1666, Jean Talon effectue le premier recensement de la Nouvelle-France et, à la suite des conclusions qui en sont tirées, il met en place une série de mesures de compensation et d'imposition afin d'encourager la nuptialité et la natalité. Il fait entre autres venir de France 800 femmes, communément appelées les « Filles du Roy » parce que dotées par le roi, lesquelles sont accueillies par Marguerite Bourgeoys. Tout au long de son intendance, il encourage la colonisation de la vallée du Saint-Laurent, en y créant et en attribuant la grande partie des seigneuries de la Nouvelle-France, tout comme les gouverneurs qui suivront. C'est ainsi qu'à partir de la fin de la première moitié du XVIIe siècle et tout au long de la seconde moitié, l'on assistera au début de la formation des régions actuelles du Québec avec l'arrivée des colons français et le développement du commerce de la fourrure. Ainsi, avec la sédentarisation des nouveaux colons canadiens et la traite de la fourrure, le site de plusieurs centres régionaux historiques actuels sera fixé. De ce fait, la Nouvelle-France assistera à la naissance des villes telles que Baie-Saint-Paul, Blainville, Boisbriand, Boucherville, Châteauguay, Lachute, Laval, Lavaltrie, Lévis, Longueuil, Matane, Montmagny, Repentigny, Rimouski, Rivière-du-Loup, Sept-Îles, Terrebonne, Varennes et Vaudreuil-Dorion, ainsi que La Tuque plus au nord, Belœil sur la rivière Richelieu et Saguenay dans le fjord du Saguenay. Bien que le régime seigneurial soit l'unique mode de division des terres, Jean Talon projette la création de trois villages en adoptant le plan de lotissement radial des Jésuites sur la seigneurie Notre-Dame-des-Anges en 1665, selon les directives du roi Louis XIV. Cette division des terres, unique en Amérique du Nord, permet le regroupement des colons afin d'assurer leur protection mutuelle contre les attaques iroquoises. Des trois villages projetés de Bourg-Royal, Bourg-Talon et Bourg-la-Reine, seul le village de Bourg-Royal voit le jour. Les terres en forme de triangle tronqué se déploient tout autour d'un quadrilatère central[70], situé à l'emplacement de l'arrondissement historique du Trait-Carré de Charlesbourg dans la ville de Québec[71].
En 1669, le roi Louis XIV crée un nouveau poste au sein de l'Ancien Régime français afin de diriger la Nouvelle-France, celui de Secrétaire d'État de la Marine. Ainsi, le Conseil souverain sera placé directement sous son autorité et recevra les ordonnances du roi par son intermédiaire. Cependant, plus tard, deux autres principaux ministres de l'État auront une autorité sur la colonie et par le fait même agiront de concert avec les secrétaires d'État de la Marine de l'époque. Il en est ainsi du Cardinal Dubois qui assumera ce rôle sous la régence du duc Philippe d'Orléans de 1715 à 1723 ainsi que d'André Hercule de Fleury qui effectuera les mêmes tâches de 1726 à 1743 sous le règne de Louis XV. Avec cette nouvelle organisation, Jean Talon réussira à diversifier l'économie grâce au système mercantile établi entre la Nouvelle-France, la métropole et les Antilles françaises. Il agrandit en outre les limites de la Nouvelle-France en chargeant des explorateurs d'étudier de nouveaux territoires. C'est ainsi que le coureur des bois Louis Jolliet et le père Jacques Marquette sont envoyés en exploration le long de la vallée du Mississippi. En revenant de leur expédition, ils s'arrêtent sur le site de la ville actuelle de Chicago (point de passage entre les Grands Lacs du Canada et le bassin du Mississippi) et y créent un poste permanent de traite de fourrure. Cependant, c'est en 1682 que René Robert Cavelier de La Salle prend possession des lieux et nommera le territoire s'étendant des Grands Lacs au Golfe du Mexique du nom de Louisiane en l'honneur du roi de France. De plus, Jean Talon envoie deux équipes à l'est afin de trouver une solution pour relier l'Acadie et le Canada par route terrestre. Dans la même lignée d'exploration, Charles Albanel, Paul Denys de Saint-Simon et Sébastien Provencher sont recrutés pour explorer les terres de la Baie du Nord (Baie d'Hudson) et pour appuyer la souveraineté de la France sur cette région au moment où la Compagnie de la baie d'Hudson y commence ses activités[72]. Simon François Daumont de Saint-Lusson a pour sa part la mission d'explorer la région de l'Outaouais et du bassin des Grands Lacs, constituant en grande partie l'ensemble de la région canadienne des Pays-d'en-Haut. Parallèlement le peuplement reste une préoccupation, même si la colonie peut compter sur une immigration de la métropole dont certaines sont dues au bannissement. Par exemple, entre 1730 et 1745, 585 faux-sauniers sont envoyés en Nouvelle-France plutôt qu'aux galères[73].
Dirigé par Pierre de Troyes, l'explorateur canadien Pierre Le Moyne d'Iberville est envoyé en expédition à la baie James[74] et se rend donc en 1686 dans la région de la baie d'Hudson avec pour mission d'y déloger les Anglais qui y avaient établi la Compagnie de la Baie d'Hudson en 1670. Ces derniers avaient indûment pris possession des territoires entourant le plan d'eau après la trahison de Médard Chouart des Groseilliers et de Pierre-Esprit Radisson à l'endroit de la France. Ces deux explorateurs français avaient suscité l'intérêt de l'Angleterre afin de prendre le contrôle du commerce de la fourrure dans la région en 1668 après le refus de Louis XIV de leur accorder les permis d'exploitation. Le seul voyage de l'Angleterre dans la région se fit en 1610 lorsque Henry Hudson navigua sur les eaux de la baie d'Hudson. Ce dernier n'avait cependant établi qu'un campement hivernal sur la rive de la baie puisque pris par les glaces, sans exploration des territoires, puis fut laissé pour mort au printemps lors de la mutinerie de son équipage qui retourna en Angleterre.
La compétition pour les territoires, les bases navales, la fourrure et la pêche devenant de plus en plus féroce, maintes guerres éclatent impliquant les Français, les Hollandais, les Britanniques et les tribus amérindiennes comme alliées. Ainsi, le XVIIIe siècle sera caractérisé en grande partie par les guerres intercoloniales (nommées French and Indian Wars en Nouvelle-Angleterre) qui apparaissent entre les Français, avec pour alliés les Hurons et les Algonquins, et les Hollandais — au début — ainsi que les Britanniques par la suite, lesquels ont pour alliée la confédération iroquoise, afin de définir le contrôle du commerce de la fourrure, notamment dans la vallée de l'Ohio. Ces guerres intercoloniales se perpétreront environ au même moment que les quatre guerres franco-britanniques en Europe entre 1689 et 1763.
Dans le but de protéger la ville de Québec contre la Nouvelle-Angleterre, le gouverneur Louis de Buade de Frontenac fait construire la première enceinte de la Citadelle de Québec en 1690. Au mois d'octobre de cette même année, le gouverneur Frontenac rejette l'offre de reddition de la ville et réussit à repousser les Britanniques de William Phips à la bataille de Québec. De plus en 1695, à l'emplacement actuel de Kingston en Ontario, celui-ci reprend la construction du Fort Frontenac qui avait été détruit en 1688 par les Iroquois, alors que l'ancien fort avait été construit après négociations entre le gouverneur Frontenac et une délégation iroquoise en 1673 afin d'étendre le commerce de la fourrure dans les Pays d'en Haut et de protéger Ville-Marie contre les Anglais.
Les nations autochtones s'étant fait la guerre entre elles tout au long du XVIIe siècle pour obtenir le privilège du commerce de la fourrure auprès des puissances européennes, la Nouvelle-France signe finalement la Paix des Braves en 1701 entre ses alliés et la confédération iroquoise, connue aussi sous le nom de Grande paix de Montréal, grâce au gouverneur Louis-Hector de Callière. Celle-ci regroupe près d'une quarantaine de nations autochtones et plusieurs milliers de délégués français et autochtones. Bien que des traités de paix furent avancés auparavant par les différents gouverneurs auprès des Iroquois, ce traité mettra définitivement fin aux guerres franco-iroquoises et de ce fait, aux guerres entre les peuples autochtones mêmes qui avaient débuté avant même l'arrivée des Européens en Amérique du Nord. Il marquera un tournant dans l'histoire dans les relations entre Français et Amérindiens alliant ainsi les Français aux Iroquois en protection contre les offensives britanniques[75],[76],[77].
Par suite de la première guerre franco-britannique (guerre de la Ligue d'Augsbourg), le traité de Ryswick de 1697 élargit les frontières de la Nouvelle-France, notamment grâce à la reconnaissance par l'Espagne de la partie ouest de Saint-Domingue (Haïti) comme étant possession française. De plus, ils mettent provisoirement fin à la guerre en restituant à la France les établissements de la baie d'Hudson et une partie de l'Acadie[78]. Puis, en cette même année, Pierre Le Moyne d'Iberville est choisi par la France pour retourner découvrir l'embouchure du fleuve Mississippi et coloniser la Louisiane, laquelle est convoitée par les Britanniques. Il y fonde le premier peuplement près de la baie de Biloxi, en compagnie de son frère Jean-Baptiste Le Moyne de Bienville. Ce dernier fondera La Nouvelle-Orléans en 1718.
Dans la même période d'exploration qui s'étend vers le Pays des Illinois en Louisiane et au moment où les Français tentent de coloniser davantage les territoires du sud pour faire face à la menace britannique dans la vallée de l'Ohio, Antoine Laumet de La Mothe, Sieur de Cadillac, fonde en 1701 la ville de Détroit avec la construction du Fort Pontchartrain. La ville de Windsor, sur l'autre rive de la rivière Détroit, sera peuplée en 1748 à même ce fort, alors que le Fort Rouillé sera érigé en 1750 à l'emplacement actuel de la ville de Toronto sous l'ordonnance du gouverneur Jacques-Pierre de Taffanel de La Jonquière, marquis de La Jonquière.
Après la victoire de la Grande-Bretagne à la Guerre de Succession d'Espagne, les Britanniques s'emparent, lors du traité d'Utrecht en 1713, de la partie française de Terre-Neuve, de la baie d'Hudson et d'une partie de l'Acadie, puis mènent à la destruction complète de la capitale de cette dernière, Port-Royal (Annapolis Royal). Les territoires acadiens à l'est de la baie Française (baie de Fundy) formeront une nouvelle colonie britannique qui prendra le nom de Nouvelle-Écosse. Cependant, par faute d'une évaluation exacte de la superficie du territoire par les Anglais, les Français conservent une partie de l'Acadie continentale, soit le Nouveau-Brunswick actuel, bien que la souveraineté de cette région sera disputée. Les îles de la Madeleine, l'île Saint-Jean (Île-du-Prince-Édouard) et l'île Royale (île du Cap-Breton) demeurent aussi au sein de la Nouvelle-France et seront unies en colonie de l'Île-Royale. Sur l'île Royale, les Français entreprennent la construction de la forteresse de Louisbourg, ville qui deviendra capitale en 1718. Lors de la guerre de Succession d'Autriche, les Britanniques venus de Nouvelle-Angleterre captureront la forteresse en 1745, mais cette dernière sera restituée à la France lors de la signature du traité d'Aix-la-Chapelle en 1748. L'année suivante, les soldats britanniques fonderont la ville d'Halifax en y créant un avant-poste militaire afin de peupler la Nouvelle-Écosse de colons britanniques.
Pendant la période de paix qui suit le traité d'Utrecht, en plus de la construction de la forteresse de Louisbourg et de la fondation de Port-la-Joie (aujourd'hui Charlottetown) par les Acadiens, les colons de la Nouvelle-France construisent le chemin du Roy en 1737 afin de relier Québec, Trois-Rivières et Montréal sur la rive nord du fleuve Saint-Laurent. Ce chemin devient la première route carrossable au Canada et est nommé ainsi dans l'espoir que le roi l'empruntera un jour. À partir de 1720, les fortifications de la ville de Québec sont par ailleurs érigées. De plus, la colonisation française commence à s'étendre le long de la rivière Chaudière, laquelle mène directement aux colonies de la Nouvelle-Angleterre à partir de la ville de Québec, et par conséquent développe la région de la Beauce, allant même jusqu'au site actuel de Lac-Mégantic. Puis, en 1738, la Nouvelle-France agrandit son territoire de plus belle avec l'exploration de l'ouest canadien. La région est explorée pour la première fois grâce à Pierre Gaultier, seigneur de Varennes et de La Vérendrye, lequel fait construire le Fort Rouge, à l'emplacement actuel de la ville de Winnipeg. En 1740, son fils François atteint les montagnes Rocheuses et explore les régions actuelles du Montana et du Wyoming. Après la guerre de Succession d'Autriche, en 1748, Pierre de Rigaud de Vaudreuil, alors gouverneur de Montréal, reçoit une seigneurie du roi Louis XV sur les terres de la ville actuelle de Saint-Hyacinthe.
La Nouvelle-France s'étend dorénavant du golfe du Saint-Laurent aux montagnes Rocheuses. Cependant, afin de prendre le contrôle du commerce de la fourrure et d'empêcher l'expansion du catholicisme en Amérique, les Britanniques tentent de plus belle de s'emparer des territoires de la Nouvelle-France, notamment en essayant de se rendre dans la vallée de l'Ohio. Lorsque la guerre de Sept Ans éclatera en 1756 entre la France et la Grande-Bretagne en Europe, le conflit fait déjà rage en Amérique.
Ainsi, la guerre de la Conquête débute au mois de mai 1754 lorsque Coulon de Jumonville est envoyé en mission de reconnaissance afin de savoir si le territoire français (dans l'actuel État de Pennsylvanie) est en effet envahi par les Anglais et, le cas échéant, pour délivrer à ces derniers une sommation de retrait au nom du roi Louis XV. Dans cette altercation qui sera connue comme étant l'Affaire Jumonville est la cause directe du déclenchement de la guerre. George Washington est accusé par les Français d'avoir ouvert le feu sur cet émissaire du roi de France. Ce conflit a pour conséquence la bataille de Fort Necessity en juillet de cette même année. Au cours de cette dernière, le commandant du Fort Duquesne (actuel Pittsburgh), Claude-Pierre Pécaudy de Contrecœur, décrète l'ordonnance d'arrestation de George Washington par un contingent de soldats commandé par Louis Coulon de Villiers, se soldant ainsi par la première victoire française.
Puis, en 1755, les soldats britanniques dirigés par Robert Monckton ouvrent une offensive et conduisent à la bataille de Fort Beauséjour dans la région du Beaubassin près du village « Le Coude » en Acadie (site de la ville actuelle de Moncton). Cette dernière mènera à la Déportation des Acadiens (surnommé le Grand Dérangement) par les Anglais, en cette même année, à partir du village de Grand-Pré dans le bassin des Mines. Alors que la résistance mènera des Acadiens à se réfugier au Canada, d'autres conflits continueront la déportation dont la prise du Fort Gaspareaux et la bataille de Petitcoudiac en cette même année ainsi que la bataille du Cran en juillet 1758. Cependant, le siège de Louisbourg commença un mois avant cette dernière et mène la Grande-Bretagne à contraindre à la reddition les Français de la forteresse de Louisbourg en juillet, laquelle deviendra un point stratégique pour la prise de la ville de Québec.
Une série d'expéditions et de batailles se succéderont pour la prise de la vallée de l'Ohio, au cours desquelles tant les Britanniques que les Français connaîtront victoires et défaites. Parmi les batailles les plus décisives de la guerre de la Conquête sur ce territoire, l'on peut nommer, entre autres, la bataille de la Monongahela, la bataille du Lac George, la bataille de Fort Bull, la bataille de Fort Oswego, l'expédition Kittanning, la bataille de Fort William Henry, la bataille de Fort Carillon, la bataille de Fort Frontenac, la bataille de Fort Duquesne, la bataille de Fort Ligonier et la bataille de Fort Niagara (dernière bataille majeure pour la possession de la vallée de l'Ohio).
Le 26 juin 1759, le siège de la ville de Québec débute lorsque les Britanniques posent pied à l'île d'Orléans. À la première tentative de débarquement pour la prise de Québec, les Anglais connaissent cependant une défaite lors de la bataille de Beauport au mois de juillet 1759. Au mois de septembre de la même année, les troupes britanniques débarquent à l'anse au Foulon, et des soldats escaladent la falaise du cap Diamant. La bataille des Plaines d'Abraham devient l'une des batailles les plus déterminantes de la guerre de la Conquête et mène à la prise définitive de la ville de Québec par le général James Wolfe sur Louis-Joseph de Montcalm, marquis de Montcalm.
Lors de la bataille de Sainte-Foy, le gouverneur de la Nouvelle-France et François Gaston de Lévis, chevalier de Lévis réussissent à repousser les Britanniques du général James Murray. Cependant, les renforts britanniques arriveront avant ceux de la France et mèneront successivement à la capitulation de Trois-Rivières et à celle de Montréal en septembre 1760 par le gouverneur Pierre de Rigaud de Vaudreuil de Cavagnial, marquis de Vaudreuil, sous les conditions du général Jeffery Amherst, quelque temps après la bataille des Mille-Îles. Un dernier espoir est toutefois donné par la France aux colons de la Nouvelle-France au mois de juillet 1760 : une petite flotte armée est envoyée dans la baie des Chaleurs, mais confrontée à une bataille vaine, la bataille de la Ristigouche.
Pendant trois ans, la Nouvelle-France est dominée par un régime militaire anglais, puis à la suite de la victoire britannique à la guerre de Sept Ans, la Grande-Bretagne s'approprie définitivement les colonies de l'Acadie, de l'Île-Royale et du Canada et la partie orientale de la Louisiane (entre le Mississippi et les Appalaches) au Traité de Paris en 1763. Ainsi, la Nouvelle-France prend fin et, bien que plusieurs vestiges de cette période passée demeurent encore aujourd'hui après la vente aux Américains du restant de la Louisiane en 1803 par Napoléon Bonaparte, le territoire des îles Saint-Pierre-et-Miquelon reste la seule possession française en Amérique du Nord. À partir de 1763, les colons français acadiens et canadiens sont coupés de tous liens avec la métropole par l'armée britannique. Du moins jusque dans les années 1760, cette situation mènera ainsi la population acadienne et canadienne-française à un manque d'approvisionnement, à une soumission inconditionnelle de survie puisque coupée de toute défense militaire et autre, et à un appauvrissement face aux Anglais qui prennent possession des terres des Français et ce, tout en étant continuellement approvisionnés par la métropole britannique.
Alors que la France et la Grande-Bretagne sont toujours en guerre en Europe, la Nouvelle-France et la Nouvelle-Angleterre sortent d'une guerre qui a duré près de sept ans. Comme les décisions administratives et politiques concernant les colonies sont prises par les métropoles respectives, le général Jeffery Amherst, à titre de commandant en chef de l'armée britannique en Amérique du Nord, établit un régime militaire provisoire en Nouvelle-France. Ce dernier ne comporte aucune réforme afin de ne pas provoquer le soulèvement des Canadiens.
Pendant cette période, la bataille de Signal Hill met définitivement fin à la présence française à Saint-Jean de Terre-Neuve en 1762. Cette même année, la France cède secrètement la Louisiane de l'ouest du fleuve Mississippi, incluant La Nouvelle-Orléans, à l'Espagne par le traité de Fontainebleau. Cette cession est effectuée afin d'éviter que le territoire ne tombe aux mains de la Grande-Bretagne, mais le territoire sera rétrocédé à la France en 1800, trois ans avant sa vente aux Américains. Après le traité de Paris de 1763, certains Acadiens iront s'établir dans la région qu'ils nommeront Acadiane, mais ils découvriront vite que le territoire sera maintenant possession espagnole. D'autres reviendront sur les terres acadiennes, rejoignant ainsi ceux s'y étant cachés tout au long du nettoyage ethnique. Cependant, ils seront contraints par l'armée britannique à se disperser en petits groupes et ils coloniseront donc les régions acadiennes actuelles de l'Île-du-Prince-Édouard, de la Nouvelle-Écosse, du Nouveau-Brunswick, du Québec et de Terre-Neuve. Cette période marquera le début de la reconstruction d'une « Nouvelle Acadie » pour le peuple acadien et mènera à la « Renaissance acadienne » au milieu du XIXe siècle avec la création d'institutions et de symboles nationaux[79]. Cette dernière sera motivée par la publication du poème Évangéline : Un conte d'Acadie de l'auteur américain Henry Wadsworth Longfellow en 1847, l'un des premiers jalons dans l'éveil de la conscience collective du peuple acadien[80].
À la suite du traité de Paris, la colonie de la Nouvelle-Écosse s'agrandit en annexant la colonie de l'Île-Royale (excepté les îles de la Madeleine, qui rejoignent la colonie de Terre-Neuve) et ce qu'il restait de l'Acadie continentale française[81]. En 1769, la colonie de l'Île-Saint-Jean, rebaptisée l'Île-du-Prince-Édouard en 1799, est séparée de la Nouvelle-Écosse[81]. En 1774, Terre-Neuve perd l'île d'Anticosti et les îles de la Madeleine, qui sont transférées à la Province de Québec[81]. Le territoire à l'ouest de la baie de Fundy se sépare aussi de la Nouvelle-Écosse en 1784 et devient la colonie du Nouveau-Brunswick et ce, pour mieux accueillir les loyalistes américains[81]. Au même moment, l'île du Cap-Breton, l'ancienne île Royale française, devient une colonie britannique indépendante, mais réintègre la Nouvelle-Écosse en 1820[81].
Après la conquête anglaise en Amérique et la fin de la Guerre de Sept Ans en Europe, la Nouvelle-France disparaît complètement et donne place à l'Empire britannique. Par la Proclamation royale faite en 1763 sous commission du roi George III, le Canada change de nom et devient la Province of Quebec ; le premier gouvernement civil y est institué avec un gouverneur général à sa tête reprenant le rôle du gouverneur et de l'intendant de la Nouvelle-France. De façon similaire au gouvernement de la Nouvelle-France, le gouverneur général est placé sous l'autorité de la Couronne britannique par l'intermédiaire du Bureau colonial (Colonial Office). De plus, le territoire est limité à la base de peuplement de la vallée du fleuve Saint-Laurent. Au cours de la période, la Nouvelle-Écosse s'étendra sur la quasi-totalité du territoire de l'Acadie jusqu'en 1784, les colonies de l'Île-du-Prince-Édouard et de l'Île du Cap-Breton seront créées, et l'on verra s'agrandir les frontières de la Terre de Rupert.
De 1763 à 1766, les Amérindiens outaouais se soulèvent contre les Britanniques, ce qui est connu aujourd'hui comme étant la rébellion de Pontiac. Au cours de cette dernière, les soldats anglais amorcent une guerre biologique auprès de la population autochtone en distribuant des couvertures infectées par le virus de la variole dans les forts amérindiens.
Dans la Province de Québec, les droits des citoyens canadiens-français sont réduits malgré l'entente de capitulation de la ville de Montréal. Les institutions canadiennes sont abolies, alors que des institutions et des cours de justice britanniques sont implantées graduellement. Par conséquent, les Canadiens français ne peuvent exercer leur religion – ainsi le serment du Test est obligatoire pour toute personne voulant occuper une charge civile - et l'utilisation de la Coutume de Paris (droit coutumier originaire du Nord de la France) est remplacée par la Common law, droit coutumier britannique.
Dès 1763, deux grands mouvements politiques font surface : le mouvement de restauration où les Canadiens français demandent la protection et la reconnaissance de leurs droits civils et religieux, et le mouvement de réforme chez les marchands britanniques venus s'installer dans la colonie où l'on demande l'implantation immédiate des institutions britanniques telles qu'une chambre d'assemblée législative. Devant les menaces d'insurrection dans la province de Québec, sous la pression de l'Église catholique romaine et pour des raisons pratiques, Londres proclame finalement l'Acte de Québec en 1774 sous les recommandations du gouverneur Guy Carleton. Ce nouvel acte élargit les frontières de la colonie en incluant les territoires de l'Ontario actuel et de la vallée de l'Ohio. De plus, l'Acte de Québec redonne aux Canadiens français certains privilèges tels que la conservation du régime seigneurial ainsi que le droit de pratiquer la religion catholique et d'utiliser la Coutume de Paris pour régir le commerce et les rapports civils. Le serment du test est aboli, mais le droit criminel et pénal britannique est toutefois maintenu. De plus, on interdit aux Canadiens de rétablir les liens avec la mère patrie.
À la bataille de Québec de 1775, les Américains attaquent les Britanniques basés à Québec et tentent vainement de s'emparer de la ville afin de soulever les Canadiens français contre la Grande-Bretagne et de gagner leur soutien dans la quête de l'indépendance des États-Unis. Malgré cette défaite, la ville de Montréal et les forts de la rivière Richelieu sont cependant contraints à la reddition. Le Congrès continental, assemblée législative commune des treize colonies de la Nouvelle-Angleterre, avait tenté à deux reprises de recruter les Canadiens français, mais la majorité de ces derniers décidèrent de demeurer neutres de l'avis du clergé catholique. Les forces révolutionnaires se retirent ensuite, seule la baie d'Hudson est attaquée en 1782 (par le français Jean-François de La Pérouse).
Bien que le mouvement de réforme ait fait relâche pendant la période de la guerre d'indépendance des États-Unis, il revient en force après la signature du traité de Paris de 1783 qui met fin à la guerre. Ce mouvement de protestation est d'autant plus amplifié lorsque près de 50 000 loyalistes de l'Empire Uni immigrent dans les colonies de la province de Québec, de la Nouvelle-Écosse, de l'Île-du-Prince-Édouard et de Terre-Neuve afin de rester fidèles à la Couronne britannique. Un projet de constitution parlementaire sera établi et mènera à l'établissement d'une assemblée législative en 1791.
D'autre part, les territoires conservés par l'Empire britannique en Amérique du Nord après la guerre d'indépendance américaine, comprenant l'ensemble des Loyalistes s'y réfugiant, seront dorénavant connus comme constituant l'Amérique du Nord britannique. La majeure partie des Loyalistes s'installeront dans l'ouest de la province de Québec, le long du lac Ontario et de l'archipel des Mille-Îles, fondant entre autres les villes actuelles de Belleville, de Brockville et de Cornwall, tout en peuplant davantage le site de la ville de Kingston. Cependant, plus à l'est, comme les Loyalistes ne sont guère les bienvenus en Nouvelle-Écosse, la partie occidentale de celle-ci se détache afin de former une nouvelle colonie, le Nouveau-Brunswick, qui les accueille en 1784. Les Loyalistes s'installeront notamment sur les sites acadiens des villes actuelles de Fredericton et de Saint-Jean. En outre, avec le début de la Conquête de l'Ouest et la cession des territoires du sud au pays nouvellement formé des États-Unis par le Royaume-Uni, la province de Québec est contrainte à réduire les limites de son territoire. Ainsi, elle perd la vallée de l'Ohio, et les nouvelles frontières du sud sont définies par les barrières naturelles que sont les Grands Lacs et la rivière Niagara.
Afin d'accommoder les loyalistes anglophones qui se sont réfugiés dans l'ouest de la province de Québec, cette dernière est divisée par l'Acte constitutionnel de 1791 en deux colonies distinctes, le Haut-Canada et le Bas-Canada. Le Haut-Canada correspond à l'Ontario actuel, majoritairement composé des Loyalistes de l'Empire Uni issus de la guerre d'Indépendance américaine. Le Bas-Canada correspond au Québec actuel, et on y retrouve une majorité de francophones nommés « les Canadiens français ». Comme toutes autres colonies, le Haut-Canada a son lieutenant-gouverneur nommé par le gouverneur général. Afin de protéger la capitale des attaques américaines, les édifices législatifs du Haut-Canada (en) déménageront de Newark (Niagara-on-the-Lake) à York (Toronto) lors du mandat de John Graves Simcoe, alors que ce dernier fondera London en 1793 pour aussi en faire la capitale, mais en vain. Le Bas-Canada est, quant à lui, dirigé par le gouverneur général lui-même siégeant à Québec, capitale de l'Amérique du Nord britannique.
Bien que chaque colonie soit théoriquement une démocratie ayant son Assemblée législative élue par la population - la Chambre d'assemblée du Bas-Canada et la Chambre d'assemblée du Haut-Canada (en) - cette dernière ne possède aucun pouvoir réel. Le régime d'État est une monarchie dont la Couronne est à Londres et dont la représentation se fait par l'intermédiaire du gouverneur général et du lieutenant-gouverneur. De plus, contrairement au Haut-Canada où tous les membres de la législature (incluant le Conseil législatif du Haut-Canada (en)) sont anglais - l'acte constitutionnel crée le Conseil législatif du Bas-Canada dont les membres sont non élus et nommés par le gouverneur général. Cette disposition a donc pour effet de créer un système bicaméral à deux chambres législatives, où le Conseil législatif a pour rôle de contrebalancer et de contrôler le pouvoir législatif donné à la majorité canadienne-française du Bas-Canada via le système démocratique et ce, en nommant des pairs britanniques.
De plus, le gouvernement des deux colonies est composé du Conseil exécutif du Bas-Canada et du Conseil exécutif du Haut-Canada (en) dont les conseillers sont nommés par le gouverneur général au Bas-Canada et par le lieutenant-gouverneur au Haut-Canada. Cette situation mènera donc la politique gouvernementale haut et bas-canadienne à une forme de ploutocratie tout au long de l'existence des deux Canadas. De plus, dans les deux colonies, le poste de secrétaire provincial (provincial secretary) est créé au sein de chacun des Conseils exécutifs. Il est placé sous l'autorité du gouverneur général et du lieutenant-gouverneur. Le secrétaire provincial de chaque colonie détiendra un rôle similaire à celui de premier ministre avant l'émergence du gouvernement responsable en 1848 et sera notamment chargé des communications entre les gouvernements colonial et impérial. La Clique du Château, nom donné au gouvernement bas-canadien, sera composée des gens de l'élite anglophone montréalaise, dont les figures les plus prééminentes seront sans doute John Molson et James McGill, afin de ne servir que les intérêts commerciaux et autres d'un petit groupe de personnes de la haute société anglaise. Alors qu'au Haut-Canada, le Family Compact (Pacte de Famille) mènera une politique monarchiste et ultra-conservatrice, dont le but sera d'établir le modèle britannique, de paralyser les Canadiens français et d'abolir le catholicisme. L'évêque anglican John Strachan en sera la figure la plus notable et verra son influence grandir après la Guerre de 1812. Ainsi, deux décennies après la création des deux Canadas, le Canada joue un rôle significatif lors de la Guerre de 1812 au cours de laquelle le Royaume-Uni tente vainement de reconquérir le territoire des États-Unis. Il se démarque, entre autres, lors de la bataille de Queenston Heights au débarquement américain sur la rivière Niagara, de la bataille de York où la ville est acculée à la capitulation, de la bataille de la rivière Thames où les forces britanniques tentent de freiner l'avance des Américains passés par Windsor, et de la bataille de Châteauguay au cours de laquelle les Canadiens français sèment des embûches aux Américains, lesquels tentent sans succès de prendre la ville de Montréal afin de couper l'approvisionnement du Haut-Canada[82],[83]. La défense du Canada lui vaut d'importants avantages à long terme, notamment quant à la création d'un sentiment d'unité et de nationalisme au sein de la population de l'Amérique du Nord britannique. Une immigration massive de la Grande-Bretagne et de l'Irlande vers le Canada se fait sentir en 1815, où les immigrants s'installent notamment sur la péninsule du Niagara et dans les environs d'Hamilton joignant ainsi les Loyalistes arrivés en 1784. En cette même année, la ville de Drummondville est fondée à mi-chemin entre Trois-Rivières et la ville de Sherbrooke (peuplée en 1793 par les Loyalistes) afin d'établir un poste de surveillance sur la rivière Saint-François, laquelle donne un accès maritime direct du fleuve Saint-Laurent aux États-Unis. Une série d'accords mèneront ensuite à de longues périodes de paix entre le Canada et les États-Unis, n'étant interrompus que par de brefs raids opérés par des insurgés politiques, les Fenians (Américains d'origine irlandaise), de 1866 à 1871 contre les autorités britanniques. Ces derniers seront notamment soutenus par l'homme politique canadien Thomas D'Arcy McGee, mais celui-ci modérera ses propos avant l'invasion fénienne à la bataille de Ridgeway sur la péninsule du Niagara en 1866.
Aux alentours du site de la ville de Saint-Boniface (annexée plus tard à la ville de Winnipeg), laquelle est fondée en 1818 et peuplée par les Métis aux abords de la rivière Rouge, le Canada assiste en 1816 à la bataille des sept chênes. Cette dernière met en scène deux compagnies rivales de traite de fourrure, la Compagnie de la Baie d'Hudson et la Compagnie du Nord-Ouest, dont le dessein — qui se solde par une victoire — est la prise de contrôle des provisions de fourrure du Fort Douglas par la Compagnie de la Baie d'Hudson. En 1822, un projet d'union législative des deux Canadas est soumis au Parlement de Londres par Lord Henri Bathurst, alors secrétaire d'État pour les colonies britanniques, Secretary of State for the Colonies. Cette disposition a pour effet de créer une minorité francophone avec la majorité canadienne-française du Bas-Canada. Des représentants bas-canadien, dont Louis-Joseph Papineau, se rendent à Londres en 1823 afin de démontrer l'opposition massive du Bas-Canada. Le projet est finalement abandonné en cette même année. Les représentants du Parti patriote (fondé par les Canadiens français au début du XIXe siècle avec la dénomination « Parti canadien ») déposent des pétitions en 1828 à la Chambre des Communes de Londres, dont les principaux intéressés se plaignent des actes arbitraires et illégaux du gouverneur général George Ramsay à l'endroit des francophones. Ce dernier est démis de ses fonctions en cette même année.
Les tentatives avortées de réforme constitutionnelle, l'absence de pouvoir réellement légiféré - le népotisme gouvernemental, les difficultés sociales et le sentiment de minorisation des francophones mènent les Patriotes canadiens, dirigés par Louis-Joseph Papineau et insatisfaits de leur position de faiblesse, à envoyer 92 résolutions à Londres en 1834 exigeant plus de pouvoirs démocratiques pour le Parlement du Bas-Canada. En 1835, le gouverneur Lord Gosford met sur pied la « commission royale d'enquête sur toutes les peines affectant les sujets de Sa Majesté dans le Bas-Canada ». Cette commission mène aux 10 résolutions de Russell en 1837, lesquelles incarnent le refus catégorique de Londres et le rejet de l'ensemble des demandes et permettent même au gouvernement colonial d'outrepasser l'autorité budgétaire de la Chambre d'assemblée du Bas-Canada. Le Parti patriote change de stratégie à la suite de ce refus et mène plusieurs assemblées de citoyens, dont les assemblées de Saint-Ours, de Saint-Laurent, de Saint-Marc et de Stanbridge Station, en plus de l'Assemblée des six-comtés où la Colonne de la liberté est érigée. Au cours de cette dernière assemblée tout comme dans les précédentes, les citoyens soutiennent l'idée des droits de l'homme, de la lutte constitutionnelle, du boycott économique et commercial et approuvent l'organisation paramilitaire des jeunes Patriotes, la Société des Fils de la Liberté. Alors réfugiés au Bas-Canada, plusieurs Acadiens participent aux assemblées de citoyens et soutiennent les Patriotes; leur apport sera d'ailleurs commémoré en 2002 par une promenade et un monument en leur hommage dans la ville de Québec[84],[85]. De plus, certains Américains, dont les frères Robert Nelson et Wolfred Nelson et certains Français recrutés par les États-Unis, dont Charles Hindenlang, se rangent du côté des Patriotes et appuient l'assemblée, laquelle mènera à la guerre civile du Bas-Canada en 1837, communément appelée la Rébellion des Patriotes. En 1838, la déclaration d'indépendance du Bas-Canada[86], écrite par Robert Nelson alors retourné aux États-Unis avec ses partisans, promulgue la séparation de l'Église et de l'État, puis mène à la création de la république du Bas-Canada. Cette volonté d'autonomie et cette révolution sont toutefois violemment réprimées par l'armée britannique et mènent à une série de conflits dont la bataille de Saint-Denis, la bataille de Saint-Charles et la bataille de Saint-Eustache. De plus, au cours de la Rébellion, les Iroquois des régions de Kahnawake et de Kanesatake déclarent leur neutralité face au conflit, mais collaborent tout de même avec les autorités britanniques. Plusieurs villages de la Montérégie sont incendiés et pillés, et des Patriotes sont pendus en 1839, dont François-Marie-Thomas Chevalier de Lorimier, sur le futur site de la prison Parthenais à Montréal. Certains iront en appeler au génocide du Bas-Canada - rappelant celui des populations autochtones de 1763 à 1766 – qui durera jusqu'à ce que la politique d'éradication linguistique et culturelle entre en vigueur en 1840 via l'Acte d'Union[87],[88],[89]. De plus, des centaines de familles canadiennes-françaises actuelles sont touchées par la déportation de plusieurs Patriotes[90]. Ces derniers sont notamment exilés en Australie, colonie pénitentiaire, alors que d'autres doivent s'enfuir aux États-Unis.
La Rébellion du Haut-Canada menée contre l'empire britannique est quant à elle de plus courte durée, et n'a pas d'incidence directe. Comme au Bas-Canada, elle a pour but de réformer le système démocratique en introduisant la responsabilité ministérielle. Elle est le fruit de l'insurrection des Écossais, menée par William Lyon Mackenzie et son Parti réformiste, et qui mène aussi à une déclaration d'indépendance, celle de la république du Canada. Les révolutionnaires fuient Toronto et vont établir le nouveau gouvernement de la république sur l'île Navy sur la rivière Niagara. Cependant, après avoir été forcés de quitter l'île par la Royal Navy, ils traversent la frontière, là où les autorités américaines les capturent et les font prisonniers pour violation des lois de neutralité entre les États-Unis et l'empire britannique.
En 1838-1839, le Canada assiste en plus à un conflit de frontière lors de la guerre Aroostook qui oppose les Britanniques aux Américains dans la région acadienne chevauchant le nord-est de l'État du Maine (Comté d'Aroostook), l'est du Bas-Canada (MRC de Témiscouata) et le nord-ouest de la colonie du Nouveau-Brunswick (comté de Madawaska), dont le centre est la ville actuelle d'Edmundston[91]. En 1842, une entente entre les États-Unis et le Royaume-Uni, le traité Webster-Ashburton, divise la région selon les trois frontières connues aujourd'hui. Cette région, communément appelée la république du Madawaska, est composée d'une population majoritairement francophone de descendance acadienne, dont les habitants sont connus sous le nom de « Brayons », contrairement aux Acadiens des autres régions qui ont conservé la même dénomination.
À la suite de l'échec des Rébellions de 1837, la reine Victoria sanctionne la création d'un conseil spécial pour administrer le Bas-Canada et donne en 1839 à John Lambton, Lord Durham, la tâche d'étudier la situation politique des deux Canadas. Le rapport sur les affaires de l'Amérique du Nord britannique (rapport Durham) traduit les principales recommandations de ce dernier. Celles-ci sont la réunification des deux colonies (ce qui permettrait de réduire la grande dette du Haut-Canada en la répartissant sur tout le territoire) et la présence plus importante de la culture britannique auprès des francophones, afin de les y noyer et de les assimiler, car ils sont considérés comme sans culture, sans histoire, sans patrie et sans littérature. C'est ainsi que l'Acte d'Union de 1840 fusionne les deux Canadas en une seule colonie quasi-fédérale, la province du Canada, communément appelée le Canada-Uni, abrogeant une partie des droits octroyés aux Canadiens français par l'Acte de Québec de 1774. En outre, l'Acte d'Union a pour conséquence de fusionner les dettes du Haut et Bas-Canada, afin de former une seule et unique dette publique. Ainsi, l'Assemblée législative de la province du Canada est dorénavant l'organe qui dirige théoriquement la colonie. Son siège sera alternativement Kingston, Montréal, Toronto et Québec, mais s'installe définitivement à Ottawa en 1866. La structure politique de la province du Canada comprend deux premiers ministres, qui agissent en tant que conseillers auprès du gouverneur général pour chacune des deux régions désignées en tant que Canada-Est et Canada-Ouest, lesquelles reprennent les mêmes limites que le Bas-Canada et le Haut-Canada respectivement. Le rôle de premier ministre du Canada-Uni est encore présent aujourd'hui, en ce sens qu'il est l'ancêtre du rôle de lieutenant du Québec, où une personnalité politique fédérale agit en tant que conseiller principal auprès de son parti sur des sujets spécifiques au Québec.
À l'Assemblée législative, le Parti Tory ou parti conservateur anglais (incarné au sein du Family Compact et de la Clique du Château), perdra peu à peu de son influence jusqu'en 1848. Cette année-là, on voit apparaître l'instauration du premier gouvernement responsable du Canada, à la suite de l'alliance entre Sir Louis-Hippolyte Lafontaine et Robert Baldwin, tous deux premiers ministres du Canada-Est et du Canada-Ouest respectivement.
De plus, en cette même année 1848, l'Institut canadien de Québec est fondé avec pour mission la promotion de la culture francophone afin de contrecarrer l'influence grandissante de la culture britannique. Des auteurs tels que François-Xavier Garneau écriront plusieurs œuvres qui perpétueront l'histoire des Canadiens français au fil des ans[92]. Depuis l'échec de la Rébellion des Patriotes, les hommes politiques canadiens-français, dont George-Étienne Cartier, tentent en outre de continuellement négocier avec le gouvernement britannique afin de retrouver leur province et leurs pouvoirs législatifs.
D'autre part, dès la fin de la première moitié du XIXe siècle, la révolution industrielle fait son apparition au Canada, tout comme dans le reste de l'Empire britannique. Les riches familles anglaises du Canada s'établiront notamment dans la ville de Montréal (capitale financière) et fonderont certaines des plus grandes entreprises canadiennes actuelles avec des Canadiens anglais aux postes de contremaîtres et des Canadiens français comme ouvriers. Pendant plus d'un siècle, la grande majorité des Canadiens français vivra repliée sur elle-même, résignée à son sort dans la pauvreté, et sera acculée aux régions rurales, où l'Église catholique jouera un rôle politique ultramontain prépondérant dans le maintien de la cohésion et dans le soutien à la société canadienne-française[93]. Ainsi, face à la croissance de l'immigration britannique au Canada, l'Église catholique tente notamment de contrer l'effet de minorisation des francophones en encourageant la natalité, ce qui est connu aujourd'hui comme étant la revanche des berceaux. Ce phénomène perdurera jusqu'à la fin des années 1950, lors de la Révolution tranquille et de la laïcisation de l'État. Au cours de cette période, l'on assistera au détachement de l'Église par les baby-boomers. Ces derniers se soulèveront contre ce qui sera perçu comme étant les abus de l'Église survenus lors de la Grande Noirceur au Québec, de 1944 à 1959.
Avec l'avènement du gouvernement responsable, on assiste à la fondation de nombreux partis politiques et, par le fait même, à la création d'un schéma rudimentaire décrivant les rouages de la scène politique canadienne actuelle. Ainsi, le Parti rouge est fondé au Canada-Est en 1848 par Antoine-Aimé Dorion en reprenant l'idéologie du Parti patriote de Louis-Joseph Papineau (à l'origine du Parti libéral du Québec). Étienne-Paschal Taché viendra quant à lui équilibrer la politique avec la création du Parti bleu selon les idées plus modérées de Louis-Hippolyte Lafontaine, lequel parti deviendra plus tard le Parti conservateur du Québec et l'Union nationale, pour finalement s'éteindre lors de la montée du mouvement souverainiste québécois dans les années 1960. Au Canada-Ouest, le parti libéral-conservateur sera fondé en 1854 par John Alexander Macdonald après la coalition du Parti réformiste (formé au cours des années 1830 en défenseur de la rébellion haut-canadienne, pour devenir aujourd'hui le Parti libéral de l'Ontario) de Robert Baldwin et William Lyon Mackenzie, et du Parti Tory (aujourd'hui le Parti progressiste-conservateur de l'Ontario). Après une gamme de fusions de partis politiques au fil des ans, ce nouveau parti mènera au Parti conservateur du Canada en 2003, au sein duquel se retrouveront les Red Tory et les Blue Tory - respectivement les partisans du progressisme et du conservatisme socio-économique. George Brown fondera quant à lui les Clear Grits (ancêtre du Parti libéral du Canada et considéré comme étant plus progressiste), à même les membres plus radicaux de la faction réformiste du Parti réformiste, en prônant la Rep by Pop (principe de la démocratie représentative où les députés sont élus au prorata de la population), et donc la minorisation des Canadiens français à l'assemblée législative du Canada-Uni. Ce parti sera perçu comme privilégiant des politiques anti-francophones, étant donné le principe défendu de la représentation selon la population, et la majorité anglaise qui existe dans l'ensemble du Canada-Uni.
Alors que le Canada Uni est au bord d'une guerre civile au début des années 1860 et que la guerre de Sécession des États-Unis fait rage, ayant été renversé par les partis d'opposition à la suite de son alliance avec le Parti rouge pour cause de sécularisme anticlérical, les Clear Grits de George Brown s'associent en 1864 avec les partis de John Alexander Macdonald (Parti libéral conservateur) et de George-Étienne Cartier (Parti bleu), lesquels forment la coalition Macdonald-Cartier. Les Clear Grits irlandais feront cependant volte-face et appuieront le Parti réformiste de William Lyon Mackenzie. Ainsi, le gouvernement de coalition sera formé et mènera à la création de la Confédération en 1867 et ce, notamment, dans le but de se prémunir des contrecoups de la guerre civile américaine au Canada.
Après que les États-Unis et le Royaume-Uni se furent entendus en 1846 pour retenir le 49e parallèle nord comme frontière séparant les États-Unis de l'Ouest de l'Amérique du Nord britannique, le gouvernement de Grande-Bretagne signa avec les États-Unis un accord de libre-échange pour le Canada-Uni en 1854. Le Traité de réciprocité canado-américain permit un regain dans l'économie en chute libre de la Province of Canada. Cet accord prendra cependant fin en 1866, et l'économie du Canada-Uni retombera à la dérive.
Le gouvernement du Royaume-Uni créa la colonie de l'Île de Vancouver en 1849 et, en 1858 la colonie de la Colombie-Britannique lors de la ruée vers l'or dans le canyon du Fraser.. Dès la fin des années 1850, les dirigeants canadiens entamèrent une série d'explorations vers l'Ouest, menées entre autres par George Dawson et Joseph Burr Tyrrell, avec l'intention de prendre le contrôle de la Terre de Rupert ainsi que de la région Arctique. Le Territoire du Nord-Ouest et le Territoire Stikine virent le jour en reprenant certains emplacements de la Terre de Rupert. La population canadienne crût rapidement grâce à un taux de natalité élevé ; l'immigration massive de l'Europe vint contrer l'effet de l'émigration vers les États-Unis. En effet, dès les années 1840 et jusqu'à la Grande Dépression de 1929, plusieurs Canadiens français migreront dans les États de la Nouvelle-Angleterre (nord-est américain) afin de fuir l'oppression anglaise et à la recherche de sécurité financière. Cet exode massif sera connu comme étant la « Grande Hémorragie »[94] Au début du XXe siècle, plusieurs de ces Franco-Américains reviendront au Canada et s'installeront dans les provinces de l'Ouest canadien[95]. De plus, durant ces années, plusieurs francophones iront s'établir dans le Canada-Ouest et peupleront les régions francophones actuelles du nord et de l'est de l'Ontario, bien que la colonisation française était déjà présente au temps de la Nouvelle-France dans les régions du sud de l'Ontario actuel[96].
Changement important à la fin de la période pré-Confédération, comme le gouvernement est maintenant imputable à la population, l'Assemblée législative du Canada-Est, majoritairement francophone, abolit le droit coutumier et introduit le code civil du Bas-Canada en 1866 à l'instar du Code Napoléon en France, afin de régir les affaires civiles. Ce nouveau code de loi connaîtra une première réforme en 1980 pour ensuite être réformé complètement en 1991 et donner le Code civil du Québec. De plus, au cours de cette période, la province du Canada connaît une période d'immigration massive provenant du sud des États-Unis à la suite de la guerre de Sécession. Les immigrants américains s'établissent principalement dans le sud du territoire québécois, peuplant ainsi davantage la région des Cantons-de-l'Est qui fut créée lorsque les Loyalistes de l'Empire-Uni s'y réfugièrent après la guerre d'indépendance des États-Unis. Puis, chevauchant la fin du régime du Canada-Uni et le début de la Confédération, on assiste à une recrudescence du développement de la région des Laurentides, lorsque l'évêque Ignace Bourget concède la paroisse de Saint-Jérôme à François-Xavier-Antoine Labelle.
Par suite de la coalition[97] lors de la Conférence de Charlottetown[98] et de la Conférence de Québec[99] en 1864, ainsi que de la Conférence de Londres[100] en 1866, les Pères de la Confédération[101] entreprennent d'unifier les colonies du Canada-Uni, de la Nouvelle-Écosse et du Nouveau-Brunswick. Le 1er juillet 1867, l'Acte de l'Amérique du Nord britannique[102] (AANB) crée le Dominion du Canada, avec quatre provinces distinctes : l'Ontario et le Québec, issus de la division du Canada-Uni, ainsi que le Nouveau-Brunswick et la Nouvelle-Écosse. Le Canada est le premier territoire de l'Amérique du Nord britannique et de tout l'Empire britannique à s'élever au statut de dominion.
Le but de cette organisation est de noyer le Québec, très populeux et concentrant les francophones, dans un groupe de petites provinces anglophones avec les mêmes pouvoirs, ainsi que de se protéger contre les idées expansionnistes des États-Unis après la Guerre civile américaine[103],[104]. Bien que la formation de la Confédération entame une certaine forme de processus d'indépendance, le régime d'État demeure monarchique, mais cette monarchie devient constitutionnelle et conserve un gouvernement responsable à régime parlementaire.
Étant l'un des Pères de la Confédération, George-Étienne Cartier, homme politique de forte influence au Canada-Est, devient le principal précurseur de la conservation du fait français dans la confédération canadienne, ainsi que de la protection du régime politique que forme l'union fédérale. Lors de la Conférence de Londres en 1866, ce régime fut appelé par les délégués anglais à être remplacé par une union législative centrale à majorité anglaise, laquelle aurait supprimé le principe de la distribution des compétences législatives connu au sein de l'union fédérale actuelle et, par le fait même, aurait annihilé tous pouvoirs législatifs réels chez les francophones, étant donné la minorité qui aurait été ainsi formée.
Par la suite, le Canada entreprend de s'agrandir considérablement en annexant les terres de la plaine située entre la province de l'Ontario et la colonie de la Colombie-Britannique. Ce projet se réalise en 1870 lorsque le pays acquiert le Territoire du Nord-Ouest et la Terre de Rupert, qui sont fusionnés en Territoires du Nord-Ouest (au pluriel)[105]. Au même moment, une petite partie de l'ancienne Terre de Rupert est détachée pour former le Manitoba, qui devient la 5e province du pays[105].
Le Manitoba reprend grosso modo le territoire de la colonie de la Rivière-Rouge, majoritairement habitée par des peuples autochtones, incluant les Métis (descendants d'Amérindiens et de Canadiens français ou parfois d'Écossais), qui vivaient dans une structure politique qui leur était propre. Par conséquent, lorsque l'armée arrive pour prendre possession des terres, certaines tensions dégénèrent en conflits ouverts, voire à la guerre. Ainsi, une crise politique majeure, la rébellion de la Rivière-Rouge, est déclenchée par le peuple métis de la plaine, ce dernier désirant conserver autorité et autonomie sur son territoire. Le gouvernement provisoire métis procède à des négociations avec le gouvernement canadien, ce qui aboutit à la création de la province du Manitoba et à son entrée au sein de la Confédération en juillet 1870. Cependant, les soldats canadiens, dont plusieurs étaient des Orangistes, ont abusé de la population métisse.[réf. nécessaire] À la suite de ces événements, plusieurs Métis sont partis plus à l'ouest. Louis Riel, le président du gouvernement provisoire, fut aussi obligé de s'exiler au Montana à cause d'une prime placée sur sa tête par le gouvernement ontarien.
Après la grande expansion vers l'ouest de 1870, le gouvernement canadien applique une politique parfois qualifiée aujourd'hui de « racis[te] » et de « génocid[aire] » envers les Amérindiens des plaines, les confinant dans des réserves, procédant à l'assimilation des enfants dans des pensionnats spécialement construits à cet effet, procédant à de nombreuses exécutions et provoquant intentionnellement des famines[106].
En 1871, la colonie de la Colombie-Britannique, laquelle incluait celle de l'Île-de-Vancouver depuis 1866, rejoint la Confédération pour devenir la 6e province du pays. La colonie de l'Île-du-Prince-Édouard fait de même en 1873. De plus, dans un but d'unification et afin d'étendre l'autorité canadienne sur l'Ouest, le gouvernement fait construire trois chemins de fer transcontinentaux — plus particulièrement le chemin de fer du Canadien Pacifique — en employant, entre autres, de nombreux immigrants chinois (devenus aujourd'hui les Sino-Canadiens). Cependant, la construction du chemin de fer mène au Scandale du Pacifique en 1873 au cours duquel le premier ministre John Alexander Macdonald est aux prises avec des accusations de corruption.
Le gouvernement encourage les immigrants européens à venir développer les Prairies canadiennes et, à cette fin, il adopte la Loi des terres fédérales en 1872 et établit l'emblématique Police montée du Nord-Ouest, aujourd'hui la Gendarmerie royale du Canada (GRC). Alors que de plus en plus d'immigrants se rendent dans la plaine à bord du train transcontinental et que la population de la région s'accroît, certaines des plus grandes villes connues aujourd'hui sont établies au courant de la décennie 1880, dont Regina, Saskatoon, Calgary et Vancouver, tandis que les villes établies plus tôt, comme Winnipeg et Victoria, commencent à prendre de l'ampleur. Par contre, la croissance démographique du pays est ralentie par l'émigration vers le sud. En effet, entre 1871 et 1896, presque un quart de la population canadienne quitte le pays pour aller s'établir aux États-Unis[107].
La décennie 1880 est témoin d'une nouvelle rébellion dans les prairies : la rébellion du Nord-Ouest. À la suite de la rébellion de la Rivière-Rouge de 1869-1870, plusieurs Métis s'étaient déplacés vers l'ouest afin de conserver leur indépendance. Ils fondèrent la colonie de Batoche sur les rives de la rivière Saskatchewan Sud, au nord du site de l'actuelle ville de Saskatoon[108]. Toutefois, avec l'arrivée des immigrants britanniques qui prirent possession des terres des Prairies dans les années 1880 et avec l'imposition du régime cadastral anglais de division des terres en cantons par le gouvernement canadien, le peuple métis se souleva une seconde fois et mena une révolte afin d'établir un État indépendant. Louis Riel, revenu de son exil en 1884, dirigea cette tentative avortée. Au cours de la rébellion, on assista à une série de conflits ouverts, dont la bataille de Duck Lake, le massacre de Frog Lake, la bataille de Fort Pitt, la bataille de Fish Creek, la bataille de Cut Knife, la bataille de Batoche, la bataille de Frenchman Butte et la bataille de Loon Lake. Le chef métis Louis Riel fut finalement capturé et pendu pour trahison en 1885 par les autorités canadiennes.
Au tournant du XXe siècle, plusieurs régions des Territoires du Nord-Ouest s'en détachent et se font accorder un nouveau statut. En 1897, le Yukon, théâtre de la ruée vers l'or dans la région du Klondike, devient un territoire à part entière. Les provinces de l'Alberta et de la Saskatchewan, quant à elles, sont formées en 1905 à partir de la partie sud-ouest des Territoires du Nord-Ouest.
L'immigration croissante entraîne des tensions dans le pays :  Dans cette motivation, les institutions politiques, la presse locale, les dirigeants reconnus et les particuliers canadiens-anglais perpétreront une première émeute anti-Chinois en 1887 à Granville (Vancouver), alors qu'une deuxième émeute de la même sorte prendra place en 1907 et sera en plus dirigée vers les immigrants originaires du Japon. Des agressions similaires se produiront jusque dans les années 1970, notamment lors de la Seconde Guerre mondiale, où des membres de la communauté allemande, italienne et japonaise seront arbitrairement internés dans l'inspiration de l'antisémitisme nazi[réf. nécessaire]. Cependant, en juin 2006, le gouvernement canadien présente des excuses officielles et des dédommagements à la communauté sino-canadienne pour la « taxe d'entrée imposée aux immigrants chinois » avec la Loi de l'immigration chinoise de 1923[110]. Toujours au point de vue social, le Canada voit apparaître le mouvement des suffragettes pour le droit de vote des femmes dès les années 1870. Par contre, ce droit n'est octroyé pour la première fois qu'en 1916 par les provinces de l'Ouest. L'année suivante, le gouvernement fédéral fera de même, et les provinces centrales et de l'Atlantique ainsi que les territoires suivront par la suite[111].Face à la convoitise des États-Unis sur les îles de l'archipel Arctique, l'explorateur Joseph-Elzéar Bernier ainsi qu'un groupe de marins canadiens-français de L'Islet-sur-Mer permet au Canada, dès le début du XXe siècle, de soutenir sa souveraineté sur une série d'îles situées au-delà de l'île de Baffin. La prise de possession effective de la majeure partie de la région arctique par ce groupe d'explorateurs permettra en plus de développer les relations diplomatiques canadiennes avec le peuple inuit[112],[113]. Toujours au point de vue territorial, au cours des années 1920, le Canada est en conflit avec le Dominion de Terre-Neuve, encore indépendant, concernant la frontière entre la province de Québec et le Labrador. Un jugement du Comité judiciaire du Conseil privé de Londres tranche finalement la question en faveur de la frontière réclamée par Terre-Neuve en 1927[114],[56]. Cependant, le Québec considère toujours le tracé de cette frontière comme n'étant pas définitif[115].
À partir de la fin du XIXe siècle, l'exploitation des ressources naturelles permet le développement de nouvelles parties du pays. On assiste d'abord à la ruée vers l'or du Klondike, qui mène à la fondation des villes de Dawson City et de Whitehorse ainsi que du territoire du Yukon dans la décennie 1890. Donnant suite au commerce de la fourrure dans la région, le développement de l'Abitibi-Témiscamingue, au Québec, se fait sentir à la fin du XIXe siècle et au début du XXe siècle avec sa colonisation par les draveurs et son développement agroforestier ainsi que, dans la période de l'entre-deux-guerres, avec un développement minier[116]. En effet, on y extrait des métaux précieux, tels que l'argent et l'or, et des minéraux industriels, tels que le cuivre et le zinc[117]. Ainsi, on verra la fondation des villes de la région telles qu'Amos, Rouyn-Noranda et Val-d'Or. Dans la même lignée, la ville de Yellowknife, capitale actuelle des Territoires du Nord-Ouest, sera fondée au courant des années 1930 lors de la découverte de mines de diamant et d'or dans la région. Les régions les plus au nord des Prairies, notamment celles de l'Alberta et de la Saskatchewan, verront pour leur part une croissance de leur population dès les années 1930 avec la découverte et l'exploitation des gisements de pétrole dans les sables bitumineux de l'Athabasca. Le nord de la Colombie-Britannique sera quant à lui développé grâce à son fort potentiel forestier, alors que le sud de la province le sera grâce à son climat propice à la culture fruitière et maraîchère, notamment dans la vallée de l'Okanagan, près de la ville de Kelowna.
Faisant partie de l'Empire britannique, le Canada est intégré à la seconde guerre des Boers en Afrique du Sud par le premier premier ministre canadien-français Wilfrid Laurier, à la fin du XIXe siècle et au début du XXe siècle. Dirigés par l'homme politique Henri Bourassa, des groupes de Canadiens français opposés à la tutelle britannique se vouent à la défense de leurs droits en tant que peuple. Ils s'opposeront notamment à l'entrée en guerre du Canada et à la création de forces navales canadiennes sous le drapeau britannique.
Dès 1914, le Canada se lance dans la Première Guerre mondiale et envoie sur le front ouest (en Belgique, sur la Somme et en Picardie) des divisions composées principalement de volontaires afin de se battre en tant que contingent national. Les pertes humaines sont si grandes que le premier ministre canadien de l'époque, Sir Robert Laird Borden, décrète la conscription en 1917, ce qui entraîne la Crise de la conscription au Québec. En effet, cette décision est extrêmement impopulaire au sein de la population québécoise, menant ainsi à une perte de popularité pour le Parti conservateur du Canada dans la province et également à la fameuse grève de Québec, souvent passée sous silence, car faisant écho à la révolte du Chemin des dames en France. Lors de l'émeute de Québec de 1918, l'armée tire sur la foule, faisant plusieurs blessés et quelques morts. Bien que les membres du Parti libéral du Canada soient profondément divisés sur l'enrôlement obligatoire, ils s'unifient et deviennent le parti dominant sur la scène politique canadienne.
En 1919, le Canada rejoint la Société des Nations de son propre chef et, en 1931, le Statut de Westminster confirme que, dorénavant, aucune loi du Parlement britannique ne s'étend à l'intérieur des frontières du Canada sans son consentement. Dans la même période, la Grande Dépression, résultat du krach boursier de 1929, affecte les Canadiens de toutes les classes sociales; la popularité croissante du Parti social démocratique du Canada en Alberta et en Saskatchewan débouche sur un état-providence tel qu'initié par Tommy Douglas ou, plus tard, par Jean Lesage dans les années 1960 au Québec. Il devient ainsi l'ancêtre du Nouveau Parti démocratique actuel et prône des politiques plus socialistes et populistes dans le pays.
Après avoir soutenu l'apaisement avec l'Allemagne à la fin des années 1930, le premier ministre libéral William Lyon Mackenzie King obtient l'approbation du Parlement pour l'entrée du Canada dans la Seconde Guerre mondiale en 1939, mobilisant ainsi les militaires avant l'invasion de la Pologne par l'Allemagne. Au début de la guerre, on avait promis au Québec que la participation à ce conflit serait volontaire. Cependant, la conscription est tout de même décrétée en 1944, ce qui mène à une nouvelle crise de la conscription. Le maire de Montréal, Camillien Houde, est mis en prison à la suite de son opposition officielle. Autre sujet de discorde, selon la Constitution canadienne, seules les provinces ont le droit de taxation et d'imposition. Or, pour faire face à l'effort de guerre, le gouvernement fédéral capte tous les pouvoirs fiscaux en promettant de les rendre à la fin du conflit. Cette promesse ne fut jamais respectée, mis à part au Québec, qui retrouve la moitié de son droit d'imposition. L'économie canadienne connaît une forte effervescence pendant la guerre, en grande partie grâce à l'énorme production de matériel militaire pour le compte du Canada, du Royaume-Uni, de la Chine et de l'Union soviétique. Le Canada termine la guerre avec l'une des plus grandes armées du monde[118].. L'économie du pays connaît des heures de gloire et ne cesse de progresser. Au même moment, le Canada modernise son système social qui devient une référence mondiale dans plusieurs domaines, dont celui de la santé.
En 1949, le Dominion de Terre-Neuve, anciennement indépendant, rejoint la Confédération en tant que dixième province du Canada. Avec l'abolition de l'Empire britannique, tous les liens impériaux sont rompus et le Canada obtient de fait son indépendance, bien que sa constitution reste à Londres.
Jusqu'au centenaire du Canada en 1967, une immigration massive d'après-guerre provenant des divers États ravagés en Europe change la courbe de la démographie du pays. En outre, tout au long de la guerre du Viêt Nam, des milliers de dissidents américains s'installent aux quatre coins du pays. L'accroissement de l'immigration — combiné au baby-boom, une force économique équivalente à celle des États-Unis dans les années 1960 et la réaction à la Révolution Tranquille au Québec — favorise l'émergence d'un nouveau type de nationalisme canadien. Les années 1960 sont aussi l'occasion pour les Québécois de se politiser, du fait de leur non-représentation dans les postes stratégiques et économiques. C'est pendant cette période que le mouvement indépendantiste qui conduit à la fondation du Parti québécois et à sa prise de pouvoir en 1976, prend son essor. À la fin des années 1960, la Commission Laurendeau-Dunton obtient le mandat de faire enquête et rapport sur l'état du bilinguisme et du biculturalisme au Canada[119]. La Loi sur les langues officielles y donne suite lorsqu'elle est adoptée en 1969 par le Parlement. Celle-ci proclame l'anglais et le français comme étant les langues officielles du Canada. Celles-ci sont à égalité devant la loi et toute personne a le droit de recevoir les services de l'administration publique fédérale ainsi que de ses sociétés d'État dans l'une ou l'autre langue. Le Commissariat aux langues officielles sera l'organe responsable de l'application de la loi et de la promotion des deux langues.
Entre la fin du XIXe siècle et 1996, plus de 150 000 enfants autochtones ont été retirés à leur famille et placés dans des pensionnats religieux. De nombreux enfants y sont morts faute de soins[120]. En outre, entre les années 1960 et 1980, 20 000 enfants autochtones ont été enlevés de leur famille et placés dans des familles non autochtones dans le cadre de la rafle dite des « Sixties Scoop[121].
Au début de la décennie 1970, une partie du mouvement indépendantiste se radicalise sous la forme du Front de libération du Québec (FLQ). Des actes terroristes seront commis, amenant à la Crise d'octobre de 1970 et à l'intervention du gouvernement du Canada. Une décennie plus tard, le référendum sur la souveraineté-association du Québec a lieu au printemps de l'année 1980. Le premier ministre du Canada, Pierre Elliott Trudeau, promet de modifier la Constitution du Canada lors de la campagne référendaire, à la condition que les Québécois votent en majorité contre la sécession du Québec. Ce référendum sera effectivement rejeté par une majorité de Québécois.
À l'occasion d'une rencontre spéciale en novembre 1981, les premiers ministres provinciaux et fédéral demandent le rapatriement de la Constitution, pour autant que les procédures d'amendement y soient désormais incluses. Après une série de négociations interprovinciales, les premiers ministres provinciaux et fédéral se rencontrent dans la nuit du 4 au 5 novembre 1981 afin de parachever les dispositions de la nouvelle Constitution. La province du Québec est cependant exclue des négociations. Cette période sera métaphoriquement connue par la suite comme étant la Nuit des Longs Couteaux du Canada. Malgré la non-ratification des modifications par la province du Québec, cette dernière sera reconnue par les Nations unies comme faisant partie de la fédération. Le Statut de Westminster de 1931 avait soumis le droit de modification constitutionnelle à l'approbation de la Couronne et du Parlement du Royaume-Uni afin d'éviter le retrait unilatéral du Québec de la Confédération. Cependant, certaines personnes soutiennent que l'imposition de la nouvelle Constitution au Québec est illégitime, étant donné le principe de la souveraineté des États membres d'une confédération et donc, celui de l'unanimité requise pour la modification du traité de l'union[122]. Le Canada rapatrie tout de même sa Constitution du Royaume-Uni le 17 avril 1982, grâce à la loi de 1982 sur le Canada, sous proclamation de la reine Élisabeth II. Cette loi du Parlement britannique crée un État entièrement souverain, bien que les deux pays partagent toujours aujourd'hui le même monarque. Ainsi, la Constitution du Canada inclut dorénavant la loi de 1982 sur le Canada, la Loi constitutionnelle de 1982, la loi constitutionnelle de 1867, le Statut de Westminster de 1931, les lois d'intégrations des provinces et autres lois constitutionnelles et décrets mis en annexe, les diverses modifications constitutionnelles ainsi que les principes constitutionnels sous-jacents reconnus par la jurisprudence. La loi constitutionnelle de 1982 comprend la Charte canadienne des droits et libertés, le Droit des peuples autochtones ainsi que le principe de péréquation. Le régime politique demeure une monarchie constitutionnelle à régime parlementaire selon les dispositions de la loi constitutionnelle de 1867. Dès l'entrée en vigueur de la loi constitutionnelle de 1982, la forme de l'État passe toutefois d'une confédération à une fédération, donnant ainsi place à un fédéralisme canadien. L'expression « Confédération canadienne » continue cependant à être utilisée de façon abusive pour désigner le Canada. L'appellation de Dominion du Canada ne figurant pas dans la nouvelle Loi constitutionnelle, ce nom tombe également en désuétude, et la Fête du Dominion est renommé Fête du Canada[44].
Après le court règne de la première femme première ministre du Canada, la conservatrice Kim Campbell qui fut en poste du 25 juin 1993 au 3 novembre 1993[123], le libéral Jean Chrétien prend le pouvoir en 1993. Très rapidement il réalisera une de ses promesses électorales, en mettant sur pieds le Programme national des travaux d'infrastructures, qui fut lancé en 1994 et qui fut financé par le gouvernement fédéral, les gouvernements provinciaux et les municipalités du Canada. Le programme canadien, qui avait initialement un budget de six milliards de dollars, intégrait dans ses objectifs de contrer la récession économique qui sévissait au pays pendant cette période de l'histoire canadienne[124],[125],[126].
Par suite du référendum de 1995 sur la sécession du Québec, la « loi de clarification » est déposée à la Chambre des Communes par le gouvernement fédéral du premier ministre Jean Chrétien et est adoptée par le Parlement en 2000. Celle-ci donne suite au « Renvoi relatif à la sécession du Québec »[127] à la Cour suprême du Canada en 1998. Cette loi a principalement pour but de définir les bases de reconnaissance de la souveraineté d'une province par le Canada advenant une victoire référendaire future pour son indépendance, notamment en déterminant « si la question permettrait à la population de la province de déclarer clairement si elle veut ou non que celle-ci cesse de faire partie du Canada et devienne un État indépendant »[128]. Cette loi sera cependant jugée inadéquate par les députés de l'Assemblée nationale du Québec, toutes tendances politiques confondues. En effet, ceux-ci créent un contrepoids en cette même année en votant à l'unanimité la « Loi sur l'exercice des droits fondamentaux et des prérogatives du peuple québécois et de l'État du Québec », laquelle édicte que « le peuple québécois détermine seul (…) les modalités de l'exercice de son droit de choisir le régime politique et le statut juridique du Québec »[129] selon une majorité référendaire.
Par suite des profonds changements sociaux et économiques ainsi que de la prise de conscience populaire survenus au Québec pendant la Révolution tranquille des années 1960, plusieurs Québécois commencent à revendiquer une plus grande autonomie provinciale sur le plan politique, et même l'indépendance totale du Québec. La Révolution Tranquille est le précurseur de l'État moderne que forme le Québec et amène les Québécois à se redéfinir non plus en tant que « Canadiens français » (expression aujourd'hui devenue obsolète et même péjorative pour certains au Québec), mais dorénavant en tant que « Québécois », ce qui mène par conséquent à la formation d'un patriotisme québécois plutôt que canadien. La société moderne se développera notamment grâce à l'État-providence et au développement d'entreprises typiquement québécoises et ce, tout en reprenant les postes stratégiques de l'administration publique, tant fédérale que provinciale. Bien que Jean Lesage soit reconnu comme étant le père du nationalisme québécois, plusieurs événements historiques remontant jusqu'au temps de la Nouvelle-France, dont la Rébellion des Patriotes, démontrent que le nationalisme québécois est en fait le fruit du nationalisme canadien-français. Dans cet esprit, René Lévesque fonde le Mouvement Souveraineté-Association en 1967 et supportera la fusion du mouvement l'année suivante avec le Ralliement national pour mener à la formation du Parti québécois. Contrairement à ce parti qui privilégie la démocratie et la voie référendaire pour atteindre l'indépendance,[réf. nécessaire] le Rassemblement pour l'indépendance nationale[N 1] sera quant à lui formé d'une faction indépendantiste qui sera plus tard connue sous le nom Front de libération du Québec[Quoi ?] et qui disparaîtra peu après la Crise d'octobre de 1970. Bien que plusieurs personnalités politiques, dont René Lévesque, aient considéré cette allocution comme étant de l'ingérence politique, le discours de l'ancien président de la République française, Charles de Gaulle, en 1967 à Montréal, a enflammé les foules et a donné un coup de main au mouvement souverainiste en présentant le Québec à la communauté internationale, notamment avec sa célèbre phrase : « Vive le Québec libre ![130] ».
Dans les années 1960, les personnalités politiques réussissent un tour de force avec l'abolition du Conseil législatif du Québec. Contrairement aux autres provinces canadiennes qui ont aboli le leur dans les premières décennies de la Confédération, le Québec réussit en 1968 à se détacher de cette chambre haute, symbole du contrôle du pouvoir législatif donné aux Canadiens français. Cette chambre donnait suite aux Conseils législatifs du Bas-Canada et de la province unie du Canada. Le lieutenant-gouverneur du Québec demeure toutefois, encore aujourd'hui, le symbole de la monarchie britannique au Québec. Dans le cadre légal de la loi constitutionnelle de 1982, cette institution ne peut cependant être abolie que par une modification de la Constitution par l'accord unanime des législatures provinciales et du Parlement fédéral, bien que la légitimité de cette loi au Québec soit sujet à débat.
Lors du premier gouvernement formé par le Parti québécois en 1976, le premier ministre René Lévesque fait la promotion de la devise nationale : « Je me souviens », laquelle avait été gravée sur la façade de l'hôtel du Parlement du Québec en 1883 par l'architecte Eugène-Étienne Taché. Au cours des années, cette devise nationale jouera un rôle important pour plusieurs Québécois dans l'élaboration et le développement du patriotisme québécois et ce, en rappelant l'histoire de l'Amérique française[131],[132]. En outre, encore aujourd'hui, certaines personnes ne reconnaissent pas la légitimité du Parlement et du gouvernement fédéral dans les affaires canadiennes-françaises. Sans compter que le Québec n'a jamais signé la loi constitutionnelle de 1982. Cependant, bien que le respect de la culture canadienne-française soit d'intérêt pour plusieurs francophones, le mouvement souverainiste crée une dichotomie dans l'idéologie des francophones du Québec et de ceux des autres provinces, bien que certains groupes, dont les Acadiens, possèdent leurs propres institutions et symboles nationaux tels que la devise « L'Union fait la force »[133]. Bien que la diaspora québécoise soit apparue dès les années 1840 en quête d'une sécurité d'emploi, plusieurs Québécois - anglophones et francophones - quittent le Québec lors de la prise de pouvoir du Parti québécois et de l'entrée en vigueur de la Charte de la langue française. Ceux-ci migrent notamment aux États-Unis ainsi que dans les provinces de l'Ontario et des Prairies. Cet exode suit le déménagement du siège social de plusieurs grandes entreprises canadiennes-anglaises quittant entre autres le centre financier de la rue Saint-Jacques de Montréal pour celui de la Bay Street de Toronto.
L'aliénation entre les deux principaux groupes linguistiques sur la question de la langue et sur les divergences sociales et culturelles est exacerbée par plusieurs événements, dont la Crise de la conscription de 1944 à la Seconde Guerre mondiale, la crise d'Octobre de 1970 au cours de laquelle la loi martiale (loi sur les mesures de guerre) est décrétée par le premier ministre fédéral - Pierre Elliott Trudeau - au Québec[134], ainsi que l'échec des deux conférences constitutionnelles de l'ancien premier ministre du Canada - Brian Mulroney, à savoir l'Accord du lac Meech de 1987 et l'Accord de Charlottetown de 1992. Ces dernières avaient pour but d'amener le Québec à ratifier la Constitution. Nonobstant le caractère sporadique de ces événements, l'attitude possessive et vindicative du Canada anglais à l'égard du Québec[135],[136], ainsi que le phénomène du dénigrement systématique du Québec, ou Quebec bashing[137], viendront quant à eux ajouter leur grain de sel à cette frustration continuelle[138],[139].
Un premier référendum en 1980 conclut que 59,6 % des électeurs, dont une majorité d'électeurs francophones, rejettent la proposition de souveraineté-association, et un second référendum en 1995 démontre que la souveraineté est rejetée à 50,6 % des voix, bien qu'elle ait été soutenue par 60 % des électeurs francophones. Les résultats du référendum de 1995 sont cependant contestés par plusieurs souverainistes et fédéralistes étant donné la faible marge séparant les deux camps. D'un côté, les souverainistes mentionnent que le gouvernement fédéral a violé les lois électorales du Québec par l'entremise, entre autres, d'Option Canada. De l'autre côté, les fédéralistes font état des irrégularités au niveau du nombre élevé de bulletins de vote rejetés dans certains comtés fortement opposés à la souveraineté, sans quoi les résultats du référendum de 1995 auraient été moins serrés. Depuis 1995, l'appui populaire à la souveraineté du Québec a reculé pour se rapprocher de celui exprimé au référendum de 1980.
D'autre part, le mouvement souverainiste québécois défend continuellement sa position affirmant que la culture canadienne-française n'est pas considérée à sa juste valeur en politique canadienne étant donné une majorité nettement plus grande de Canadiens anglais, et étant donné les événements historiques. Dans le but de faire front commun et de défendre les intérêts du Québec sur les sujets tombant sous la compétence législative fédérale et ce, en travaillant de concert avec son homologue provincial (le Parti québécois), le Bloc québécois est fondé en 1991 par le député Lucien Bouchard (ultérieurement premier ministre du Québec de 1996 à 2001). Ce nouveau parti souverainiste fait son entrée à la Chambre des Communes en 1993 en tant qu'opposition officielle (1993-1996); depuis lors, ce dernier a toujours récolté plus de la majorité des sièges alloués au Québec jusqu'à l'élection du 14 octobre 2008. Principalement, c'est un parti qui se dit social-démocrate et qui prône le droit à l'autodétermination des peuples tel que déclaré par le président américain Woodrow Wilson, après la Première Guerre mondiale, dans le respect de la décolonisation, et qui est un principe reconnu dans le droit international de l'Organisation des Nations unies.
De plus, la discorde entre Canadiens anglais et Québécois entraîne la province de Québec à ne déléguer pratiquement aucune de ses compétences législatives à des organismes de collaboration interprovinciale, tendant ainsi à créer une société totalement distincte et se dissociant ainsi de la plupart des accords interprovinciaux et fédéraux qui pourraient compromettre le droit du Québec de faire valoir la culture et le savoir-faire canadiens-français au sein de groupes politiques où les décisions sont prises à la majorité des voix. D'autre part, dans un but de promotion des affaires canadiennes-françaises, le Québec a su tirer profit de sa position géopolitique particulière où il est le seul état majoritairement de langue française en Amérique du Nord, contrairement aux francophones des autres provinces et des États-Unis qui sont souvent noyés et assimilés à la masse d'expression anglaise et pour qui un territoire les circonscrivant est souvent quasiment indéfinissable. De la même façon qu'un pays indépendant, il n'est pas rare de voir des personnalités politiques québécois se porter à la défense des minorités francophones des autres provinces[140] et territoires. Le gouvernement du Québec s'engage même dans des accords extraterritoriaux, voire internationaux, en se donnant pour mission la promotion et l'accroissement des échanges entre personnes de langue française. Ainsi, on peut assister, par exemple, à des ententes conclues avec la Société nationale de l'Acadie, avec les gouvernements des provinces à l'ouest du Québec en matière d'affaires francophones, et même avec les communautés francophones des États-Unis telles que celles des États de la Louisiane et du Maine.
Depuis quelques années, différentes scissions sont apparues au sein du mouvement souverainiste sur la question nationale. Cependant, la souveraineté demeure le but de toutes les divisions. Non seulement de nouveaux partis politiques ont été fondés tels que Québec solidaire, mais des organisations telles que le Conseil de la souveraineté du Québec, les Jeunes Patriotes du Québec et le Réseau de Résistance du Québécois ont été formées afin de regrouper les militants, de promouvoir l'indépendance et d'agir, non pas contre, mais indépendamment de l'aile parlementaire. Ces organisations viennent donc s'ajouter aux Sociétés Saint-Jean-Baptiste ainsi qu'à leur fédération, le Mouvement national des Québécoises et des Québécois, fondés respectivement en 1834 et en 1947.
Dans l'optique de l'avancement continuel vers la souveraineté, certains acteurs du mouvement ont, depuis peu, avancé l'idée de la gouvernance souverainiste pour contrer les inconvénients des référendums populaires[141]. Dans cette stratégie, l'indépendance du Québec est perçue comme une question de fait alors qu'un référendum est une formalité administrative. Par conséquent, un gouvernement souverainiste élu du Québec, notamment du Parti québécois, sera porté à créer des institutions et politiques (constitution, citoyenneté, indépendance politique régionale, etc.) répondant à cette vision de l'avenir du Québec tout en conservant à l'esprit les exigences constitutionnelles canadiennes.
Au printemps 2006, le nouveau gouvernement conservateur du Canada a signé un accord avec le gouvernement du Québec afin que la province joigne les rangs de l'UNESCO en tant que membre associé. Ce faisant, le fédéralisme asymétrique est désormais présent en politique fédérale. De plus, le 27 novembre 2006, la Chambre des communes du Canada a voté, presque unanimement, en faveur d'une motion qui reconnaît que « les Québécois forment une nation au sein d'un Canada uni », une démarche surtout symbolique mais qui constitue un grand pas en avant pour la consolidation du concept de statut particulier de la province francophone. Au Canada anglais, les critiques ont fusé, beaucoup craignant qu'on ne donne de nouvelles armes aux indépendantistes québécois.
Depuis la fin du XIXe siècle, les Amérindiens possèdent des territoires quasi-autonomes, appelés réserves, octroyés par le gouvernement fédéral. Les peuples autochtones vivant dans ces zones ne paient ni impôts, ni taxes provinciales. Souvent isolées, ces réserves disposent de peu de services publics. En conséquence de ce manque de services et de plusieurs traités souvent signés sous l'influence de l'armée britannique, divers heurts surviennent encore aujourd'hui au sujet de revendications territoriales et du respect de la place des peuples autochtones au sein du Canada. Ces différends entre les peuples autochtones et le gouvernement dégénèrent quelquefois en conflits ouverts, notamment lors de la Crise d'Oka en 1990, de la Crise d'Ipperwash en 1995, de la Crise de Kanesatake de 2004 à 2005 et de la Crise de Caledonia en 2006. La reconnaissance des droits ancestraux ou issus de traités quant aux revendications territoriales a été confirmée dans la loi constitutionnelle de 1982. Depuis les années 1990, le Canada assiste à une importante crise de la contrebande de tabac, en plus d'un important trafic d'armes à feu et de stupéfiants transitant notamment par la réserve d'Akwesasne, laquelle chevauche la frontière canado-américaine. Alors que ces sujets s'avèrent être très sensibles pour les personnalités politiques, les gouvernements ont souvent été accusés de laxisme dans les médias et par la population[142].
L'intégration économique avec les États-Unis se renforce après 1940. L'Accord de libre-échange nord-américain (ALENA) de 1994 est un moment culminant dans l'élaboration d'une intégration économique entre les deux pays. Toutefois, le conflit du bois d'œuvre demeure un enjeu politique et commercial depuis la fin des années 1980. De plus, l'économie canadienne est en croissance continue grâce aux secteurs de l'immobilier et des ressources minières et naturelles ainsi qu'aux réserves de pétrole dans les sables bitumineux de l'Athabasca, bien qu'elle fût affectée à la baisse par la crise économique asiatique de 1997-1998, par les attentats terroristes de 2001 aux États-Unis et par la perte de valeurs des titres technologiques en 2002. Le Canada a été le pays hôte du Sommet des Amériques en 2001, lequel s'est tenu dans la ville de Québec, afin de pourvoir aux dispositions d'une éventuelle Zone de libre-échange des Amériques (ZLEA).
D'autre part, depuis les années 1980, les Canadiens se préoccupent de leur autonomie culturelle puisque les compagnies, la télévision et les films américains sont omniprésents. Cependant, faisant contraste avec le reste de l'Amérique du Nord, certaines provinces du Canada s'alignent sur un système universel de soins de santé. De plus, la Charte canadienne des droits et libertés contraint les tribunaux à la conservation du multiculturalisme dans leurs jugements.
Le Canada participe à la guerre du Golfe de 1990-1991 ainsi qu'aux missions de paix de l'Organisation des Nations unies en Ex-Yougoslavie et au Rwanda dans les années 1990. Depuis 2001, le Canada participe activement à la guerre d'Afghanistan au sein de la coalition occidentale formée à la suite des attentats terroristes du 11 septembre 2001 perpétrés par une faction islamique talibane aux États-Unis. De ce fait, le Canada entre officiellement dans la guerre contre le terrorisme en tant qu'allié des États-Unis, mais se restreindra à la campagne militaire menée en Afghanistan.
Le Canada signe le Protocole de Kyoto en 2002 dans le respect de la Convention-cadre des Nations unies sur les changements climatiques et devient le 99e pays à y adhérer[143]. En 2006, le gouvernement conservateur de Stephen Harper succède au gouvernement Libéral de Jean Chrétien et désire retirer le pays du protocole de Kyoto considérant les objectifs comme trop idéalistes et inatteignables[144],[145]. Le gouvernement conservateur apporte son appui aux pétrolières de l'ouest du pays, les principaux opposants au protocole. De plus, le gouvernement Harper stoppe les programmes de sensibilisation et de recherche scientifique et adopte une politique laxiste concernant les émissions de gaz à effet de serre (GES)[146]. Plutôt que de suivre les recommandations environnementales, les conservateurs de Harper s'alignent avec la Coopération économique pour l'Asie-Pacifique (APEC) donnant ainsi préséance au Partenariat Asie-Pacifique conclu par les membres de l'Association des nations de l'Asie du Sud-Est (ASEAN)[147],[148]. En 2011, Le gouvernement conservateur devient un gouvernement majoritaire et annonce en décembre de la même année que le pays se retirerait du protocole de Kyoto en 2012. Le retrait n'a pas plu à certaines provinces profondément impliquées dans le protocole, comme le Québec qui en retirait des avantages économiques et commerciaux avec l'Europe. Ainsi, le Canada a laissé le choix à ses provinces de réintégrer le protocole, car les provinces ont les pouvoirs nécessaires pour émettre des législations favorables à la réalisation des demandes de ce protocole. Seules les provinces du Québec et du Manitoba le réintègrent. Des provinces comme la Colombie-Britannique et le Nouveau-Brunswick étaient désireuses d'en savoir davantage à son propos avant de prendre leur décision[149].
Dans un autre ordre d'idées, un phénomène populaire invite plusieurs Canadiens à leur migration dans les États du sud des États-Unis tels que l'Arizona et la Floride, dans les pays des Caraïbes et de l'Océanie ainsi que sur l'île de Vancouver où les hivers sont généralement plus doux. Ce phénomène s'étend à toute catégorie d'âge sans restriction bien qu'il soit présent notamment auprès des personnes retraitées. Ces gens que l'on nomme les Snowbirds y passent l'hiver, généralement six mois, et reviennent au Canada en été afin de conserver leur statut légal de résidence. La présence canadienne étant d'autant plus remarquée avec les Québécois en Floride que des institutions financières québécoises s'y sont installées en plus de journaux du Québec qui sont livrés chaque matin dans certaines régions « québécoises » de la Floride, comme à Hallandale Beach, Pompano Beach, Lauderhill, Fort Lauderdale, Fort Myers et Key West. Depuis la première moitié du XXe siècle, des relations économiques et de développement d'affaires se sont développées et sont maintenant présentes tout au long de l'année. On y retrouve en plus plusieurs centaines de milliers de propriétés québécoises. De plus, une forte présence de Québécois peut être remarquée dans le Maine, à Old Orchard Beach et à Ogunquit, lors des vacances d'été.
Depuis le 17 octobre 2018, le Canada est le second pays au monde, après l'Uruguay, à légaliser totalement le cannabis pour tous usages, y compris récréatifs. Les lois encadrant le cannabis diffèrent selon chaque province[150].
En 2017, les Canadiens célèbrent les 150 ans de la Confédération.
Quatre journées de fêtes canadiennes ont été ainsi plus grandioses qu'à la normale pour souligner cet anniversaire. La Journée nationale des peuples autochtones, la Saint-Jean Baptiste, la Fête nationale du Québec et de la francophonie canadienne, et la Journée canadienne du multiculturalisme ont vu leurs programmations souligner les 150 ans du pays. Enfin, le point culminant de ces célébrations majeures a été le 1er juillet, lorsque les Canadiens au pays et à l’étranger ont célébré la Fête du Canada.
Plusieurs autres activités ont été prévues pour les 150 ans du Canada dans différentes communautés[151]. De plus, pour souligner l’événement, les différents parcs nationaux du Canada ont été libres d'accès tout au long de cette année anniversaire 2017[152].
Le Canada est une monarchie constitutionnelle qui, en tant que royaume du Commonwealth, reconnaît Charles III comme roi du Canada depuis le 8 septembre 2022.
Le pouvoir exécutif est, quant à lui, constitué du Conseil privé, chargé de conseiller le gouverneur général dans sa prise de décisions. Les conseillers privés sont nommés par le gouverneur général. Parmi eux, des conseillers sont assermentées pour former le cabinet ministériel, dirigé par le premier ministre. Les membres du Cabinet sont les seuls conseillers privés autorisés à agir officiellement par décrets au nom du gouverneur général et ont la responsabilité d'un ministère.
Charles III, roi du Canada depuis le 8 septembre 2022.
Mary Simon, gouverneure générale depuis 2021.
Justin Trudeau, premier ministre depuis 2015.
En sa qualité de représentant du roi ou de la reine, chef de l'État, le ou la gouverneur général et à ce titre Commandant en chef des Forces armées canadiennes, assume les prérogatives royales lorsque le roi ou la reine ne se trouve pas au Canada. Le ou la gouverneur général est nommé par le roi ou la reine sur conseil du premier ministre. Rideau Hall est sa résidence principale d'Ottawa alors que la Citadelle de Québec est sa résidence à Québec. Bien que le ou la gouverneur général conserve certaines prérogatives royales, ses devoirs et obligations sont définies par la Constitution du Canada, laquelle consiste en une série de lois constitutionnelles enchevêtrées, celles-ci étant composées de textes écrits et de traditions et conventions non écrites. Dans les faits, le poste de gouverneur général est surtout symbolique, et ne possède pas de pouvoir réel. Depuis quelques années un débat subsiste, à savoir si le poste de gouverneur général et celui des lieutenant-gouverneur devraient être abolis.
Le poste de premier ministre, chef du gouvernement du Canada, revient de facto au chef du parti politique dont la représentation à la Chambre des Communes est la plus grande, ce qui peut mener à des situations où le parti du gouvernement peut être majoritaire comme minoritaire. Le premier ministre choisit ensuite les membres du conseil des ministres. Les nominations ministérielles sont effectuées par le gouverneur général sur les recommandations du premier ministre, ces dernières étant habituellement de facto respectées bien qu'elles puissent de jure être rejetées. Les membres du Cabinet proviennent généralement du parti politique du premier ministre, et fort majoritairement des députés de la Chambre des communes, bien que certains puissent aussi provenir du Sénat, ou même dans de rares cas, ne faire partie d'aucune Chambre du Parlement. Bien qu'il n'y ait aucun texte écrit à cet effet, et comme la tradition dicte au gouverneur général de nommer au poste de premier ministre le chef de la majorité politique élue à la Chambre des communes, et aux postes de conseillers privés et de ministres les gens dont il approuve la nomination, certains juristes soutiennent que de nos jours, cette disposition unit constitutionnellement le gouverneur général.
Comme le Canada est une monarchie parlementaire, le gouverneur général est le responsable du pouvoir exécutif en l'absence du Roi. Cependant, au fil des années, son rôle a évolué et s'est empreint de conventions non écrites qui lui ont fait perdre le pouvoir qu'il détenait autrefois. Bien qu'aujourd'hui son rôle soit apolitique et purement symbolique et protocolaire, le gouverneur général est tout de même assisté par le Bureau du secrétaire du gouverneur général afin d'accomplir son mandat et [de] s'acquitter de ses responsabilités en qualité de chef d'État et en ce qui concerne la constitution, le cérémonial et les autres responsabilités traditionnelles[153].
Le Conseil privé du Roi ou de la Reine pour le Canada a été créé par la loi constitutionnelle de 1867 afin d'aider et aviser le gouverneur général en conseil dans l'administration du gouvernement. Il est le principal organe du pouvoir exécutif après la Couronne. Le ministre des Affaires intergouvernementales est par tradition le président du Conseil privé. En plus des candidats aux postes de ministre, le premier ministre peut recommander la nomination d'autres personnes au titre de conseillers privés telles que des personnalités de marque, d'anciens membres du gouvernement ou tout simplement des gens à titre honorifique. Le Conseil privé pourrait (…), s'il était actif, être un organisme important et politiquement encombrant (…) avec des membres allant toujours à contre-courant des autres. Il s'est sorti de cette malencontreuse situation d'une façon simple et efficace, en ne réunissant que très rarement tous ses membres[154]. Ainsi, la dernière fois que le Conseil privé a réuni tous ses membres fut en 1981 afin de consentir officiellement au mariage royal du prince Charles, prince de Galles, et de Lady Diana Spencer. Suivant l'annonce des fiançailles du prince à la duchesse de Cornouailles, Camilla Parker Bowles, en 2005, le ministère de la Justice du Canada annonça que le Conseil privé n'avait pas à se réunir puisque le mariage n'aboutirait pas à une descendance et donc, n'affecterait pas l'ordre de succession pour la Couronne du Canada. Selon David Brown, dirigeant au Bureau du Conseil privé en 1981, si le Conseil privé avait rejeté le mariage de 1981, il y aurait eu division dans la lignée royale ainsi qu'avec les autres pays membres du Commonwealth. Par conséquent, aucun descendant du Prince de Galles n'aurait été reconnu comme légitime successeur au Trône. Cette situation aurait amené le Canada à créer sa propre monarchie ou à changer son régime d'État pour celui d'une république[155].
Le gouvernement est composé du cabinet ministériel, organe exécutif du Conseil privé et lequel est dirigé par le premier ministre. Dans ces tâches, ce dernier est soutenu par le Bureau du Conseil privé. À sa tête se trouve le greffier du Conseil privé, le plus haut fonctionnaire apolitique du gouvernement du Canada. En tant que secrétaire du Cabinet, le greffier du Conseil privé joue un rôle central dans la gestion de l'État et a pour tâches de conseiller de façon impartiale et seconder le premier ministre et le Cabinet, et diriger la fonction publique.
Le gouvernement du Canada est en plus assisté par le Conseil du Trésor, comité du Cabinet composé du président du Conseil du Trésor et de ministres. Le Conseil du Trésor est chargé de l'imputabilité et de l'éthique, de la gestion des finances, du personnel et de l'administration, de la fonction de contrôle ainsi que de l'approbation des règlements et de la plupart des décrets en conseil[156]. Ainsi, tout comme le Bureau du Conseil privé, le Conseil du Trésor joue un rôle central dans la gestion de l'État, mais constitue un organe politique dont les membres sont élus contrairement à ce dernier. Le Conseil du Trésor est donc directement imputable au Parlement.
Bien que chaque ministère soit responsable de son portefeuille respectif, trois ministères jouent un rôle central dans la gestion des finances publiques de l'État et supporte ainsi les travaux du Conseil du Trésor et des autres ministères. Ainsi, le ministère des Finances est responsable de toutes les questions en matière de finances publiques qui ne sont pas attribuées de droit au Conseil du Trésor, tel l'établissement du budget fédéral et de la politique économique et financière du pays. L'Agence du Revenu du Canada est quant à elle responsable du contrôle d'application de la législation fiscale. Alors que le ministère des Travaux publics et des Services gouvernementaux est un organisme de services communs destinés à aider les ministères à réaliser leurs programmes. Le ministre de ce dernier ministère est par tradition le Receveur général du Canada et donc, responsable de l'émission et de la réception de tous les paiements faits et reçus par le gouvernement et, responsable de la préparation et de la publication des comptes publics.
D'autres organismes existent dans la gestion centrale et le contrôle de l'État, mais ceux-ci relèvent directement du Parlement et sont donc indépendants du gouvernement. Tel est le cas du Bureau du commissaire à l'éthique, du Bureau du vérificateur général et du Commissariat aux langues officielles.
Le Canada est un régime parlementaire fédéral avec une tradition démocratique héritée de la démocratie anglaise du XVIe siècle. Le pouvoir législatif est constitué du Parlement, lequel comprend la roi ou la reine (en son absence le gouverneur général du Canada), le Sénat et la Chambre des communes[157]. La représentation du pouvoir législatif se fait par la Colline du Parlement, là où se situent tous les édifices parlementaires.
Avant d'entrer en fonction, le premier ministre du Canada ainsi que tous les membres de son cabinet ministériel sont assermentés par le gouverneur général en conseil d'abord en tant que conseillers privés au sein du Conseil privé de la Reine pour le Canada, et ensuite en tant que membres du Cabinet. Le premier ministre exerce des pouvoirs nombreux, notamment quant à la nomination des responsables au sein du Gouvernement et de l'administration publique.
La tradition veut que le prédicat « le très honorable » précède le nom du premier ministre.
Justin Trudeau, chef du Parti libéral du Canada est premier ministre depuis le 4 novembre 2015. Chacun des ministres a la responsabilité de son ministère respectif. Ainsi, chaque ministre est chargé de la nomination des responsables au sein du ministère, dont le sous-ministre. Ce dernier constitue la plus haute autorité administrative non élue du ministère. Son rôle est de conseiller le ministre et de rendre compte à celui-ci des activités du ministère.
La Chambre des communes du Canada est composée de députés élus au scrutin uninominal majoritaire à un tour dans chacune des circonscriptions électorales (jadis appelées « comtés »). Des élections générales sont déclenchées par le gouverneur général en conseil après que celui-ci a dissous la Chambre des communes pour l'une des raisons suivantes :
Les membres du Sénat, dont les sièges sont octroyés sur une base régionale, sont choisis par le premier ministre et assermentés à vie par le gouverneur général en conseil pour servir jusqu'à l'âge maximum de 75 ans.
À l'issue de l'élection fédérale canadienne de 2015 les cinq partis politiques du Canada siégeant actuellement au Parlement sont, en ordre décroissant de représentation à la Chambre des communes : le Parti libéral du Canada (184), le Parti conservateur du Canada (99), Nouveau Parti démocratique du Canada (44), le Bloc québécois (10) et le Parti vert du Canada (1). Aucun député indépendant n'a été élu le 19 octobre 2015. Bien que plusieurs autres partis ne soient pas représentés au Parlement, la liste des partis historiques avec représentation est substantielle (voir Partis politiques canadiens).
Le président de la Chambre des communes est responsable de la direction des affaires parlementaires de la Chambre. Il veille au bon déroulement de la Chambre, à l'interprétation impartiale des règles et à la défense des droits et privilèges de tous les députés. Siégeant au centre de la Chambre avec des greffiers adjoints, sous-greffiers et légistes, le Greffier de la Chambre des communes relève du président et, a pour tâches de conseiller de façon impartiale le Président et les députés sur l'interprétation des règles, des usages et de la jurisprudence parlementaires. Il est aussi responsable de l'enregistrement des décisions et des débats de la Chambre dans le Hansard et de faire parvenir les publications à la Gazette officielle. Le sergent d'armes assiste quant à lui le greffier de la Chambre des communes dans son rôle de chef de la cité parlementaire, notamment dans les fonctions protocolaires telles que le cérémonial de la masse au début et à la fin de chaque séance, dans la sécurité et l'entretien des édifices parlementaires.
Le Sénat fonctionne sensiblement de la même façon que la Chambre des communes. Cependant, comme c'est la Chambre haute du Parlement, la représentation de l'autorité est plus présente. À titre de membre du service interne de la maison royale, l'huissier du bâton noir agit en tant que serviteur personnel de la Reine et sert de messager parlementaire afin de convoquer les députés au discours du Trône et à la cérémonie de la sanction royale. Il est responsable des détails protocolaires, logistiques et administratifs entourant tous les événements d'envergure nationale, tels que l'ouverture des législatures, l'investiture du gouverneur général, les funérailles nationales et la réception des dignitaires et officiels étrangers par le gouverneur général.
Dans l'exercice de leurs tâches, les parlementaires sont assistés par la Bibliothèque du Parlement, laquelle offre des services objectifs d'information juridique, budgétaire et d'estimation de coûts.
En matière de responsabilité ministérielle, les membres du Cabinet doivent rendre compte des activités de leur ministère lors d'une période de questions et de réponses orales à chaque jour de travail de la Chambre des communes. Ainsi, une période de la journée est allouée où tous les membres du gouvernement, dans la mesure du possible, siègent en Chambre afin de répondre aux questions de l'Opposition officielle et des autres partis d'opposition.
Une période similaire existe au Sénat où les membres de l'Opposition interrogent le leader parlementaire du gouvernement au Sénat.
À l'ouverture de chaque session parlementaire (deux par année), le gouverneur général en conseil prononce le discours du Trône à même la Salle du Trône, la chambre du Sénat. Ce discours stipule les grandes lignes d'intervention du gouvernement tout au long de la session parlementaire. Ce discours est ensuite passé au vote par les députés et, en général, soumis à une motion de confiance. Cette motion a pour effet d'induire au vote la qualité de faire tomber le gouvernement si le vote s'avère négatif. Dans cette situation, le gouverneur général en conseil doit dissoudre la Chambre des communes et déclencher des élections générales. Si le vote est positif, le gouvernement peut cependant mettre en œuvre ces directives.
En plus des travaux législatifs effectués en Chambre, différents comités sénatoriaux et comités des Communes existent afin d'approfondir les études. Ces comités permettent aux députés et aux sénateurs de rencontrer et d'interroger (dans un cadre formel) des experts ou groupes de citoyens pouvant apporter une opinion sur le sujet débattu. Ces comités déposent ensuite leurs rapports à la Chambre et au Sénat pour examen subséquent. Sauf exception, telle le vote du budget, l'approbation des deux chambres législatives est nécessaire pour que le gouverneur général en conseil sanctionne la création, la modification ou l'abrogation d'une loi.
La Constitution inclut la Charte canadienne des droits et libertés garantissant aux Canadiens les droits et libertés qui y sont énoncés, et qui ne peuvent être enfreints par aucun niveau de gouvernement au Canada. « Ils ne peuvent être restreints que par une règle de droit, dans des limites qui soient raisonnables et dont la justification puisse se démontrer dans le cadre d'une société libre et démocratique » (Extrait du premier article de la Charte). En outre, une disposition de dérogation octroie au Parlement fédéral ainsi qu'aux législatures provinciales le pouvoir de légiférer en tout temps, et dans la mesure convenue par les législateurs, en outrepassant temporairement certaines dispositions de la Charte — dans les libertés fondamentales, les garanties juridiques ou les droits à l'égalité — pour une période de cinq ans renouvelable.
La Cour suprême du Canada est la juridiction d'appel de dernier ressort du pays et/ou la plus haute cour au Canada. Elle tranche des questions de droit d’importances pour le public. Elle contribue aussi à l'évolution de tous les domaines du droit au Canada. Elle est composée de 9 juges dont le juge en chef du Canada. La Cour suprême du Canada constitue une institution nationale majeure qui se trouve au sommet du pouvoir judiciaire de l’État canadien[158].
La Constitution garantit un partage des compétences législatives entre le Parlement et les législatures provinciales. Chacun des paliers possède l'autorité suprême sur leurs compétences respectives bien que les deux aient une compétence égale dans les matières sur l'immigration et l'agriculture. Afin de faire respecter ce partage des juridictions, plusieurs débats se traduisent souvent en Cour suprême. Au cours des années, il s'est cependant avéré que les provinces possèdent significativement plus de pouvoirs que le gouvernement fédéral au point où les provinces ont le pouvoir d'influencer indirectement l'impact de certaines compétences fédérales à l'intérieur des limites de leur territoire. Tel est le cas en matière de réglementation du trafic et du commerce (compétence fédérale) et d'octroi de permis d'exercice et de pratique commerciale (compétence provinciale) ou d'élaboration contractuelle (compétence provinciale). Ainsi, dans cet exemple, les principes du libre-marché promus par les gouvernements fédéraux peuvent être annihilés par des politiques provinciales.
Le ministère des Affaires intergouvernementales est un organisme du Bureau du Conseil privé et, est responsable des affaires parlementaires touchant les relations fédérales-provinciales-territoriales telles que le fédéralisme fiscal, l'évolution de la fédération et l'unité canadienne.
Dans un but de développement de la fédération, les provinces, avec la participation des territoires, ont créé le Conseil de la fédération en 2003. Bien que cette organisation n'ait pas été institutionnalisée, elle permet aux provinces et territoires de consolider leurs forces et de travailler en collaboration sur tous sujets tombant dans leur juridiction législative en favorisant entre autres les échanges interprovinciaux. De plus, elle permet aux provinces et territoires de faire front commun lorsque vient le temps de négocier avec le gouvernement fédéral, notamment en matière de péréquation et de développement de projets nécessitant la coopération du gouvernement fédéral.
Chaque province est un État à part entière avec un régime également parlementaire. Le régime est constitué du pouvoir législatif, c'est-à-dire un parlement unicaméral comprenant une assemblée législative élective et du représentant du roi ou de la reine du Canada appelé lieutenant-gouverneur. Le pouvoir exécutif, c'est-à-dire le gouvernement, est composé du représentant de la reine et d'un conseil des ministres dirigé par le premier ministre provincial.
L'assemblée législative de chaque province est composée de députés élus au scrutin uninominal majoritaire à un tour dans chacune des circonscriptions électorales provinciales (différentes des circonscriptions fédérales, à l'exception de l'Ontario, où plusieurs circonscriptions provinciales coïncident avec les circonscriptions fédérales depuis 1999).
En 2001, la Colombie-Britannique a été la première province canadienne à adopter le principe d'élections générales à date fixe. Depuis, trois autres provinces, soit l'Ontario (2004), Terre-Neuve-et-Labrador (2004) et le Nouveau-Brunswick (2007) ont fait de même. En novembre 2006, le gouvernement fédéral modifiait lui aussi sa loi électorale afin que soit déterminée d'avance, la date des futures élections. De nos jours, seule la Nouvelle-Écosse n'a pas adopté de loi prévoyant des élections générales à date fixe. Il est cependant à noter que la mise en place d'élections à date fixe au Canada n'a pas pour effet d'empêcher un gouvernement de perdre la confiance de son assemblée législative élective. Par conséquent, si un gouvernement est renversé en chambre, la date initialement prévue pour la tenue d'une élection générale est devancée même si le principe d'élections à date fixe a été adopté. Cette caractéristique de ce dernier dans le contexte canadien se distingue de celui appliqué aux États-Unis où la date prévue, par exemple, pour la tenue des élections présidentielles, ne peut être devancée, et ce, même si le président américain n'est plus en mesure d'exercer son mandat[159].
Le système démocratique et parlementaire d'une province est par défaut celui défini dans la loi constitutionnelle de 1867, à savoir un système similaire à celui du parlement fédéral. Cependant, chaque parlement provincial a le pouvoir de modifier sa propre constitution provinciale.
La judicature du Canada est définie dans la loi constitutionnelle de 1867. Elle joue un rôle important dans l'interprétation des lois, et possède le pouvoir d'invalider les lois qui transgressent la Constitution. Tous les tribunaux provinciaux et fédéraux sont organisés en une seule pyramide à quatre niveaux. La Cour suprême du Canada, constituée en 1875, est la plus haute instance judiciaire du pays, et en l'occurrence, la cour de dernier ressort nationale. , chapeautant la Cour d'appel fédérale ainsi que toutes les cours d'appel provinciales. Sous ces juridictions viennent la Cour fédérale, la Cour canadienne de l'impôt ainsi que les cours supérieures de compétence générale des provinces et des territoires. Puis au bas de la pyramide viennent les cours typiquement décrites comme des cours provinciales. , entre autres en matière de relations de travail.
D'autre part, le mandat de la judicature est de pourvoir à la primauté du droit de façon impartiale et accessible à tous[161] ; la primauté du droit étant assurée grâce à l'indépendance de la magistrature face aux institutions politiques[162].
La juge en chef du Canada, Beverley McLachlin, de même que les huit autres juges puînés de la Cour suprême, sont assermentés par le gouverneur général en conseil après avis du premier ministre. Tous les juges des cours d'appel, provinciales et fédérale, et des cours supérieures sont aussi assermentés de la même manière, après avis du premier ministre et du ministre de la Justice, à la suite de la consultation des organismes non gouvernementaux. Le Cabinet fédéral nomme les magistrats des cours supérieures aux niveaux provincial et territorial. Les postes des tribunaux du bas de la pyramide judiciaire, aux niveaux provincial et territorial, sont comblés par les gouvernements respectifs.
Le fondement légal du Canada repose dans la Constitution du Canada. Celle-ci est composée de textes écrits, de traditions et de conventions non écrites dont les origines proviennent du droit anglais, et dont la pertinence au sein du droit canadien est de nos jours confirmée par la jurisprudence grâce au premier paragraphe du préambule de la Loi constitutionnelle de 1867 :
« Considérant que les provinces du Canada, de la Nouvelle-Écosse et du Nouveau-Brunswick ont exprimé le désir de contracter une Union Fédérale pour ne former qu'une seule et même Puissance (Dominion) sous la couronne du Royaume-Uni de la Grande-Bretagne et d'Irlande, avec une constitution reposant sur les mêmes principes que celle du Royaume-Uni[163]. »
Cependant, la Proclamation royale de 1763 est le point de départ du droit canadien pour tous sujets en litige, tels que les conflits frontaliers des provinces. Par suite de la cession du territoire par la France, la proclamation a imposé au Canada toutes lois en vigueur en 1763 au Royaume-Uni et a amené avec elle tous les droits détenus par les Britanniques dans l'Empire. Encore aujourd'hui, plusieurs lois ou traditions britanniques en vigueur en 1763 font partie intégrante du contexte légal canadien. Par exemple, plusieurs institutions politiques existantes de nos jours, telles que les Assemblées législatives, trouvent leur source dans des traditions en vigueur au XVIIIe siècle au Royaume-Uni et dans l'ensemble de l'Empire britannique. Ces lois et traditions britanniques n'ont, pour la plupart, de représentation écrite dans le droit canadien que la jurisprudence des tribunaux. De plus, la Magna Carta de 1215 et la Déclaration des droits (Bill of Rights) de 1689 octroient certains droits fondamentaux aux Canadiens, dont la liberté d'expression, la liberté de presse, le droit de vote et le principe de l'Habeas Corpus. Les droits de tout homme et femme canadien sont par ailleurs repris et confirmés par écrit dans la Charte canadienne des droits et libertés, créée en 1982. Par conséquent, en plus des droits énoncés par écrit dans les textes constitutionnels, il existe une théorie judiciaire en jurisprudence canadienne qui fait intervenir des droits intrinsèques à la Constitution afin de reconnaître les lois et traditions existantes lors de l'entrée en vigueur de la loi constitutionnelle de 1867 et même avant, de la Proclamation royale en 1763. Cette théorie est mieux connue sous le nom de Charte des droits implicite. Elle a préséance tant et aussi longtemps qu'elle n'est pas contredite par des dispositions écrites de la Constitution même, par une loi du Parlement ou d'une législature provinciale, ou par des conventions non écrites.
Le droit positif canadien est ainsi composé de quatre grandes sphères : la législation, la jurisprudence, la doctrine et la tradition. Le bijuridisme est un trait particulier du droit canadien. En effet, la common law est l'unique loi civile au Canada - comme dans la plupart des pays anglophones, sauf dans les compétences législatives du Québec où la préséance est donnée exclusivement au Code civil du Québec. Cependant, seuls les principes de la common law régissent la Constitution. Par conséquent, les dispositions constitutionnelles sont muables et constamment en modification jour après jour. Autrement dit, les écrits constitutionnels ne forment que des points de départ auxquels les classes médiatiques, politiques et sociales se rattachent pour ensuite influencer les comportements populaires de façon à conserver ou modifier la Constitution par voie de conventions non écrites. De cette façon, ses principes s'appliquent, par exemple, aux compétences législatives provinciales pour que celles-ci demeurent aux provinces ou soient transférées au Parlement fédéral, ou à l'inverse pour que des compétences législatives fédérales soient conservées par le Parlement fédéral ou transférées aux législatures provinciales.
La Constitution du Canada inclut donc aujourd'hui la loi de 1982 sur le Canada, la loi constitutionnelle de 1867, la loi constitutionnelle de 1982 et tous les documents figurant à son annexe, les modifications constitutionnelles subséquentes, ainsi que les principes constitutionnels sous-jacents.
Les lois civiles du Canada sont issues des lois civiles britanniques puis ont été adaptées au fil du temps au contexte socio-culturel et géopolitique du Canada. Exception faite des compétences législatives du Québec, la common law est le système de droit civil qui prévaut dans les compétences législatives provinciales de l'ensemble des provinces et territoires du Canada. La common law s'applique toutefois à toutes les provinces et territoires en matière de droit constitutionnel et de compétences législatives fédérales. Common law ne se traduit pas en français. Bien que certaines personnes soient portées à parler de droit coutumier, la common law est plus qu'un système de droit coutumier, c'est aussi un système de droit jurisprudentiel, de droit législatif et de droit populaire. Ainsi, on parle de droit jurisprudentiel lorsque les normes légales sont promulguées par arrêts des tribunaux, de droit législatif lorsque les lois sont édictées par le Parlement fédéral ou les législatures provinciales et de droit populaire lorsque les normes légales ont pour source le peuple (les citoyens). Dans le droit populaire, l'on parle de conventions non-écrites pour définir les normes émanant du peuple.
Dans le système de common law, le droit coutumier sert à chacun des groupes ci-haut mentionné pour acquérir de l'autorité sur certaines matières, ou en perdre, dépendamment de la tradition ou de la coutume établie au fil du temps. Cette tradition, ou coutume, se définit par un comportement ou une omission répétée pendant une période de temps. Par exemple, une législature provinciale pourrait détenir l'autorité sur certaines dispositions de la sécurité routière et ainsi établir une loi qui interdit de tourner à gauche sur les feux rouges, mais si l'autorité responsable (les forces policières) ne prend pas les mesures appropriées pour l'application de cette loi et que son omission provoque la création d'une coutume où les conducteurs tournent à gauche sur les feux rouges malgré l'interdiction législative, cette coutume risque de prendre le dessus sur la législation et ainsi devenir une convention non-écrite qui rend la loi de la législature invalide. Le droit populaire s'étant imposé, toutes interventions futures des forces policières pour faire respecter la législation deviendraient illégales. L'influence médiatique et la propagande deviendraient les seules façons valides pour faire changer cette coutume afin d'obtenir l'approbation d'une quantité suffisante de personnes, le cas échéant. Par conséquent, il est à noter que la coutume peut être d'ordre national, provincial, régional, local, et même individuel - un individu peut créer sa propre loi. Ainsi, dans l'exemple ci-dessus, tourner à gauche sur les feux rouges est d'ordre courant dans la grande région de Toronto si le virage s'effectue lorsque le trafic en sens inverse empêche les trois premières voitures de tourner à gauche sur un feu vert ou jaune, bien que la législation l'interdise.
Parfois, dans les situations de plus grande portée, les gouvernements sont aux prises avec l'obligation d'entretenir ou de sonder l'opinion publique conformément à leur idéologie, car celle-ci sert de justification légale à certaines de leurs actions ou inactions. Tel est le cas, par exemple, sur les questions de reconnaissance de la nation québécoise ou sur les droits des minorités francophones hors-Québec.
Finalement, dans un système de common law, chaque acteur tente généralement de conserver son pouvoir en plus d'acquérir ou de retrouver un pouvoir légal, voire constitutionnel, par voie de traditions et de conventions non écrites. Par conséquent, l'influence, la pression, la manipulation et la propagande sont les moyens utilisés au sein de la population, des médias et des instances politiques afin de parvenir à ces buts. Ces derniers peuvent notamment être atteints en tentant de :
En complément de la common law existent les principes de l'équité. Alors que la common law élabore les normes légales, l'equity est un ensemble de principes permettant aux tribunaux canadiens de pourvoir à l'application des normes légales. Des instruments tels que des brefs d'évocation, cautions, décrets, injonctions, mises en demeure et outrages au tribunal sont utilisés par les tribunaux afin de rendre leurs jugements effectifs. Ainsi, il est possible pour les tribunaux d'établir une forme de justice naturelle qui va au-delà des sanctions imposées par la législation, telles les dommages-intérêts ou l'emprisonnement. Dans un souci de justice et d'équité, les tribunaux invoquent la présomption d'innocence du défendeur et donc, les principes par lesquels chacune des parties adverses doit être traitée équitablement jusqu'à preuve de culpabilité de la partie défenderesse. Par exemple, en présumant son innocence, le suspect d'un meurtre pourrait être libéré sous caution en attendant son procès tout en promettant de comparaître dans les délais prévus en garantie de ne pas quitter le pays, ou une région donnée. Un tribunal pourrait aussi, par exemple, émettre une injonction interlocutoire provisoire empêchant la partie plaignante d'user d'un droit légal qui, si le jugement du tribunal était favorable au défendeur, rendrait ce jugement ineffectif.
Dans un même souci de justice et d'équité, les peines imposées à un coupable déclaré peuvent être réduites ou augmentées par un tribunal après évaluation des causes affligeantes et réductrices. Par exemple, le coupable déclaré à une infraction pourrait voir sa peine réduite de façon substantielle par un tribunal en considération du traitement médiatique qui lui a été infligé. À l'inverse, sa peine pourrait être augmentée significativement dans la situation où le coupable ne démontre aucun signe apparent de remords.
Les principes de l'equity existent dans toutes les provinces et territoires du Canada et s'appliquent tant au droit constitutionnel qu'aux compétences législatives fédérales et provinciales. Cependant, eu égard au droit civil du Québec, on parle de procédure civile et de procédure pénale dans les champs de juridictions québécoises. L'Assemblée nationale du Québec reprend toutefois les mêmes principes de l'equity, mais en les définissant et en les balisant par écrit dans le Code de procédure civile et le Code de procédure pénale du Québec. Contrairement à l'equity qui suit les règles de modification de la common law, les procédures ne peuvent être modifiés que par l'Assemblée nationale du Québec.
Contrairement au reste du Canada, le Québec est unique en ce sens où toute loi civile est écrite. Par conséquent, seule une modification dans le respect des lois existantes par les législateurs est nécessaire. L'autorité ultime appartient à l'Assemblée nationale du Québec, dans les limites de ses compétences législatives. Cependant, pour les lois fédérales, seuls les principes de common law s'appliquent. Ainsi, toutes les lois de l'Assemblée nationale du Québec ont préséance en tout temps, tant et aussi longtemps qu'elles suivent les règles constitutionnelles et qu'elles ne sont pas modifiées par les législateurs. Cependant, le Code civil du Québec ainsi que d'autres lois provinciales délèguent parfois aux tribunaux le rôle de créer un droit jurisprudentiel, coutumier et populaire, d'abord et avant tout, encadré par la législation. Ce type de droit constitue un système juridique mixte quasi-unique dans le Monde au côté du droit écossais et de celui de la Louisiane.
Le droit jurisprudentiel et coutumier du Québec est très présent pour légiférer et réglementer certaines industries membres du système corporatiste, en particulier les professions libérales, à savoir la médecine, le droit, le notariat, la comptabilité et la planification financière. Il en est ainsi, par exemple, lorsque les législateurs insèrent dans une loi des dispositions qui insistent sur le caractère raisonnable ou pertinent d'une action, sans nécessairement donner plus de détails. Les tribunaux sont ainsi appelés à créer des lois sur ces points en litige en jugeant de la raisonnabilité ou de la pertinence selon les us et coutumes de l'industrie respective. Dans plusieurs contextes, les principes de droit jurisprudentiel, coutumier et populaire s'appliquent aussi à tous les citoyens. Par exemple, les tribunaux sont appelés à établir les droits de la personnalité en plus de ceux énoncés au Code civil et d'établir les situations constituant des atteintes à la réputation.
Ce système mixte a entre autres pour effet de combler certains vides juridiques auxquels les législateurs n'auraient pas pensé, et ce, en plus de permettre au droit de s'adapter plus rapidement à l'évolution de la société. Il encourage en plus la rapidité d'innovation des pratiques sous la concertation des experts des industries concernées. Autrement dit, la codification des lois québécoises intègre les principes de droit commun en permettant au peuple d'établir lui-même ses principes fondamentaux de justice grâce à la « loi de la nation » (law of the land), principe décrit dans la Magna Carta de 1215 et dans la Coutume de Paris de la Nouvelle-France. Par exemple, en matière de responsabilité civile, le premier paragraphe de l'article 1457 du Code civil indique que  L'ensemble de la nation est ainsi amené à établir démocratiquement au sein de sa population diverses normes socio-culturelles qui prennent de fait une valeur légale avec le temps. Ainsi, selon les normes socio-culturelles en vigueur, cette seule phrase du Code civil légifère sur des sujets tels que l'abus de confiance, l'abus de pouvoir, le harcèlement psychologique, l'assistance à autrui, la divulgation de renseignements, la modification des lois par les législateurs, etc.
Par conséquent, contrairement aux provinces de pure common law, aucun système de droit coutumier et de droit populaire n'existe dans les compétences législatives du Québec si ce n'est ceux établis par le droit législatif. Cependant, pour avoir force de loi, la jurisprudence des tribunaux est seule compétente, dans les limites de la législation, pour reconnaître la « loi de la nation » et le respect des dispositions législatives.
Les lois criminelles sont uniquement de juridiction fédérale, et sont par conséquent uniformes dans tout le Canada. Celles-ci sont entièrement codifiées dans le Code criminel du Canada. Seuls les principes de la common law s'appliquent au droit criminel ainsi qu'aux autres lois fédérales partout au Canada, même au Québec. Ainsi, l'autorité populaire s'étend à l'ensemble des citoyens canadiens plutôt qu'à une seule province ou région.
Dans le cas des lois criminelles, seules les normes culturelles du Canada définissent si un comportement, ou une omission, est criminel. Cette désignation s'effectue principalement sur la base de l'affectation du bien-être collectif de la société. Bien que les lois criminelles soient représentées dans le Code criminel du Canada, le droit populaire (la mentalité des citoyens) détient aujourd'hui la pleine autorité sur la totalité des matières criminelles. Ainsi, pour que le Parlement puisse effectuer une modification du Code criminel du Canada, la mentalité des citoyens doit être conforme aux modifications effectuées. Un changement de mentalité au sein de la population est donc impératif pour que le Parlement puisse effectuer un amendement quelconque en toute légitimité. Cependant, la mentalité populaire n'a force de loi que si le Code criminel a préalablement été modifié en conséquence par les législateurs. L'influence médiatique, la pression et la propagande sont donc des moyens utilisés partout au Canada au sein de la population sur des sujets tels que l'avortement, l'euthanasie, la prostitution et la peine de mort.
Quant à l'application des lois criminelles, le pouvoir est détenu par la législation (les forces policières), la jurisprudence (les tribunaux) et la population (les citoyens). Ainsi, les forces policières sont responsables des enquêtes et du maintien de l'ordre et de la paix alors que les tribunaux pourvoient à l'établissement de l'equity ou de la procédure, le cas échéant. De plus, dans les limites légales établies par la common law, tout citoyen canadien a le pouvoir d'arrestation, de dénonciation et de détention de tout suspect à tout acte criminel. Par exemple, un individu commettant un vol qualifié auprès d'un employé de dépanneur pourrait être poursuivi et arrêté par une tierce personne témoin de la perpétration du crime pour ensuite être détenu par celle-ci jusqu'à l'arrivée des forces policières.
L'application de la loi au Canada, incluant les cours de compétences criminelles, est de juridiction provinciale, mais dans la plupart des provinces, cette application est donnée à contrat à la police fédérale, la Gendarmerie royale du Canada (GRC). Seuls l'Ontario, le Québec et Terre-Neuve-et-Labrador possèdent en propre un corps policier provincial, respectivement la Police provinciale de l'Ontario, la Sûreté du Québec et la police Royal Newfoundland Constabulary (« Police royale de Terre-Neuve »). La GRC n'applique que les lois fédérales sur leur territoire, et seulement les lois criminelles d'une étendue nationale, voire internationale, telles que celles relatives au crime organisé, à la sécurité nationale et frontalière, à la jeunesse, aux communautés autochtones et à l'intégrité financière. Cependant, à Terre-Neuve-et-Labrador, la Gendarmerie est aussi responsable de l'application des lois provinciales, mais seulement à l'extérieur des centres urbains de la province.
Certaines municipalités possèdent leur propre corps policier ; il est chargé de l'application des règlements municipaux dans son district judiciaire. Pour les municipalités, régions, comtés… n'ayant pas de corps policier, les services policiers sont donnés à contrat soit à la GRC ou, dans le cas de l'Ontario et du Québec, à leurs polices provinciales respectives. Pour ce qui est des réserves indiennes, chacune possède son corps policier mandaté par le gouvernement fédéral, et ayant juridiction sur tout le territoire de la réserve. Ces policiers sont communément appelés les Peacekeepers.
L'Unité de police militaire du Canada est quant à elle le corps policier responsable de l'ordre et du maintien de la paix au sein des Forces armées canadiennes. Le Service national des enquêtes des Forces canadiennes est, lui, responsable des enquêtes. Pour ce qui est de l'armée, elle peut être déployée auprès de la population lorsque des états de crise ou d'urgence sont en vigueur. Par exemple, lors de la Crise du verglas en 1998 dans le sud du Québec, les Forces canadiennes ont servi à des opérations de secours et de support aux sinistrés.
L'Agence des services frontaliers est de son côté l'organisation responsable d'assurer la protection civile à la frontière du Canada. La Garde côtière canadienne travaille en collaboration avec la Gendarmerie royale du Canada et l'Agence des services frontaliers afin d'assurer la sécurité publique en mer, et la protection civile sur les frontières maritimes. Le Service canadien du renseignement de sécurité est l'agence des services secrets responsable des enquêtes approfondies d'ordre national, voire international, afin d'assurer la protection civile du Canada.
Le Canada entretient une relation de premier plan avec les États-Unis, pays avec lequel il partage la plus longue frontière non défendue du monde, frontière où les échanges économiques transfrontaliers sont parmi les plus importants du globe. Cette collaboration économique, établie à plusieurs niveaux entre le Canada et les États-Unis, permet les relations économiques les plus importantes de l'accord de libre-échange nord-américain. Le Canada partage aussi de longues relations avec le Royaume-Uni et nourrit depuis peu, surtout à travers le Québec, des rapports privilégiés avec la France. La Grande-Bretagne et la France sont les deux puissances coloniales européennes les plus significatives de sa fondation. Ces relations s'étendent aussi avec d'autres anciens membres des Empires britanniques et français, par l'entremise du Commonwealth et de la Francophonie.
Parmi les nombreuses et fructueuses collaborations économiques, notons les travaux communs entre Bombardier et Alstom (France) dans le domaine de la fabrication de matériel de transport en commun ayant permis la mise en place du premier TGV en sol américain dans le corridor Washington-Boston. Plus récemment, Hydro-Québec a signé un accord permettant l'application de ses nouvelles recherches par les industriels Heuliez et Dassault pour la fabrication d'une voiture électrique très prometteuse.
Le peuple Inuit du Canada a également constitué une alliance avec les autres peuples Inuit du pourtour arctique.
Le Canada s'est régulièrement illustré en Afrique grâce à l'Agence canadienne de développement international (ACDI).
Pendant les soixante dernières années, le Canada s'est fait le défenseur du multilatéralisme, faisant des efforts pour résoudre les conflits internationaux en collaboration avec les autres nations. Cela est clairement démontré lors de la Crise de Suez en 1956 quand l'ancien premier ministre Lester B. Pearson, alors ministre des Affaires étrangères, apaise les tensions en proposant des interventions de maintien de la paix et l'envoi de la Force de maintien de la paix des Nations unies. Dans cet esprit, le Canada développe et tente de maintenir un rôle de chef de file dans les interventions de l'ONU. Le Canada a participé à 50 missions de la sorte, participant à chaque opération de l'ONU jusqu'en 1989. Les contributions du Canada au programme de maintien de la paix de l'ONU diminuèrent pendant les premières années du XXIe siècle.
Par ailleurs le Canada joue un rôle significatif auprès de l'Organisation de l'aviation civile internationale dont le siège social se situe à Montréal. En outre, le Canada fut représenté depuis le 1er juillet 2004 jusqu'au 30 juin 2008 par Louise Arbour, originaire du Québec, pour assurer le poste de Haut-commissaire des Nations unies aux droits de l'homme.
Enfin, le Canada fait partie du Commonwealth, ce qui l'intègre à un ensemble politique mondial qui est chapeauté par le roi Charles III[164].
En 1985, l'Institut antarctique du Canada est mis en place pour faire pression sur le gouvernement canadien afin d'améliorer les recherches sur l'Antarctique[165].
Membre fondateur de l'Organisation du traité de l'Atlantique nord (OTAN), le Canada s'est doté d'une armée défensive sans armes nucléaires. Il emploie actuellement environ 75 000 militaires permanents au sein des Forces armées canadiennes, et 30 000 dans la réserve militaire. Les Forces canadiennes (FC) comprennent l'armée, la marine et la force aérienne. La majeure partie de l'équipement déployé des Forces inclut 1 500 véhicules de combat blindés, 34 vaisseaux de combat, et 861 appareils d'aviation.
En plus des grandes participations à la seconde guerre des Boers, à la Première Guerre mondiale, à la Seconde Guerre mondiale, et à la guerre de Corée, le Canada maintient des participations substantielles dans les missions internationales sous le commandement des Nations unies et de l'OTAN depuis 1950, incluant des missions de maintien de la paix, diverses missions en ex-Yougoslavie, et un soutien aux Forces de la Coalition lors de la Première guerre du Golfe. Depuis 2001, le Canada a des troupes déployées en Afghanistan en partenariat de la Force de stabilisation des États-Unis et de la Force internationale d'assistance à la sécurité, dirigée par l'OTAN et autorisée par l'ONU. L'Équipe d'intervention en cas de catastrophe (DART) a participé à trois importantes opérations de secours après le tsunami de décembre 2004 en Asie du Sud-Est, après l'ouragan Katrina en septembre 2005 sur les côtes américaines, après le tremblement de terre du Cachemire en octobre 2005 et celui d'Haïti en janvier 2010.
Le Canada et les États-Unis sont partenaires sur le projet de Commandement de la défense aérospatiale de l'Amérique du Nord (NORAD) qui vise à la défense de la souveraineté aérienne de l'Amérique du Nord depuis 1957 en administrant les plans de vols commerciaux et privés. De plus, le Canada est depuis quelques années approché par le gouvernement américain afin de mettre en œuvre leur projet de bouclier antimissile visant à la défense de l'Amérique du Nord contre les attaques de missiles balistiques.
Bien que le gouvernement du Canada affirme constamment son autorité sur l'ensemble du territoire, certains endroits ne sont pas universellement reconnus par la communauté internationale comme étant sous la souveraineté du Canada. Bien que la Convention des Nations unies sur le droit de la mer jette les bases de reconnaissance des droits souverains des pays sur les eaux entourant leur territoire, l'interprétation de cette convention diffère entre le Canada et d'autres pays. Par exemple, bien que la zone économique exclusive soit respectée, certains États considèrent les eaux entourant l'archipel de la région arctique, lesquelles constituent le Passage du Nord-Ouest, comme étant libres de passage pour tous. De plus, les États-Unis affirment que le Canada agit à l'encontre de la convention sur la partie de la mer de Beaufort et de l'océan Arctique qui s'étend jusqu'au Pôle Nord géographique. Un conflit similaire a existé jusqu'en 1992 entre le Canada et la France sur la zone économique exclusive entourant l'archipel Saint-Pierre-et-Miquelon.
Certaines parcelles de terre sont en outre contestées par les pays limitrophes du Canada. Bien que la frontière maritime de la baie de Baffin entre le Canada et le Groenland ait été délimitée après entente avec le Danemark, les deux pays se disputaient toujours l'île Hans dans le détroit de Nares reliant la mer de Baffin à la mer de Lincoln. Cette dispute a été réglée en 2022 par la division l'île entre les deux pays. La Commission de la frontière internationale est quant à elle la société créée par le Canada et les États-Unis afin d'entretenir la frontière terrestre canado-américaine. Cependant, les deux pays sont en désaccord sur le droit de propriété de l'île Machias Seal dans le golfe du Maine.
S'y ajoute le mémorial de Vimy (France) dont le terrain d'assise ainsi qu'une centaine d'hectares alentour offert par la France en 1922.La première différence entre une province et un territoire réside dans leur autonomie. En effet, les provinces ont des compétences déléguées par la loi constitutionnelle de 1867, et donc qui ne peuvent être octroyées au gouvernement fédéral que par une modification constitutionnelle. Ainsi, dans la limite de leurs compétences législatives, et des droits et privilèges qui leur sont accordés par la Constitution, les provinces sont indépendantes du gouvernement fédéral et des unes par rapport aux autres. En d'autres mots, bien qu'une procédure normale puisse mener à la modification de la Constitution selon la loi constitutionnelle de 1982, la modification est sans effet dans les limites d'une province ayant déclaré son désaccord à son égard si cette modification est dérogatoire aux droits, privilèges et compétences de la province. Cette disposition a donc pour effet d'annihiler l'emprise de certaines provinces, ou du gouvernement fédéral, sur d'autres provinces jouissant de pouvoirs et droits acquis par la Constitution. De plus, dans les limites définies par la loi constitutionnelle de 1982, une législature a compétence exclusive pour modifier sa propre constitution provinciale, laquelle est définie par défaut par la loi constitutionnelle de 1867. Quant aux territoires, ceux-ci sont le fruit d'une simple loi du Parlement fédéral, et donc directement sous son pouvoir.
Les provinces sont, à l'intérieur de leurs limites, responsables de la plupart des programmes sociaux du Canada tels que l'administration de la santé, l'éducation, et « généralement toutes les matières d'une nature purement locale ou privée dans la province ». Au total, les provinces regroupées ensemble encaissent davantage de revenus que le gouvernement fédéral, une structure quasi-unique parmi les fédérations dans le monde. Les paiements de péréquation sont accordés par le gouvernement fédéral dans le but d'assurer que des standards uniformes et raisonnables de services publics et d'imposition soient conservées entre les provinces les plus riches et celles les plus pauvres.
Bien qu'une province puisse, exclusivement, « faire des lois relatives aux matières tombant dans ses catégories de sujets », le gouvernement fédéral peut initier des politiques nationales dans les champs de compétence provinciale, telle que la loi canadienne sur la santé. Cependant, chaque province a le choix de se retirer du programme, ou de ne pas y adhérer. De plus, tous accords internationaux signés par le gouvernement fédéral, notamment en matière de commerce, sont subordonnés aux compétences législatives de chacun des paliers de gouvernement du Canada. Considérant que les relations internationales constituent une compétence fédérale, les provinces ont néanmoins le pouvoir de signer des accords internationaux dans la limite de leurs pouvoirs législatifs. Le Québec est d'ailleurs un ardent défenseur de ce pouvoir au travers duquel il entretient notamment des relations avec la France et les États du Nord-Est des États-Unis, et grâce auquel il établit des Délégations générales dans plusieurs villes du monde.
Toutes les provinces ont une législature élue et monocamérale qui est chapeautée par un premier ministre choisi de la même façon que le Premier Ministre du Canada. Chaque province a aussi son lieutenant-gouverneur, représentant de la Couronne et homologue provincial du gouverneur général du Canada, assermenté sous l'avis du premier ministre du Canada, et depuis quelques années avec des consultations croissantes avec les gouvernements provinciaux. Toutes les provinces et territoires ont leurs propres emblèmes.
La gouvernance locale relève directement et uniquement des législatures provinciales. De ce fait, chaque province ou territoire a son propre système d'administration territoriale et de subdivisions de son territoire. Dans certaines provinces, des compétences législatives provinciales sont déléguées par la législature aux unités territoriales ainsi formées. Dans la désignation de ces unités, l'on parlera dépendamment de comtés, districts, districts municipaux, districts régionaux, districts d'amélioration, districts municipaux d'opportunité, municipalités régionales, municipalités régionales de comté, régions, régions administratives, villes, villages, paroisses, hameaux, cantons, agglomérations, arrondissements, communautés métropolitaines, etc. De plus, bien que des unités territoriales puissent être désignées sous un même nom générique, tel que comté, ces unités n'ont pas nécessairement la même définition légale ou les mêmes compétences d'une province à l'autre. Par exemple, les régions de l'Ontario désignées sous le générique comté n'ont pas autant de pouvoir que les gouvernements locaux des comtés du Nouveau-Brunswick.
Bien que le gouvernement fédéral n'ait pas de compétence en matière d'administration territoriale, celui-ci subdivise tout de même le pays afin d'accomplir ses propres mandats, notamment de statistiques nationales. Dans ce cas, l'on parle de régions métropolitaines de recensement, régions économiques, agglomérations de recensement, divisions de recensement, subdivisions de recensement, secteurs de recensement et aires de diffusion. Ces unités géographiques respectent dans la plupart des cas les unités territoriales créées par les législatures provinciales bien qu'il puisse y avoir plusieurs divergences. Par exemple, pour ce qui est des divisions de recensement, dans les provinces de la Colombie-Britannique, de l'Île-du-Prince-Édouard, du Nouveau-Brunswick, de la Nouvelle-Écosse et de l'Ontario, ces divisions correspondent aux unités formées par chacune des provinces. Au Québec, la plupart des divisions correspondent aux municipalités régionales de comté, sauf pour onze d'entre elles qui sont regroupées en seulement cinq divisions de recensement différentes. Quant aux provinces de l'Alberta, du Manitoba, de la Saskatchewan et de Terre-Neuve-et-Labrador, aucun gouvernement supra local ou régional n'est établi par leur législature. Le gouvernement fédéral a cependant conclu un accord avec ces provinces afin d'y créer des divisions de recensement. Il en est de même pour le Nunavut et les Territoires du Nord-Ouest. Le Yukon forme quant à lui une seule division de recensement[167]. D'autre part, dans chacune des provinces, les commissions scolaires ont aussi leur propre territoire qui peut différer des unités de gouvernance locale formées par les législatures provinciales. De plus, les circonscriptions électorales fédérales, provinciales, régionales, municipales et autres diffèrent l'une de l'autre et des unités de gouvernance locale dans plusieurs cas.
Ottawa est la capitale nationale du Canada et le siège du gouvernent fédéral. Au cœur de la ville, la colline du Parlement est le centre d'intérêt de la vie politique et culturelle de la ville d’Ottawa. Siège de la majorité des Ministères et organismes fédéraux, Ottawa est aussi le siège de plusieurs institutions nationales. Enfin la ville accueille aussi le siège de plusieurs organisations internationales.
Ottawa est le point de rencontre des deux cultures nationales d'importance, à savoir la culture francophone et la culture anglophone. La capitale nationale est un lieu historique à la culture cosmopolite[168].
Au moment de l'Acte d'Union, c'est Kingston qui est désignée, de 1841 à 1843, comme la capitale du nouveau pays. Trop à l'étroit à Kingston les députés déménageront en 1844 à Montréal mais la loi d'indemnisation des citoyens du Bas-Canada viendra soulever la population anglaise contre cette mesure et elle incendiera le siège montréalais du gouvernent canadien. Par mesure de sécurité la capitale déménagera de nouveau à Toronto cette fois.
En 1852, c'est à Québec que se déplace la capitale du pays. En 1855 c'est à nouveau à Toronto. En 1857 la reine Victoria désigne finalement Bytown (qui allait devenir Ottawa) comme la capitale officielle du Pays. Ce n'est qu'en 1866 que les députés s'y installent après 5 ans de nouveau à Québec de 1860 à 1865 qui prend alors le titre de capitale provisoire. C'est enfin en 1866 qu'Ottawa devient la dernière capitale du Canada-Uni pour devenir en 1867 la capitale du Canada[169].
Le Canada occupe la majeure partie de la portion septentrionale de l'Amérique du Nord, s'étendant de l'océan Atlantique à l'est jusqu'à l'océan Pacifique à l'ouest, et au nord vers l'océan Arctique. Il partage une frontière terrestre commune avec les États-Unis au sud, et une autre avec ce même pays au nord-ouest (entre l'Alaska et le Yukon d'une part et le nord de la Colombie-Britannique d'autre part). Depuis 2022, il partage aussi une frontière terrestre avec le Danemark (Groenland) grâce au partage entre les deux pays de l'Île Hans. Il partage aussi une frontière maritime avec la France (Saint-Pierre-et-Miquelon). Depuis 1925, le Canada réclame la portion de l'Arctique s'étalant entre les méridiens 60ºO et 141ºO ; cette réclamation n'est toutefois pas universellement reconnue. L'établissement le plus nordique du Canada, et du monde, se situe à Alert (Nunavut), base des Forces armées canadiennes, au sommet de l'île d'Ellesmere (latitude 82,5ºN à 834 kilomètres — 450 milles marins — du Pôle Nord). Le Canada est le plus grand pays du monde après la Russie[170].
La densité de la population, environ 3,5 habitants par kilomètre carré, est l'une des plus faibles au monde. La région la plus densément peuplée du pays est celle du Corridor Québec-Windsor, le long des basses-terres du fleuve Saint-Laurent et des Grands Lacs au sud-est. Au nord de cette région se trouve le vaste bouclier canadien, une région de roc nettoyé par la dernière ère glaciaire, dépourvue de terres fertiles, riche en minéraux, et dotée de lacs et rivières. On y retrouve la forêt boréale, immense forêt nordique composée majoritairement d'épinettes et de sapins. Le Canada possède sur son territoire plus de lacs que tout autre pays dans le monde et possède une importante réserve d'eau douce[171],[172],[173].
Dans l'est du Canada, le fleuve Saint-Laurent se verse dans le golfe du Saint-Laurent, le plus grand estuaire du monde ; l'île de Terre-Neuve se situe en son embouchure, alors que l'Île-du-Prince-Édouard se situe au sud de celui-ci. Le Nouveau-Brunswick et la Nouvelle-Écosse sont séparés par la baie de Fundy, laquelle connaît les marnages les plus grands du monde. Ces quatre provinces maritimes s'avancent à l'est de la péninsule gaspésienne du Québec. L'Ontario et la baie d'Hudson dominent le centre du Canada, alors qu'à l'ouest se trouvent les vastes plaines des Prairies canadiennes, passant par le Manitoba, la Saskatchewan et l'Alberta pour se rendre jusqu'aux montagnes Rocheuses qui les séparent de la Colombie-Britannique.
Au nord du 60e parallèle se trouvent les trois territoires canadiens — Nunavut, Territoires du Nord-Ouest et Yukon — parsemés de plusieurs lacs (dont le grand lac de l'Ours et le grand lac des Esclaves) et traversés par le plus long fleuve du pays, le fleuve Mackenzie. De plus, les terres continentales du Nord canadien sont bordées au nord d'un grand archipel, l'archipel Arctique comprenant certaines des plus grandes îles du monde. Les détroits se trouvant entre ces îles constituent le passage du Nord-Ouest, du détroit de Davis à la mer de Beaufort en passant par la baie de Baffin. En outre, c'est dans cette région que l'on retrouve, en majorité au large et entre les îles de la Reine-Élisabeth, le pôle nord magnétique ainsi qu'une partie de la glace polaire.
La végétation passe des forêts de feuillus dans le sud de l'Ontario aux forêts mixtes et laurentiennes, et diminue graduellement vers le Nord canadien passant de la taïga — forêts boréales ou ceinture de conifères — à la toundra et finalement aux sols arides du Grand Nord.
Pour ce qui est du relief, le pays se dessine principalement de par les plaines des Prairies, et des plateaux du bouclier canadien. Les basses terres continentales de la Colombie-Britannique ainsi que la chaîne des Rocheuses occupent la région à l'ouest des Prairies alors que les montagnes des Appalaches s'étalent du Sud du Québec vers les provinces maritimes.
Sept principaux types de climat se retrouvent au Canada. La majeure partie du territoire du pays est dotée d'un climat subarctique qui sévit dans la partie méridionale des Territoires du Nord-Ouest et du Nunavut, de même que dans le nord-est de la Colombie-Britannique, le nord des Prairies, de l'Ontario et du Québec, ainsi que le Labrador. La zone septentrionale du Yukon, des Territoires du Nord-Ouest et du Nunavut connaît un climat arctique. Un climat de type continental sec se retrouve dans la partie sud des Prairies, tandis que les provinces centrales (Ontario et Québec) possèdent dans leur partie sud un climat continental humide, tout comme dans la région nord-ouest du Nouveau-Brunswick. Un climat maritime caractérise la zone côtière (pacifique) de la Colombie-Britannique (climat maritime de l'ouest, doux et humide), de même que les provinces atlantiques (Nouveau-Brunswick dans sa région sud-est, l'Île- du-Prince-Édouard, Nouvelle-Écosse et l'île de Terre-Neuve (climat maritime de l'est). Un climat de montagne (climat alpin) couvre la plus grande partie de la Colombie-Britannique et du Yukon et le sud-ouest de l'Alberta[174],[175].
La moyenne des températures absolues hivernales et estivales diffèrent largement d'une région à l'autre. L'hiver peut être très rude dans certaines régions du pays, avec des températures moyennes mensuelles pouvant descendre à −15 °C dans la partie méridionale du pays, bien qu'il soit aussi possible d'atteindre des températures de −45 °C avec de forts vents glaciaux. Les chutes de neige annuelles peuvent atteindre plusieurs centaines de centimètres en moyenne (par exemple, une moyenne de 337 cm à Québec). La côte de la Colombie-Britannique, notamment l'île de Vancouver, constitue une exception, et jouit d'un climat tempéré avec des hivers doux et pluvieux. Pour ce qui est des étés, les températures peuvent grimper jusqu'à 35 °C, voire 40 °C en tenant compte de l'indice humidex. La température la plus froide jamais observée au Canada est de −63 °C à Snag, dans le territoire du Yukon[176].
En 2018, des scientifiques alertent sur la fonte des glaciers du Yukon, celle-ci étant deux fois plus rapide que prévu et pouvant avoir des « conséquences dramatiques » dans la région. Cette accélération a déjà provoqué des changements dans la région : en 2016, en raison du manque d'eau, la rivière Slims qui alimente le lac Kluane a été complètement asséchée. Depuis, le niveau du lac a baissé, provoquant la disparition de milliers de poissons[177].
Le réchauffement climatique est deux fois plus rapide au Canada que sur le reste de la planète selon un rapport commandé par le gouvernement canadien. D'après les experts auteurs du rapport, le réchauffement climatique « augmentera la sévérité des vagues de chaleur et contribuera à augmenter les risques de sécheresses et de feux de forêt. Même si les inondations à l’intérieur des terres résultent de multiples facteurs, des précipitations plus intenses augmenteront le risque d’inondation en milieu urbain »[178].
En 2019, les chercheurs constatent la fonte du pergélisol des îles arctiques du Canada. Les modèles climatiques établis par le Groupe d'experts intergouvernemental sur l'évolution du climat (GIEC) ne prévoyaient pas un tel dégel avant 2090[179]. 
En 2019, le gouvernement autorise l’élevage de saumons transgéniques. Ces saumons qui produisent plus d'hormone de croissance grandissent deux fois plus rapidement qu'un saumon normal[180].
En juin 2019, le gouvernement annonce la relance des travaux d'agrandissement de l'oléoduc Trans Mountain vers la côte Ouest canadienne. La décision est saluée par l'industrie pétrolière, mais décriée par les écologistes. Une fois achevé, le projet pourrait dégager environ 330 millions d'euros par an mais aussi entraîner une hausse additionnelle de 15 millions de tonnes des émissions de gaz à effet de serre du Canada[181].
En août 2014, la catastrophe du Mont Polley endommage considérablement la biodiversité. Des milliards de litres d’eaux usées et de boues toxiques se sont déversées dans les lacs et les cours d’eau environnants. La rupture de la digue est survenue malgré un rapport officiel de 2011 alertant le propriétaire de la mine, Imperial Metals, sur la nécessité de trouver une solution durable pour évacuer les eaux usées s’accumulant dans le bassin.
Toronto est la plus grande ville du Canada devant Montréal et la capitale de la province de l'Ontario. La ville est située sur la rive nord-ouest du lac Ontario. Avec plus de 2,5 millions d'habitants[182], Toronto est la cinquième plus grande ville en Amérique du Nord. En 2006, 5 113 149 personnes vivaient dans la région du Grand Toronto ainsi que 8,1 millions dans la mégapole du Croissant d'or (nommée Golden Horseshoe, en anglais). Toronto est la principale place financière et économique du Canada et la deuxième en Amérique du Nord. La capitale de l'Ontario possède une bourse qui se nomme le Toronto Stock Exchange (abrégé TSX). La plupart des compagnies minières émettent des actions dans cette bourse, ainsi que des compagnies gazières et pétrolières. Le TSX est le meneur mondial dans les échanges boursiers dans ces deux catégories. En août 2012, le TSX comptait plus de 1 577 entreprises enregistrées, avec une capitalisation totale qui est de 1 989 562 971 807 $CAN.
Dans le monde du sport, il y a le Toronto FC en MLS, les Maple Leafs de Toronto dans la LNH, les Blue Jays de Toronto dans la LMB, le Rock de Toronto dans la ligue de crosse, les Raptors de Toronto dans la NBA et les Argonauts de Toronto dans la LCF.
Montréal est la métropole du Québec. La ville est située sur une île dont la superficie atteint 500 km2. L'île est située entre la rivière des Prairies (au nord-ouest) et le fleuve Saint-Laurent (au sud-est)[183]. Elle constitue un centre majeur du commerce, de l'industrie, de la culture, de la finance et des affaires internationales. Montréal a accueilli l'exposition universelle de 1967 et les Jeux olympiques d'été de 1976. On peut y voir aussi le Festival International de Jazz de Montréal, le Festival Juste Pour Rire, le Heavy Montréal, le Festival international Nuits d'Afrique, le Festival Montréal en lumière et le Grand Prix de Formule 1 du Canada. Le Vieux-Montréal a été déclaré arrondissement historique en 1964.
Dans le milieu sportif professionnel, Montréal est bien représenté par l'équipe des Canadiens de Montréal dans la LNH, l'équipe de soccer Club de Foot Montréal dans la MLS et les Alouettes de Montréal dans la LCF. La ville avait autrefois une équipe de LMB, les Expos de Montréal.
Montréal est la quatrième[184] agglomération francophone de langue officielle dans le monde[185] et la seule métropole francophone en Amérique du Nord. Elle est aussi, après Toronto, la deuxième agglomération canadienne en importance ainsi qu'une des villes financières les plus importantes dans le monde. Les secteurs économiques les plus importants sont la finance, les télécommunications, le secteur aérospatial, les transports, les médias, les arts, le cinéma, la production de séries télévisées, la publication de documents, l'informatique, les jeux vidéo, la recherche médicale, l'éducation, le tourisme et les sports.
Vancouver est une ville portuaire et la métropole de l'Ouest canadien. Elle est la troisième agglomération canadienne en importance. Vancouver a accueilli les Jeux olympiques d'hiver de 2010. Le sport professionnel est présent dans la ville de par l'équipe des Whitecaps de Vancouver en MLS et l'équipe des Canucks de Vancouver dans la LNH.
Relativement peu occupé par l'activité humaine, le Canada jouit d'une grande biodiversité. Sa superficie est protégée par quarante-deux parcs nationaux dont deux aires marines. Au total, 681 espèces d'oiseaux habitent son territoire, à l'année ou une partie de l'année[186]. L'un d'eux, le plongeon huard, décore d'ailleurs l'envers de la pièce de 1 dollar canadien.
Le Canada est l'une des nations les plus riches du monde, un membre de l'Organisation de coopération et de développement économiques (OCDE) et du Groupe des huit (G8). En 2022, le Canada est classé en 15e position pour l'indice mondial de l'innovation[187]. Le Canada possède une économie de marché qui subit légèrement plus d'interventions gouvernementales que l'économie américaine, mais beaucoup moins que la plupart des pays d'Europe. Le Canada eut historiquement un produit intérieur brut (PIB) par habitant plus faible que celui de son voisin du sud — bien que la richesse soit plus équitablement distribuée, mais plus élevée que dans les grandes économies d'Europe occidentale. À partir des années 1990, grâce à une réforme de l'État réussie et d'une gestion de l'État rigoureuse (voir Politique de rigueur), inspirée des principes du libéralisme économique[188], la dette fédérale passe de 68,4 % du PIB en 1994 à 38,7 % en 2004, à la suite d'une série d'excédents budgétaires ; en parallèle à la baisse du déficit, la part des dépenses publiques fédérales dans le PIB est passée de 19 % à 12 %, les dépenses publiques totales baissant d'environ 10 % entre 1992 et 2004[189]. L'économie canadienne connaît une période d'effervescence économique, avec un taux de croissance élevé et un faible taux de chômage. En 2008, le Canada ressemble fortement aux États-Unis quant à son orientation-marché dans son système économique, à ses moyens de production, et à son haut niveau de vie. Alors qu'en janvier 2008, le taux de chômage national du Canada était à son plus bas depuis 1974, se chiffrant ainsi à 5,8 %, les taux de chômage provinciaux variaient entre 3,2 % et 12,2 %. La crise frappe le pays au cours de l'année 2008 et le chômage touchait 6,2 % de la population active en novembre selon l'OIT[190].
Au cours du XXe siècle, l'impressionnante croissance des secteurs manufacturiers, miniers et des services transforme la nation d'une économie largement rurale à une économie principalement industrielle et urbaine. Tout comme les autres nations modernes et industrialisées, l'économie canadienne est dominée par l'industrie des services, laquelle emploie environ les trois quarts des Canadiens. Cependant, le Canada est, contrairement aux pays industrialisés, exceptionnel quant à l'importance qu'a le secteur primaire dans son économie, avec deux de ses plus importantes industries, le pétrole et le bois d'œuvre. Par ailleurs, le pays possède d'importantes réserves d'eau douce dont l'exportation vers les États-Unis fait l'objet d'un débat récurrent[191].
À l'inverse de la plupart des nations développées, le Canada est un exportateur net d'énergie. Le Canada a de vastes réserves de gaz naturel sur la côte est, et de grandes ressources de gaz et de pétrole principalement situées en Alberta, en Colombie-Britannique et en Saskatchewan. Les grandes étendues de sables bitumineux dans la région d'Athabasca placent le Canada au huitième rang des pays producteurs de pétrole (2006)[192]. En Colombie-Britannique, au Manitoba, en Ontario, au Québec et à Terre-Neuve-et-Labrador se trouve une source d'énergie renouvelable, abondante et à faible coût : l'énergie hydroélectrique.
Le Canada est l'un des fournisseurs les plus importants du monde en produits agricoles, avec la région des Prairies, qui est l'un des plus grands fournisseurs de blé et céréales grâce à la Commission canadienne du blé. Le pays était aussi sixième au palmarès des producteurs mondiaux de céréales au milieu des années 2010, toujours dominé par son voisin, les États-Unis.
Le Canada est le deuxième producteur de diamants au monde, le plus grand producteur de zinc et d'uranium, et un chef de file dans plusieurs autres ressources naturelles telles que l'or, le nickel, l'aluminium et le plomb. Plusieurs des villes, pour ne pas dire toutes les villes de la région nordique du pays, où l'agriculture est difficile, subsistent grâce à une mine tout près ou à une source de bois d'œuvre. Le Canada a aussi un imposant secteur manufacturier concentré principalement dans le sud de l'Ontario, avec un important tissu industriel de l'automobile grâce à la présence des constructeurs américains et japonais, et au Québec, avec un fort réseau d'industries aérospatiales grâce à une industrie nationale et provinciale forte.
En partie le résultat de son important secteur primaire, le Canada est hautement dépendant du commerce international, spécialement le commerce avec les États-Unis. L'Accord de libre-échange (ALE) de 1989, avec ce dernier, et l'Accord de libre-échange nord-américain (ALENA) de 1994, qui inclut le Mexique, déclenchent une impressionnante croissance de l'intégration commerciale et économique du Canada avec les États-Unis. Mis à part le ralentissement économique de 2001 qui n'a techniquement pas été considéré comme une récession puisqu'il a duré moins de deux trimestres consécutifs, le Canada n'a pas connu de récession depuis 1991, et maintient dans l'ensemble la meilleure performance économique du Groupe des huit (G8) et ce jusqu'en 2008.
Selon l’Institut de recherche en économie contemporaine (IRÉC), les stocks d’actifs canadiens dans les sept principaux paradis fiscaux ont été multipliés par 37,6 entre 1987 et 2014. Entre 1999 et 2013, une demi-douzaine de projets de loi ont été déposés pour limiter ou mettre un terme à l’évasion fiscale, mais tous ont été rejetés[193]. Une étude réalisée en 2020 par un groupe d’économistes note que des entreprises canadiennes ont accumulé un montant total de 381 milliards de dollars en 2019 dans les 12 principaux paradis fiscaux de la planète. Les actifs d’entreprises déclarés dans ces États ont augmenté de 135 % entre 2010 et 2020. D'après l’économiste Toby Sanger « Il y a un “momentum” pour que les gouvernements récupèrent les recettes fiscales qui leur échappent, mais le Canada n’est pas un leader, pas du tout. C’est un “suiveux” qui fait le minimum de ce qu’on attend de lui »[194].
En 2016, les cent patrons canadiens les mieux rémunérés ont gagné en moyenne 10,4 millions de dollars canadiens, soit plus de 200 fois le revenu moyen d'un travailleur canadien en 2016. De 2015 à 2016, la rémunération moyenne des PDG a augmenté de 8%[195]. Le déclin du syndicalisme au Canada aurait entraîné une hausse de 15 % des inégalités de revenu du travail[196].
En 2015, le Canada possède le dixième revenu par habitant le plus élevé[197] et est classé neuvième par le PNUD en termes d'indice de développement humain[198]. Néanmoins, l'endettement des ménages canadiens est particulièrement élevé : le ratio de la dette contractée par chaque ménage en proportion de ses revenus disponibles atteint 174 % en 2019[199].
Le Canada possède la troisième réserve de pétrole prouvée au monde. En 2019, Ottawa a décidé de racheter le pipeline TransMountain, qui transporte le pétrole de l'Alberta vers le Pacifique, afin de pouvoir lancer des travaux d'extension[200]. Bien que certaines nations contestent le processus de consultation en cour fédérale, la plupart des communautés autochtones potentiellement affectées par le projet ne s'opposent pas à celui-ci[201].
Le lait et les produits laitiers canadiens sont très diversifiés.
L'industrie laitière canadienne jouit d'une excellente réputation internationale sans doute parce qu'elle est liée à l'application de normes de qualité rigoureuses.
Cette industrie est aussi liée à des engagements spécifiques en regards de la protection des animaux et du développement durable.
Fondée sur un système de gestion de l’offre, l'industrie laitière canadienne s'oriente sur la planification de la production nationale, des prix réglementés et des contrôles à l'importation des produits laitiers.
Au troisième rang du secteur agricole canadien après les secteurs des céréales et oléagineux et des viandes rouges, l’industrie laitière canadienne a importé pour 899,4 millions de dollars de produits laitiers en 2015 pour des exportations totalisant 211,1 millions de dollars[202].
La Banque du Canada est la banque centrale du Canada. Son rôle consiste à favoriser la prospérité économique et financière du pays. Les quatre grandes sphères de responsabilités de la banque sont les suivantes :
Pour 2017 le budget du Canada est de 330,2 milliards de dollars avec un déficit prévu de 28,5 milliards de dollars. Dans ce budget, 3,4 milliards de dollars sont accordés aux autochtones du Canada[204].
L'industrie minière est fortement soutenue par le gouvernement. Le pays accueille les sièges sociaux et principales filiales d’une majorité de sociétés minières dans le monde. La législation fédérale et celles des provinces favorisent la mise en valeur des titres boursiers en permettant des formes de publicité sur les gisements plus souples qu’ailleurs, et la fiscalité est favorable aux entreprises. En outre, le réseau diplomatique canadien soutient politiquement à l’étranger toute société minière inscrite au pays[205].
Le premier ministre Stephen Harper (en fonction de 2006 à 2015) a déclaré vouloir faire du Canada l’un des plus grands exportateurs de ressources naturelles au monde. Il lui a été reproché d’affaiblir délibérément les protections environnementales en vigueur afin de favoriser l’industrie, notamment minière[206].
Le Canada participe officiellement au développement de l'industrie spatiale depuis 1989 grâce à la création de l'Agence spatiale canadienne. L'agence est essentiellement chargée de coordonner les politiques et programmes spatiaux civils au nom du gouvernement du Canada[207]. Le Canada est également connu pour la conception du Bras canadien (Canadarm).
Le recensement du Canada de 2021 (en) dénombre une population de 36 991 981 personnes[208], ce qui est comparable à celle de la Pologne et du Maroc. 91,2 % des habitants sont des citoyens canadiens[209]. Parmi eux, 74,4 % sont citoyens de naissance, tandis que 16,8 % le sont suite à une naturalisation[209]. La croissance de la population canadienne s'accomplit largement grâce à l'immigration[210] et, dans une moindre mesure, par la croissance naturelle.
Environ trois quarts des Canadiens vivent à moins de 160 kilomètres de la frontière avec les États-Unis. Une proportion semblable vit dans les zones urbaines concentrées dans le corridor Québec-Windsor (notamment les régions métropolitaines de recensement Toronto-Hamilton, Montréal et Ottawa-Gatineau), les basses terres continentales de la Colombie-Britannique (de la région de Vancouver jusqu'au bout de la vallée du fleuve Fraser) ainsi que dans le corridor Calgary-Edmonton en Alberta.
Depuis 1867, la population historique du Canada est passée de 3,4 millions à 38,2 millions. La densité de population reste très faible : elle est de 3,6 par km2, dix fois inférieure à celle des États-Unis et trente fois inférieure à celle de la France du fait de l'immense grand nord canadien très peu urbanisé et sauvage.
Le 16 juin 2023, Statistique Canada indique que la population canadienne vient d'atteindre les 40 millions d'habitants[211]. Il se base sur leur « horloge démographique », un outil visuel qui modélise en temps réel les changements dans le nombre de naissances, de décès et d'immigrants avec répartition par provinces et territoires[212].
Selon le recensement de 2016, les Canadiens possèdent majoritairement des ancêtres du même nom (la grande majorité des répondants à cette catégorie sont des blancs anglophones ou francophones de l'est du pays : leur présence est trop lointaine pour connaitre une partie ou l'ensemble de leurs ascendances avec exactitude, en conséquence, ils choisissent l'origine « canadienne » à 32 %. La plupart du temps, les personnes cochant cette catégorie descendent en partie ou en totalité des plus anciennes lignées de colons français ou britanniques). Il est suivi par anglais (18,3 %), écossais (13,9 %), français (13,6 %), irlandais (13,4 %), allemands (9,6 %), chinois (5,1 %), italiens (4,6 %), amérindiens (4,4 %), indiens (4,0 %) et ukrainiens (3,9 %). Les répondants doivent préciser toutes les origines ethniques qui s'appliquent et jusqu'à six origines ethniques par personne sont retenues lors du recensement.
D'après le même recensement, les Blancs constituent le groupe racial majoritaire (72,9 %), suivis des Asiatiques (17,7 %), des Autochtones (4,9 %), des Noirs (3,1 %), des Latino-Américains (1,3 %) et des Pacifiens (0,2 %)[213]. 22,3 % de la population appartient à une minorité visible non autochtone[214]. En 2016, les plus grands groupes de minorités visibles sont les Sud-Asiatiques (5,6 %), les Chinois (5,1 %) et les Noirs (3,5 %)[214]. Entre 2011 et 2016, la population des minorités visibles a augmenté de 18,4 %. En 1961, moins de 2 % de la population canadienne (environ 300 000 personnes) étaient membres d'un groupe de minorités visibles[215]. Les peuples autochtones ne sont pas considérés comme une minorité visible dans les calculs de Statistique Canada[216].
À la fin de 2007, le Canada comptait près de 54 500 réfugiés et demandeurs d'asile[217]. Le programme de réinstallation des réfugiés du Canada a accepté 11 100 réfugiés en 2007, dont 2 040 provenaient de l'Afghanistan, 1 790 du Myanmar/Birmanie, et 1 650 de la Colombie[217].
Le Canada présente le taux d'immigration le plus élevé des pays du G7 avec pour objectif d'accroitre la main d’œuvre disponible. Les autorités prévoient que les habitants originaires d'un pays étranger représenteront 30 % de la population canadienne en 2030. Les candidats à l'immigration doivent cependant prouver détenir des compétences professionnelles spécifiques faisant défaut au niveau local, ou être en mesure de créer des entreprises au Canada. En raison de cette politique, 36 % des médecins, 41 % des ingénieurs et un chef d'entreprise sur trois au Canada sont issus de l'immigration (statistiques de 2023). Cette politique migratoire présente l'inconvénient de priver les pays pauvres de leur personnel qualifié et ainsi de ralentir leur développement[218]
Le Canada a connu jusque dans les années 1950 des clauses restrictives en matière immobilière qui interdisaient la vente de propriétés à des Noirs, à des juifs ou à des immigrants chinois. Certaines discriminations perdurent encore de nos jours en matière d’accès à l’emploi ou à l’éducation pour la minorité noire du pays[219].
La conquête par les Britanniques de l'Acadie péninsulaire, en 1713 (traité d'Utrecht), puis du reste de la Nouvelle-France, en 1763 (traité de Paris), a amené sur le territoire canadien une nouvelle dualité religieuse : la dualité entre les protestants et les catholiques. Sous le régime britannique, les protestants représentent la religion chrétienne réformée. La confession anglicane, qui est alors la branche du protestantisme sans doute la plus significative, incarne la religion officielle de la royauté.
Aujourd’hui, l'Église anglicane du Canada est dirigée par la primat Linda Nicholls (en). L'Église catholique au Canada, quant à elle, a à sa tête le primat Gérald Cyprien Lacroix[220].
Les Canadiens adhèrent à une grande variété de religions. Selon le recensement de 2011, 67,3 % des Canadiens s'identifient comme chrétiens ; de ceux-ci, les catholiques constituent le plus grand groupe avec 38,7 % des Canadiens. Le catholicisme est suivi par le protestantisme et l'Église protestante la plus importante est l'Église unie du Canada. Environ 24 % des Canadiens déclarent n'avoir aucune affiliation religieuse. Les religions minoritaires importantes incluent l'islam (3,2 % de la population), l'hindouisme (1,5 %), le sikhisme (1,4 %), le bouddhisme (1,1 %) et le judaïsme (1,0 %)[221].
Au Canada, les provinces et territoires sont responsables de l'éducation ; en l'occurrence, le Canada n'a pas de ministère national pour l'éducation. Chacun des treize systèmes d'éducation est similaire aux autres, et ce, tout en reflétant l'histoire, la culture et la géographie régionale de sa province. Une des grandes différences existantes est celle du Québec, où les études post-secondaires débutent au cégep (collège d'enseignement général et professionnel), une institution scolaire préparant aux études universitaires et formant les techniciens spécialisés. L'âge pour l'éducation obligatoire varie à travers le Canada, mais se situe généralement aux alentours de 5–7 ans jusqu'à 16–18 ans, contribuant ainsi à un taux d'alphabétisation de 99 % chez les adultes[réf. nécessaire]. Néanmoins, selon ABC Canada, 24 % des Canadiens sont limités à des lectures très simples[222]. Le pays est un des meneurs en matière de recherche scientifique[223] et se range parmi les plus éduqués du monde en étant classé premier par le nombre d'adultes possédant une éducation post-secondaire, avec 51 % d'entre eux ayant au moins atteint un diplôme post-secondaire chez sa population âgée de 25 à 64 ans[224].
Chaque province est responsable d'organiser la gestion de ses écoles. Cependant, dans le cadre des dispositions constitutionnelles de la Charte canadienne des droits et libertés, moyennant certaines conditions et restrictions, les citoyens canadiens membres d'une minorité francophone ou anglophone dans la province où ils résident ont le droit à l'instruction dans la langue de la minorité de leur province dans toutes les communautés où le nombre est suffisant pour justifier le financement des écoles à même les fonds publics. Toutes les provinces canadiennes ont mis en place des commissions scolaires, des conseils scolaires ou des centres de services scolaire pour assurer la prestation des services en matière d'éducation. Parmi les provinces canadiennes, c'est au Québec et en Ontario qu'il y a le plus de ces entités : 72 pour chacune de ces provinces[225]. L'éducation post-secondaire est la responsabilité des gouvernements provinciaux et territoriaux, lesquels fournissent la majeure partie du financement ; le gouvernement fédéral fournit du financement additionnel du fait des subventions à la recherche. En 2002, 43 % des Canadiens âgés entre 25 et 64 ans ont déjà eu accès à l'éducation post-secondaire ; pour ceux âgés entre 25 et 34 ans, la réalisation d'études post-secondaires atteignait 51 %.
Les deux langues officielles du Canada, l'anglais et le français, sont respectivement les langues maternelles de 57,8 % et 22,1 % de la population[229]. Le 7 septembre 1969, sous la Loi sur les langues officielles, le français obtient un statut égal à celui de l'anglais dans toutes les instances gouvernementales fédérales. Ceci déclenche un processus qui mène le Canada à se redéfinir officiellement en tant que nation bilingue[230].
L'anglais et le français ont un statut égal dans les tribunaux fédéraux, le Parlement et toutes les sociétés d'État fédérales ainsi que les autres institutions gouvernementales du Canada. Le public a le droit de recevoir, là où il y a une demande suffisamment importante, des services du gouvernement fédéral dans l'une ou l'autre langue. L'utilisation de la signalisation routière bilingue varie d'une province à l'autre. Alors que le multiculturalisme est une politique d'immigration officielle du Canada, devenir citoyen canadien nécessite de parler aisément soit le français soit l'anglais — 98,5 % des Canadiens parlent au moins une des deux langues (anglais seulement : 67,5 %, français seulement : 13,3 %, les deux : 17,5 %).
Bien que le français soit principalement parlé dans la province du Québec, d'autres provinces — le Nouveau-Brunswick qui est d'ailleurs la seule province considérée officiellement bilingue, l'est et le nord de l'Ontario ainsi que le sud du Manitoba — ont une part substantielle de personnes francophones. De tous ceux qui parlent le français comme première langue, 85 % vivent au Québec. Le français est la langue officielle de la province du Québec. Plusieurs langues autochtones ont un statut officiel dans les Territoires du Nord-Ouest ; l'inuktitut est la langue de la majorité de la population au Nunavut et l'une des trois langues officielles de ce territoire.
Les langues non officielles sont aussi importantes au Canada, avec près de 5 200 000 personnes en parlant au moins une en tant que première langue. Les langues non officielles listées comme étant des langues maternelles incluent le mandarin (853 745 locuteurs natifs), l'italien (469 485 locuteurs natifs), l'allemand (438 080 locuteurs natifs) et le pendjabi (271 220 locuteurs natifs).
Le terme Premières Nations désigne les peuples autochtones du Canada qui sont autres que Métis ou Inuits. En 2011 cette population représentait environ 1,3 million d'individus[231].
Les migrations autochtones remontent à environ 15 000 ans[232] lors de la dernière glaciation, qui a abaissé le niveau des océans et créé un pont terrestre reliant l'Eurasie à l'Amérique, permettant à ceux-ci de s'installer[233].
Les Premières Nations du Canada sont actuellement regroupées et représentées par l'Assemblée des Premières Nations fondée en 1982. Le chef actuel est Perry Bellegarde[234].
En octobre 2019, le Canada est épinglé par l'ONU pour les conditions de logement des peuples autochtones. La commission d’enquête souligne que les Autochtones sont « plus susceptibles d’être mal logés et d’avoir des problèmes de santé qui découlent de cette situation ». En outre, « le pourcentage des sans-abri parmi eux est disproportionnellement élevé et ils sont extrêmement vulnérables aux expulsions forcées, à l’accaparement de terres et aux effets des changements climatiques. Lorsqu’ils défendent leurs droits, ils sont souvent la cible de violences extrêmes ». Les communautés autochtones sont aussi fortement victimes de la pénurie de logements qui touche le nord du Canada : communautés à dormir en alternance. « Quinze personnes vivent dans une maison de la taille d’une caravane. [...] Elles doivent dormir à tour de rôle quand il n’y a pas beaucoup de place »[235].
Le Canada est membre de l'Organisation internationale de la francophonie et de l'Assemblée parlementaire de la francophonie. De plus, l'Association francophone des municipalités du Nouveau-Brunswick, la Fédération québécoise des municipalités, l'Union des municipalités du Québec, la Fédération canadienne des municipalités de même que les villes de Lévis, Québec, Sherbrooke, Gatineau, Montréal, Saguenay et Trois-Rivières sont membres de l'Association internationale des maires francophones[236].
Les groupes francophones du Canada incluent :
On retrouve dans les Maritimes plusieurs communautés francophones dont Chéticamp, Edmundston et Évangeline, plus connues, font partie[237].
En Ontario, on retrouve une communauté francophone significative à Toronto même, de même que dans les villes de Windsor, Ottawa et Sault-Sainte-Marie[238].
Au Manitoba, la principale communauté francophone se retrouve à Saint-Boniface, un quartier français de Winnipeg[239].
Au Nunavut, la communauté francophone est surtout concentrée à Iqaluit[240].
Enfin pour l'Ouest canadien, c'est surtout à Kamloops, dans le Nord-Ouest de l'Alberta surnommé « Le Petit Québec » ou au lieu historique de Batoche que se concentrent les francophones[241].
La culture canadienne est historiquement influencée par les cultures et traditions anglaises, françaises, irlandaises, écossaises et autochtones, ainsi que par la culture américaine en raison de la proximité et des échanges de capital humain existant entre les deux pays. Plusieurs formes de médias et de divertissements américains sont populaires et omniprésents au Canada, surtout la partie anglophone. À l'inverse, plusieurs produits et divertissements culturels canadiens ont de grands succès aux États-Unis et partout dans le monde. Plusieurs produits culturels sont maintenant généralement commercialisés vers un marché nord-américain unifié, ou un marché global, bien que certaines régions conservent leur spécificité. Notamment, la région des Maritimes qui conserve un folklore aux airs de la culture celte irlandaise et écossaise et qui, par le fait même, s'harmonise avec le principal trait de la culture de l'Acadie et du Québec dont le folklore est empreint de rythmes gallo-romains de la Gaule celtique.
Par ailleurs, une différence flagrante domine toujours pour beaucoup de personnes en les fondements français du Canada. Ceux-ci donnent une spécificité particulière au continent américain et à la nature même du Canada, ce qui fait sous-entendre à plusieurs personnes que la ville de Montréal est en l'occurrence la plaque tournante de la culture de langue française en Amérique. Ce faisant, plusieurs artistes francophones fusent des quatre coins du pays (Québec, Acadie, Ontario, Manitoba, etc.), des États-Unis (notamment du pays des Cadiens, en Louisiane) ainsi que des Caraïbes pour faire carrière à Montréal tant dans les domaines littéraire, musical, cinématographique, etc. Sans compter que de nombreux artistes provenant de l'Europe, du Proche-Orient et de l'Afrique viennent aussi se tailler une place afin de faire épanouir encore davantage la culture latine au Canada.
La création et la conservation d'une culture canadienne distincte est partiellement influencée par des programmes du gouvernement fédéral, des lois et des institutions politiques telles que la Société Radio-Canada (SRC), l'Office national du film du Canada (ONF) ainsi que le Conseil de la radiodiffusion et des télécommunications canadiennes (CRTC).
Le Canada possède plusieurs orchestres de renommée internationale, tels l'Orchestre symphonique de Québec, l'Orchestre symphonique de Toronto et surtout l'Orchestre symphonique de Montréal dirigé par Kent Nagano. Le Canada possède aussi plusieurs groupes musicaux de rock alternatif extrêmement importants sur la scène mondiale tel Arcade Fire, Crystal Castles, Simple Plan, Hot Hot Heat, The Dears, Malajube, Godspeed You! Black Emperor, The Stills ou The Sam Robert's Band. Ainsi que des groupes évoluant sur la scène punk alternative international tels que Billy Talent, Alexisonfire, Silverstein ou encore Cancer Bats.
La culture canadienne est aussi partiellement influencée par l'immigration récente de personnes provenant des quatre coins du monde. De nombreux Canadiens prisent le multiculturalisme, ce qui fait croire aux yeux de certaines personnes que la culture du Canada est proprement dite multiculturelle. De surcroît, le patrimoine multiculturel du Canada est protégé par l'article 27 de la Charte canadienne des droits et libertés. Les symboles nationaux sont largement le fruit d'influences provenant de sources naturelles et historiques, ainsi que des peuples autochtones. Plus particulièrement, l'utilisation de la feuille d'érable comme symbole national canadien remonte au début du XVIIIe siècle, et est illustrée sur les anciens drapeaux du Canada, sur son drapeau actuel, sur le cent (prononcé [sɛnt], ou communément [sɛn]), ainsi que sur les armoiries. D'autres symboles importants incluent le castor, la bernache du Canada, le plongeon huard, la Couronne et la Gendarmerie royale du Canada (GRC).
La province du Québec produit de très nombreuses variétés de fromages[242]. Le Canada fut le premier à introduire des législations strictes sur les critères de propreté et d'hygiène régissant la traite du lait et l'élaboration des fromages[243].
Le Canada est également un grand producteur de vin de glace et de cidre de glace ainsi que de spécialités rustiques à base de gibier[244].
De Calgary jusqu'à Vancouver on peut aussi découvrir une cuisine typique du Canada influencée par les tendances européennes et françaises à base d'ingrédients locaux et nuancés par les communautés culturelles canadiennes.
En marge de cela l'Ouest canadien révèle une cuisine semblable à ce que l'on retrouve du côté américain[245].
Le Canada est le premier producteur mondial de sirop d'érable (sirop produit à partir de sève brute de l'érable, l'eau d'érable), la province du Québec étant le chef de file de cette industrie. Dans la seule année 2011, cette dernière province a produit près de 102 millions de livres de sirop d'érable, qui ont principalement été exportés aux États-Unis[246]. Le sirop d'érable est produit pendant une courte période de quatre à six semaines s'échelonnant de la mi-mars à la fin avril, « saison » désignée au Québec comme le temps des sucres. L'industrie acéricole génère également des produits dérivés : tire d'érable, beurre d'érable, sucre d'érable, liqueur d'érable. Le temps des sucres est, pour de nombreux canadiens, et plus particulièrement au Québec, l'occasion de rencontres autour d'un joyeux festin à la cabane à sucre où la gastronomie est dominée par l'usage du sirop d'érable dans la préparation des différents plats. Le sirop d'érable est fréquemment utilisé dans la confection de desserts. On le voit d'ailleurs de plus en plus dans la gastronomie et de nombreux restaurateurs l'incluent dans leurs menus.
D'influence chrétienne, française, britannique et américaine la fête de Noël au Canada passait inaperçue au début du XIXe siècle et prit l'importance qu'on lui connaît de nos jours au début du XXe siècle. C'est environ à cette période (1870) que Noël perdit son aspect religieux pour devenir une fête familiale et communautaire. La reine Victoria fut à l'origine, à la même période, de l’introduction de nombreuses caractéristiques familières du Noël moderne dont le fameux sapin de Noël et la dinde rôtie. L'influence américaine est surtout notable au niveau où les États-Unis ont les premiers transformé saint Nicolas en Père Noël et sont principalement à l'origine la commercialisation de Noël[247],[248].
Les États-Unis ont influencé la nature du père Noël au Canada.
Le dinde de Noël au Canada est d'origine britannique.
Le sapin de Noël canadien est aussi d’origine britannique.
Célébré chaque année à Ottawa, le festival est le plus grand au monde dédié spécifiquement à la tulipe. Il trouve son origine dans un don de tulipes au Canada en provenance des Pays-Bas pour célébrer la fin de la Seconde Guerre mondiale[249],[248].
Chaque lundi précédant le 25 mai, en souvenir de l'anniversaire de naissance de la reine Victoria qui est née un 24 mai, le Canada, par un jour férié officiel, célèbre la première reine du Canada et, lors de son règne, la reine Élisabeth II[249],[248].
Depuis 1868, la fête du Canada est célébrée chaque année le premier juillet. De nombreuses festivités ont lieu ce jour-là[249],[248].
Le 11 novembre de chaque année, les Canadiens célèbrent la mémoire des 100 000 Canadiens morts au combat lors de la Première Guerre mondiale[249],[248].
Le sociologue Gérard Bouchard rapporte que selon plusieurs observateurs, [250].
Selon Gérard Bouchard, là où la plupart des autres pays ont des mythes fondateurs forts (comme la Révolution en France et la Guerre d'indépendance aux États-Unis), le Canada en est dénué, et s'est bâti plutôt sur des « micro-ancrages (l'octroi d'une Chambre d'assemblée, l'Acte d'Union, [...], la Confédération, la participation à la guerre 1914-1918, le Statut de Westminster, etc.) dont aucun ne prédomine vraiment[254]. Il en résulte que le Canada s'est fondé essentiellement sur trois grands axes, dont deux se définissent par comparaison ou opposition au pays voisin :
Bouchard conclut : « Il en résulte une situation un peu paradoxale et fascinante, à savoir le fait d'une identité qui semble se manifester davantage dans l'ardeur mise à la revendiquer que dans la réalité[250]. »
Cette revendication a été portée par de nombreuses personnalités politiques (Lester B. Pearson) et artistiques (Margaret Atwood, Northrop Frye, « Groupe des sept ») canadiennes. Outre l'opposition aux États-Unis, on la fait reposer notamment sur le pacifisme, le multiculturalisme, et le système universel de soins de santé.
Les sports nationaux officiels du Canada sont le hockey et la crosse[256]. Le hockey est le sport le plus populaire au pays. Les sept plus grandes régions métropolitaines du Canada — Toronto, Montréal, Vancouver, Ottawa, Calgary, Edmonton et Winnipeg — sont les villes-maison d'équipes faisant partie intégrante de la Ligue nationale de hockey (LNH). Au sein de toutes les équipes de la Ligue (trente-deux équipes réparties entre le Canada et les États-Unis), plus de 50 % de tous les joueurs sont originaires du Canada. Outre le hockey, la saison froide est au Canada propice aux sports d'hiver tels le patin à glace, le ski alpin et le ski nordique, la raquette, la randonnée en motoneige. Certains centres de ski ont acquis une renommée internationale Whistler, Mont-Tremblant, Mont Sainte-Anne. La pratique de la motoneige s'est initialement développée au Canada, à la suite de l'invention de la motoneige par Joseph-Armand Bombardier.
D'autres sports canadiens populaires comprennent le curling, le football canadien (en particulier la Ligue canadienne de football) ainsi que le vélo de montagne (le Canada serait en partie le lieu de naissance du VTT[réf. souhaitée], notamment des disciplines comme le Freeride ou encore le Northshore.), plus communément appelé VTT, qui se pratique beaucoup dans la ville de Whistler. Le basket-ball et le baseball sont considérablement joués à des niveaux amateurs et de jeunes, mais ne connaissent pas autant de popularité sur la scène professionnelle que les autres sports. Le Canada est le pays hôte de la Coupe du monde de football (soccer) des moins de 20 ans 2007, des Jeux olympiques d'hiver de 2010 à Vancouver en Colombie-Britannique et de ceux de 1988 à Calgary en Alberta. Il a également reçu les Jeux olympiques d'été de 1976 à Montréal au Québec. Depuis les années 1990 la ville de Toronto accueille une franchise de basket-ball évoluant dans le championnat professionnel nord-américain, la NBA, les Raptors de Toronto, qui ont reçu le NBA All-Star Game 2016. De plus, il y a trois franchises de la MLS au Canada. Soit le Toronto FC, les Whitecaps de Vancouver et l'Impact de Montréal.
La course à pied est un sport populaire en été pour les canadiens. Parmi les nombreuses courses organisés au Canada, trois sont ont reçu le label d'or de l'IAAF, soit le 10 km d'Ottawa, le marathon d'Ottawa et le marathon de Toronto[257].
Montréal accueille chaque année le Grand Prix automobile du Canada en Formule-1 sur le circuit Gilles-Villeneuve, et Toronto le Grand Prix automobile de Toronto en IndyCar Series.
Le budget consacré aux soins de longue durée est considéré comme le  du système de santé au Canada, où il n’existe pas de financement spécifique, contrairement à la France et à d'autres pays. Les emplois sont souvent mal rémunérés et le personnel insuffisamment nombreux[258].
Le système de santé canadien est décentralisé, ce sont les provinces et les territoires qui sont responsables du système de santé. Le gouvernement fédéral met en place des normes nationales et assure une part du financement[259].
L'accès aux soins de santé est garanti par des régimes d'assurance maladie publics[259]. Depuis 2006, à la suite du jugement Chaoulli v. Québec, certaines cliniques offrant des soins de santé privée sont autorisées dans la province de Québec[260].
Un sondage Ipsos réalisé en 2021 concluait que 71% des Canadiens jugent le système de santé trop bureaucratique pour répondre aux besoins de la population[261].
À la suite de la pandémie de Covid-19, plusieurs voix se sont fait entendre, exigeant l’autorisation de l’assurance médicale duplicative ainsi qu’une plus grande place au secteur privé dans le système de santé[262].
Drapeau du Canada, couramment appelé l'Unifolié.
Grand Sceau du Canada.
Drapeau du Gouverneur général du Canada.
Cocarde de l'Aviation royale canadienne[264].
Épreuve de 1 cent en laiton représentant une feuille d'érable.
Épreuve de 5 cents en laiton du Canada représentant un castor.
Épreuve de 25 cents en laiton représentant un caribou.
Police montée en uniforme d'apparat.
Pour les articles homonymes, voir Russie (homonymie) et Russia (homonymie).
Fédération de Russie(ru) Росси́йская Федера́ция / Rossiïskaïa Federatsiïa Écouter modifier La Russie (en russe : Россия, Rossiïa prononciation), en forme longue la fédération de Russie[b] (en russe : Российская Федерация, Rossiïskaïa Federatsiïa prononciation), est un État fédéral transcontinental, le plus vaste État de la planète, à cheval sur l'Asie du Nord (80 % de sa superficie) et sur l'Europe (20 %).
D'ouest en est, son territoire s'étend de la mer Baltique (exclave de Kaliningrad) au détroit de Béring (district autonome de Tchoukotka) sur plus de 6 600 kilomètres, avec une superficie de 17 234 033 km2, soit 11,5 % des terres émergées.
Du fait de cette étendue, la Russie connait une variété de climats allant du climat subtropical humide sur les rives de la mer Noire jusqu'à des climats beaucoup plus froids dans la zone de toundra limitrophe du cercle polaire arctique, ainsi qu'en Sibérie, en passant les zones arides et semi-arides du désert Ryn et de la steppe eurasienne au sud. La majorité du territoire russe est caractérisée par un climat continental avec des hivers froids et neigeux et est occupée par la taïga.
La population russe est estimée à près de 146 millions d'habitants en 2021[3] ce qui en fait le neuvième pays le plus peuplé de la planète. 78 % de ses habitants vivent en Russie européenne[10].
Après la chute de la Rus' de Kiev au XIIIe siècle, la grande-principauté de Moscou unifie plusieurs territoires voisins et devient le tsarat de Russie au XVIe siècle, fondé par Ivan le Terrible. Le pays s'élargit rapidement en conquérant l'Asie du Nord au XVIIe siècle. En 1721, le tsar Pierre Ier le Grand établit l'Empire russe qui devient le troisième plus grand empire de l'histoire ainsi qu'une puissance majeure en Europe. La révolution russe, à la suite de la Première Guerre mondiale, mène à la chute de la dynastie impériale en mars 1917, puis à la prise de pouvoir des bolcheviks dirigés par Vladimir Lénine et l'établissement de l'Union des républiques socialistes soviétiques (URSS) en 1922, dont la république socialiste fédérative soviétique de Russie est le principal constituant.
Après la Seconde Guerre mondiale, l'URSS devient une des deux superpuissances de la guerre froide, fer de lance de l'idéologie communiste et du « socialisme réel » face au monde capitaliste dirigé par les États-Unis. L'Union soviétique développe l'arme nucléaire dès 1949, stupéfie le monde par son avance dans le domaine spatial et s'implique dans de nombreux conflits afin de maintenir et d'étendre son influence, s'engageant notamment dans une guerre en Afghanistan en 1979, une des causes de son effondrement dans les années 1990-1991.
Fin 1991, l'URSS éclate en quinze États indépendants, dont la fédération de Russie qui reprend la place de l'URSS dans les institutions internationales, notamment le siège permanent au Conseil de sécurité des Nations unies. Elle assume également le passif financier de l'URSS et prend en charge l'armement nucléaire soviétique. Elle est aussi à l'origine de la Communauté des États indépendants (CEI) qui rassemble dix des ex-républiques soviétiques. La Russie adopte alors une économie de marché et un régime parlementaire pluraliste. Depuis la démission du premier président, Boris Eltsine, en 1999, la vie politique de la Russie est dominée par Vladimir Poutine, souvent qualifié de dirigeant autoritaire et accusé de violations des droits de l'homme, ainsi que de corruption et d'ingérences.
La Russie actuelle est une fédération constituée de 89 entités, les « sujets de la Fédération », disposant d'une autonomie politique et économique variable. Le découpage territorial, qui tient compte entre autres de la présence de minorités, reprend celui de la Russie soviétique.
Aspirant à s'insérer dans la mondialisation, la Russie fait partie des BRICS (Brésil, Russie, Inde, Chine et Afrique du Sud). Elle se considère par ailleurs comme un pont entre l'Europe et l'Asie. En 2019, la Russie est la onzième puissance économique mondiale en termes de PIB à valeur nominale[11] et la sixième en parité de pouvoir d'achat[12].
Le territoire de la Russie est constitué majoritairement de vastes plaines où prédominent les steppes au sud, la forêt au nord et la toundra le long des rivages de l'océan Arctique. Les principaux massifs montagneux se situent le long de la frontière méridionale : ce sont le Caucase, dont le point culminant, le mont Elbrouz (5 642 mètres) est également le sommet le plus élevé d'Europe et les montagnes de l'Altaï au sud de la Sibérie. À l'est se trouvent le massif de Verkhoïansk et la chaîne de volcans de la presqu'île du Kamtchatka, dominée par le Klioutchevskoï, un stratovolcan de 4 835 mètres. L'Oural, qui sépare selon un axe nord-sud la Russie d'Europe de la Russie d'Asie, est un massif montagneux érodé riche en ressources minières.
L'énorme ceinture forestière d'une largeur de 1 200 km en « Russie européenne » dont l'Oural est la barrière naturelle, et de 2 000 km en Sibérie constitue la plus grande réserve forestière de la planète. Les surfaces cultivées présentent 8,9 % de la surface cultivable de la planète.
Le littoral de la Russie a une longueur de 37 653 km : il s'étire essentiellement le long de l'océan Arctique et de l'océan Pacifique ; il comprend également de relativement petites portions de côtes sur la mer Baltique, la mer Noire, la mer Caspienne et la mer d'Azov.
Les principales îles et archipels comprennent en océan Arctique la Nouvelle-Zemble, l'archipel François-Joseph, l'archipel de Nouvelle-Sibérie, et dans le Pacifique l'île Sakhaline et l'archipel des Kouriles, dont les îles les plus méridionales sont revendiquées par le Japon.
Plus de 100 000 rivières arrosent la Russie dont certaines figurent parmi les plus importantes de la planète. La Volga, qui draine un bassin versant de 1,4 million de kilomètres carrés, est le plus long fleuve d'Europe (3 350 km) et a joué un rôle majeur dans l'histoire du pays. Les grands fleuves sibériens figurent parmi les géants de la planète : ce sont l'Ienisseï (débit moyen 19 800 m3/s), l'Ob, la Léna et l'Amour tous caractérisés par des débits énormes et des débâcles particulièrement violentes lorsque l'été arrive, et remet ainsi en mouvement les eaux prises dans les glaces. Les principales étendues d'eau sont le lac Baïkal, qui contient 20 % de l'eau douce lacustre de la planète, le lac Ladoga et le lac Onega.
Plus de la moitié du pays est située au nord du 60° de latitude tandis que seule une faible partie se trouve au sud du 50° de latitude. Les montagnes qui ferment les frontières méridionales (Altaï…) empêchent la remontée des masses d'air chaud venues des régions plus méridionales ; par contre, les plaines qui dominent dans le nord du pays laissent pénétrer loin à l'intérieur des terres les masses d'air refroidies par l'océan Arctique. Il en résulte une température moyenne de −5,5 °C avec une grande amplitude thermique entre l'hiver et l'été.
Dans pratiquement tout le pays, il n'existe que deux grandes saisons : l'hiver et l'été ; le printemps et l'automne sont généralement de très courte durée et le passage des températures les plus chaudes aux températures les plus froides est extrêmement rapide. Le mois le plus froid est janvier (février sur les côtes). Les températures hivernales vont en s'abaissant à la fois du sud au nord et de l'ouest à l'est (beaucoup plus continental) : on relève ainsi une température moyenne en février de −8 °C à Saint-Pétersbourg située à l'extrême-ouest, −27 °C dans les plaines de Sibérie occidentale, et −43 °C à Iakoutsk située en Sibérie orientale à peu près à la latitude de Saint-Pétersbourg. Le record du froid est détenu par la ville de Oïmiakon (−72 °C relevés). Le vent du sud, généré par l'anticyclone qui stationne en hiver sur la majeure partie de la Russie, réduit les différences de température entre les régions situées à des latitudes différentes. En été, le mois le plus chaud est généralement juillet (la température moyenne en Russie est de 20 °C). Les températures peuvent être très élevées dans les régions continentales (jusqu'à 38 °C au sud). L'amplitude des températures est généralement extrêmement élevée. L'été peut être très chaud et humide y compris en Sibérie. Une petite partie de la côte de la mer Noire près de Sotchi a un climat subtropical.
Le climat continental limite fortement la pluviométrie. Si à l'ouest les précipitations annuelles sont de 600 mm dans les régions baltiques et de 525 mm à Moscou, elles tombent à 425 mm à Novossibirsk (en Sibérie).
La durée de l'hiver, le froid intense et les variations brutales de température ont un énorme impact sur le mode de vie de la population et le fonctionnement de l'économie. Dans la partie la plus froide du pays, le sous-sol ne dégèle jamais : on parle de pergélisol (permafrost en anglais, merzlota en russe) ; l'eau stagne en surface et crée de gigantesques marécages – paysage récurrent de la Sibérie ; la présence du sous-sol gelé génère des contraintes très coûteuses sur le mode de construction des bâtiments et des infrastructures. Les grands fleuves sont généralement pris par les glaces d'octobre/novembre à avril/mai bloquant toute circulation fluviale ; au printemps, la débâcle des glaces entraîne souvent des inondations catastrophiques sur les plus grands fleuves sibériens.
Du fait de sa taille, le pays présente de nombreux types de paysages parmi lesquels prédominent des étendues relativement plates couvertes selon la latitude de toundra, de taïga, de forêts ou de steppes. La Russie d'Europe, définie de manière arbitraire comme la partie du pays située à l'ouest de l'Oural, présente successivement en allant du nord au sud les paysages suivants : au nord la partie la plus froide est le règne de la toundra à laquelle succèdent en allant vers le sud les forêts de conifères, puis les forêts mixtes (feuillus et conifères), les prairies, et enfin la steppe semi-désertique (près de la mer Caspienne). Le changement de végétation suit celui du climat. La Sibérie – la partie située à l'est de l'Oural – présente la même succession de paysages, mais c'est surtout la taïga, forêt plus ou moins clairsemée composée majoritairement de conifères, qui prédomine.
La Russie est une fédération constituée de 89 sujets de la fédération de Russie qui sont des unités territoriales du niveau supérieur de la fédération de Russie :
Les sujets de la fédération de Russie ont un pouvoir exécutif (un chef, un gouverneur, un maire), un pouvoir législatif (parlements régionaux) et un pouvoir judiciaire. Les républiques ont une Constitution tandis qu'on parle de statut pour les autres sujets de la fédération. Chaque sujet de la fédération de Russie envoie deux représentants au Conseil de la fédération (le sénat de la fédération de Russie).
Quatre nouvelles régions sont rattachées à la fédération de Russie le 5 octobre 2022 :
Ce rattachement est cependant contesté par l’Ukraine et une grande partie de la communauté internationale.
Pour des raisons différentes, les sujets de la fédération de Russie sont regroupés en :
La Russie est limitrophe (frontières terrestres) de 14 pays. Dans l'ordre inverse des aiguilles d'une montre, en partant de la Norvège (latitude Nord la plus élevée), ce sont : la Norvège sur 196 km, la Finlande sur 1 313 km, l'Estonie sur 290 km, la Lettonie sur 292 km, la Biélorussie sur 959 km, la Lituanie sur 227 km, la Pologne sur 206 km, l'Ukraine sur 1 576 km, la Géorgie sur 723 km, l'Azerbaïdjan sur 284 km, le Kazakhstan sur 6 846 km, la Chine sur 3 645 km, la Mongolie sur 3 441 km et la Corée du Nord sur 19 km[17].
Elle est aussi limitrophe de deux républiques séparatistes en Géorgie (l'Abkhazie et l'Ossétie du Sud-Alanie dont la Russie a reconnu l'indépendance en 2008).
En ce qui concerne les frontières maritimes, la Russie est très proche du Japon (détroit entre les îles de Sakhaline et d'Hokkaido) et des États-Unis (détroit de Bering).
La Rus' de Kiev ou principauté de Kiev (Ruthénie) est le premier État organisé à s'être formé dans la région occupée aujourd'hui par l'Ukraine, la Biélorussie et une partie de la Russie occidentale (862). Fondée par des Vikings venus de Scandinavie (les Varègues en russe) puis dirigée par la dynastie des Riourikides, elle forme un État peu structuré dont les sujets sont les tribus de Slaves orientaux vivant dans la région et qui seront progressivement conquis. Les princes varègues développent la route commerciale qui relie la mer Baltique et la mer Noire en empruntant le fleuve Dniepr (la route des Varègues). Ils réussissent, par la force des armes, à s'imposer à l'Empire byzantin en tant que partenaires commerciaux. La principauté de Kiev doit combattre les peuples nomades des steppes venus de l'est : Petchenègues, Coumans, etc. Sous le règne de Vladimir, le territoire s'étend et en 988, ce grand prince se convertit à la religion de l'Empire byzantin, le christianisme orthodoxe : celle-ci deviendra religion d'État et sera l'un des facteurs de l'unité nationale russe. La Principauté de Kiev se désintègre au fil des années sous les coups de boutoir des peuples nomades après une longue période d'instabilité interne en raison des partages successoraux entre les descendants de Vladimir. Elle fait place à une quinzaine de principautés situées sur les territoires des actuelles Ukraine, Biélorussie et de la partie européenne de la Russie. Ainsi, en 1276, la grande-principauté de Moscou voit le jour.
Les princes, qui dirigent ces principautés et ont la propriété éminente de la terre, emploient des armées encadrées par des boyards qui deviendront progressivement des propriétaires terriens. Ils règnent sur une masse de paysans à cette époque généralement libres. La principauté de Vladimir-Souzdal et surtout la république de Novgorod toutes deux situées au nord de la Principauté de Kiev vont profiter de leur indépendance pour se développer. La république de Novgorod, cité-État dotée d'un système de gouvernement original, prospère grâce à ses échanges commerciaux avec les pays de la Baltique. Elle repousse à plusieurs reprises les tentatives d'expansion des chevaliers teutoniques.
En 1226, un peuple nomade guerrier venu de Mongolie, appelé Tataro-Mongols par les Russes, attaque les principautés. Entre 1237 et 1242, le khan Batou petit-fils de Gengis Khan, défait les unes après les autres les armées des princes et réduit en cendres les principales villes dont Vladimir, Kiev et Moscou[18]. Les populations sont massacrées ou réduites en esclavage. Seule Novgorod et dans une certaine mesure Pskov, situées au nord-est, réussissent à conserver une certaine autonomie. Les Mongols n'occupent pas les territoires vaincus, mais les principautés doivent payer tribut et reconnaître la suzeraineté des Mongols qui fondent un État au sud de la Volga : la Horde d'or. Cette vassalité ne prendra fin que trois siècles plus tard.
Les Mongols tatars ont profondément marqué la Russie, ethniquement avec l'installation de peuples de langues turciques, culturellement avec l'islamisation des peuples de l'est de Moscou, entre Vladimir et Kazan qui renforcera le poids de l'Église face à l'occupation musulmane. Le vocabulaire russe s'enrichit de nombreux termes de la langue mongole tels que yam (poste) et tamga (péage). Administrativement, les Russes intègrent les tribus ainsi que les levées de troupes. Comme les Mongols, les princes russes iront jusqu'à imposer à leurs sujets de maintenir un service de relais de poste. Enfin, militairement, l'armée russe reprendra à son compte l'usage de la cavalerie légère[19].
Du XIIIe au XVIe siècle, l'une de ces principautés, la Moscovie (dont la capitale est Moscou), dirigée par des princes habiles, annexe progressivement toutes les autres pour devenir la Russie. Le prince Dimitri IV de Russie vainc une première fois les Mongols à la bataille de Koulikovo (1380). Toutefois, ce mouvement d'unification se heurte aux rivalités et à la tradition de partage des territoires entre les différents fils du prince, ce qui engendra une guerre civile entre 1425 et 1453. Monté sur le trône en 1462, Ivan III, qu'un voyageur vénitien décrit comme un « homme de haute taille, penché en avant et beau », libère la Moscovie du joug des Mongols dont l'empire est désormais fragmenté en plusieurs khanats, puis absorbe les principales principautés russes encore indépendantes dont Novgorod (1478) et Principauté de Tver (1485). En 1485, Ivan III prend le titre de « souverain de toute la Rus' », désirant montrer sa volonté de reconstituer tout l'héritage de Vladimir. À la fin du règne d'Ivan III le territoire de la Moscovie a quadruplé. Son fils Vassili III (1505–1533) poursuit l'extension territoriale en annexant la cité-État de Pskov (1510) et la principauté de Riazan (1521) ainsi que Smolensk (1514). Ivan le Terrible, premier prince à se faire désigner sous le titre de tsar, parachève ces conquêtes en s'emparant des principaux khanats mongols, mais il perd l'accès à la mer Baltique face à une coalition de l'Empire suédois avec la Pologne et la Lituanie. Désormais l'expansion de la Russie vers l'est n'a plus d'obstacle sérieux. La colonisation par les paysans russes du vaste bassin de la Volga et de l'Oural prend son essor. Des paysans et fugitifs, les cosaques, s'installent sur les marges et s'organisent en « armée » tout en jouant les rôles de pionniers et de garde-frontières. Ivan le Terrible se considère alors logiquement comme l'unique héritier de Vladimir, bien qu'il ne possède pas la ville de Kiev aux mains de la dynastie lituanienne des Jagellon. Cette dernière avait conquis la plupart des territoires de la Rus' occidentale.
L'extinction de la dynastie des descendants de Riourik (qui remontait aux mythiques princes varègues) déclenche le Temps des troubles jusqu'à ce qu'une nouvelle dynastie, les Romanov, monte sur le trône (1613). Plusieurs souverains brillants vont aux XVIIe et XVIIIe siècles accroître la taille de l'Empire russe avec l'aide des cosaques. Pierre Ier le Grand (1682–1725), au prix d'une longue guerre avec la Suède, obtient un accès à la mer Baltique ; il fait construire Saint-Pétersbourg qui devient à compter de 1712 la nouvelle capitale, symbolisant ainsi l'ouverture du pays vers l'Europe. Une puissante industrie métallurgique, la première d'Occident à l'époque, est édifiée dans l'Oural et permet de soutenir l'effort de guerre.
Catherine II de Russie (1762-1796), autocrate éclairée, achève la conquête des steppes situées au bord de la mer Noire après avoir défait l'Empire ottoman et le khanat de Crimée et repousse vers l'ouest les frontières de l'empire russe grâce au partage de la Pologne.
L'actuelle Ukraine et la Russie Blanche (Biélorussie) sont désormais entièrement en territoire russe. Durant toute cette période, les cosaques occupent progressivement la Sibérie et atteignent l'océan Pacifique en 1640. Irkoutsk au bord du lac Baïkal est fondé en 1632, la région du détroit de Béring et l'Alaska sont explorés dans les années 1740.
Un code édicté en 1649 lie désormais le paysan et ses descendants à la terre et à son propriétaire généralisant le servage, à contresens de l'évolution du statut du paysan en Europe occidentale. En contrepartie, les propriétaires terriens sont astreints à servir leur souverain. Catherine II confirme et renforce ces dispositions. Le mécontentement des paysans et d'une classe naissante d'ouvriers, exploités par leurs propriétaires et lourdement taxés par la fiscalité d'un État en pleine croissance déclenchent aux XVIIe et XVIIIe siècles de nombreuses révoltes paysannes dont la plus importante, menée par le cosaque Pougatchev, parvient à menacer le trône avant d'être écrasée (1773). L'Église à l'époque joue un rôle essentiel dans la société russe et possède plus des deux tiers des terres. La réforme du dogme orthodoxe russe par le patriarche Nikon (1653) est à l'origine du schisme des vieux-croyants sévèrement réprimé.
Pierre le Grand puis Catherine II font venir un grand nombre de colons allemands (par exemple les Allemands de la Volga), d'artisans et de savants occidentaux souvent allemands, pour moderniser le pays, édifier des industries et jeter les fondements des établissements d'enseignement et de diffusion du savoir. Les bases de la langue littéraire russe sont définies par Mikhaïl Lomonossov. Les premiers journaux sont publiés à cette époque. La noblesse russe s'occidentalise, surtout sous l'influence de la philosophie allemande et de la langue française, et certains de ses membres s'enthousiasmeront pour les idées des Lumières, et parfois même de la Révolution française.
L'Empire russe joue un rôle décisif durant la guerre de Sept Ans puis, cinquante ans plus tard, dans les guerres napoléoniennes ; ces conflits font de la Russie une puissance européenne. Mû, comme tous les souverains européens, par une idéologie conservatrice et hostile aux idées de la Révolution française, Alexandre Ier participe à deux coalitions contre Napoléon Ier et essuie des défaites coûteuses. Alexandre Ier choisit alors, par renversement d'alliance, le camp de la France (traités de Tilsit), mais la paix ne durera que cinq ans (1807–1812). Il profite de cette pause pour attaquer la Suède et annexer la Finlande.
En 1812, les hostilités reprennent. La Grande Armée de Napoléon parvient à s'emparer de Moscou, mais doit en repartir, chassée par l'incendie de la ville. Les armées russes harcèlent alors un ennemi décimé par la faim et le froid et, en 1814, elles occupent Paris.
L'Empire russe joue un rôle majeur dans le congrès de Vienne et la Sainte-Alliance, qui veut régir le destin de l'Europe post-napoléonienne : il s'oppose à la reconstitution de l'État polonais et participe militairement à la répression des soulèvements contre les monarchies (Hongrie 1849), à l'instar de l'empire d'Autriche.
L'Empire russe poursuit, sous son règne et celui de ses successeurs, son expansion dans le Caucase et vers les bouches du Danube, au détriment de l'Empire perse et de l'Empire ottoman. La Géorgie (vassale de l'Empire perse) est annexée en 1813 (traité de Golestan). La partie orientale de la Principauté de Moldavie (vassale de l'Empire ottoman) est annexée en 1812 et forme le gouvernement de Bessarabie (Traité de Bucarest de 1812). L'Arménie, le Daghestan et une partie de l'Azerbaïdjan sont annexés en 1828 au terme d'un conflit de quatre ans avec l'Empire perse (Traité de Turkmantchaï). Au décès d'Alexandre (1825), des officiers réformistes, les décembristes, se soulèvent en vain pour demander une réforme de la monarchie. Cette tentative de soulèvement d'officiers issus de l'aristocratie va servir aussi de modèle à de nombreux intellectuels russes au cours du siècle suivant, inspirés par la philosophie de Hegel ou de Kropotkine. En 1829 l'Empire russe se fait céder par l'Empire ottoman les Bouches du Danube. Nicolas Ier bénéficie d'une bonne croissance économique, mais renforce l'appareil répressif. Il écrase violemment un soulèvement armé de la Pologne (1831). Le déclin de l'Empire ottoman, qui attise les convoitises des puissances européennes, est à l'origine d'un conflit entre la Russie et les autres puissances européennes, Grande-Bretagne en tête: la guerre de Crimée. Défait à Sébastopol (1856), Alexandre II, le successeur de Nicolas, doit céder le sud de la Bessarabie avec les Bouches du Danube, et perd les droits de passage entre la mer Noire et la Méditerranée. Un dernier conflit victorieux avec l'Empire ottoman (1878), déclenché par l'insurrection bosniaque de 1876[20], lui permet de retrouver un accès au Danube et parachève la conquête du Caucase. Ce conflit inquiète cependant les investisseurs, car la Turquie refuse de signer le protocole élaboré à Londres par les grandes puissances.
La Russie obtient aussi la création dans les Balkans d'un royaume de Bulgarie, et la reconnaissance par les Ottomans de l'indépendance de la Serbie et de la Roumanie. Cet accroissement d'influence ravive l'hostilité du Royaume-Uni (Le Grand Jeu).
De nombreuses jacqueries, contre l'aristocratie terrienne endettée et attachée de ce fait au système du servage, ont lieu durant cette période. L'industrie se développe surtout dans les mines et le textile, mais reste très en retrait par rapport à l'Angleterre et à l'Allemagne (environ 600 000 ouvriers vers 1860). Une nouvelle classe de commerçants et de petits industriels – souvent d'anciens serfs libérés par rachat – apparaît, mais ses effectifs sont relativement peu nombreux.
L'enseignement se répand dans les classes les plus aisées et de nombreuses écoles supérieures sont fondées. La littérature russe connaît un premier épanouissement avec des écrivains majeurs comme Tourgueniev, Pouchkine ou Gogol qui témoignent des tourments de la société russe. Cet essor culturel s'étend également à l'architecture et à la musique (Glinka).
Alexandre II tente de tirer les leçons de la défaite de Crimée. Le pays, qui s'étend désormais sur 12,5 millions de kilomètres carrés et compte 60 millions d'habitants, est handicapé par son fonctionnement archaïque. Des réformes structurelles sont mises en train par le tsar : la mesure la plus importante est l'abolition du servage de 1861 qui inclut l'attribution à l'ancien serf d'une terre, souvent trop petite pour le nourrir, au prix d'un endettement à long terme vis-à-vis de l'État. Des conseils locaux élus au suffrage censitaire – les Zemstvos – sont créés à compter de 1864 : dotés de pouvoir leur permettant de gérer les affaires locales et de construire routes, écoles et hôpitaux, ils peuvent lever des impôts pour les financer. Ce type de structure est étendu par la suite aux villes (douma urbaine). Enfin le code juridique introduit les procédures d'accusation et de défense et crée une justice théoriquement indépendante du pouvoir jusqu'à l'échelon du district. Le régime conserve malgré tout un caractère autocratique et fortement policier. Les réformes vont d'ailleurs attiser la violence de groupes d'intellectuels nihilistes et Alexandre finira par tomber sous leurs coups (1881). Sous son règne, l'empire a poursuivi son expansion coloniale en Asie centrale : après l'annexion des terres des Kazakhs achevée en 1847, les trois khanats du territoire ouzbek (Kokand, Boukhara et Khiva) sont conquis au cours des trois décennies suivantes puis annexés ou placés sous protectorat (1876). Cette avancée place les limites de l'empire russe aux portes de l'Empire britannique aux Indes. La tension (Grand Jeu) entre les deux pays va rester très vive jusqu'à ce qu'un accord soit trouvé en 1907 (convention anglo-russe). La Pologne se soulèvera sans succès en 1863.
Alexandre II est principalement connu pour ses réformes, notamment l'abolition du servage. Malgré les grandes réformes libérales mises en place, il est assassiné, le 13 mars 1881, lors d'un attentat organisé par le groupe anti-tsariste Narodnaïa Volia.
Alexandre III, lorsqu'il monte sur le trône en 1881, mène en réaction à l'assassinat de son père une politique de contre-réformes. Les dispositions autoritaires sont maintenues ou renforcées : les partis politiques et les syndicats sont interdits, le droit de circulation est limité, la presse est censurée. Sur le plan économique, l'industrie se développe rapidement grâce, entre autres, aux investissements étrangers et à la construction d'un réseau ferroviaire qui atteint 30 000 km en 1890. De nouvelles régions s'industrialisent (Ukraine) tandis que certaines renforcent leur caractère industriel comme la région de Saint-Pétersbourg et surtout celle de Moscou. Mais la main-d'œuvre abondante dégagée par l'abolition du servage et la croissance démographique ne trouve pas entièrement à s'employer dans l'industrie (trois millions d'ouvriers en 1913). De nombreux paysans viennent coloniser les terres vierges de l'empire situées dans le sud et l'est (vallée inférieure de la Volga, Oural, Sibérie) de l'empire. Le Transsibérien permet de désenclaver les immenses territoires de la Sibérie et facilite cette migration, tandis que le financement de l'industrialisation se fait principalement par les emprunts russes venus surtout de France.
Le premier tronçon du Transsibérien ouvre dès 1888 et Moscou émet quatre emprunts de 500 millions de francs-or. En 1904, la France compte 1,6 million de créanciers du réseau ferré, de l'État et des municipalités russes[21], tandis que l'alliance franco-russe mise en place en 1892 tente de faire pièce à la Triplice.
L’agriculture a toujours un poids écrasant : en 1897, la Russie compte 97 millions de paysans pour une population totale de 127 millions d’habitants.[réf. nécessaire] Ceux-ci ne possèdent généralement pas les terres qu'ils cultivent (25 % seront propriétaires en 1914). Le taux d'alphabétisation est très faible et la mortalité infantile est élevée (environ 180 ‰). L'excédent démographique est absorbé par les villes dont le nombre croît rapidement : à la veille de la Première Guerre mondiale, la population citadine dépasse les 25 millions d'habitants. La Russie continue d'accroître son aire d'influence : en Chine et en Corée, elle se heurte aux intérêts japonais. La guerre russo-japonaise qui s'ensuit se termine par une défaite complète (1905 à Tsushima) : la modernisation du Japon a été sous-estimée et l'éloignement du champ de bataille a créé d'énormes contraintes logistiques.
La défaite de Tsushima de 1905 déclenche le premier soulèvement généralisé de la population russe contre le régime. La révolution russe de 1905 est d'abord un mouvement paysan qui touche essentiellement la région des terres noires. Les ouvriers se joignent au mouvement par la suite[22]. La loyauté des forces armées va sauver le régime. Nicolas II, qui est monté sur le trône en 1894, est obligé de donner des gages d'ouverture. Une assemblée (douma) élue est dotée de pouvoirs législatifs. Mais les élections de deux doumas successives donnent une large majorité à l'opposition. La loi électorale est alors modifiée pour obtenir une chambre des députés favorable au pouvoir.
L'évolution économique et sociale du pays avait fait monter les oppositions libérales, démocrates, socialistes et révolutionnaires au régime tsariste. La fusillade meurtrière du Dimanche rouge à Saint-Pétersbourg mit le feu aux poudres. Le régime impérial survécut à cette première attaque d'envergure, mais le mécontentement grandit et l'opposition se radicalisa. La grève générale d'octobre 1905 réussit à faire céder le régime. Une constitution libérale fut octroyée ; mais dans les deux ans qui suivirent, la contre-attaque de Nicolas II réduisait à néant les espoirs soulevés par cette révolution.
La mutinerie du cuirassé Potemkine, immortalisée en 1925 par Le Cuirassé Potemkine, film de Sergueï Eisenstein, en est restée un symbole.
La Russie entre en guerre contre l'Allemagne et l'Autriche-Hongrie en 1914 pour venir en aide à la Serbie, son alliée. L'Empire russe déclenche une offensive en Pologne orientale mais est sévèrement battu. Les troupes russes doivent abandonner la Pologne. Début 1917 éclatent des mouvements sociaux, suscités par le poids de la guerre sur l'économie, les pertes sur un front réduit à une stratégie défensive, l'instabilité des dirigeants et la défiance vis-à-vis du tsar. Le refus des troupes de réprimer les manifestations et la lassitude des classes dirigeantes obligent le tsar Nicolas II à abdiquer; ainsi éclate la Révolution de Février 1917 et la Russie devient une république. Un gouvernement provisoire est alors constitué, présidé par Alexandre Kerenski. Tout en esquissant des réformes, celui-ci tente malgré tout de respecter les engagements de la Russie vis-à-vis de ses alliés en poursuivant la guerre. L'impopularité de cette dernière mesure est exploitée par le parti des bolcheviks qui, le 25 octobre 1917 (7 novembre dans le calendrier grégorien), renverse le gouvernement à Saint-Pétersbourg (alors capitale de la Russie) par les armes (révolution d'Octobre). La paix est signée avec les Allemands (à Brest-Litovsk, en Biélorussie actuelle) au prix d'énormes concessions territoriales (Pologne, partie de l'Ukraine, pays baltes, etc., soit environ 800 000 km2). Une tentative de Lénine d'exporter la révolution en Occident déclenchant la guerre contre la Pologne se termine par un cuisant échec: l'armée russe presque deux fois plus nombreuse que la polonaise est battue aux portes de Varsovie en août 1920. C'est le "miracle de la Vistule" et la "dix-huitième bataille décisive dans l'histoire du monde" due au génie stratégique du Maréchal Joseph Pilsudski. Une guerre civile va opposer pendant trois ans les Russes blancs (républicains ou monarchistes), assistés par les puissances occidentales, aux bolcheviks. Après leur victoire, le 22 décembre 1922, les bolcheviks instaurent l'Union des républiques socialistes soviétiques ; la Russie devient une des républiques de l'Union.
Dès la prise du pouvoir, le nouveau régime tourne à la dictature réprimant toute opposition même au sein du parti bolchevik. L'ensemble des moyens de production industrielle est placé sous le contrôle de l'État. À la fin de la guerre civile en 1921, le pays est exsangue : la désorganisation des transports et les réquisitions agricoles déclenchent une famine qui fait un million de victimes autour de la Volga. Le mécontentement gagne et le régime doit assouplir son programme : c'est la NEP qui autorise une forme limitée d'économie privée. En quelques années, les productions agricole et industrielle se rétablissent. Lénine, mort en 1924, laisse sa « succession » ouverte. Staline va en quelques années se hisser au pouvoir en éliminant physiquement ses rivaux. Le plan de collectivisation est repris avec vigueur et les terres agricoles sont regroupées par la force au sein de grandes coopératives. Une nouvelle famine éclate, cette fois-ci majoritairement en Ukraine (1932–1933) et dans le Kouban. Le développement de l'économie est désormais planifié de façon centralisée et le pouvoir, qui se concentre à Moscou (redevenue capitale du pays en 1918), mène un vaste programme d'industrialisation (surtout dans le domaine de l'industrie lourde) à l'aide des plans quinquennaux. Le gouvernement incite les travailleurs au dépassement des normes de productivité (stakhanovisme) au nom de l'avenir radieux. La machine de propagande communiste fonctionne à plein régime. En même temps, Staline mène une politique répressive qui envoie au goulag ou à la mort plusieurs millions de personnes avant le déclenchement de la Seconde Guerre mondiale, ce qui ne l'empêche pas d'instaurer un véritable culte de personnalité. C'est la montée du stalinisme.
Le Pacte germano-soviétique, signé le 23 août 1939, pacte de non-agression entre l'Allemagne nazie et l'Union soviétique contient des protocoles secrets établissant les modalités de partage de la Finlande, l'Estonie, la Lettonie, la Lituanie, la Pologne et la Roumanie entre le Reich et l'Union soviétique. La Pologne est ainsi partagée en deux en septembre 1939. De même, Staline annexe les trois États baltes et force la Roumanie à lui céder la Bessarabie et les régions moldaves. Ces protocoles sont mis en œuvre sans difficulté véritable, sauf en ce qui concerne la Finlande (qui doit être placée sous influence soviétique), où se déroule la guerre d'Hiver. Ainsi, l'Union soviétique et l'Allemagne nazie se partagent une partie de l'Europe, sans que cela ne déclenche de réaction notoire de la part de la France et de la Grande-Bretagne.
Staline, qui a signé avant le début de la Seconde Guerre mondiale un pacte de non-agression avec Hitler comprenant une clause de partage de la Pologne et des pays baltes, est attaqué par l'Allemagne en juin 1941 (opération Barbarossa). L'Armée rouge sous-équipée et désorganisée par les purges staliniennes recule en essuyant des pertes qui se chiffrent en millions. L'avancée allemande est bloquée devant Stalingrad en janvier 1943, puis repoussée vers l'ouest, notamment à la suite de la bataille de Koursk opposant du 5 juillet au 23 août 1943 les forces allemandes aux forces soviétiques sur un immense saillant de 23 000 km2 situé au sud-ouest de la Russie, à la limite de l'Ukraine, entre Orel au nord et Belgorod au sud. C'est l'une des batailles qui ont déterminé l'issue de la Seconde Guerre mondiale en Europe. Les généraux soviétiques reprennent progressivement l'initiative et l'Armée rouge, renforcée par des livraisons d'armes alliées, reconquiert les territoires perdus, libère les pays de l'Europe orientale puis rentre victorieuse dans Berlin (mai 1945), au prix d'un terrible bilan de 20 à 30 millions de victimes (dont presque la moitié de civils). Staline et ses alliés occidentaux ont conclu un accord sur un partage de l'Europe en zones d'influence qui entérine le rôle joué par l'URSS dans le conflit (conférence de Yalta). Les pays d'Europe orientale et l'Allemagne de l'Est se voient bientôt imposer un régime socialiste piloté par l'URSS.
La guerre a saigné l'URSS (plus de 20 millions de victimes dont une majorité de civils) et détruit une bonne partie de ses installations industrielles et de ses villes. L'immédiat après-guerre est une période de reconstruction. Le pays retrouve son niveau de production industrielle d'avant-guerre puis le double en 1952. L'industrie nucléaire se développe, avec la création du complexe nucléaire Maïak. L'URSS effectue son premier essai nucléaire en 1949, accédant ainsi au rang de seconde puissance nucléaire mondiale.
Dans le même temps, le culte de la personnalité est porté à son comble par Staline. Peu après le décès de celui-ci en 1953, Nikita Khrouchtchev accède au pouvoir (1953) et dénonce les excès de son prédécesseur. Sur le plan intérieur commence une période de relative prospérité ; les droits des citoyens sont mieux respectés, c'est le début d'une certaine libéralisation. L'URSS stupéfie le monde par son avance dans le domaine spatial en mettant en orbite le premier Spoutnik et en y envoyant Youri Gagarine, premier homme dans l'espace. Sur le plan international, l'URSS élargit son influence à de nombreux pays du tiers monde et parvient par des investissements massifs dans l'armement à faire jeu égal avec les États-Unis, notamment dans le domaine nucléaire et des missiles balistiques. Cette période de guerre froide se traduit par de nombreux conflits ou tensions un peu partout dans le monde entre les deux superpuissances et leurs alliés. La crise de Cuba en 1962 manque de dégénérer en un conflit nucléaire. L'accession de Léonid Brejnev au pouvoir (1964) se traduit par une relative détente entre les deux grands, grâce aux travaux de la Conférence sur la sécurité et la coopération en Europe mais également, sur le plan intérieur, par une réduction des tentatives de réforme qui n'avaient pas réussi à son prédécesseur (la campagne des terres vierges entre autres). L'écart entre le niveau de vie des Soviétiques et celui des habitants des pays occidentaux s'accroît. La tension entre les deux superpuissances reprend à compter de 1979 à la suite de l'invasion de l'Afghanistan et de l'arrivée de Ronald Reagan à la tête des États-Unis en 1980.
Mikhaïl Gorbatchev arrive au pouvoir en 1985 en prenant la tête du PCUS avec la volonté de réformer le régime pour combattre la stagnation économique et les reliquats du stalinisme, mais ses réformes donnent des résultats plutôt mitigés. La perestroïka (restructuration économique) n'a pas atteint les objectifs escomptés ayant aggravé les pénuries de biens de consommation et les inégalités sociales entraînant un mécontentement populaire, tandis qu'une démocratisation du régime, amorcée avec la glasnost (transparence), déclenche des conflits interethniques et la montée des nationalismes, mal perçus par les Russes. En 1986, le pays est confronté à la Catastrophe nucléaire de Tchernobyl.
Les élections législatives de 1989 sont marquées par l’émergence des réformateurs et des nationalistes. La même année, les troupes soviétiques se retirent d’Afghanistan. Le mur de Berlin chute le 9 novembre 1989 sans que l’URSS ne s’y oppose et dès 1990, les trois républiques baltes déclarent leur indépendance.
Vers 1991, un véritable dualisme du pouvoir s'installe au Kremlin - la puissance montante des structures étatiques russes libérées de la tutelle du PCUS, avec Boris Eltsine en tête, face aux organes du pouvoir soviétique et communiste, archaïque et conservateur, essayant en vain de freiner les réformes gorbatchéviennes et de préserver le système soviétique. Lors d'un référendum organisé le 17 mars 1991, 77,85 % des électeurs votent pour la préservation de l'Union soviétique. Un nouveau traité devait être signé entre les républiques le 20 août 1991. Cependant un coup d'État mené par les conservateurs le 19 août échoue mais empêche l'adoption du projet et accélère la chute du pays.
L'URSS s'effondre : les républiques qui la constituaient prennent leur indépendance, le Conseil d'assistance économique mutuelle créé en 1949 et le Pacte de Varsovie (1955) ne sont plus. Mikhaïl Gorbatchev démissionne le 25 décembre 1991. La fédération de Russie reprend de l'ancienne superpuissance mondiale les trois quarts de son territoire, plus de la moitié de sa population, les deux tiers de son industrie et la moitié de sa production agricole. État continuateur de l'URSS, elle occupe désormais sa place dans les institutions internationales, dont le siège permanent au Conseil de sécurité des Nations unies, mais assume également le passif financier de l'ancienne URSS. Une union politique et économique, la Communauté des États indépendants (CEI), est fondée quelques jours avant la démission de Gorbatchev pour tenter de maintenir des liens privilégiés entre les anciennes républiques soviétiques.
Bien que la Russie, dirigée par Boris Eltsine à partir de 1991, soit l’héritière de l'Union soviétique, elle ne peut endosser son rôle de superpuissance. Elle est en effet confrontée à de nombreux problèmes internes, parmi lesquels l'élaboration laborieuse d'un système politique démocratique et une guerre de sécession en Tchétchénie, et laisse la grande politique mondiale aux Américains et à leurs alliés.
Eltsine, le premier président de la Russie postsoviétique, donne une inflexion libérale au nouveau régime. La société russe, qui a dû abandonner le socialisme, est profondément bouleversée. Quelques oligarques construisent des fortunes. Mais le déclin de l'outil économique, l'affaiblissement de l'État fédéral provoquent une chute catastrophique du niveau de vie des Russes.
La transition vers l'économie de marché est alors apparue inéluctable pour la fédération russe née fin 1991. Deux approches économistes s'opposaient vis-à-vis des modalités de cette transition vers le capitalisme[23]:
Appuyés par les instances internationales (FMI, BERD, etc.), les partisans de la « thérapie de choc » (Jeffrey Sachs) l'emportèrent et conseillèrent le gouvernement russe.
À partir de 1992, la Russie privatisa massivement, la thérapie de choc étant mise en œuvre de façon complète à partir de 1994 : à cette époque, plus de 50 % du secteur public (112 625 entreprises d'État) avaient été privatisés. Au niveau économique, la planification dirigiste et centralisée de l'économie est ainsi abandonnée sans transition au profit d'un mode de fonctionnement s'inspirant des thèses libérales des économistes de l'école de Chicago. Les moyens de production ont été en grande partie privatisés, dans des conditions souvent obscures.
La transition rapide vers une économie de marché capitaliste provoque au cours des années 1990 un effondrement total de l'économie. Le PIB est divisé par deux en quelques années, et une crise financière majeure en 1998 plonge une grande partie de la population dans de graves difficultés (exceptée une infime minorité de nouveaux riches, surnommés « nouveaux Russes »). La privatisation assortie de l'ouverture des marchés des capitaux facilite la ruée des capitaux hors du pays.
Si l'on prend le seuil de pauvreté de 2 $ par jour, 23,8 % de la population vit désormais dans la pauvreté sous le nouveau régime capitaliste, contre seulement 2 % en 1989 sous le régime communiste[24]. Le chômage, qui s'élevait à moins de 0,1 % de la population active au début des années 1990, a grimpé à 0,8 % en 1992 et jusqu'à 7,5 % en 1994, quatre fois plus vite qu'en Biélorussie (0,5 % en 1992 et 2,1 % en 1994), qui a elle adopté une méthode plus graduelle de libéralisation.
Pour le prix Nobel d'économie Joseph E. Stiglitz, la thérapie de choc a été une grave erreur. Dans son livre, La Grande Désillusion il écrit ainsi : .
Lorsque la présidence de Boris Eltsine touche à sa fin, l'économie russe est au plus bas. Le PIB a baissé de 7,5 % par an en moyenne entre 1990 et 1998, à une époque où la Chine, autre pays en transition, connaissait un taux de croissance annuel moyen de 10 %.
De plus, l'armée est tenue en échec dans le conflit qui l'oppose aux séparatistes islamistes en Tchétchénie. Les élections de 1993 se traduisent par une montée du courant nationaliste (22,92 % des votes vont au Parti libéral-démocrate de Russie de Vladimir Jirinovski, contre 7,81 % en juin 1991) et le maintien d'un vote communiste important (12,40 % des votes, contre 16,85 % en juin 1991). Une nouvelle constitution, adoptée en décembre 1993 après une grave crise constitutionnelle et la mise au pas du Congrès des députés du peuple à l'aide de l'armée, donne un tour plus présidentiel au régime. La période est également caractérisée par de grands mouvements de population entre les États composant l'URSS (population russe des États voisins se repliant en Russie, émigration des Russes de religion juive ou d'origine allemande, fuite des cerveaux) et au sein même de la Russie (abandon des campagnes et des zones les plus éloignées en Sibérie). Le désordre économique et politique se prolonge jusqu'en 1998, date à laquelle le système financier russe s'effondre : entre 1990 et 1998 le PIB aura chuté de 45 %.
Vladimir Poutine, porté au pouvoir en 2000, se donne pour objectif de rétablir le fonctionnement de l'État et de l'économie par le biais d'un régime présidentiel fort. Le nouveau président bénéficie de l'envolée du cours des matières premières, dont la Russie est le plus grand producteur. Il lance des réformes structurelles visant entre autres à rétablir la « verticale des pouvoirs ». Des mesures ont été prises contre la fraude fiscale, ce qui s'est traduit par l'arrestation de certains oligarques. Depuis 2000, la Russie connaît une croissance forte (augmentation du PIB de 7 % en moyenne) étroitement liée à la montée des prix des matières premières et plus particulièrement du pétrole et du gaz. L'afflux de revenus qui en découle permet le développement du secteur tertiaire (banque, assurance, distribution) et la croissance de la consommation intérieure. Vladimir Poutine tente de redonner à la Russie un rôle de premier plan sur la scène internationale en profitant, entre autres, des déboires américains en Irak, et de renouer des liens privilégiés avec les anciennes républiques composant l'URSS en maniant alternativement la manière forte (Biélorussie, Ukraine) et une approche plus diplomatique. Son successeur, Dmitri Medvedev, élu en mars 2008, est plus libéral, mais continue d'appliquer la politique générale de Poutine. Par ailleurs, la guerre d'Ossétie en 2008 étend l'influence russe dans le Caucase, en particulier en Abkhazie et en Ossétie du Sud-Alanie. Vladimir Poutine lui succède à nouveau après l'élection présidentielle de mars 2012.
Vladimir Poutine est réélu au premier tour de l'élection présidentielle du 18 mars 2018.
En 2014, à la suite de l'annexion de la Crimée, le gouvernement de Vladimir Poutine est critiqué par les autres pays du G8 qui suspendent son adhésion au groupe, reformant ainsi temporairement le G7.
À la suite de l'invasion de l'Ukraine par la Russie en 2022, l'Occident adopte des sanctions contre l'économie russe qui entre en récession de 4 % au second semestre de 2022 d'après l'agence de statistique Rosstat[25]. L'Europe et les États-Unis livrent également de grandes quantités d'armement pour permettre à l'Ukraine de résister face à l'invasion[26].
La Constitution de 1993, adoptée à la suite de la crise constitutionnelle russe de 1993 qui avait opposé le président Boris Eltsine à l'Assemblée et n'avait pu être résolue que par l'intervention des chars, définit la Russie comme une fédération et une république présidentielle dans laquelle le président de la fédération, en tant que chef de l'État, dirige la Nation et le président du gouvernement dirige le gouvernement. Le pouvoir exécutif est exercé par le chef de l'État.
Le président de la fédération est élu au suffrage universel pour une période de six ans depuis 2012. Son mandat est renouvelable une seule fois. La dernière élection présidentielle a eu lieu le 18 mars 2018.
Le pouvoir législatif est représenté par l'Assemblée fédérale, un parlement bicaméral composée de :
La constitution russe garantit l'égalité de tous les citoyens devant la loi et l'indépendance des juges. Les procès doivent être publics et le droit de la défense est garanti aux accusés.
Selon la loi fédérale, le dirigeant d'un sujet de la fédération de Russie est élu soit par les citoyens de la fédération de Russie résidant dans ce sujet sur la base du suffrage universel, égal et direct, au scrutin secret, soit par les députés de l'organe législatif de ce sujet sur la proposition du président de la fédération de Russie, qui a aussi le droit de destituer le dirigeant d'un sujet de la fédération de Russie et d'en désigner un par intérim (jusqu'aux élections prochaines dans ce sujet de la fédération de Russie).
Les principaux partis sont le parti progouvernemental « Russie unie » (325 sièges à la Douma aux élections de 2021), le Parti communiste de la fédération de Russie (57 sièges), Russie juste (27 sièges), et le Parti libéral-démocrate de Russie (21 sièges). La majorité des trois quarts est nécessaire à la destitution du chef de l'État.
Le président de la Russie est Vladimir Poutine (réélu le 18 mars 2018, pour son quatrième mandat). Depuis son arrivée au pouvoir en 2000, la Russie a connu un recul démocratique jusqu'à devenir un État autoritaire et autocratique, certains allant jusqu'à la qualifier d'État totalitaire[2],[27],[1].
L'actuelle armée russe, formée en 1992, est l'héritière de l'ancienne Armée rouge qui fut l'Armée soviétique de 1922 à 1991, année de la dislocation de l'URSS. Elle a hérité de l'armement et de l'équipement de l'armée soviétique située sur le territoire russe, ainsi que de la totalité de l'arsenal nucléaire soviétique qui lui a été transféré par le Kazakhstan, l'Ukraine et la Biélorussie[réf. souhaitée].
La Russie est l'un des cinq pays reconnus officiellement par le Traité sur la non-prolifération des armes nucléaires (TNP) comme possédant l'arme nucléaire. Elle possède d'ailleurs le plus vaste arsenal nucléaire au monde avec plus de 16 000 têtes nucléaires[28] dont 3 500 sont opérationnelles. Au cours de son histoire, l'URSS aura produit quelque 50 000 têtes nucléaires[réf. nécessaire].
Après la chute de l'URSS, malgré la baisse des effectifs et du budget, l'armée russe reste une armée de premier plan à l'échelle mondiale[réf. souhaitée].
La Russie est en tête des exportations d'armes, devant la France (5,2 milliards de dollars) et derrière les États-Unis (26,9 milliards de dollars) avec un excédent de 7,2 milliards de dollars américains, émanant de son secteur de l'armement[29]. Les principaux clients de la Russie sont l'Inde et la Chine en tête, puis notamment l'Iran, le Venezuela et l'Algérie[30],[31].
La Russie considère devoir se protéger contre diverses « menaces » : indépendantistes au sein de la Russie, rivalités avec ses voisins de l'Ouest, d'Asie Mineure, du Japon, de Mongolie et de Chine. Elle surveille de près les détroits turcs pour accéder à la Méditerranée, le « verrou » danois pour accéder à l'océan Atlantique et à l'Est, le « verrou » japonais pour l'océan Pacifique, et l'Arctique notamment pour le pétrole[32].
Durant l'invasion de l'Ukraine, la Russie perd énormément d'équipements militaires au combat[34].
La Russie a des relations amicales avec les républiques d'Asie centrale, la Biélorussie, l'Azerbaïdjan et l'Arménie.
Ces relations sont plus froides avec l'Estonie, la Lettonie et la Lituanie, qui ont intégré l'Union européenne et l'OTAN en 2004, ainsi qu'avec la Géorgie où une guerre a eu lieu en 2008.
Les relations sont difficiles avec la Moldavie, en raison du problème de la république séparatiste prorusse de Transnistrie.
Les relations avec l'Ukraine, tendues depuis la révolution de Maïdan, sont devenues hostiles avec l'annexion de la Crimée par la Russie en 2014 et l'instauration des républiques séparatistes du Donbass, aboutissant à l'état de guerre avec l'invasion russe de février 2022. Les sanctions économiques contre la Russie depuis l'occupation de la Crimée et la guerre du Donbass ont été aggravées immédiatement après cette invasion par les États-Unis et l'Union européenne.
Depuis la chute de l'URSS, la Russie s'est engagée dans plusieurs conflits de la sphère ex-soviétique : guerre civile du Tadjikistan (1992-1997), conflit en Ossétie du Nord de 1992, guerre du Dniestr (1992), première guerre de Tchétchénie (1994-1996), invasion du Daghestan (1999), seconde guerre de Tchétchénie (1999-2009), deuxième guerre d'Ossétie du Sud (2008), guerre du Donbass (2014-2022), invasion de l'Ukraine (2022).
Vladimir Poutine a soutenu Bachar el-Assad dans la guerre civile syrienne en intervenant militairement.
D'après Taline Ter Minassian, professeure d'histoire contemporaine à l'INALCO, [35].
De l'égalité initiale, en l'an 1000, avec l'Europe[réf. souhaitée], le PIB par habitant russe n'a cessé de fléchir. Les Mongols rassemblés sous Gengis Khan ruinent son économie au XIIIe siècle. Les tsars fondent un empire fondé sur la puissance militaire et le féodalisme, mais ne peuvent « rattraper » le retard pris sur l'Europe. Les efforts entrepris par Moscou ont permis à l'époque soviétique quelques rebonds au XXe siècle, vite retombés lors de la dislocation de l'URSS, le PIB par habitant russe n'atteignant que 50 % du PIB franco-allemand et 40 % du PIB américain au début du XXIe siècle[36]. De son passé soviétique, la Russie a hérité d'une industrie métallurgique lourde puissante et concurrentielle, d'un savoir-faire pointu dans les domaines de l'aéronautique, de l'armement et de l'énergie[réf. souhaitée].
La Russie fait partie des pays économiquement développés : PIB de 2 056 milliards de $ en 2010 (nominatif), 2 097 milliards de $ (en parité de pouvoir d'achat[17], 7e rang en 2007). Son économie est marquée par le poids des industries extractives : gaz naturel (1er producteur et exportateur mondial), pétrole (1er producteur)[37], charbon (6e pays producteur), métaux non ferreux.
L'agriculture, longtemps handicapée par la collectivisation des exploitations agricoles sous le régime soviétique, malgré le labourage des terres vierges dans les années 1970, composant avec un environnement naturel globalement peu favorable et immense, est structurellement déficitaire (déficit en valeur de 10 milliards de $). Mais la Russie peut être considérée comme une puissance agricole forte – la Russie est le premier producteur mondial d'orge, de framboise, de groseille. Elle est aussi un gros producteur de betteraves, de blés et de pommes de terre.
La répartition du PIB (secteur primaire 7 % – secondaire 37 % – tertiaire 56 %) reflète la montée en puissance des services.
Le fonctionnement de l'économie russe a subi des transformations radicales après les réformes entamées par Gorbatchev dans la 2e moitié des années 1980 (perestroïka), caractérisées par le passage d'une économie planifiée (dont l'ensemble des moyens de production étaient contrôlés par l'État) à un mode de fonctionnement basé sur l'économie de marché.
Ce processus de transformation est à l'origine d'une crise économique profonde, culminant avec la crise financière en 1998, dont la Russie s'est progressivement relevée depuis : le PIB a retrouvé en 2007 son niveau de 1990. L'évolution du prix des matières premières a grandement favorisé la reprise économique amorcée en 1998. Avec une croissance du PIB supérieure à 6 % en moyenne depuis cette date, l'État russe a pu régler par anticipation les emprunts contractés au plus fort de la crise financière et ramener la dette publique à 8 % fin 2007[réf. souhaitée].
La Russie s'est constituée la troisième réserve de change du monde (504 milliards de $ en février 2012[réf. souhaitée]) grâce à une balance des paiements excédentaire de 10 % du PIB durant cette période[réf. souhaitée]. Le budget de l'État, régulièrement excédentaire grâce à une gestion prudente[réf. souhaitée] de la manne financière constituée par des rentrées fiscales plus efficaces[réf. souhaitée] et au prix assez élevé des hydrocarbures, a permis la constitution en 2004 d'un fonds de stabilisation qui se montait à 130 milliards de dollars en septembre 2007. L'État russe a retrouvé des moyens financiers permettant de lancer des projets d'envergure (infrastructures, soutien à l'investissement)[réf. souhaitée].
Des secteurs importants de l'industrie russe sont, depuis la libéralisation de l'économie, confrontés à la concurrence des entreprises étrangères : celle-ci n'est freinée que dans des domaines jugés stratégiques[réf. souhaitée] (construction automobile, ressources minières et énergétiques, industrie de l'armement). La Russie reste le deuxième exportateur mondial d'armes (avions de chasse, sous-marins, etc.)[29]. Mal préparée, l'industrie légère russe a vu ses parts de marché fondre sur le marché national. Le phénomène touche également des industries de pointe comme la construction aéronautique. Les exportations sont désormais en grande partie composées de produits à faible valeur ajoutée (hydrocarbures et métaux représentaient en 2005 82 % des exportations en volume et non en valeur). La croissance de cette économie peu diversifiée est très sensible aux évolutions du prix des matières premières[réf. souhaitée].
Le PIB par habitant s'élevait en 2007 à 12 200 $ et le taux de chômage à 6,6 % (2006). Mais ce PIB est très inégalement réparti. La libéralisation de l'économie a accentué un phénomène qu'avait jusqu'à présent contrebalancé le régime socialiste. La richesse s'est plutôt concentrée au cours de la décennie dans quelques régions favorisées : les deux métropoles de Moscou et Saint-Pétersbourg, les régions sibériennes où sont situés les gisements d'hydrocarbures et quelques régions industrielles (Tatarstan, Iekaterinbourg, Samara, etc.). La ville de Moscou concentre à elle seule 22 % du PIB russe[38].
Le taux de TVA est rehaussé de 18 % à 20 % en 2019[39].
En 2022, la Russie est classée en 47e position pour l'indice mondial de l'innovation[40].
Les statistiques officielles de la fédération de Russie reconnaissent trois formes d'exploitations agricoles. Les organisations agricoles, les fermes privées et les lopins de terre. La culture du blé et des pommes de terre en représente une large part. La Russie est cinquième au palmarès des producteurs mondiaux de céréales au milieu des années 2010, dominé par les États-Unis, car elle a très fortement augmenté ses récoltes entre 2012 et 2016[réf. souhaitée].
L'élevage porcin et de volaille est également très répandu. En revanche, l'élevage de bovins est essentiellement destiné à la production laitière[réf. nécessaire]. La betterave sucrière est également une réussite du secteur agricole. Sur les six premières années de la décennie 2010, le pays a confirmé sa huitième place au palmarès des grands producteurs mondiaux de sucre[41], grâce à une progression de près d'un cinquième des volumes de betterave récoltés.
Les conditions climatiques de la Russie ne lui permettent une mise en culture de ses terres que sur une période relativement courte (environ sept mois de l'année). La dimension de sa surface agricole utilisée et le facteur climatique permettent sans doute d'expliquer que son agriculture est plutôt extensive qu'intensive[réf. souhaitée].
Les variations paysagères et structurales de l'espace agricole russe se font largement suivant un gradient nord-sud défini essentiellement par le climat[réf. souhaitée]. Cette variation régionale est visible par le degré de mise en culture du territoire, par la densité de population et par la taille des bourgs[réf. souhaitée].
La Russie est riche en ressources énergétiques. Elle possède les plus grandes réserves de gaz naturel du monde (32 % des réserves prouvées, 23 % des réserves probables), ainsi que les deuxièmes plus grandes réserves de charbon (10 % des réserves prouvées, 14 % des réserves probables), les huitièmes pour le pétrole (12 % des réserves prouvées, 42 % des réserves probables), et 8 % des réserves prouvées d'uranium[42].
La production d'énergie de la Russie atteignait en 2011[43] un total de 1,31 milliard de Tep, dont 42 % pour le gaz naturel, 39 % pour le pétrole, 14 % pour le charbon ; le nucléaire (3,5 %) et les énergies renouvelables pèsent peu à côté de ces mastodontes, bien que la Russie compte plusieurs centrales hydroélectriques et nucléaires parmi les plus puissantes du monde.
La Russie était en 2012 le 2e producteur de pétrole et de gaz naturel du monde, 6e pour le charbon, 3e producteur d'électricité nucléaire en 2011 et 5e pour l'hydroélectricité[44].
Une part importante (45,6 %) de cette production est exportée : 48 % du pétrole, 30 % du gaz naturel et 45 % du charbon ; la Russie était en 2011 le 2e exportateur de pétrole du monde et en 2012 le 1er exportateur de gaz naturel et le 4e de charbon[43].
La consommation d'énergie de la Russie est très élevée : 5,15 Tep par habitant en 2011 (France : 3,88) et les émissions de gaz à effet de serre de la Russie étaient de 11,65 tonnes de CO2 par habitant (France : 5,04 tCO2/hab ; États-Unis : 16,94 tCO2/hab ; Chine : 5,92 tCO2/hab)[44].
La compagnie des chemins de fer russes (RJD) connecte toutes les villes importantes de Russie.
Le Transsibérien est le plus célèbre des trains russes. Le train rapide Allegro relie Helsinki à Saint-Pétersbourg depuis 2010. Le réseau ferroviaire régional se nomme « Elektritschka ».
Principales plates-formes de correspondances :
Compagnies aériennes :
L'industrie numérique russe connaît une croissance notable, grâce notamment à des entreprises comme Yandex, Kaspersky et Mail.ru Group, toutes trois connues à l'étranger.
On peut aussi citer VKontakte, un réseau social qui constitue en Russie une alternative à Facebook.
Dans le contexte de l'invasion de l'Ukraine, le gouvernement russe bloque l'accès aux réseaux sociaux Facebook et Instagram, utilisés par 62 millions de Russes[45].
Bien que la Russie soit signataire de la Déclaration universelle des droits de l'homme, sa gestion de la démocratie et des Droits de l'homme est très critiquée par les organisations internationales. Des ONG comme Amnesty International ou Human Rights Watch considèrent que la Russie n'a pas assez d'attributs démocratiques et n'accorde pas assez de droits politiques et de libertés civiles à ses citoyens[47],[48].
Les statistiques de la Cour européenne des droits de l’homme (CEDH) indiquent que la Russie est le pays d’Europe qui compte le plus grand nombre de violations des droits de l’homme. Loin devant la Turquie ou l’Ukraine[49].
Dans son rapport annuel de 2020, Amnesty International indique notamment que la torture est endémique et que les tortionnaires restent quasiment totalement impunis, que la modification de la législation a réduit l'indépendance de la justice, que les personnes LGBTI restent discriminées et persécutées et que de nouveaux éléments confirmant les allégations de crimes de guerre par les forces Russes en Syrie ont été apportés[47]. En 2021, Human Rights Watch affirme que la répression n'a jamais été aussi importante depuis la chute de l'URSS en 1991. La censure des médias d'oppositions, le harcèlement des manifestants pacifiques, les campagnes de diffamation contre les groupes indépendants, l'interdiction de nombreuses organisations étrangères font partie des griefs portés par l'ONG[48].
La liberté de la presse est très limitée, avec notamment une télévision d'État entièrement sous contrôle[49]. En 2022, la Russie est classée 155e sur 180 par le classement mondial de la liberté de la presse de Reporters sans frontières[50].
En ce qui concerne la corruption, la Russie est classée, en 2021, 139e sur 180 pays par Transparency International avec un indice de perception de la corruption de 29 sur 100, ce qui en fait le pays le plus corrompu du continent européen[46],[51].
Cela s'explique en partie par l’économie de réseau héritée de la période des privatisations des années 1990. Cette distribution des ressources et des entreprises a façonné les rapports de force dans les plus hautes sphères du pouvoir en Russie au profit de certains oligarques faisant désormais partie intégrante de la réalité socio-économique et politique du pays. De plus le manque de certains attributs d’un État de droit (justice indépendante, liberté d’association et de presse) laisse une forme d’anomie autoritaire se dessiner en Russie, affaiblissant la constitution d’un appareil déjà déficient de lutte contre la corruption[52].
Depuis l'invasion de l'Ukraine commencée le 24 février 2022, la Russie est accusée d'attaques constitutives de crimes de guerre[53]. Elle est suspendue du Conseil de l'Europe le 25 février 2022, puis son exclusion est votée à l'unanimité le 16 mars parce qu’elle ne respecte plus les valeurs de l'organisation, ni aucun des engagements qu’elle a pris lors de son adhésion. Les citoyens russes n'ont désormais plus accès à la CEDH, les recours russes représentant un quart des cas (dont par exemple, celui de l'activiste anti-corruption Alexei Navalny)[54].
L'ONG Global Initiative Against Transnational Organized Crime publie en avril 2022 un rapport qui relève qu'à la suite des sanctions économiques et financières qu'elle a subies après l'invasion de l'Ukraine, la Russie, avec l'aide des Émirats arabes unis (EAU), utilise son influence en Afrique de l'Ouest pour exploiter les marchés illicites de l'or afin de générer des bénéfices et déplacer des capitaux, ceci malgré la condamnation par les EAU de l'invasion de l'Ukraine. Le marché de l'or des EAU, qui a augmenté pour devenir l'un des plus importants au monde au cours de la dernière décennie, conserve une réglementation inadéquate susceptible de permettre un commerce non éthique[55].
Le 26 septembre 2022, un rapport du Comité pour la protection des journalistes révèle que des journalistes russes ont été arrêtés alors qu'ils couvraient des manifestations de protestation contre la « mobilisation partielle » des réservistes annoncée par Vladimir Poutine le 21 septembre[56].
La population de la Russie s'établit approximativement à 146,17 millions d'habitants en 2021[3], avec un taux d'urbanisation élevé (73 % de la population). La densité est de 8,5 hab./km2, mais la population est très inégalement répartie sur le territoire : de 26,9 en Russie d'Europe (Oural compris) elle tombe à 2,5 en Russie d'Asie. L'urbanisation tend à dépeupler la « Russie profonde » au profit de grandes métropoles et plus particulièrement des villes de la Russie européenne.
Après la Seconde Guerre mondiale, qui avait entraîné la mort d'environ 27 millions de personnes[57] (civils et militaires), la population avait retrouvé son niveau d'avant-guerre en 1955 (111 millions), puis s'était accrue de près de 35 % en atteignant son maximum en 1992 (148,7 millions). Cependant plusieurs phénomènes sont venus modifier cette dynamique démographique dont la plus importante est sans doute la « normalisation » de la fécondité russe qui a effectué à compter de 1988 sa transition démographique et présente désormais un taux de natalité proche de celui des autres pays d'Europe de l'Est, c'est-à-dire très bas.
La population de la Russie augmente depuis 2009 du fait de l'immigration, d'une hausse de la natalité et d'une baisse de la mortalité. En 2013, le taux de natalité s'établit à 13,3 ‰ tandis que le taux de mortalité s'élève à 13,1 ‰. L'indice de fécondité est de 1,7 enfant par femme. Depuis 2007, pour enrayer la diminution de la population, l'administration Vladimir Poutine octroie un capital maternité de 267 500 roubles (environ 6 300 euros) à la naissance du second enfant. Depuis 2019, la population russe rebaisse.
Le déficit naturel est en partie compensé par des flux migratoires en provenance des pays issus de l'éclatement de l'URSS. L'immigration, qui était dans les années 1990 essentiellement le fait de russophones, a aujourd'hui des origines plus mélangées (immigration chinoise et ouzbèke)[réf. nécessaire]. En 2008, la Russie comptait quelque 10 millions d'immigrés[58]. La crise économique, l'augmentation du chômage et la redéfinition de l'identité russe provoquent une montée de la xénophobie dans le pays : 74 meurtres à caractère raciste ont été recensés en 2007, 114 en 2008[58], ce qui est à mettre en perspective avec les statistiques inférieures des autres pays européens connaissant désormais eux aussi ce phénomène. Par ailleurs, le courant d'émigration en direction d'Israël, des États-Unis et de l'Allemagne, très important durant les années 1990, s'est aujourd'hui pratiquement tari et fut bien inférieur à certaines prévisions[59].
L'espérance de vie est inférieure à la moyenne européenne pour les femmes (75 ans) mais l'est surtout pour les hommes : pour ceux-ci, l'âge moyen au décès est de 63 ans (inférieur de 9 ans à la moyenne européenne et de 14 ans à la moyenne française) soit un taux de mortalité de 15 ‰ pour un taux de natalité de 9 ‰. L'espérance de vie a connu une chute dramatique pendant la période de chaos politique et économique des années 1990, à la suite de la disparition de l'Union soviétique. Cela s'explique par divers facteurs : l'alcoolisme de masse, le suicide, un système de santé déficient qui ne réussit pas à stopper le développement rapide du SIDA et la tuberculose[60]. Ainsi, la Russie a connu pendant la crise de la période de transition quatre fois plus de morts violentes que les États-Unis[60] : en effet, elle se classait à l'époque au deuxième rang mondial pour les homicides (28,4 pour 100 000 habitants en 2000[61]) et troisième pour les suicides (38,4 pour 100 000 habitants en 2002[62]). L'arrivée, plus tardive qu'à l'Ouest, de certaines épidémies comme le sida explique aussi la situation : à la fin de 2005, la Russie enregistrait près de 350 000 infections au VIH[63]. L'alcoolisme en Russie était un problème de santé publique de premier plan. Un plan gouvernemental restreignant la vente d'alcool, en augmentant le prix et en interdisant la publicité, a fait baisser la consommation de 43 % entre 2003 et 2016[64],[65]. Cette baisse a contribué à la hausse de l'espérance de vie, qui a atteint un niveau record en 2018, pour s'établir à 78 ans pour les femmes et 68 ans pour les hommes. Au début des années 1990, l'espérance de vie des hommes n'était que de 57 ans[64].Face à cette situation, le gouvernement russe a inscrit dans son programme la mise en place d'une politique nataliste reposant sur des incitations financières pour la naissance des 2e et 3e enfants. Ainsi, en 2009, la population russe a augmenté pour la première fois depuis 1995, sous l'effet conjugué depuis quatre ans d'une remontée de la natalité et d'une baisse de la mortalité[66].
Après le rattachement de la Crimée à la fédération le 18 mars 2014, la population russe a subitement augmenté d'environ deux millions d'habitants, portant le total à environ 146,5 millions d'habitants.
Malgré la faiblesse de la densité moyenne, la Russie est un pays fortement urbanisé : près des trois quarts des Russes (73 %) résident en ville, soit 106,5 millions de ses habitants au sein d'environ 1 100 villes et 1 400 bourgs. Quelque 20 % des Russes se concentrent dans des villes de plus d'un million d'habitants et 45 % dans des zones urbaines de plus de cent mille âmes[67].
Le russe est la langue d'État de la fédération. Par ailleurs 37 langues ont un statut de langue d'État dans les républiques, et 15 autres langues ont un statut officiel. Un grand nombre d'autres langues sont parlées en Russie.
La Russie compterait en 2014, environ 77 % de croyants (dont 70 % de chrétiens)[68].
Le dénombrement des pratiquants est toutefois difficile et le CIA Factbook donnait l'évaluation suivante en 2006[17] :
Le taux d'alphabétisation est dans la moyenne des pays de l'OCDE : 99,7 %  en 2019[69].
Les populations n'appartenant pas à l'ethnie russe sont souvent bilingues (exemples : russe et tatar, russe et oudmourte, russe et iakoute, russe et arménien).
Entre 2003 et 2008, le taux de scolarisation brut pour les hommes et femmes est de 96 % et le taux de scolarisation net est de 91 %. Dans un cadre éducatif, 21 % des enfants scolarisés utilisent un accès à Internet, en 2007[réf. souhaitée]. Le taux de suivi en dernière année d'école primaire est de 99 %, entre 2003 et 2008, d'après les données administratives russes[réf. souhaitée]. D'après l'Unicef, le taux de scolarisation en secondaire est de 85 % pour les hommes et de 83 % pour les femmes entre 2003 et 2008[70].
La littérature russe prend son essor à Saint-Pétersbourg avec Alexandre Pouchkine, qui est considéré comme l'un des fondateurs de la littérature moderne russe et est parfois surnommé le « Shakespeare russe ». Parmi les poètes et écrivains russes les plus célèbres figurent Nicolas Gogol, Mikhaïl Lermontov, Fiodor Dostoïevski, Léon Tolstoï et Anton Tchekhov. Les écrivains les plus marquants de la période soviétique sont Boris Pasternak, Alexandre Soljenitsyne, Vladimir Maïakovski, Mikhaïl Cholokhov et les poètes Evgueni Evtouchenko et Andreï Voznessenski.
Un grand nombre de groupes ethniques vivant en Russie ont des traditions folkloriques très variées. La musique russe du XIXe siècle est caractérisée par l'existence de deux courants musicaux : celui représenté par le compositeur Mikhaïl Glinka et ses successeurs, dont le Groupe des Cinq, qui ont inclus des éléments folkloriques et religieux dans leurs compositions et la Société musicale russe dirigée par Anton et Nikolaï Rubinstein aux accents plus traditionnels. La tradition du romantisme tardif incarnée par Tchaïkovski ou encore Nikolaï Rimski-Korsakov (bien qu'également successeur de Glinka), fut prolongée au XXe siècle par Sergueï Rachmaninov, l'un des derniers grands compositeurs de musique romantique.
Les compositeurs du XXe siècle de renommée mondiale comprennent Alexandre Scriabine, Igor Stravinsky, Sergueï Rachmaninov, Serge Prokofiev et Dmitri Chostakovitch. À l'époque soviétique, la musique était sous surveillance constante du régime, car elle était un moyen d'éduquer les masses socialistes, et elle ne devait pas être influencée, selon la propagande officielle, « par la décadence bourgeoise ». Les conservatoires de Russie ont produit des générations de solistes de renommée mondiale. Parmi les plus connus figurent les violonistes David Oïstrakh, Leonid Kogan et Gidon Kremer, le violoncelliste Mstislav Rostropovitch, les pianistes Vladimir Horowitz, Sviatoslav Richter et Emil Gilels et la cantatrice Galina Vichnevskaïa.
Tchaïkovski composa des ballets connus dans le monde entier comme Le Lac des cygnes, Casse-Noisette et La Belle au bois dormant. Au début du XXe siècle, les danseurs russes Anna Pavlova et Vaslav Nijinski devinrent célèbres et les déplacements à l'étranger des Ballets russes influencèrent fortement le développement de la danse dans le monde. Le ballet soviétique a préservé à la perfection les traditions du XIXe siècle et les écoles de chorégraphie de l'Union soviétique ont fait naître de grandes étoiles, admirées partout comme Maïa Plissetskaïa, Rudolf Noureev et Mikhaïl Barychnikov. Le ballet du Bolchoï à Moscou et le celui du Mariinsky à Saint-Pétersbourg sont universellement prisés.
Alors que le cinéma a souvent été considéré comme une forme de divertissement bon marché à destination des classes populaires, la production cinématographique en Russie a eu dès 1917 un rôle culturel important : immédiatement après la révolution de 1917, le cinéma soviétique a exploré les possibilités et les limites du montage avec par exemple des films comme Le Cuirassé Potemkine. Le régime utilisait cet art pour former les masses, mais il tenta cependant de le faire avec des formes nouvelles et une grande créativité. Des réalisateurs soviétiques comme Sergueï Eisenstein et Andreï Tarkovski marquèrent leur époque et eurent une grande influence sur les cinéastes contemporains. Eisenstein fut l'élève du metteur en scène et théoricien Lev Koulechov, qui mit au point les principes du montage cinématographique dans la première école du cinéma créée au monde, l'institut du cinéma de l'Union à Moscou. En 1932, Staline promulgua le réalisme socialiste soviétique comme fondement de l'art soviétique, ce qui freina la créativité, mais beaucoup d'œuvres produites à cette époque sont des réussites artistiques comme Tchapaev, Quand passent les cigognes et la Ballade du soldat.
Le cinéma soviétique fut en crise dans les années 1980 et 1990. Les réalisateurs russes n'étaient plus obligés d'affronter la censure, mais les réductions des subventions d'État ne leur permettaient de produire qu'un nombre réduit de films. Le début du XXIe siècle quant à lui se caractérisa par un accroissement des entrées en salle et en conséquence une prospérité accrue de l'industrie cinématographique.
L'art de la vidéo est très populaire dans la Russie moderne. La Russie est l'un des marchés prioritaires pour YouTube[71]. L'épisode le plus populaire de la série animée russe Masha et Michka a plus de 3 milliards de vues[72]. La chaîne « +100500 », qui héberge des critiques de vidéos pour des vidéos amusantes et BadComedian, qui fait des critiques pour des films populaires, est particulièrement populaire. De nombreuses bandes-annonces de films russes ont été nominées aux Golden Trailer Awards[73],[74]. Beaucoup de vidéos de Nikolay Kurbatov (en), éditeur russe, poète et publiciste, le fondateur de bande-annonce de poétique et de construction de la bande-annonce[75] ont été téléchargés sur les grandes chaînes YouTube, ont été utilisés comme bandes-annonces principales et entrés dans le livre des records[76],[77].
Voir aussi : Culture russe (catégorie) – Musique russe (catégorie) – Théâtre russe (catégorie) – Cinéma russe (catégorie) – Théâtre russe – Cinéma russe et soviétique
Depuis la christianisation de la Rus' de Kiev, l'architecture russe a été influencée par l'architecture byzantine pendant de nombreuses années. Outre les fortifications (kremlins), les principaux bâtiments en pierre de l'ancienne Rus' étaient des églises orthodoxes avec leurs nombreux dômes, souvent dorés ou peints de couleurs vives.
Aristotile Fioravanti et d'autres architectes italiens ont introduit la mode de la Renaissance en Russie depuis la fin du XVe siècle, tandis que le XVIe siècle vit le développement des églises uniques en forme de tente (chatior) culminant dans la cathédrale Sainte-Basile-le-Bienheureux[78].
À cette époque-là, la conception du dôme en forme d'oignon, ou clocher à bulbe, était complètement développée[79].
Au XVIIe siècle, le « style ardent » de l'ornementation a prospéré à Moscou et à Iaroslavl, ouvrant progressivement la voie au Baroque Narychkine dans les années 1690. Après les réformes de Pierre Ier le Grand, le changement de style architectural en Russie a généralement suivi celui de l'Europe occidentale.
Le goût du XVIIIe siècle pour l'architecture rococo a conduit aux œuvres ornées de Bartolomeo Rastrelli et de ses disciples. Les règnes de Catherine II et de son petit-fils Alexandre Ier ont vu fleurir l'architecture néoclassique, notamment à Saint-Pétersbourg, capitale de la Russie à cette époque. La seconde moitié du XIXe siècle a été dominée par les styles néo-byzantin et néo-russe. Les styles prédominants du XXe siècle étaient l'Art nouveau, le constructivisme russe et l'architecture stalinienne.
Durant la période stalinienne, la tradition de préservation a été brisée. Les sociétés de préservation indépendantes, même celles qui ne défendaient que des sites séculiers tel l'OIRU basé à Moscou, ont été dissoutes à la fin des années 1920. Une nouvelle campagne antireligieuse, lancée en 1929 coïncide avec le collectivisme agricole ; la destruction des églises dans les villes atteint un sommet vers 1932. Un certain nombre d'églises ont été démolies, dont la Cathédrale du Christ-Sauveur de Moscou.
À Moscou seulement, les disparitions d'édifices notables de 1917 à 2006 sont estimées à plus de 640 (dont 150 à 200 bâtiments classés, sur un inventaire total de 3 500) - certains ont complètement disparu, d'autres ont été remplacés par des répliques en béton.
En 1955, un nouveau leader soviétique, Nikita Khrouchtchev, a condamné les « excès » de l'ancienne académique d'architecture et la fin de l'ère soviétique a été dominée par le fonctionnalisme simple en architecture. Cela a quelque peu aidé à résoudre le problème du logement, mais en créant une grande quantité de bâtiments de faible qualité architecturale, contrastant significativement avec les styles lumineux antérieurs. En 1959, Nikita Khrouchtchev a lancé la campagne antireligieuse. En 1964, plus de 10 000 églises sur 20 000 ont été fermées (principalement dans les zones rurales) et beaucoup ont été démolies. Sur 58 monastères et couvents en activité en 1959, seulement 16 restaient en 1964, sur 50 églises de Moscou en activité en 1959, 30 ont été fermées et 6 démolies.
Outre ces jours fériés, il existe un grand nombre de fêtes de corporations (Профессиональные праздники). Ces jours ne sont pas chômés, mais les plus importants sont célébrés officiellement (12 avril : journée de la cosmonautique ; 28 mai : jour des gardes-frontières ; 5 octobre : jour des enseignants ; 10 novembre : jour de la police…).
La Russie a pour code :
Bangkok (en thaï : กรุงเทพฯ, Krung Thep, « Cité des anges » ou บางกอก), officiellement Krung Thep Maha Nakhon depuis 2022, est la capitale de la Thaïlande. La ville a également le statut de province. Les Thaïlandais l'appellent communément et plus simplement Khrung Thep.
La ville occupe une superficie de 1 569 km2 dans le delta du fleuve Chao Phraya en Thaïlande centrale et son nombre d’habitants est supérieur à 9 millions, plus de 15 millions de personnes habitent l’aire métropolitaine de la capitale, soit plus que tous les autres centres urbains du pays.
Les racines de Bangkok remontent à un petit comptoir commercial créé durant le royaume d'Ayutthaya au XVe siècle au bord du fleuve Chao Phraya qui prend de l’importance avant de devenir le site d’une première capitale, Thonburi, en 1768. Mais la date officielle de sa fondation par Rama Ier, premier roi de la dynastie Chakri, est le 6 avril 1782, sur l’autre rive du fleuve. Bangkok s’inscrit au XIXe siècle au cœur du mouvement de modernisation du royaume de Siam, alors que le pays subit la pression des nations colonisatrices européennes. La ville est ensuite au XXe siècle le théâtre de l’évolution politique de la Thaïlande, notamment avec l’abolition de la monarchie absolue, l’adoption d’une constitution, et plusieurs soubresauts politiques parfois violents. La ville a connu une formidable croissance à partir des années 1960 et exerce aujourd’hui une influence centrale sur la vie politique, économique, culturelle, universitaire et médiatique de la Thaïlande[1].
Le boom économique asiatique des années 1980 et 1990 a amené beaucoup d’entreprises multinationales à installer leur siège régional à Bangkok. La ville est un important pôle d’affaires. C’est également une plateforme internationale pour les transports et la santé, tout comme pour les arts, la mode, les spectacles et le tourisme. Bangkok fait partie des villes les plus visitées au monde.
Il existe plusieurs explications pour le nom de « Bangkok » :
Néanmoins le nom entier composé à partir de deux anciennes langues indiennes, le pali et le sanskrit[4] donne :
กรุงเทพมหานคร อมรรัตนโกสินทร์ มหินทรายุธยา มหาดิลกภพ นพรัตน์ราชธานีบุรีรมย์ อุดมราชนิเวศน์มหาสถาน อมรพิมานอวตารสถิต สักกะทัตติยะวิษณุกรรมประสิทธิ์
« Krungthep mahanakhon amon rattanakosin mahintara ayuthaya mahadilok phop noppharat ratchathani burirom udomratchaniwet mahasathan amon piman awatan sathit sakkathattiya witsanukam prasit »
« Ville des dieux, grande ville, résidence du Bouddha d’émeraude, ville imprenable du dieu Indra, grande capitale du monde ciselée de neuf pierres précieuses, ville heureuse, généreuse dans l’énorme Palais Royal pareil à la demeure céleste, règne du dieu réincarné, ville dédiée à Indra et construite par Vishnukarn. »
Le Livre Guinness des records le note comme le nom de lieu le plus long au monde[5].
Une signification est souvent proposée pour le terme Krungthep (écouter)  « Ville des anges ». Thep signifie « ange » ; « fée » ; « dieu » ; « déité » ; « être divin »...
Krungthep mahanakhon (กรุงเทพมหานคร) ou Phra Nakhon, c’est-à-dire « la capitale », est le titre couramment abrégé de la ville en tant que capitale royale. Ainsi, la ville Ayutthaya, qui fut également capitale du Siam, avait pour nom complet Krungthep mahanakhon bawonthawarawadi si ayutthaya mahadilok phopnoppharatana ratchathani burirom.
L’appellation courante de Krungthep ou Krungthep Mahanakhon est reflétée dans un chant populaire (กรุงเทพมหานคร/Krungthep Mahanakhon par อัสนี-วสันต์ โชติกุล/Asanee-Wasan Chotikul).
L'historien Chris Baker, spécialiste de la Thaïlande, explique que l'on peut utiliser indifféremment le nom international ou le nom thaï de la capitale mais il précise : « Krung Thep Maha Nakhon évoque une histoire très royale, alors que Bangkok ne veut pas dire grand chose. C'était le nom d'un village, un nom prosaïque et très typique.»[6]
En 2022, un changement de ponctuation dans l'appellation officielle lance une rumeur prétendant que la ville ne s'appellerait plus Bangkok mais uniquement Krung Thep Maha Nakhon (thaï : กรุงเทพมหานคร, « la grande cité des anges »)[7]. À la suite de nombreuses incompréhensions, les autorités précisent alors que la ville s'appelle toujours Bangkok en alphabet latin[8],[9].
Bangkok était un village situé sur la rive est du fleuve Chao Phraya. Il fut occupé par les troupes françaises le 16 octobre 1687[10], puis cédé officiellement par traité le 1er décembre par le roi d’Ayutthaya Narai. Cependant, durant son agonie, en juin 1688, son successeur Phetracha fit le siège de la forteresse de Bangkok, réussissant à les en expulser en novembre (siège de Bangkok).
Après la destruction par les Birmans de la capitale Ayutthaya en 1767, le général Taksin, devenu roi, se replia en aval beaucoup plus au sud près de l'embouchure du Chao Phraya sur le site de Thonburi et décida d’y fonder la nouvelle capitale. Taksin fut déclaré fou et renversé par un coup d’État. Il fut exécuté le 6 avril 1782 et remplacé par le général Chakri qui, sous le nom de Phra Yot Fa, devint le premier roi de la nouvelle dynastie Chakri, qui règne encore aujourd’hui. Phra Yot Fa reçut à titre posthume le nom de « Rama Ier » de son successeur Rama III dans le cadre du système d’appellation royale utilisé aujourd’hui. Il décida, pour des raisons de défense, de transférer la capitale sur la rive gauche à Bangkok et commença des constructions fortifiées, aujourd’hui le Grand Palais (Palais royal). Le 6 avril 1782 est retenu comme date de fondation de la ville-capitale Krungthep (Bangkok)[11].
Le nom de Bangkok est utilisé par les Thaïlandais dans les documents officiels en anglais[12], mais le nom qui est utilisé dans le documents officiels en thaï est กรุงเทพมหานคร (Krungthep Mahanakhon).
De 1782 aux années 1850, l'essentiel de la surface actuelle de la ville était couverte de champs, de rizières et de khlongs ou klongs (canaux). Bangkok était une ville flottante : hormis les palais et les pagodes, il n'y avait quasiment pas de maisons construites sur la terre ferme ; la ville était principalement concentrée sur le fleuve et les canaux, sur des maisons flottantes ; ainsi, on comptait par exemple jusqu'à quatre rangées de maisons flottantes de chaque côté du fleuve.
Bangkok était alors surnommée "La Venise de l'Orient".
À partir des années 1850, Bangkok s'ouvre largement aux étrangers : ceux-ci construisent sur la terre ferme des bâtiments coloniaux comme l'ambassade de France (1857) ou l'hôtel Oriental (1876)... et certains quartiers que nous connaissons aujourd'hui surgissent eux aussi de terre : les principales artères du quartier chinois (Sampeng, Yaowarat), le quartier de Silom... La première vraie rue pavée est tracée en 1861 : c'est Charoen Krung appelée "New Road".
Au début du XXe siècle, la population de Bangkok est de 500 000 à 600 000 habitants. Ils se déplacent essentiellement à pied ou en bateau ou en calèche et utilisent aussi comme transport en commun le tramway électrique (construit en 1903). La voiture est quasi-inexistante : en 1908, on ne totalisait qu'environ 300 automobiles. Des années 1920 à 1950-1960, l'habitat est principalement constitué des fameux "compartiments chinois" (en anglais : shophouse) de deux, trois ou quatre niveaux : boutique sur la rue, entrepôt et séjour à l'arrière ; et appartements dans les étages supérieurs[13].
Après la seconde guerre mondiale, à partir des années 1950, la capitale se peuple massivement et s'étend. On construit des immeubles et les canaux sont progressivement comblés pour y construire à la place des rues. Le tramway est définitivement abandonné en 1965 pour faire place à l'ère du tout voiture.
Dans les années 1970-1980 émerge de la société une classe moyenne qui va vivre en banlieue dans des lotissements de maisonnettes avec jardinets, toutes construites sur le même modèle en dizaines, centaines voire milliers d'exemplaires ; et on construit aussi des HLM pour reloger la population des bidons-villes.
Dans les années 1980 apparaissent les condominiums, logements de luxe pour les étrangers et les premiers gratte-ciel pour les bureaux[14].
La population officielle de Bangkok en 2010 est de 8,25 millions d’habitants pour la ville elle-même et de 14,6 millions d’habitants pour la métropole, appelée « Grand Bangkok ». Ce nombre est toutefois considéré comme très sous-estimé, puisqu’il ne prend en compte que les habitants de la ville officiellement enregistrés dans celle-ci et non leur nombre réel (bon nombre de Thaïlandais restent enregistrés dans leur ville de naissance). Les estimations les plus sérieuses, bien que spéculatives, estiment que la population vivant dans les limites de la métropole de Bangkok est de 18 millions d’habitants[réf. nécessaire].
Bangkok vue par satellite en 1988
Bangkok vue par satellite en 2022
La superficie de cette province est de 1 568,7 km2, dont l’essentiel est considérée comme constituant la ville de Bangkok, ce qui en fait une des plus grandes villes du monde. La superficie de la métropole s’étend elle sur 7 762 km2. La capitale thaïlandaise se trouve à 532 km à l'ouest-nord-ouest de Phnom Penh, à 577 km au sud-est de Rangoun, à 980 km au sud-ouest d'Hanoï, à 1 181 km au nord de Kuala Lumpur et à 2 917 km à l'est-sud-est de New Delhi, la capitale indienne.
Bangkok est traversée par le fleuve Chao Phraya, qui détermine deux grandes zones, la rive droite (l’ancienne Thonburi) restée plus traditionnelle, parcourue de nombreux khlong, canaux reliés au fleuve qui avaient valu à la ville le surnom de , et la rive gauche, plus développée, où se trouvent presque tous les attraits touristiques, le centre des affaires, le réseau de métro et les grandes tours modernes.
Bangkok n’est située qu’à 2 mètres au-dessus du niveau de la mer, ce qui provoque des inondations en période de mousson. De plus, construite sur une zone autrefois marécageuse, la ville s’enfoncerait de 1, voire 2 cm par an[15]. La modernisation de la ville entraîne un bétonnage du sous-sol et l’assèchement des khlong. L’eau arrivant en aval, par la Chao Phraya et les autres rivières, s’écoule difficilement à travers la ville, qui ne peut désormais plus absorber le débit du fleuve, en périodes de fortes moussons, sans inondations. Ce phénomène est aggravé par la quantité considérable de déchets plastiques qui se déversent dans Bangkok[16], rendant le système de drainage insuffisant et endommageant les machines des stations de pompages[17],[18]. Autres causes aggravant l'affaissement de Bangkok : le pompage d'eau dans la nappe phréatique, le poids des immeubles[19].... Selon un rapport publié par l'OCDE, Bangkok se classe septième parmi les 136 villes du monde les plus menacées par les inondations côtières d'ici à 70 ans[20].
Cette situation commence à devenir problématique. En 2011, Bangkok a connu une période d’inondations sans précédent due à de fortes moussons[21]. Pendant près de six mois, toutes ses rues ont été partiellement inondées et les autorités ont été obligées d’entreprendre dans l’urgence de gigantesques travaux de déviation des cours d’eau, en périphérie de la ville ; 10 % des 12 millions de résidents de la capitale ont décidé de quitter la ville etc. Bangkok semble vouée à un sort similaire à Venise, protégée de façon très précaire par des digues et des barrages[22].
En 2016, les moussons provoquent une nouvelle fois des inondations, bloquant notamment l’accès à l’aéroport Don Muang et empêchant environ 200 passagers de prendre leur vol. Le premier ministre Prayut Chan-o-cha mobilise la police pour fluidifier la circulation et porter assistance aux automobilistes en difficulté et promet des mesures à long terme pour lutter contre les inondations[23].
La pollution de l'air est essentiellement causée par les émissions de dioxyde de carbone et de micro particules fines des véhicules même si l'activité industrielle, les chantiers et les crémations ainsi que le brûlage des cultures[24] sont des sources non négligeables[25]. Il faut aussi ajouter les pluies acides dues au dioxyde de soufre et au dioxyde d'azote etc. En 2004, un rapport officiel informait que 900 000 personnes souffraient à Bangkok d'insuffisance respiratoire ; et depuis, les épisodes de pollutions de l'air sont de plus en plus fréquents[26] : par exemple, en mars 2023, la capitale est noyée pendant plusieurs jours dans un smog toxique, un brouillard opaque qui présente un taux de particules fines nettement supérieur au seuil de prévention et qui incite la population à porter le masque dehors ; des dizaines de milliers de personnes sont hospitalisées[27]...
La pollution de l'eau a transformé la "Venise de l'Orient" (comme la Venise d'Italie) en une vaste fosse septique : en effet, en 2004, la plupart des eaux usées de la ville étaient déversées directement sans traitement dans les canaux (khlongs) et le fleuve Chao Phraya. S'ajoute la pollution par les plastiques, les insecticides et pesticides etc. Pour améliorer la qualité de l'eau et ramener l'eau à la vie, une vaste programme de stations d'épuration est mis en place, des opérations d'assainissement des canaux sont régulièrement organisées etc.
La pollution des sols due à la surconsommation est aussi importante : en 2022, chaque citoyen de Bangkok produit 4 kg de déchet par jour soit 1 460 kg par an (contre 2,2 kg par jour soit 803 kg par an à Singapour[28] et 580 kg par an en France selon l'ADEME[29] (dont 254 kg recyclés)). Ces ordures sont souvent stockées en vrac dans des décharges légales ou illégales. Le thème écologique des "montagnes" de déchets plastiques à Bangkok, en particulier des bouteilles, est par exemple abordé dans le film thaïlandais Citizen Dog. Le tri des déchets n'en est qu'au stade de l'expérimentation[30],[31]
Le climat de la ville est un climat tropical marqué par deux saisons : la saison sèche de novembre à avril et la saison des pluies de mai à octobre. En 2017, Bangkok a été nommée capitale la plus chaude du monde avec une moyenne de 28 °C.
La ville de Bangkok est divisée en 50 circonscriptions nommées khet  (thaï : เขต ; anglicisme : district ; français : quartier, arrondissement).
La ville est dirigée par un gouverneur, personnage important, élu pour quatre ans. Son administration métropolitaine est aussi dirigée par un conseil métropolitain, et aussi renouvelée tous les quatre ans.
Les élections de janvier 2009 puis de mars 2013 ont été remportées par Sukhumbhand Paribatra, membre du Parti démocrate ; en octobre 2016, la junte militaire ayant effectuée le coup d'état de 2014 remplace le gouverneur de Bangkok par Aswin Kwanmuang, un ancien policier[32] ; L'élection de mai 2022 est remportée par Chadchart Sittipunt, ancien membre du Parti Pheu Thai de 2012 à 2019[33],[34],[35],[36].
Bangkok est désormais le centre d’une zone de co-urbanisation le Bangkok Metropolitan Area (en thaï : กรุงเทพมหานครและปริมณฑล), ou Grand Bangkok, qui regroupe les cinq provinces adjacentes.
La ville possède essentiellement près de 400 temples bouddhistes[37], tous postérieurs à la fin du XVIIIe siècle :
Le grand Bouddha couché.
Temples dans l’enceinte du Grand Palais.
Un cycliste défile à coté du Wat Phra Kaeo (Bangkok). Septembre 2022.
Bangkok est le siège de nombreuses universités, dont les plus connues sont l’université Thammasat, l'université de Kasetsart et l’université Chulalongkorn.
Il existe également l’AIT Bangkok, c’est-à-dire l’Asian Institute of Technology fondée en 1959, elle est basée au nord de Bangkok.
Enfin, l'université de Silpakorn est la première université artistique de Thaïlande. Tous les arts y sont enseignés grâce à l'action de son fondateur, le sculpteur italien Silpha Bhirasri, artiste invité par le roi Rama IV pour développer l'enseignement artistique en Thaïlande au début du XXe siècle.
Bangkok abrite aussi le lycée international français de Bangkok.
Le développement de la ville sur la rive gauche a été plutôt empirique et réalisé sans plan d’urbanisme. Les grandes voies ont été créées en bétonnant les anciens khlong. Les îlots ainsi créés ont été pourvus de voies secondaires appelées soï. Ceux-ci se finissent très souvent en impasse, certains auteurs[38] soutiennent d’ailleurs que le mode de développement traditionnel de Bangkok consiste justement en ces ramifications erratiques à partir des artères principales[39]. Bangkok est sans doute l’une des villes les plus bruyantes, les plus polluées et les plus embouteillées du monde à cause de cette circulation automobile anarchique : en 2004, la mégapole comptait 4,8 millions de véhicules utilisés en moyenne 3h 12 mn par jour ; et en 2019 elle compte près de 10 millions de véhicules dont 2,5 roulent au diesel[40].
L’urbanisation par quartiers et la présence de l’ancien parcellaire limité par des canaux (khlong) transformés pour la plupart en rues explique la difficulté de circulation. En effet, en dehors des boulevards importants circonscrivant les quartiers, les rues intérieures (soï) ne communiquent pas entre elles et finissent souvent en impasse, excluant la possibilité d’itinéraires secondaires de délestage, ce qui engorge d’autant plus les grandes voies.
Enfin, la ville a récemment développé de nouveaux modes de transports en commun, plus propres et permettant de désengorger les avenues de la ville.
Bangkok est connue pour ses innombrables taxis disponibles partout et à toute heure, dont la grande majorité sont des Toyota Corolla aussi bien roses que jaunes, bleues ou vertes. Depuis que la législation leur impose de rouler au gaz, la pollution a nettement diminué, bien que restant relativement élevée. La ville possède aussi de nombreux motos-taxis (entre 100 000 et 200 000)[41] ainsi que des tuk-tuk[42] dans les endroits les plus touristiques.
Plusieurs compagnies de bus sillonnent les différents points de la ville, ainsi que le reste du pays.
Bangkok compte des lignes de bus régulières, aux bus non climatisés et bon marché.
Des bus climatisés et mini-bus sillonnent également la ville.
Bangkok est desservie par un réseau ferré national.
La gare de Hua Lamphong est la gare principale de Bangkok. Cet édifice, en forme d'arc, a été construit entre 1910 et 1916. La gare Bang Sue, qui doit ouvrir en novembre 2021 en fera le principal nœud ferroviaire de Thaïlande en remplacement de Hua Lamphong.
Ouvert en 2004, le métro de Bangkok relie plusieurs attractions touristiques principales aux quartiers où se trouvent les hôtels et les marchés, ainsi qu'au quartier des affaires. Il circule tous les jours de 6h00 à minuit.
Par ailleurs, Bangkok possède depuis 1999 un métro aérien, appelé Bangkok Mass Transit System (BTS), ou Bangkok Skytrain. Celui-ci est composé de trois lignes en viaduc, dont des extensions sont en cours de construction. Un métro souterrain existe également avec l’ouverture d’une première ligne semi-souterraine en août 2004. La construction du métro de Bangkok est une entreprise particulièrement difficile, en raison du sous-sol de la ville, constitué d’alluvions gorgées d’eau.
Le BTS assure des liaisons rapides et bon marché vers les quartiers populaires de Bangkok. Les deux lignes existantes se croisent à Siam Square. Le prix des billets varie de 10 à 40 bahts.
Bangkok est desservie par 2 aéroports : l’aéroport de Bangkok-Suvarnabhumi et l’aéroport international Don Muang.
L’aéroport de Bangkok-Suvarnabhumi situé à 32 km à l’est du centre-ville de Bangkok a accueilli 60,8 millions de passagers en 2017. Il sert de base pour les compagnies Thai Airways International et Bangkok Airways.
L’aéroport international Don Muang est situé à 24 km au nord de Bangkok il sert de base pour les compagnies Nok Air, AirAsia et Thai Lion Air.
Les bateaux-taxis (เรือจ้าง) naviguent sur le fleuve Chao Phraya[43]. Certains bateaux sont des ferries qui traversent le fleuve. D'autres desservent plusieurs embarcadères situés sur les deux rives et parcourent un itinéraire allant jusqu'à la banlieue nord de Nonthaburi.
Depuis 2001, la ville abrite une troupe d’opéra : l’Opéra de Bangkok qui a donné un grand nombre d’œuvres célèbres.
Depuis 2003 s’y déroulent chaque année le Festival international du film de Bangkok et le Festival mondial du film de Bangkok.
Le festival de Songkran célèbre le nouvel an thaïlandais les 13, 14 et 15 avril. D'après la tradition, les enfants versent de l'eau sur les mains de leurs aînés en demandant leur bénédiction avant de s'amuser à s'asperger dans les rues de la ville ou aux bord du fleuve ou des canaux.
A Bangkok, la statue de Bouddha la plus vénérée, « Phra Buddha Sihing », est exposée à Sanam Luang et arrosée par des centaines de Bouddhistes. Les passants s'affrontent à coups de jets d'eau.
Le livre Les Nobles de Dokmai Sot traduit par Wanee Pooput et Annick D'Hont nous fait découvrir Bangkok dans les années 1930[45].
Les romans policiers de John Burdett  Bangkok 8 (2003), Bangkok Tatoo, Bangkok Psycho, Le parrain de Katmandou, Le pic du vautour, Le Jocker etc. ont pour cadre le district no 8 de Bangkok[46] ; le thriller La cité de l'ange noir (2017) de Harlan Wolff[47],[48] ; le livre de terreur Temple de la nuit de S.P. Somtow etc.
Le roman policier Meurtre et méditation[49] de Nick Wilgus[50], journaliste au Bangkok Post dans les années 1990, raconte une enquête sur un crime dans un temple bouddhiste au cœur de Bangkok[51].
Le roman de science-fiction La Fille automate de Paolo Bacigalupi, se situe à Bangkok, dans un avenir incertain. La ville est protégée de la montée des océans par une gigantesque digue de béton.
L’auteur français Thomas Day a également placé l’intrigue de son roman court de science-fiction Dragon dans la ville de Bangkok, à moitié inondée par le cours du fleuve Chao Phraya. Un assassin y tue clients et tenanciers de bordels clandestins en signant ses actes d’une carte de visite à son nom, Dragon.
Louise Pichard-Bertaux, diplômée de l'Inalco en thaï et birman et chercheuse au CNRS, présente, dans son ouvrage Écrire Bangkok. La ville dans la nouvelle contemporaine en Thaïlande, la traduction de 10 nouvelles sur Bangkok écrites par les écrivains thaïlandais Atsiri Thammachot, Chart Korbjitti, Sila Komchai (ou Khomchai), Wanich Jarungidanan et Win Lyovarin[52],[53].
L'écrivain Pitchaya Sudbanthad dans Bangkok Déluge, un livre navigant entre traumatisme de l'Histoire et anticipation d'un avenir proche, nous décrit une Bangkok en perpétuel mouvement, ville tentaculaire tour à tour piège et refuge.
Bangkok est évoqué dans plusieurs chansons dont :
Certains films voient leur intrigue se dérouler en tout ou partie à Bangkok[54] :
Le tourisme est très important et celui de la chirurgie vient s’y ajouter, notamment pour l’hôpital Bumrungrad.
Bangkok veut devenir une des places fortes de l’Asie, principalement avec le nouvel Aéroport international de Suvarnabhumi ouvert en septembre 2006.
Une importante communauté chinoise (300 000 personnes) est présente à Bangkok.
Bangkok concentre 50 % de la richesse nationale[55].  Mais, comme dans toute la Thaïlande, les inégalités sociales sont très fortes : par exemple, en 2000, l'administration du Grand Bangkok (BMA), qui compte alors à cette époque près de dix millions d'habitants, a distribué des carnets de santé "à titre de cadeau de nouvel an" à plus d'un million de défavorisés (foyers vivant avec moins de 2800 bahts par mois, célibataire gagnant moins de 2000 bahts par mois, écoliers dans le besoin, infirmes, personnes âgées, vétérans, bonzes etc.) afin que ceux-ci puissent avoir accès aux soins gratuitement[56].
[1]
Pour la province, voir La Mecque (province).
Pour l’article ayant un titre homophone, voir Lamèque.
La Mecque[1] (en arabe : مكة, Makka, /makːa) est une ville de l'Ouest de l'Arabie saoudite, située dans une cuvette de l'Asir, non loin de la dépression séparant cette même chaîne de montagnes du Hedjaz, et à 84 km de la mer Rouge (66 km à vol d'oiseau). Elle est la capitale de la province de la Mecque.
La Mecque serait le lieu de naissance, selon la tradition islamique, du prophète de l'islam Mahomet à la fin du VIe siècle[2], elle abrite la Kaaba au cœur de la mosquée Masjid Al-Haram (« La Mosquée sacrée »). Selon les musulmans, la ville aurait un lien originel avec Ibrahim, la fondation de la ville reposant uniquement sur des traditions religieuses. 
La Mecque est devenue la ville sainte de l'islam où l'accès en est interdit aux personnes qui ne sont pas de confession musulmane ainsi qu'aux femmes seules, même musulmanes[3].
L'histoire pré- islamique de la ville demeure obscure et inaccessible en raison de l'absence totale de preuves archéologiques indubitables, La Mecque n'existait pas aux siècles précédant sa création supposée vers le VIIe siècle[4].
C'est annuellement et depuis le VIIe siècle le lieu du pèlerinage de La Mecque (hajj) qui rassemble, depuis la fin du XXe siècle, des millions de fidèles des différentes confessions de l'islam, venus du monde entier. C'est également le lieu vers lequel se tournent pour leurs prières quotidiennes les croyants musulmans[5].
Depuis les années 1970, des spéculateurs immobiliers construisent des infrastructures, avec un gigantisme comparable à celles de Las Vegas, pour permettre d'accueillir plus de fidèles, mais détruisant dans le même temps des sites historiques islamiques[6],[7].
La ville de La Mecque a pour nom officiel actuel مكة المكرمة, Makkat al-Mukarrama[8], signifiant littéralement à la fois « Makka la Noble », « l'Ennoblie », « la Bénie »[9] ou encore « l'Honorée »[10]prononcé localement /makːa almʊkarːama/.
Le géographe grec Ptolémée (IIe siècle) mentionne dans son ouvrage Géographie VI, 7, 31-37[11] une « Makoraba » située en Arabie de l'Ouest, soit dans le Hijaz, soit à proximité[Note 1],[12]. Certains chercheurs y voient la première mention de la Mecque, une thèse qui reste débattue[12].
L'étymologie de Macoraba est incertaine. Certains chercheurs la font remonter à l'arabe yéménite maqrab qui signifierait « sanctuaire »[13] d'autres à l'éthiopien mikrab, « le temple »[14]. Mekwarb peut signifier le « palais », le « lieu sacré » ou la « synagogue », sans être antinomique du sens surbaissé pour désigner « le lieu du sanctuaire »[15]. Makoraba peut suggérer la présence d'une ressource en eau pérenne qui attirait une population sédentaire et près de laquelle la Kaa'ba fut bâtie à une époque indéterminée[16]. Cependant, à partir de la langue afar, une langue du même groupe que l'arabe yéménite appartenant au groupe de l'afrasiatique, la traduction du mot maqrab (pl. maqrooba) signifie « amas de pierres rouges et plates »[17].
En 1987, dans son livre Meccan trade and the Rise of Islam, l'historienne Patricia Crone remet en question l'identification de la Makoraba avec le site de la Mecque[18] : selon elle, une évolution de la racine krb de « Makoraba » en mkk de Mekka est impossible[18]. En 2010, Mikhaïl D. Bukharin, soutient que le nom Macoraba pourrait dériver — via le grec — de l'arabe maghrib (« ouest »), conjecturant que le sud-ouest du Hedjaz et la région de La Mecque étaient connus des Romains et des Grecs de la seconde moitié du IIe siècle sous le nom de « l'Ouest »[19], dans une hypothèse « curieuse »[20] qui, identifiant la Mecque à la Macoraba de Ptolémée[21], demeure spéculative et conforte paradoxalement[22] les positions de Crone[12].
En 2018, l'historien Ian D. Morris relève que le consensus sur l'identification entre la Mecque et Macoraba a pu exister, car la question, peu approfondie[23], n'a pas été réexaminé avant Crone et que cette attribution, arbitraire et fragile, doit être requestionnée[24]. En 2019, l'historien Guillaume Dye, citant Morris, remet également cette hypothèse en question dans Le Coran des historiens[21].
Selon les commentateurs musulmans, le nom la Mecque apparaîtrait à deux reprises dans des passages tardifs du Coran sous la dénomination de Makka (48,24) et sous celle, plus controversée, de Bakka (3,96).
Le verset 3,96 mentionne : « Certes, le premier sanctuaire qui ait été édifiée pour les gens, c'est bien celle de Bakka bénie et une bonne direction pour l'univers ». L'exégèse traditionnelle du Coran (Tafsir) assimile également ce nom de Bakka à celui de Makka/La Mecque pour revendiquer une grande antiquité à la ville, qui en ferait ainsi le premier centre cultuel du monde. Cette assimilation et l'interprétation du nom sont également débattues[25]. Selon une tradition musulmane, Bakka, Makka et Haram sont trois espaces concentriques de tailles différentes centrés sur la Ka'ba[26]. Ainsi, selon ces interprétations, Bakka désignerait l’esplanade où la Kaaba fut construite, tandis que Makkah (pour « La Mecque » en arabe) désigne l’ensemble de la cité[27].
Pour Tesei, si les commentateurs ont relié Bakka à la Mecque[Note 2], celui-ci considère ce lien comme douteux[28] et Segovia la considère comme « hautement problématique »[29]. Ce toponyme apparaît, en effet, dans le psaume 84 sans référence à une ville arabe. Ce passage semble davantage lié à l’autel d’Abraham sur une montagne du pays de Moriyya. Pour Reynolds, il est possible que le Coran, par ce passage, « transfère ces traditions à un lieu en Arabie » et il n’est pas impossible que la Mecque ait été choisie comme site de la « Maison », par la correspondance avec certains traits de la description coranique[28]. Pour Holmgren, dans la Bible, la vallée de Baca désigne une vallée au sud-ouest de Jérusalem (Psaume 84:6)[30].
La seule mention du terme Makka dans le Coran se trouve dans la sourate 48[31]. Le philologue Christoph Luxenberg, dans un article daté de 2012, traduit même l'expression « dans la vallée de la Makka » par « au milieu d'un conflit ». Ainsi, selon cette approche, le nom de La Mecque ne serait pas même cité dans le Coran[32].
Dans les parties géographiques des puranas, dont la composition s'étend entre l'an 300 et l'an mille ap. J.-C., on trouve La Mecque sous le nom de मकेश्वर / Makeśvara. Pour le texte hindou, la pierre noire qui y est entreposée est l’emblème du dieu Shiva (le Lingam)[33].
Selon le philologue Christoph Luxenberg qui théorisa en 2007 une origine syro-araméenne du Coran, le nom de la Mecque proviendrait plutôt de la racine araméenne Makk désignant une dépression topographique, soit notamment et justement une « vallée » qui accréditerait l'origine syro-araméenne[34].
La ville de La Mecque se situe à l'ouest de l'Arabie saoudite, sur les pentes de la chaîne d'al-Sarawat, entre les massifs du Hedjaz et de l'Asir, plus précisément dans la vallée de l'oued Ibrahim au pied de collines de 60 à plus de 500 mètres de hauteur. Le port de Djeddah n'est distant que de 80 kilomètres. La partie est de la ville se situe entre 194 et 310 m d'altitude. La partie ouest, à 400 m, se caractérise par la présence de certains monts qui peuvent atteindre jusqu'à 900 m d'altitude comme le mont Jabal Tarki (qui est la plus haute montagne de la Mecque) et le Jabal Khandama qui culmine à 914 m. La partie centrale a une altitude moyenne de 294 m et la Kaaba est à 300 m. Cette partie est caractérisée par le mont Jabal Thor (759 m) qui a joué un rôle important dans la vie du prophète de l'islam Mahomet.
Le point antipodal de La Mecque (21° 25′ 21″ S, 140° 10′ 26″ O[35]) se situe dans l'océan Pacifique, en Polynésie française, à 55 kilomètres à l'est-nord-est de l'atoll de Tematangi (archipel des Tuamotu, commune de Tureia).
La Mecque possède un climat subtropical désertique avec des étés très chauds et très secs et des hivers chauds et secs. Contrairement aux autres villes d'Arabie saoudite, La Mecque conserve une température moyenne maximale supérieure à 30 °C en période hivernale. En hiver, il peut y avoir de grands changements de température, qui peuvent par exemple en quelques jours passer de 45 °C à 30 °C. Il peut faire rarement 25 °C, tout comme les températures peuvent souvent dépasser les 40 °C. En été, les températures moyennes maximales dépassent 40 °C et les températures nocturnes restent supérieures à 28 °C. La température maximale peut atteindre 48 °C l'été et un minimum de 18 °C l'hiver, avec une moyenne comprise entre 29,9 et 31 °C, ce qui en fait une des villes les plus chaudes du monde après In Salah en Algérie. Depuis 2004, de septembre à mai, les températures minimales observées dépassent toujours les 29 °C.
En hiver, la neige est inexistante, de même que le gel. À la période estivale, si les pluies sont rares, celles-ci peuvent survenir souvent brutalement, ayant un caractère  orageux provoquant de fortes inondations. Cependant, les orages secs y sont fréquents.
Sa population est estimée à 1 547 360 habitants (estimation 2012)[37]. La ville de La Mecque prospère surtout grâce aux millions de pèlerins qui s'y rendent chaque année. Le sanctuaire de La Mecque atteint sa pleine capacité de deux millions et demi de personnes lors des nuits du mois de ramadan (pour les prières nocturnes tarawih), ou lors du pèlerinage hajj.
Les origines de la fondation de la ville sont sujettes à caution car, à ce jour, aucune preuve épigraphique ou archéologique ou historique ne vient à étayer de manière formelle les récits légendaires issues de la tradition religieuse tardive de l'islam. 
Une autre difficulté très sérieuse est liée à son absence dans les cartes de l'Antiquité tardive. Alors qu'une ville du désert comme Pétra, ancienne cité nabatéenne, est solidement attestée par la cartographie antique  comme dans la Carte du Monde de
Ptolémée du IIe siècle [38],[39] ou la Table de Peutinger du IVe siècle, La Mecque n'y figure jamais, même pas comme une oasis dans le Désert d'Arabie, attestant d'une fondation tardive au plus tôt au VIIe siècle[40],[41].
En 2009 une importante découverte paléontologique a lieu près de la Mecque, le paléontologue Iyad Zalmout[42], université du Michigan aux États-Unis, qui était à la recherche de fossiles de baleines et de dinosaures, a découvert un crâne fossilisé datant de 29 à 28 millions d’années, qui correspond à l'espèce Saadanius hijazensis[43], un primate catarrhinien apparenté à l’ancêtre commun des Grands Singes (dont l'Homme) et des Cercopithèques. Les chercheurs vont accentuer les recherches dans cette région d'Asie et particulièrement dans les strates géologiques de cette période.
On appelle généralement « Tradition musulmane » ou « islamique » « l'ensemble des textes produits ou enregistrés aux premiers siècles de l'islam »[44]. Selon le Coran et les hadith, la ville aurait été fondée avant la période islamique par Ibrahim et Ismaël. En renouvelant une alliance en leur faveur, le dieu de l'islam convoque les hommes à « la Maison », al-bayt (البيت), transposition directe du syro-araméen bayta (ܐܬܝܒ) pour qu’ils adoptent « pour lieu de prière ce lieu où Abraham se tint » (s. 2/v. 125). Cette tradition coranique correspond à un dogme religieux selon lequel la première « demeure (bayt) divine » terrestre aurait été créée par Abraham à La Mecque[Note 3], supputation à l'origine de la polémique entre Mahomet et la communauté judaïque de Médine, ainsi dépossédée de la figure patriarcale fondatrice désormais islamisée[4].
Les récits anciens transmis par la tradition musulmane expliquent que c'est une source miraculeusement apparue grâce à une intervention divine qui est à l'origine de la ville. L'histoire rapportée par les Qisas Al-Anbiya, le Livre des Prophètes, rejoignant partiellement un récit de la Genèse[Note 4], explique que l'épouse d’Abraham (Ibrahim), Sarah (Śāra), exigea de celui-ci qu'il exile sa concubine Agar (Hajar) et l'enfant qu'elle lui a donné, Ismaël (Ismāʿīl)[45].
Le patriarche s'exécuta et, au terme d'une longue marche, abandonna son enfant et sa concubine à la providence divine dans un endroit inhabité, désertique et sauvage. Agar chercha âme qui vive entre les collines de Safâ et Marwah mais c'est à une intervention de l'ange Gabriel (Djibril) qu'elle dut son salut : celui-ci lui apparut et donna un coup de talon sur le sol d'où jaillit la source connue aujourd'hui sous le nom de Zamzam[45]. La source attira bientôt des nuées d'oiseaux qui attirèrent à leur tour l'attention de la tribu de Jurhum à laquelle Agar donna accès à la source en échange de leur protection pour elle et son fils. Les membres de cette tribu, surnommée par l'historiographie musulmane les « Vrais Arabes », installèrent leur campement à cet endroit et sont considérés comme les premiers habitants de la Mecque[45]. Ismaël, devenu un homme, prit pour épouse une Jurhum. Abraham vint le visiter une fois par an et, au cours de l'un de ses séjours, reçut l'injonction divine de construire le sanctuaire de la Kaaba. Les deux hommes se firent aider par les Jurhum qui se convertirent alors au monothéisme du Patriarche[46].
L'histoire pré-islamique de La Mecque est assez obscure[4]. Dans les dernières décennies du XXe siècle, les vestiges antiques, médiévaux et modernes de la ville ont été détruits systématiquement et l'on ignore dès lors tout de son archéologie[47]. Cette histoire pré-islamique repose sur des traditions musulmanes tandis que plusieurs chercheurs remettent en cause l’existence de La Mecque à cette époque. Pour R. Simon, 
Selon la tradition musulmane, son implantation ne devrait rien à une oasis. Cette singularité qui serait relevée par le Coran[49] participerait de son caractère sacré, préexistant à l'implantation de l'islam. Mais, en vérité, la ville s'est probablement structurée dans cette région aride autour d'un point d'eau qui, d'ailleurs, existe toujours et qui est à l'origine de la Kaaba, lieu sacré où se seraient retrouvées les caravanes et les tribus. Et il reste la trace d'une divinité protectrice. Il accueille à proximité et à une date indéterminée un bétyle — une « demeure (bayt) du dieu (el) » — qui fait l'objet d'un pèlerinage aux environs de l'équinoxe de printemps[4].
Reprenant la tradition musulmane, la Mecque serait, aux VIe et VIIe siècles, un centre économique modeste au regard des grandes cités caravanières comme Palmyre et Pétra, ses ressources apparaissent limitées et on y souffre régulièrement de la faim[44]. Mais c'est un centre sanctuaire et cultuel polythéiste qui abrite la Kaaba et accueille des pèlerinages donnant lieu à de grands rassemblements, notamment au cours des trêves, coïncidant avec la tenue d'importantes foires[50].
La tradition musulmane présente une Arabie préislamique misérable et anarchique appelée l'« Âge de l'Ignorance », traduisant une période de crise, d'appauvrissement et de dérèglements qui a probablement existé mais seulement pendant quelques dizaines d'années avant l'hégire[51]. Des populations nouvelles auraient alors pris la place de populations plus anciennes, dispersées ou disparues. À La Mecque, c'est Qusay qui, ayant uni les différentes tribus qurayshites au début du VIe siècle[52], prend le contrôle de la ville, six générations avant Mahomet[53]. À la veille de l'islam, la ville est passée de la domination du clan Hashîm et de la tribu Quraysh, au sein duquel Mahomet voit le jour, à celle du clan Umayya[54] qui a bénéficié du commerce caravanier renaissant[55].
Sur le plan religieux, la tradition atteste du polythéisme mecquois des qurayshites dont le panthéon se compose d'idoles que l'on trouve dans l'enceinte sacrée — le Haram — panthéon dominé par le dieu ancestral Hubal, accompagné de Manaf (en), Isaf et Na'ila (culte de Isaf et Na'ila)[56]. S'y superposent les divinités propres à l’association cultuelle, dite Hums, qui unit les tribus d'Arabie occidentale au sanctuaire mecquois[57] ; on compte parmi elles Allâh[58] — dieu qui a pour sanctuaire la Kaaba et qui donne la victoire à Quraysh lors de la « campagne de l'Éléphant » — ainsi que les déesses Allât, al-Uzzâ et Manât, ces dernières n'ayant ni idole ni sanctuaire dans la ville[56]. À l'époque de la naissance de Mahomet et à l'instar du paganisme arabe ancien, le polythéisme mecquois est en déclin, et il semble que les principales références intellectuelles et culturelles de la région soient essentiellement juives et issues des différentes confessions chrétiennes, ce qu'atteste notamment la familiarité des auditeurs de Mahomet avec les récits bibliques[59]. À côté de l'adoption de cultes monothéistes existants, on constate également une tendance à adapter les cultes anciens à l'exigence monothéiste, tout en conservant les formes ancestrales de la religiosité locale, une tendance dont relèvent plusieurs réformateurs religieux parmi lesquels Mahomet[60].
Bien que la région autour de la Mecque soit complètement aride et déserte, selon la tradition musulmane, la cité était riche, et la plus riche parmi les tribus installées dans cette partie de l'Arabie, grâce au puits Zamzam, dont l'eau a toujours été abondante et à sa position géographique sur la route des grandes caravanes. Au Ve siècle, les Quraychites auraient pris le contrôle de la Mecque pour devenir des marchands et commerçants très habiles. Jusqu'au début du VIIe siècle, le dieu principal de la mythologie arabe est Hubal[61]. Toujours selon la tradition, La Mecque était une place commerciale importante sur la route reliant le Yémen à la Mésopotamie. Les Quraychites participèrent au commerce lucratif des épices au VIe siècle. La route des épices de plus en plus menacée sur mer (piraterie) s'était déplacée sur des voies terrestres plus sûres. La Mecque devint un important centre de commerce surpassant les villes de Pétra (Jordanie) et Palmyre (Syrie)[62].
Ce lieu d'échanges aurait été à l'origine d'alliances entre les marchands de la Mecque et les tribus nomades qui commerçaient par caravanes de chameaux avec des villes de Syrie et d'Irak auxquelles ils apportaient du cuir, du bétail et des métaux qu'ils tiraient des mines locales dans les montagnes. Des récits historiques confirment le passage des marchandises venant d'Afrique plus particulièrement d'Algérie et d'Asie (médecines, tissus, épices, cuirs, esclaves) grâce à des accords commerciaux avec les Byzantins et les Bédouins qui rapportaient des céréales, du vin, des armes ensuite redistribués en Arabie[63].
Selon la Sunna, c'était aussi une ville sacrée du paganisme arabe, la Kaaba étant vénérée pour les idoles qu'elle contenait, dont la Pierre noire. Les pèlerinages étaient l'occasion de rassemblement pacifique entre les clans nomades qui, le reste du temps, s'affrontaient fréquemment. Une fois par an avait lieu un pèlerinage qui rassemblait les tribus nomades afin de célébrer les différentes déités arabes. Cet événement permettait le développement des relations sociales et des foires. S'est créée ainsi une notion d'appartenance et d'identité qui a fait de la Mecque un endroit important dans la péninsule. À la fin du VIe siècle, le commerce de la Mecque était à son apogée et représentait le pouvoir principal qui liait les habitants de la péninsule arabique[64].
Le royaume d'Axoum, conduit par le général éthiopien chrétien Abraha tente d'envahir La Mecque mais ses troupes sont décimées par la peste. Les tribus menacées craignant une nouvelle attaque font appel au roi perse Khosro Ier : l'intervention des Sassanides en 575 fait échouer une nouvelle tentative d'invasion[65]. Les études sur cette expédition montrent qu'elle ne concernait pas directement la Mecque mais est 
Aucune source (grecque, syriaque ou araméenne...) antérieure ou contemporaine de la naissance de l'islam ne mentionne la Mecque[66],[31],[Note 5]. Les sources concernant l'histoire de La Mecque sont dépendantes du matériau islamique et sont tardives puisqu'elles datent à partir de la fin du VIIe siècle. Jacqueline Chabbi rappelle que ces récits traditionnels ont des dimensions mythiques et légendaires[67]. Cette absence dans les textes musulmans les plus anciens interroge les chercheurs quant à la place prétendument centrale de celle-ci[68].
H. Holma déclarait : . A.-L. de Prémare rappelle que la charte de Médine ne mentionne jamais le nom de La Mecque et que la recherche sur les origines de l'islam est dépendante du matériau islamique à ce sujet. Il déclare : [70].
Bien que l'Arabie eut une importance politique et religieuse au VIe siècle, il n'est pas fait mention des Quraychites ni du centre commercial de La Mecque dans toute la littérature grecque et latine de l'époque. Certains chercheurs, tels Patricia Crone, Alfred-Louis de Prémare, Günter Lüling, Christoph Luxenberg, Claude Gilliot et Edouard-Marie Gallez, remettent en cause l’existence de La Mecque du vivant de Mahomet. Elle aurait été fondée vers le milieu du VIIe siècle[71]. L'archéologie montre que les premières constructions datent du VIIIe siècle. Lors de la construction du complexe Abraj Al Bait Towers des fouilles ont été entreprises sur le site.
La première mention historique de La Mecque apparaît en 741 dans la chronique Continuatio Byzantia Arabica . La ville de La Mecque y est citée dans le cadre de la guerre civile entre 'Abd al-Malik b. Marwan et 'Abd Allah b. al-Zubayr (685-692)[29]. Notons que l'auteur du document situe La Mecque (Maccam) entre Our en Chaldée  et Carrhes, ce qui en ferait une ville mésopotamienne et non pas arabe.  Le chrétien Jean Damascène en parle dans son Traité des Hérésies en 746 également comme un lieu situé en plein désert[72].
Défendant la thèse d'une existence préislamique, R. Simon, étudiant la géostratégie de l'Arabie préislamique, considère . Pour lui, La Mecque n'avait pas de commerce indépendant et elle était dirigée par les marchands de la Hira[48]. Il semble même plutôt que les habitants aient pris les nomades à leur service, établissant de multiples réseaux d'alliances commerciales et religieuses[73]. L'importance ainsi que le poids commercial et économique de la ville à cette époque ont été réévalués à la baisse depuis les travaux de Patricia Crone. La chercheuse montre la limitation des ressources et la modestie relative de la taille de cette cité dont on ne trouve d'ailleurs pas, pour cette période, d'attestation dans la littérature non musulmane[44]. La Mecque semble néanmoins avoir été, avec Najran et Adan, une ville active de la région, témoignant d'une relative sécurité et prospérité[74]. Combattant la tradition musulmane présentant La Mecque comme une cité riche située sur la route des grandes caravanes Jacqueline Chabbi est plus nuancée : . De même, elle n'est pas un lieu d'étape des grandes caravanes : .
La Tradition, dont les plus anciennes sources proviennent d'Irak dans la seconde moitié du VIIIe siècle, voit la naissance de Mahomet dans cette ville en 570 dans une famille influente de marchands caravaniers. Lorsque Mahomet fait état de premières révélations divines qu'il impute à l'ange Gabriel dans la grotte de Hira à Jabal al-Nour (située à 4 km au nord-ouest de la cité), il rencontre peu d'adhésion de la part de la majorité de ses concitoyens (juifs, chrétiens nestoriens et polythéistes), en général les plus riches[réf. nécessaire], mais il en rencontre de sa femme, très fortunée, de la part des pauvres[réf. nécessaire] de la ville et d'esclaves chez qui sa religion se répand assez vite[Combien ?]. Mahomet est l'arrière petit-fils de Hâchim, prince des Quraychites, gouverneur de La Mecque et intendant de la Ka`ba. Il est marchand. Khadija ou Khadidja bint Khuwaylid, sa première épouse, est commerçante issue d'une famille chrétienne, et aussi son employeur.
Les Quraychites hostiles à cette nouvelle religion menaçant l'ordre établi, donc peut-être aussi leur aisance et leur commerce, le chassèrent, peu après la mort de sa première femme, avec ses premiers compagnons. Ils s'exilent vers l'oasis de Yathrib (Médine) le 16 juillet 622. Cet évènement appelé « hégire » sera le point de départ du calendrier musulman.
Après des campagnes militaires victorieuses accompagnées de conversions, Mahomet revient en 630 à La Mecque à la tête d'une armée de dix-mille hommes pendant le mois de ramadan de la huitième année de l'hégire. Il entoure la ville de nuit avec des torches allumées. Les habitants effrayés lui envoient un parlementaire, Abû Sufyân, qui se convertit à l'Islam et revient annoncer aux Mecquois que s'ils se rendent, aucun mal ne leur sera fait. Ainsi donc Mahomet et ses partisans pénètrent dans la Mecque et épargne les Quraychites, qui l'avaient auparavant chassé. Il leur offre son pardon[77]. Les Mecquois se convertissent alors en nombre à la nouvelle religion monothéiste, la plupart sans combattre[78]. Lors d'une escarmouche, quelques hommes et une femme furent tués. Une tribu ralliée aux Quraychites, celle des Bakrites, eut cependant à souffrir de la vengeance des Khuzâ'ites, ralliés aux musulmans, qui entendaient réparer une attaque par traîtrise commise lors de la trêve d'Hudaibîyah, à laquelle avaient adhéré les deux tribus. Voyant les excès commis durant ces représailles, Mahomet intervint et proclama la paix générale[79].
Une légende ponctue ce voyage dans les récits musulmans et concerne l'attitude du prophète envers les animaux. Il aurait posté un soldat près d'une chienne allaitant ses petits afin de la protéger. Il est à noter que selon les ahadith Mahomet insistait beaucoup sur le bien-être des animaux et les respectait[80] ; « Il n’y a point un moineau ou un animal plus gros, que l’homme ne tue sans excuse, sans qu’Allah ne lui demande des comptes le jour de la résurrection au sujet de ce qu’il a tué. » (rapporté par An-Nassa'i).
Après avoir pris la cité, Mahomet la consacre ville sainte. Les idoles païennes de la Kaaba sont détruites en janvier 630 (sauf la Pierre noire et une icône de la Vierge à l'Enfant, selon une tradition rapportée par Al-Azraqi (en)[81]). La Ka'ba à la Mecque sera interdite aux seuls païens l'année suivante, décision accompagnée de la sourate 9, verset 28[82]. L'interdiction à tous les non-musulmans de la Mecque et de Medine sera postérieure aux quatre premiers califes. Muhammad Hamidullah la date de , rappelant au passage que le deuxième calife Omar recevait les plaintes des dhimmis au sein même de la mosquée de la Kaaba et qu'un peu plus tard un médecin chrétien disposait d'un cabinet au pied du minaret de cette dernière[83]. À la suite d'une nouvelle révélation alléguée par le Prophète, le pèlerinage à la Ka'ba devient l'un des cinq piliers de l'islam pour les musulmans sunnites et l'une des dix pratiques de la foi (ou Furû' ad-Dîn) pour les chiites duodécimains.
À la mort de Mahomet (632), l'islam commence une expansion géographique et La Mecque prend de l'importance. Auparavant, elle n'était qu'une ville médiocre, non située sur la route de pèlerinages importants, qui ne comportait que quelques milliers d'habitants à la fin du VIe siècle. Tous appartenaient à la même tribu et s'étaient établis là parce que s'y trouvait un point d'eau (un puits)[84]. Les Mecquois n'étaient ni de grands marchands ni de grands caravaniers, plutôt de petits trafiquants qui exerçaient à l'échelle locale. La Mecque attire alors davantage de nouveaux convertis venus en pèlerinage et gardera son caractère de capitale religieuse et de cité commerciale. Cependant, la ville ne sera jamais un centre politique, ni même la capitale d'un quelconque califat, y compris pendant la période ottomane.
Elle tombe quelque temps sous la domination d'Abd Allah ibn az-Zubayr, compagnon de Mahomet et neveu de sa femme Aïcha, qui refuse de faire allégeance aux Omeyyades et se proclame lui-même calife. Il est vaincu, décapité puis crucifié par Al-Hajjaj ben Yusef à l'automne 692.
À partir de la fin du IXe, la 
En 930, les Qarmates — une secte ismaélienne originaire de l'est de la péninsule arabique qui, dénonçant les inégalités et les privilèges du califat, prêche le partage équitable des biens — se livrent, sous le commandement d’Abou Tahir, au sac de la ville sainte au cours d'un raid[86]. Considérant le pèlerinage à La Mecque comme une superstition et la ville elle-même corrompue[87], ils massacrent les pèlerins et habitant puis empoisonnent la source de Zamzam avec des cadavres[88]. Dans l'attente de l'arrivée imminente du mahdi, ils emportent la Pierre noire de la Kaaba dans leur capitale Al-Hassa[87], qui ne sera restituée que vingt ans plus tard contre une rançon payée par les Abbassides[86].
À partir de 1201, la Mecque devient un chérifat chiite zaïdite dirigé par les hassanides[89],[90] (des descendants de Hassan, le petit-fils de Mahomet).
En 1349, la ville sainte est touchée par la Peste noire.
En 1517, le chérif de La Mecque, Barakat II ibn Muhammad al-Hachimi, reconnaît la souveraineté du nouveau calife ottoman Sélim Ier, mais obtient un fort degré d'autonomie locale (c'est également autour de cette période que les hassanides passent du chiisme zaïdite au rite shâfi'îte de l'islam sunnite[89],[90]). Cependant, la création du premier État saoudien en 1744, mais surtout la prise de La Mecque par les Wahhabites en 1803 porte un rude coup au prestige des Turcs. Ceci, jusqu'à ce que Méhémet Ali, le vice-roi d'Égypte, reprenne son contrôle en 1813. Un second État saoudien sera aussitôt créé en 1824, six années après la disparition du premier, mais ne réussira cependant pas à prendre le contrôle des deux villes saintes et s'effondrera à son tour le 24 janvier 1891, défait par l'émirat de Haïl lors de la bataille de Mulayda (en).
C'est à faveur du premier conflit mondial, que la révolte arabe contre la domination turque éclate en 1916. Le chérif de La Mecque, Hussein ben Ali proclame la même année l'indépendance du royaume du Hejaz à la suite de la bataille du 10 juin au 4 juillet 1916 et fait de La Mecque sa capitale. Cette année-là, Hussein Ibn Ali se déclare lui-même roi du Hejaz (reconnu internationalement le 10 août 1920) alors que son armée combat les Turcs et les expulse de la péninsule arabique, avec d'autres forces militaires arabes et celles de l'Empire britannique.
Mais cette indépendance est de courte durée puisqu'en octobre 1924 Hussein ibn Ali est vaincu lors d'une deuxième bataille (en) par Abdelaziz Al Saoud, fondateur du troisième État saoudien, l'actuelle Arabie saoudite. Le nouveau souverain supprime alors le poste de chérif de La Mecque et se proclame lui-même gardien des deux saintes mosquées.
La prise de la Mecque par Abd al Aziz ben Abd al Rahman Al Saoud en 1924 inaugure une nouvelle ère. La ville sainte dut en effet se mettre à l'heure de l'imam Abdelwahab, l'ardent théologien du XVIIIe siècle (ère chrétienne), la tête pensante et légiférante de la dynastie Séoudite (monuments rasés, pratiques cultuelles « païennes » abolies, tabac et musique bannis…). Les ressortissants des diverses obédiences minoritaires islamiques — chiites, druzes, etc. — sont tolérés dans le sens propre du terme. Ce sont des « fautifs » qu'on supporte. Le wahhabisme devient, de facto, le seul courant de l'islam à administrer et gérer la ville à compter de cette date[91],[92],[93].
En décembre 1975 l'explosion d'une bouteille de gaz provoque un incendie dans un camp de tentes ce qui tue plus de 200 pèlerins.
Le 20 novembre 1979, 1er jour de l'an 1400 de l'hégire, 200 militants islamistes armés, opposants au régime monarchiste, prennent le contrôle de la grande mosquée, prenant des dizaines de milliers de pèlerins en otages. Ils reprochent à la dynastie des Al-Saoud « son culte de l’argent, sa corruption et sa déviance religieuse[85] » mettant ainsi en péril la « vraie foi ». Le siège dure deux semaines, et le régime saoudien se résout finalement à employer la force pour reprendre le contrôle de la mosquée, demandant discrètement l'aide de la France. Ainsi, le Groupe d'Intervention de la Gendarmerie Nationale français (GIGN) assura une « aide technique et logistique » durant l'opération[94]. Les membres du GIGN participant à l'opération durent toutefois « se convertir » à l'islam en récitant la shahada avant d'être autorisés à combattre dans la mosquée interdite (indiscrétion du magazine Le Point suivant article du journaliste Jean-Michel Gourevitch en date du 28 janvier 1980)[95],[96]. L'assaut terminé le 4 décembre[97] a fait plus de 244 morts[98].
Le 31 juillet 1987, une manifestation anti-américaine organisée par des pèlerins iraniens tourne à l'émeute, lorsque la police saoudienne ouvre le feu contre les manifestants non armés. Il y eut ce jour-là 402 morts (275 pèlerins iraniens, 85 saoudiens — y compris les policiers —, et 45 pèlerins en provenance d'autres pays) et 649 blessés (303 pèlerins iraniens, 145 saoudiens — y compris policiers — et 201 pèlerins en provenance d'autres pays). Des violences ont régulièrement opposé, depuis la révolution islamique de 1979, les forces saoudiennes aux pèlerins iraniens accusés de transformer le pèlerinage en tribune politique anti-israélienne, anti-américaine et hostile au régime saoudien. Ces affrontements de 1987 ont provoqué une rupture de plusieurs années des relations entre Ryad et Téhéran[99],[100].
Le 2 juillet 1990, le jour de l'Aïd al-Adha, la fête du sacrifice, 1 426 pèlerins meurent piétinés et asphyxiés dans le tunnel reliant Mina à La Mecque.
En 1994, 270 personnes meurent dans une bousculade ; en 1998, il y a 118 morts ; en 2004, 251 morts.
Le 12 janvier 2006 362 personnes périssent piétinées ou étouffées à l’entrée nord du pont Djamarat, à Mina, dans une vallée étroite qui s’ouvre à l’extérieur de La Mecque, pendant la cérémonie de la lapidation des stèles. L'incident fait également 289 blessés parmi les pèlerins[101]. Des failles dans la sécurité et l'encadrement sont alors mises en cause.
Le 11 septembre 2015, la chute d'une grue de chantier fait 107 morts parmi les fidèles[102].
Le 24 septembre 2015, lors du pèlerinage près de la Mecque (rituel de la lapidation de Satan), un très grand nombre de pèlerins trouvent la mort lors d'une bousculade dans la vallée de Mina (2 432 morts selon le Middle East Eye (en)[103]).
Les deux principales branches de l'islam, sunnite et chiite, considèrent cette ville comme sainte, puisqu'elle est la ville natale du prophète de l'islam et se rapporte à la période d'avant le schisme.
La Mecque est un centre fondamental de la vie religieuse musulmane. Le cinquième pilier de l'islam[104] dispose en effet que tout croyant doit faire un pèlerinage à La Mecque, s'il en a les moyens. Ce pèlerinage porte le nom de hajj (ou hadj, selon les graphies). Il réunit plusieurs millions de fidèles depuis la fin du XXe siècle qui s'y recueillent entre le 8 et le 12 ou le 13 du mois de dhou al-hijja, douzième mois du calendrier musulman ou hégirien. Ce dernier étant lunaire, contrairement au calendrier grégorien occidental — donc onze jours plus court — le pèlerinage se déplace sur l'année du calendrier grégorien sur une période de trente ans ; alors qu'à l'origine, le pèlerinage était saisonnier[104]. Ce pèlerinage n'est réalisé chaque année que par une minorité de musulmans : en 2012, sur plus d’1,5 milliard de musulmans dans le monde, 3 millions de pèlerins ont été recensés, soit 0,2 %[85].
Un nombre élevé de croyants accomplissent également le pèlerinage mineur (la oumra), qui peut être exécuté à tout moment de l'année, mais plus particulièrement pendant le ramadan. Cet afflux a profondément modifié les infrastructures de la ville, devenue un centre cosmopolite[105],[106]. Environ 30 000 pèlerins partent de France et 30 000 d'Algérie vers l'Arabie saoudite par an[107].
La Mecque est aussi la direction, la qibla, vers laquelle les musulmans qui prient se tournent au cours de leurs prières. L'historien Dan Gibson soutient dans son livre Qur'anic Geography que la ville sainte originale du Coran serait Pétra, située dans une vallée, et que la relocalisation de la pierre noire par Abd Allah ibn az-Zubayr à l'emplacement actuel de La Mecque aurait été la cause du changement de la qibla des mosquées de Petra vers La Mecque au deuxième siècle après l'hégire[108].
Lors de funérailles musulmanes, le défunt est inhumé sur le côté droit, en direction de La Mecque.
L'accès à La Mecque est interdit aux non-musulmans[7],[85], c'est un « territoire sacré » (en arabe : البَلَد الحرام, al-balad al-harām)[109]. Afin de garantir cette interdiction, des postes de contrôle sur les routes surveillent l'accès à la ville. De plus, les autorités saoudiennes exigent désormais la présentation d'un « certificat de conversion à l'islam » pour toutes les personnes converties qui souhaitent pénétrer dans le « périmètre sacré ». Ce document est normalement délivré dans n'importe quelle mosquée, après entretien et contrôle des connaissances mais n'est pas nécessaire lorsqu'on porte un nom et un prénom musulmans arabes. Il est préférable de faire cette attestation auprès des grandes mosquées, ou à défaut, auprès des associations.
C'est la Mosquée sacrée, le premier lieu saint de l'islam, qui comporte en son centre la Kaaba.
L'esplanade de la Grande mosquée de La Mecque s'étend sur 368 000 m2. Sa capacité d'accueil, actuellement de plus de 1,5 million de fidèles, devrait passer à plus de deux millions dans le cadre d'un plan d'extension en cours d'exécution. Au moins 1 500 caméras de surveillance ont été installées autour de la Grande mosquée à La Mecque et ses environs[110].
À cause des travaux à la Grande mosquée et sur l'esplanade, le nombre de visas accordés aux fidèles est réduit en 2013 par les autorités saoudiennes. Mais il est possible qu'il s'agisse aussi de prévention sanitaire en relation avec l’épidémie du coronavirus.[réf. nécessaire]
Ces travaux ont entraîné la destruction de certaines anciennes parties de la Grande mosquée, et notamment des dizaines de sites historiques majeurs remontant à la naissance de l'islam, et, relève l'islamologue Robert M. Kerr, des anciennes inscriptions sud-arabiques[111] et des colonnes datant de l'Empire ottoman[6].
En arabe, « kaaba » signifie « cube ». Il s'agit toutefois d'un parallélépipède dont la base est un rectangle de 10 mètres par 12 mètres et de 15 mètres de haut. Selon la tradition musulmane, la Kaaba fut construite par Adam, premier prophète et premier homme sur Terre, et fut reconstruite plus tard par Ibrahim (Abraham) et son fils Ismaïl.
Une pierre noire, creuse, est enclose dans l'un des angles de la Kaaba.
La ville sainte a construit un complexe comportant plusieurs tours, les Abraj Al Bait Towers, qui surplombent la Sainte mosquée. La plus élevée d'entre elles, la Makkah Clock Royal Tower, fait 601 mètres de haut, ce qui, au moment de son inauguration, la classait à la deuxième place des plus hautes tours au monde, après la Burj Khalifa de Dubai. L'édifice, qui abrite un hôtel, a ouvert ses portes en 2010[112].
Cet hôtel est surmonté, depuis le 10 août 2010, d’une horloge (Arabian Standard Time) six fois plus grande que celle de Big Ben, à Londres[113]. D’un diamètre de plus d'une quarantaine de mètres, l’horloge est visible jusqu’à 30 km à la ronde de nuit, et jusqu'à 11 à 12 km le jour[114]. Elle est dotée de quatre cadrans de 46 mètres de diamètre en partie recouverts d'or[115] et décorés de 98 millions de pièces de mosaïque. Deux millions d'ampoules électriques éclairent l'inscription « Au nom d'Allah », présente sur chaque cadran de l'horloge. Pour appeler les fidèles à prier, 21 000 luminaires verts et blancs s'illuminent cinq fois par jour au sommet de la tour[115]. Le tout est surmonté d'une flèche portant un croissant, symbole de l'islam.
La ville sainte est desservie par l'aéroport international King Abdulaziz de Djeddah, qui se situe à 93 kilomètres (par route ou autoroute) au nord-ouest de la ville.
Une ligne ferroviaire à grande vitesse, la LGV Haramain, fonctionne depuis le 11 octobre 2018 et relie La Mecque à Djeddah, à la Ville économique du roi Abdallah à Rabigh et à Médine.
La Mecque s'est dotée d'une ligne de métro aérienne. Elle dessert les principaux lieux de pèlerinage, a été inaugurée en novembre 2010 pour l'Aïd et a une longueur de 18,1 km. D'après l'AFP, l'ouvrage, [116]. Il a été réservé la première année aux seuls pèlerins des monarchies du Golfe[116]. D'un coût de 1,8 milliard de dollars[116], cette ligne de métro fait partie d'un vaste projet de développement de la ville sainte, estimé à 20 milliards de dollars.
Un appel d'offres auprès d'entreprises internationales a été lancé début 2013 pour la construction d'un métro à La Mecque. C'est un projet de 16,53 milliards de dollars. Osama Al Bar, le maire de la ville, a annoncé que les quatre lignes du futur métro couvriront au total 182 kilomètres. La première étape du projet, qui concerne 122 kilomètres du réseau, devrait être construite en trois ans. Le projet total devrait être réalisé sous dix ans[117].
« She [...] was unable because she lacked a mahram, or male guardian — usually a husband, brother or father — to accompany her; male pilgrims can come alone.[...] She got here only because the Saudi government allows some women over 45 to come with an older female companion.. »
Pour les articles homonymes, voir Arctique (homonymie).
L'océan Arctique, naguère appelé océan Glacial arctique, s'étend sur une surface d'environ 14 millions de km2, ce qui en fait le plus petit des océans. Il recouvre l'ensemble des mers situées entre le pôle Nord et les régions septentrionales de l'Europe, de l'Asie et de l'Amérique. Il communique avec le nord de l'océan Atlantique, recevant de grandes masses d'eau à travers la mer de Barents et le détroit de Fram. Il se trouve aussi en contact avec l'océan Pacifique à travers le détroit de Béring.
La banquise arctique le recouvre en grande partie avec des variations saisonnières. En son centre, la banquise mesure jusqu'à 4 mètres d'épaisseur. Cette épaisseur est atteinte par le glissement de plaques de glace les unes sur les autres. On constate depuis 1979 une importante réduction de sa surface et de son albédo en raison du réchauffement climatique et d'un réchauffement régional (deux à trois fois plus rapide que dans le reste du monde pour la période 2010-2014). Ceci cause une hausse de l'absorption du rayonnement solaire par cette région du monde, ce qui pourrait accélérer le réchauffement de l'eau et la libération d'hydrates de méthane[1].
Le nom de l'océan provient du grec ancien ἄρκτος (árktos) qui signifie ours. En effet, c'est en raison de la présence près du pôle céleste des deux constellations de la Grande Ourse et de la Petite Ourse que l'océan fut nommé ainsi. Il est toutefois remarquable que l'ours polaire vit uniquement sur la « Terre des Ours », qui désigne l'Arctique, et non en Antarctique.
Selon l'Organisation hydrographique internationale, l'océan Arctique est délimité de la façon suivante[2] :
Selon l’Organisation hydrographique internationale, les mers ci-dessous sont des dépendances de l'océan Arctique:
D'une profondeur moyenne de 1 038 mètres[3], l'océan Arctique atteint une profondeur maximale d'environ 4 000 mètres.
Le long de l'Arctique sibérien, les eaux sont relativement peu profondes, ne dépassant guère 200 m. Pendant les glaciations quaternaires, jusqu'à une époque récente, une épaisse couche de glace permanente recouvrait le sol. Le relief sous-marin est accidenté en mer de Kara, plus plat, hormis quelques failles, dans la partie orientale. Le littoral est arasé, bas et peu découpé, sauf dans les régions montagneuses de la Nouvelle-Zemble, la péninsule de Taïmyr, la terre du Nord et la péninsule Tchouktche. Pendant l'été, la côte est couverte par une mince couche brun-vert de végétation de toundra ; pendant l'hiver, elle est entièrement enneigée sauf des zones de roches noires[4]. Du côté nord-américain, les côtes de l'Alaska, de la terre de Baffin et du Groenland sont découpées en fjords[5]. 
L'océan Arctique reçoit plusieurs fleuves importants venus d'Eurasie et d'Amérique du Nord : leur régime hydrologique et leurs sédiments jouent un grand rôle dans l'écosystème arctique. Leur apport annuel en eau douce est estimé à 2 603 km3 (y compris le Yukon qui a son embouchure légèrement au sud du détroit de Behring mais interagit avec l'écosystème arctique), légèrement supérieur à celui des précipitations évalué à 2 375 km3 [6]. Bien que la sédimentation fluviale soit limitée à quelques mois de l'année, elle est particulièrement intense pendant la débâcle du fait des plaques de glace flottante qui arrachent aux rives des quantités de débris minéraux : l'érosion peut atteindre 20 m par an sur les rives des mers des Laptev, de Sibérie orientale et de Beaufort. Elle est accentuée par le réchauffement climatique qui entraîne la dislocation du pergélisol[7]. Les fleuves charrient aussi des bois flottés qui, jusqu'à une époque récente, constituaient la seule ressource en bois des populations arctiques[8].
Le climat polaire est caractérisé par le froid persistant et de faibles variations annuelles de températures ; l'hiver est caractérisé par l'obscurité continue, les conditions froides et stables, et un ciel dégagé ; l'été est caractérisé par la lumière du jour continue, l'humidité importante, le temps brumeux et les cyclones faibles avec pluie ou neige.
Les températures hivernales sont proches de −50 °C du fait des forts vents provenant de Sibérie (Russie) ; tandis qu'en été elles peuvent dépasser 0 °C.
Selon des études effectuées par des spécialistes de l'université d'Oxford (Grande-Bretagne) et du Royal Netherlands Institute for Sea Research (NIOZ), l'océan Arctique jouissait, il y a 70 millions d'années, de températures proches de 15 °C, semblables à celles de la mer Méditerranée ; et de températures de 20 °C, il y a 20 millions d'années.
Les chercheurs sont arrivés à cette conclusion après avoir étudié des matériaux organiques trouvés dans les boues d'îlots de glace de l'océan Arctique. Bien qu'on ne connaisse pas les raisons de telles températures, on présume qu'elles proviennent d'un effet de serre provoqué par la forte concentration du dioxyde de carbone dans l'atmosphère. Cette conclusion est confirmée par des scientifiques de l'université du Michigan (États-Unis), qui affirment que les taux de dioxyde de carbone, il y a quelques millions d'années, étaient de trois à six fois supérieurs à la teneur actuelle.
On sait que l'épaisseur de la couche de glace de l'océan Arctique a fortement diminué durant les cinquante dernières années. Pendant l’été 2012, la glace ne couvre que 3,4 millions de kilomètres carrés, soit le plus bas niveau jamais constaté. Cette superficie de glace est inférieure de 18 % au minimum enregistré en 2007 et 50 % inférieure à la moyenne des années 1980[9].
De plus, les résultats indiquent que cette évolution va se poursuivre, la fonte des glaces étant plus rapide que ce qu'avait prévu le rapport du Groupe d'experts intergouvernemental sur l'évolution du climat (GIEC). Alors que de nombreux scientifiques pensaient que la glace de l'Arctique disparaîtrait totalement à l'horizon 2100, les prévisions plus actuelles placent plutôt cet événement à 2035[9]. Les conséquences sont sérieuses sur l'équilibre écologique de la région et pour l'habitat de certaines espèces, comme l'ours polaire qui a besoin d'une banquise pour survivre et chasser ses proies.
Des chercheurs prédisent que dans moins de 50 ans, l'océan Arctique sera parfaitement navigable pendant l'été. En effet, la couche de glace qui recouvre cette masse océanique devient chaque année plus mince, du fait de la durée des fortes chaleurs de plus en plus longue. Un bouleversement des écosystèmes et le risque d'apparition de maladies émergentes[10] (dont parasitoses et zoonoses pouvant à la fois toucher les animaux et les hommes) sont attendus[11],[12]. Les chercheurs invitent les populations et décideurs à s'y préparer[13].
Des observations par satellite montrent que ces banquises perdent de la superficie dans l'océan Arctique[14]. Par ailleurs, un amincissement de ces banquises, en particulier autour du pôle Nord, a été observé[15].
L'âge moyen des glaces, sur la période 1988-2005, est passé de plus de six ans à moins de trois ans[16].
La réduction de l'étendue moyenne de la banquise arctique depuis 1978 est de l'ordre de 2,7 % par décennie (plus ou moins 0,6 %), son étendue minimale en fin d'été diminuant de 7,4 % par décennie (plus ou moins 2,4 %)[17].
Le réchauffement dans cette région est de l'ordre de 2,5 °C[18] (au lieu de 0,7 °C en moyenne sur la planète), et l'épaisseur moyenne des glaces a perdu 40 % de sa valeur entre les périodes 1958-1976 et 1993-1997[19].
2007 marque un minimum de la banquise en été[20]. Cette année-là, les observations satellitaires constatent une accélération de la fonte de la banquise arctique, avec une perte de 20 % de la surface de la banquise d'été en un an[21].
Les observations menées pendant l'expédition Tara, une initiative privée sous l'égide du programme européen Damoclès (Developping Arctic Modelling and Observing Capabillities for Long-term Environmental Studies)[22] de septembre 2006 à décembre 2007, indiquent que les modifications entamées dans l'océan Arctique sont profondes et irréversibles[23]. Par ailleurs, le Groenland a vu ses glaciers se réduire de 230 à 80 milliards de tonnes par an de 2003 à 2005, ce qui contribuerait à 10 % des 3 mm actuels d'élévation annuelle du niveau des mers[24].
La disparition de la banquise en été diminue l'albédo de l'Arctique, renforçant le réchauffement de l'océan Arctique pendant cette saison. Une partie de la chaleur accumulée est transmise à l'atmosphère pendant l'hiver, modifiant la circulation des vents polaires. Ces changements entraîneraient des incursions d'air arctique aux latitudes moyennes expliquant les épisodes hivernaux rudes ayant touché les États-Unis ou l'Europe pendant les hivers 2010 à 2012. Cependant, les statistiques sur ces phénomènes sont encore trop récentes pour tirer une conclusion définitive[25].
Dans le Guardian, du 17 septembre 2012, Peter Wadhams, directeur du département de physique de l'océan polaire à l'université de Cambridge, en Angleterre, affirme que la banquise arctique pourrait avoir totalement disparu en été d'ici 2016[26].
On observe un réchauffement et une fonte partielle du pergélisol arctique. Entre un tiers et la moitié du pergélisol de l'Alaska n'est plus qu'à un degré de la température de dégel. En Sibérie, des lacs issus de la fonte du pergélisol se forment, provoquant des dégagements importants de méthane. Le dégagement de méthane est de l'ordre de 14 à 35 millions de tonnes par an sur l'ensemble des lacs arctiques. L'analyse au carbone 14 de ce méthane prouve que celui-ci était gelé depuis des milliers d'années[27].
Il existe environ 400 espèces animales dans l'Arctique. Parmi les plus connues l'ours blanc ou polaire, le plus grand carnivore dans cette zone. Il peut avoir une masse de 800 kg. Bien qu'il se nourrisse de phoques et de poissons, il lui arrive en cas de mauvaise chasse de les remplacer par des mousses et des lichens.
Six espèces de phoque vivent dans cet océan, bien que leur nombre ait diminué depuis le XIXe siècle du fait de la chasse intensive à laquelle l'a soumis l'Homme, chasse qui a pour but de récupérer la peau et la graisse de l'animal. Un autre habitant typique de la zone est la baleine, également menacée et qui, actuellement, est protégée de la chasse.
On trouve aussi un habitant plus discret, le krill, qui joue un rôle important dans la chaîne alimentaire.
Les animaux et les végétaux de l'Arctique sont, par leur physique et leur comportement, adaptés aux conditions particulières des régions au nord du cercle Arctique (66° 32′ Nord). La courte saison de croissance est certainement le facteur le plus contraignant pour la faune et la flore arctiques, limitées par la température, la lumière et la calotte glaciaire. La productivité marine au pôle est plus ou moins importante selon les années, les saisons et la proximité au pôle[28], ainsi la croissance de la biomasse ne dépasse pas 100 mgC/m2/jour au centre du bassin polaire, elle est de deux à cinq fois moins importante que dans la zone ouverte de l'océan Arctique. La présence de la vie sous cette partie de la banquise n'est pas plus importante qu'en haute mer ; la production primaire mesurée en Méditerranée occidentale est équivalente, contrairement aux zones de très haute production comme les côtes froides du Pérou où la biomasse est cent fois plus productive[28].
Cette biomasse arctique est principalement constituée de zooplancton tel que les amphipodes benthiques se nourrissant du phytoplancton (dinoflagellés et diatomées) qui poussent dans les couches inférieures et sous la surface submergée de la glace flottante. Même durant l'hiver austral, certaines algues peuvent continuer leur processus de photosynthèse en profitant des très faibles lueurs de la nuit polaire. Cette production attire les poissons, les cétacés et les phoques durant l'été austral, parfois même à proximité du pôle[29]. Les récits témoignant d'une présence animale autour du pôle Nord demeurent tout de même anecdotiques, mais on observe une importante perturbation de la productivité causée par le réchauffement climatique[30]. En effet, on peut observer plus de 275 espèces de plantes et d'animaux se rapprochant du pôle durant l'été austral en raison du réchauffement[31]. La dirette de parin, une espèce des grandes profondeurs des régions tropicales, a ainsi été retrouvée à plusieurs reprises à proximité du Svalbard au début du XXIe siècle[32]. 
La pêche halieutique autour de cette zone est favorisée tandis que la mégafaune est défavorisée. L'ours blanc se déplace rarement au-delà de 82° de latitude Nord, en raison de la rareté de la nourriture, bien que des traces soient parfois observées près du pôle Nord[33]. Une expédition en 2006 a signalé avoir observé un ours blanc à un peu plus d'un kilomètre du pôle[34]. Le phoque annelé a également été observé près du pôle, et un renard polaire a été vu à moins de 60 kilomètres, à 89° 40′ Nord[35].
Parmi les oiseaux observés près du pôle, plusieurs espèces ont été signalées : des bruants des neiges, des fulmars boréaux et des mouettes tridactyles, bien que certaines observations puissent être faussées par le fait que les oiseaux ont tendance à suivre les navires et les expéditions[33]. Des poissons ont été vus dans les eaux au pôle Nord, mais ils sont probablement peu nombreux[33]. Bien que certaines espèces puissent comporter un grand nombre d'individus, le froid polaire ralentit leur métabolisme et elles peuvent mettre jusqu'à deux ans en eau polaire avant d'atteindre leur maturité sexuelle[29].
La pollution des eaux arctiques a également un impact important sur la natalité via la chaîne alimentaire du cercle polaire. Certains métaux lourds tels que le zinc, le cadmium, le mercure et le sélénium sont concentrés dans l'océan Arctique par les courants marins provenant des océans Atlantique et Pacifique. Les polluants bioaccumulés dans le métabolisme d'un individu augmentent avec l'absorption des niveaux inférieurs du réseau alimentaire océanique[29]. Ainsi, des contaminants peuvent être présents en quantité infime dans le zooplancton, mais on observe des taux anormalement concentrés dans le métabolisme des espèces superprédatrices comme les oiseaux de mer, les phoques, les ours et même à l'extrémité de la chaîne, chez les humains. Des prélèvements de sang de cordon des nouveau-nés inuits révèlent un taux de polychlorobiphényles quatre fois plus grand et un taux de mercure quinze à vingt fois plus élevé que chez les bébés nés plus au sud. Ces polluants présents dans les métabolismes ont un impact sur le taux de natalité, et peuvent provoquer des déficits de neurotransmission et divers problèmes cognitifs[36].
L'écosystème de l'océan Arctique est un des plus vulnérables de la planète, de plus en plus fragilisé par la fonte des glaces qui entraîne une hausse des activités industrielles causant une forte pollution. L'océan Arctique est aussi un grand vivier de poissons pour les populations indigènes qui s'en nourrissent et dont la santé est au premier chef menacée par cette pollution. De plus, une forte contamination radioactive est enregistrée dans la région et constitue une des principales préoccupations pour l’environnement. Mais la radioactivité n'est pas que dans l'eau, elle est aussi incrustée dans les glaces, et la fonte de la banquise arctique ainsi que le dégel du pergélisol risquent de libérer ces substances radioactives dans les écosystèmes.
Les premières explorations en vue de trouver des voies maritimes en Arctique remontent à 1539 pour le passage du passage du Nord-Ouest au nord du Canada et du Groenland. Elles sont plus tardives mais plus prometteuses pour le passage du Nord-Est, désormais connu sous le nom de route maritime du nord, au nord de la Russie.
La Russie dispose d’une flotte de brise-glaces, dont plusieurs à propulsion nucléaire permettant d’augmenter la période de navigabilité de la route maritime du nord et donc permettant d’envisager une réduction des temps de trajets par rapport au passage par le canal de Suez ou le Cap de Bonne-Espérance.
La pêche hauturière est pratiquée par les pays européens dans les eaux arctiques. Ce sont 2,3 millions de tonnes de poissons qui sont pêchées chaque année par les pays européens, principalement l’Islande, la Norvège, le Danemark et la Russie[37]. Mais grâce à la fonte des glaces, la pêche est susceptible d’augmenter de façon considérable, car l’on constate actuellement un déplacement vers le nord d’espèces de poissons habituellement observées en Atlantique et Pacifique. Une étude prévoit une possible hausse de 30 à 70 % des captures de poissons dans les hautes latitudes, y compris l'Arctique, d'ici 2055[9]. Les espèces de poissons de l’Arctique seront donc inévitablement touchées par cette pêche.
Le développement du tourisme devient une préoccupation majeure pour l’équilibre de cet environnement fragile. Depuis la fin de la guerre froide, le tourisme est en pleine croissance dans les régions arctiques. Avec la fonte des glaces, certaines zones deviennent encore plus accessibles. Des croisières sont organisées pour permettre aux touristes de découvrir le Grand Nord. Au Canada et en Alaska, le nombre de touristes a doublé durant les années 1990. En Islande, il a triplé entre 1970 et 1995, passant de 50 000 à 165 000. Au Spitsberg, il a augmenté de 12 % par an, pour atteindre aujourd’hui plus de 30 000 visiteurs. Enfin, l’Arctique sibérien s’ouvre progressivement aux touristes avec quelques voyages à bord de brise-glaces. En Arctique, le nombre de touristes est passé d’environ 1 million au début des années 1990 à plus de 1,5 million en 2007[38]. Il existe de réelles préoccupations en ce qui concerne les sols, la faune et la flore, l’eau et d’autres besoins élémentaires. Le piétinement de la végétation entraîne une érosion des sols et une dégradation du permafrost. Ainsi, au Spitsberg, le site de la baie de la Madeleine a été interdit d’accès (jusqu’à 20 000 touristes y étaient débarqués chaque année) car près d’un hectare de fragile toundra a été dégradé par le piétinement.
D’autres problèmes se font jour tels le problème de la gestion des déchets en l’absence de centre de recyclage ou l’émission de matières polluantes avec l’usage grandissant des motos-neige. Mais les inquiétudes concernent aussi les pollutions locales : le nombre de touristes ayant largement dépassé celui des populations hôtes, le maintien des pratiques culturelles locales est en danger ce qui pourrait porter préjudice à ces populations[38].
Il existe de réelles préoccupations quant à la dégradation environnementale en milieu polaire (surtout en Arctique) liée à l'industrie touristique en pleine expansion, particulièrement en ce qui concerne les terres, la faune et la flore, l'eau et d'autres besoins élémentaires[9].
À cause du réchauffement climatique, la fonte des glaces en été de la banquise arctique s'accentue depuis plusieurs décennies et se constate dès 1979, date à laquelle l'on connait précisément sa superficie[39]. Une des conséquences est l'ouverture de nouvelles voies maritimes. Ces nouvelles routes du Nord-Ouest et du Nord-Est entre le Japon, la Russie, l’Europe et l’Amérique du Nord représentent un risque additionnel pour l'environnement de cette région si le trafic devait se densifier[40].
Les perturbations liées à la pollution industrielle sont principalement dues à l’exploitation des ressources énergétiques, au transport maritime et à la pêche intensive. En effet, la fonte des glaces laisse un accès plus facile aux ressources naturelles comme le gaz, les hydrocarbures et les minéraux, ce qui aggrave la disparition de la faune et la flore. L’océan Arctique devient un lieu d’exploitation prisé, puisque 30 % des ressources mondiales non découvertes se trouvent en Arctique[9].
On observe aussi une grave détérioration de l'état des eaux côtières. Chaque année, environ 10 millions de mètres cubes d'eau non traitée sont déversés dans les eaux de l’océan Arctique. Ces eaux usées provenant d'entreprises industrielles contiennent des substances chimiques toxiques comme du pétrole, du phénol, des composés de métaux lourds, de l'azote et d'autres substances[41].
Des débris plastiques de provenance mondiale, et notamment d'Europe de l'Ouest et des États-Unis, polluent l'ensemble des océans, dont l’océan Arctique. Les chercheurs y estiment la quantité totale de fragments flottant en surface entre 100 tonnes et 1 200 tonnes[42].
Il y a plusieurs menaces de contamination radioactive dans l'océan Arctique. Le climat polaire qui sévit dans la région étant peu accueillant, il préserve le lieu des regards indiscrets. C'est pourquoi la Nouvelle-Zemble a été choisie par le gouvernement russe pour accueillir des essais nucléaires pendant la guerre froide, mais également pour y stocker des déchets radioactifs[43].
Entre 1954 et 1990, l'océan Arctique accueille aussi des sites de tests nucléaires. En 1955, la Nouvelle-Zemble a été vidée de sa population pour accueillir les expérimentations de l'URSS qui transforme ce lieu en polygone de tir lors de la Course aux armements nucléaires de la guerre froide. Pendant cette période, plus de 200 explosions nucléaires ont été réalisées, équivalant à 100 fois l'ensemble des explosifs utilisés pendant la Seconde Guerre mondiale[43]. Lancé le 30 octobre 1961 au-dessus de l'archipel, le test le plus puissant effectué dans cette zone est celui de la Tsar Bomba, d’une puissance d’environ 57 mégatonnes. C'est la bombe la plus puissante ayant jamais explosé.
Entre 1988 et 1989, à la suite du glasnost, les essais ont été révélés au monde. Déposé le 5 février 2001, un rapport commun de l'Assemblée nationale et du Sénat français atteste que les essais nucléaires atmosphériques de Nouvelle-Zemble se chiffrent au nombre de 91 et représentent une puissance totale de 239,6 mégatonnes, soit 97 % de la puissance des essais atmosphériques soviétiques et près de 55 % de la puissance dégagée par la totalité des essais atmosphériques mondiaux (440 Mt)[44].
Ces nombreuses activités militaires ont créé une importante pollution radioactive. On y trouve surtout de grandes quantités de Strontium 90 et de Césium 137, mesurés en Sibérie et en Carélie. Aujourd’hui encore, la dose de césium est dix fois plus élevée que la norme dans ces régions[45].
Le 21 janvier 1968, un accident aérien impliquant un bombardier B-52 de l'United States Air Force (USAF) se produit près de la base aérienne de Thulé sur le territoire danois du Groenland. L'appareil transporte quatre bombes à hydrogène et s'écrase sur la banquise arctique dans la baie North Star au Groenland, ce qui déclenche la détonation des explosifs conventionnels à bord, la rupture et la dispersion des charges nucléaires, et donc une contamination radioactive. La neige contaminée est envoyée aux États-Unis.
Dans les années 1960, au nord de la péninsule de Kola, l'Union soviétique avait installé trois piscines de stockage des combustibles usés provenant de sous-marins et de brise-glaces nucléaires. Des fuites se sont produites dans deux d'entre elles, provoquant une très forte contamination. Depuis, des entreposages à sec de 22 000 assemblages de combustibles ont été aménagés, mais dans des conditions très précaires[46].
Entrée en vigueur le 30 août 1975, la convention de Londres sur les déchets nucléaires interdit tout dépôt de déchets de ce type dans l’océan. Pourtant, en 1982, la Russie, signataire de cette convention, a immergé le sous-marin nucléaire K-27. Ce dernier gît par 33 mètres de fond au large de la Nouvelle-Zemble. S’ajoutent à cela entre 11 000 et 17 000 conteneurs de déchets radioactifs, 15 réacteurs qui appartenaient à des sous-marins nucléaires et au brise-glace Lénine, et 5 autres réacteurs officiellement immergés[46],[43]. De plus, plusieurs autres sous-marins nucléaires gisent dans la mer de Kara et la mer de Barents, tels le K-278 Komsomolets (1989), le K-141 Koursk (2000), et le K-159 (2003).
Selon les autorités russes, le niveau d’activité global des produits immergés dans la mer de Kara est de 2 à 3 milliards de gigabecquerels (soit environ 0,1 milliard de curies). En revanche, l’état exact de confinement et le type de contrôle effectué sont inconnus[43].
Au large de la péninsule de Kola, ce sont 200 navires abandonnés et sabordés qui sont sources de pollution. Sur les rives de l'océan Arctique se trouvent environ 12 millions de barils, souvent remplis de carburant, d'huile et de matières premières chimiques[41].
En 1991, les huit pays riverains de l'Arctique[47] se rencontrent pour signer la Stratégie pour la Protection de l'Environnement Arctique (en) (Arctic Environmental Protection Strategy, AEPS). Ceci est le premier pas vers la déclaration d'Ottawa, ratifiée en 1996, qui établit formellement le Conseil de l'Arctique, forum traitant notamment du développement durable et de la protection de l'environnement dans l'Arctique. Cette déclaration prévoit la participation des différentes communautés autochtones et des autres habitants de l'Arctique.
Depuis 1988, le Programme des Nations unies pour l'environnement (PNUE) soulignait les principaux problèmes environnementaux liés à l'évolution du climat[48]. Dans l'Arctique ces problèmes sont principalement : la fonte des glaces et le changement climatique, la pollution de l'eau par des déchets d'huile de mers du Nord et chimiques, la réduction des populations d’animaux et les changements dans leur environnement.
Le 28 mai 2008, un sommet ministériel réunit à Ilulissat au Groenland cinq pays riverains de l’océan Arctique qui affirment leur volonté de dépasser leurs différends et de coopérer pour protéger l’environnement de cet océan. À l'issue de la réunion, les ministres et représentants du Canada, du Danemark, des États-Unis, de la Norvège et de la Russie se sont .
L'océan Arctique symbolise le monde des ténèbres et le royaume des morts dans les peintures mythologiques de certains peuples ouraliens et sibériens (Finno-ougriens, Samoyèdes...). Pour eux l'univers n'est pas vertical, comme dans la mythologie grecque, mais horizontal. Il s'organise autour d'une rivière mondiale : l'océan Arctique. Sa source est issue du monde de la lumière, d'où venaient le printemps et les oiseaux migrateurs apportant l'âme des nouveau-nés dans le monde des humains. Les âmes des défunts partaient en bas de la rivière, dans le royaume des morts.
La mythologie indo-iranienne, et notamment celle du peuple aryen, a conservé quelques échos de contacts avec les civilisations voisines du nord. Certaines montagnes citées dans la mythologie aryenne sont à rapprocher des montagnes de l'Oural. Au pied de ces montagnes se trouvaient l'océan Mondial (probablement l'océan Arctique) et l'île des bienheureux. Dans le Mahâbhârata, il est noté que sur le versant nord du mont Meru se trouve la côte de l'Océan de lait[49]. Selon certains chercheurs, les éléments de cette mythologie ont été empruntés à la Grèce ancienne et transmis aux peuples indo-iraniens via les Scythes, et sont basés sur les mythes des monts Riphées et des Hyperboréens[50],[51].
Les évocations de l'océan Arctique en Europe de l'Ouest dans l'Antiquité et au Moyen Âge sont plus vagues et plus mythifiées. Ses côtes étaient considérées comme le bord d'un monde où habitaient différents monstres issus du chaos primitif. Ces légendes ont été évincées dans les traditions russes par des données objectives, la mise en valeur de la région et des contacts fréquents avec la population locale.
Des informations sont également conservées dans la tradition géographique arabe. Au milieu du XIIe siècle, l'explorateur arabe Abu Khamid Al-Garnati s'est rendu dans le Khanat bulgare de la Volga et rapporte avoir entendu parler d'un territoire situé près de « la mer des ténèbres », à savoir les rives de l'océan Arctique.
L'espace arctique n'est pas directement réglementé à l'échelle internationale. Il est fragmenté par différents accords juridiques internationaux, principalement concentrés sur des problématiques environnementales et les diverses législations nationales des pays de l'Arctique. L'océan Arctique est directement adjacent à 6 pays : le Danemark (Groenland), le Canada, la Norvège, la Russie, les États-Unis et l'Islande (qui ne revendique aucun droit de sol sur l'océan Arctique)[52].
Il y a deux différentes manières de distinguer les droits des États riverains de l’Arctique sur le territoire de l’océan Arctique[53] :
Pour assurer le respect de la CNUDM, la Commission des limites du plateau continental (CLPC) des Nations unies est créée durant cette convention afin d’analyser l’évolution des plateaux du Danemark, de la Norvège et de la Russie[55]. En 2008, ces trois pays, ainsi que les États-Unis et le Canada signent la Déclaration d'Ilulissat dont l’un des principaux objectifs est le blocage de tout « nouveau régime juridique compréhensif international de gestion de l’océan Arctique »[56]. Les cinq signataires se mettent cependant d’accord pour procéder à une coopération à dessein environnemental dans l’Arctique. Ils décident de coordonner leurs efforts dans d’éventuelles futures opérations de sauvetage dans la région.
En 1880, la Grande-Bretagne remet officiellement au Canada ses possessions arctiques d'Amérique du Nord par décret confirmé dans l'Imperial Colonial Boundaries Act de 1885[57]. La plupart des îles de l'Arctique canadien sont cependant découvertes au fil des années par des chercheurs américains et norvégiens, une menace pour la souveraineté du Canada dans la région.
Désireux de légiférer rapidement, le Canada est finalement le premier à définir un statut juridique de l'Arctique en 1909[58]. Il s'octroie alors toutes les terres et les îles situées à l'ouest du Groenland, entre le Canada et le pôle Nord[58].
En 1926, ces droits sont légalement fixés par décret royal et interdisent aux pays étrangers de se livrer à toute activité dans les terres et les îles de l’Arctique canadien sans une autorisation spéciale du gouvernement canadien[54]. En 1922, le pays annonce sa souveraineté sur l’île Wrangel. L’URSS s’oppose à cette déclaration et appose le drapeau soviétique sur l’île en 1924[56].
En 2015, le Canada déclare posséder : le bassin de drainage de la rivière Yukon, toutes les terres au nord de 60° N, y compris l’archipel arctique canadien, ses détroits et ses baies, et les régions côtières de la baie d’Hudson et de la baie James. La superficie des propriétés polaires du Canada s’élève ainsi à 1,43 million de kilomètres carrés.
En 2007, le Premier ministre canadien, Stephen Harper, décide de renforcer la souveraineté canadienne dans l’océan Arctique. Deux ans plus tard, le Parlement national adopte la « Stratégie pour le Nord du Canada », qui en plus d’une composante politique importante, se concentre sur le développement économique de la région Arctique, avec un accent particulier mis sur la recherche scientifique[59].
En décembre 2013, le Canada dépose une demande à l’ONU afin d’établir son droit de propriété sur 1,2 million de kilomètres carrés de plateau continental (comprenant le pôle Nord)[60].
En 1993, le Danemark revendique un droit souverain sur le Groenland et les îles Féroé et les inclut dans son espace arctique[61]. Son territoire polaire représente alors 372 000 kilomètres carrés.
Le pays entre ensuite en conflit avec le Canada au sujet de l'île Hans, située au centre du passage Kennedy[62]. Ce conflit a été résolu en 2022 par la division de l'île entre les deux pays.
En 2002, le Danemark présente une demande à l'ONU, réclamant la propriété de 62 000 kilomètres carrés de plateau continental (comprenant le pôle Nord)[63].
En 1924, les États-Unis souhaitent ajouter le pôle Nord à leur zone de contrôle dans la région, se référant au fait que le pôle Nord est une extension de l’Alaska. Aujourd’hui, les États-Unis revendiquent le contrôle de la zone située au nord du cercle polaire arctique et au nord et à l’ouest de la frontière formée par la Porcupine, le Yukon et les rivières Kuskokwim, ainsi que toutes les mers adjacentes, y compris l’océan Arctique, la mer de Beaufort et la mer des Tchouktches[64].
La superficie des propriétés arctique des États-Unis s’élève à 126 000 kilomètres carrés.
Les États-Unis et le Canada se disputent les frontières des pays de la mer de Beaufort. Les États-Unis insistent sur le fait que, selon la loi, le passage du Nord-Ouest fait partie des eaux internationales, contrairement à l’affirmation du Canada selon laquelle il appartient à ses eaux territoriales[53].
Établie par l’accord de 1990, la frontière maritime entre les États-Unis et la Russie s’étend à travers la mer des Tchouktches et continue vers le sud par la mer de Béring. Cet accord doit encore être ratifié par la Russie, mais il est respecté par les deux parties depuis sa signature[65].
En 1922, 42 pays signent à Paris un traité établissant la souveraineté norvégienne sur l’archipel de Svalbard. Trois ans plus tard, la Norvège annonce officiellement l’ajout de Spitzberg sur son territoire[52] et établit une zone économique de 200 miles autour de l’archipel, que l’Union soviétique, et plus tard la Russie, ne reconnaît pas. Le 15 février 1957, l’Union soviétique et la Norvège signent un accord concernant la frontière maritime entre les deux pays dans la mer de Barents[66].
En 1997, les ministres de l’Environnement des États riverains à l’Arctique déterminent ensemble que les propriétés arctiques de la Norvège englobent les zones de la mer de Norvège au nord de 65°N, pour une superficie de 746 000 kilomètres carrés.
En septembre 2010, la Norvège et la fédération de Russie signent le « traité de délimitation maritime et de coopération en mer de Barents et dans l’océan Arctique », qui aboutit à l’affiliation entre les deux pays d’espaces maritimes d’une superficie totale d’environ 175 000 kilomètres carrés[67].
Le statut légal de la zone arctique russe est d’abord défini dans une note du ministère des Affaires étrangères de l’Empire russe, le 20 septembre 1916. Elle déclare la propriété russe de toutes les terres situées sur le prolongement nord du plateau continental sibérien. Un mémorandum est publié le 4 novembre 1924 par le Commissariat du peuple aux Affaires étrangères de l’URSS qui confirme la position de la note de 1916[68].
Le 15 avril 1926, un décret du Præsidium du Soviet suprême de l’URSS définit le statut juridique des terres arctiques de l’Union soviétique ainsi :
« Sont déclarés territoires de l’Union des Républiques socialistes et soviétiques toutes les terres et toutes les îles, actuellement découvertes ou qui pourront l’être ultérieurement, qui sont situées dans l'océan glacial arctique au nord du littoral de l’URSS et jusqu’au pôle Nord, dans une zone comprise entre le méridien 32° 44′ 35″ Est de Greenwich, lequel passe le long de la partie orientale de la baie de Vaïda, par le point de repère géodésique du cap Kekourek, et le méridien 168° 49′ 30″ Ouest de Greenwich, lequel passe par le milieu du détroit séparant l’île Ratmanov de l’île Krusenstern (archipel des îles Diomède), dans le détroit de Béring, et qui, à la date de la publication du présent décret, ne sont pas reconnues par le gouvernement de l’URSS comme étant territoire d’un État étranger »[69].
La superficie totale des territoires polaires appartenant à l’URSS s’élève ainsi à 5 842 millions de kilomètres carrés.
En 2001, la Russie est le premier pays à soumettre à la Commission des Nations unies une demande afin d’étendre les frontières du plateau continental Arctique de 1,2 million de kilomètres carrés. Elle réclame les dorsales du Lomonossov et de Mendeleïev. Sa demande est rejetée, car manquant d'informations[60].
Les conventions bibliographiques ne sont pas respectées (décembre 2014).
La bibliographie et les liens externes sont à corriger. Améliorez-les !
La mer du Nord est une mer épicontinentale de l'océan Atlantique, située au nord-ouest de l'Europe, et qui s'étend sur une superficie d'environ 575 000 km2.
Les pays qui bordent la mer du Nord sont le Royaume-Uni (île de Grande-Bretagne) à l'ouest ; les îles Orcades et Shetland au nord-ouest ; la Norvège au nord-est ; le Danemark à l'est ; l'Allemagne au sud-est ; enfin les Pays-Bas, la Belgique et la France (avec 50 km de littoral entre Calais et la frontière belge) au sud. Elle communique avec la Manche au sud-ouest ; avec l'océan Atlantique au nord-ouest et la mer de Norvège au nord ; avec le Skagerrak à l'est. Le canal de Kiel permet aux navires de rejoindre la mer Baltique.
Elle constitue une zone de fort transit maritime, d'exploitation pétrolière et de pêche. La mer du Nord et son littoral forment un milieu naturel très riche, mais la pollution marine, la surpêche, l'industrie pétrolière (plates-formes offshore) et le tourisme sont sources de menaces pour l'avenir. Elle est en aval du centre de l'Europe industrielle, de l'estuaire du Rhin aux fjords norvégiens et aux falaises du nord de la Grande-Bretagne. Le secteur Manche/Sud-mer du Nord, incluant le pas de Calais est considéré comme représentatif de mers mégatidales peu profondes, caractérisées par un fort courant et une eau très turbide (en raison des courants et phénomènes de renversement de marées), ce qui en fait une zone écologiquement particulière, mais également vulnérable au risque maritime en raison d'un intense trafic maritime (marchand et passager).
L'Organisation hydrographique internationale définit les limites de la mer du Nord de la façon suivante[2] :
Au nord, elles sont déchiquetées, récemment dépouillées par des glaciers des périodes glaciaires ; les montagnes norvégiennes plongent en mer, donnant naissance, au nord de Stavanger à des fjords profonds et à des archipels aux multiples îles et îlots. Au sud, elles sont plus douces : recouvertes de débris glaciaires déposés ou directement par la glace ou redéposés par la mer.
Au sud de Stavanger, le trait de côte et son relief s'adoucissent, alors que les îles deviennent moins nombreuses. La côte écossaise orientale présente encore une allure déchirée, mais moins marquée qu'en Norvège. À partir de la « tête de Flamborough » (Flamborough Head), dans le Nord-Est de l'Angleterre les falaises s'amenuisent et leur matériau (de moraine souvent), moins résistant, s'érode plus facilement, donnant des formes plus arrondies. En Hollande, en Belgique et dans l'Est de l'Angleterre (Est-Anglie) le littoral devient bas et localement marécageux (avec zones de polders), et les estuaires s'élargissent. Les côtes est et sud-est de la mer du Nord (la mer des Wadden) sont principalement sablonneuses et rectilignes, notamment en Belgique et au Danemark.
La mer du Nord repose en grande partie sur le plateau continental et possède donc peu de zones profondes de plus de 100 m. Toutefois, vers le nord, à partir de la latitude 53°24", d'une manière générale, le fond de la mer du Nord descend irrégulièrement. Vers le sud, il s'incline vers le pas de Calais.
Les profondeurs rencontrées dans sa partie méridionale sont de l'ordre de 40 m. Ce secteur est composé de nombreux hauts-fonds ou bancs probablement des ères glaciaires mouvant au gré des fortes marées. Ceux-ci représentent d'importants dangers maritimes.
À l'est de la Grande-Bretagne, le vaste plateau morainique du Dogger Bank s'élève jusqu'à -15 à -30 m, formant ainsi une région très poissonneuse[3].
Les plus grandes profondeurs se trouvent dans la fosse norvégienne qui longe la côte norvégienne de la mer de Norvège jusqu'à Oslo. Large de 25 à 35 km, elle est profonde de 300 m environ au large de Bergen, et atteint jusqu'à 700 m dans le Skagerrak[4].
On trouve aussi des grandes profondeurs dans la partie occidentale de la mer du Nord, comme le trou du diable[5] (Devil's Hole) au large d'Édimbourg, jusqu'à plus de 460 m ; ou quelques tranchées au large de la baie The Wash. Ces couloirs pourraient avoir été formés par les cours d'eau pendant la dernière glaciation. En effet, à cette époque de glaciation, le niveau de la mer du Nord se trouvait plus bas que le niveau actuel (régression). Les fleuves auraient alors érodé certaines parties alors découvertes que la mer recouvre aujourd'hui (transgression). Ce qui est le plus probable est qu'ils soient des restes de vallées tunnels maintenues ouvertes par les courants de marée.
Du carbone s'y est accumulé sous forme de craie, mais aussi d'hydrocarbures ; notamment dans des schistes et charbons profonds situés sous le gisement. Ces hydrocarbures se sont probablement formés à partir de sédiments qui se sont accumulés au Jurassique moyen et au supérieur[6].
Lors de la formation du rift de la mer du Nord, et du graben central de la mer du Nord (graben inversé, propice à la formation de pétrole et gaz), alors que le fond marin s'enfonçait, ces hydrocarbures ont été peu à peu piégés par des schistes du Jurassique supérieur et des marnes du Crétacé inférieur[6]. Ils sont aujourd'hui exploités par des forages profonds.
La mer du Nord est encore en cours d'ouverture. Elle possède un petit rift et est sismiquement active (assez fortement au nord, sur la côte occidentale de la Norvège[7]). Elle l'est plus que les zones continentales qui l'entourent, sans toutefois pouvoir être comparée aux fonds marins islandais, japonais ou indonésien très actifs.
Un réseau de sismographes à courte-période a été déployé sur les côtes d’Écosse et de Norvège (fin des années 1960 et début des années 1970). Il a mis en évidence une activité sismique, non uniforme en mer du Nord. Cette activité a fait l'objet d'une surveillance approfondie durant 10 ans[8]. Des séismes . Les données disponibles suggèrent [9] impliqués dans les activités offshore et portuaires. Des vibrations importantes sont perçues sur les plates-formes offshore[8] et plusieurs séismes petits ou moyens sont enregistrés chaque année en mer du Nord.
Un tremblement de terre de 6,1 sur l'échelle de Richter a eu lieu en 1931 sur la zone du « Dogger Bank » (voir Dogger Bank earthquake).
Le graben central, exploité pour ses ressources gazières et en hydrocarbures légers (condensats de gaz naturel), est traversé par une série de failles perpendiculaires. C'est là qu'est situé la plate-forme de forage offshore du champ d'hydrocarbures d'Elgin-Franklin dont le puits G4 d'Elgin foré  (ou plus de 5 530 mètres selon le Schéma du puits publié par Total[10]) a fui en 2012.
Des variations de pressions et de tensions sur la lithosphère sont induites par la déglaciation et le rebond induit. Ce sont des sources de stress locaux pour les structures géologiques (flexion lithosphérique). On observe ainsi une pression horizontale sur le côte ouest du Viking Graben et une dépression horizontale sur le côté est[11].
Le risque sismique pour les installations offshore a été évalué au Royaume-Uni via dix ans de « surveillance séismologique de la mer du Nord » (de 1979 à 1989) réalisé par le British Geological Survey[8], en lien avec le Bergen Seismological Observatory[8]. Cette surveillance visait :
Sur la base des données historiques, de relevés sismiques[7] et d'un catalogue[12] récent et fiable des séismes en mer du Nord, l'étude faite pour le HSE (Health and safety executive) a conclu que des séismes de magnitude 4,1 à 4,4 sur l'échelle de Richter ont une probabilité annuelle d'occurrence de 0,7 en mer du Nord[8].
De plus, la fonte rapide de la calotte polaire nord, et la montée de la mer pourraient par le jeu de rééquilibrages isostatiques et eustatiques réveiller des failles depuis longtemps inactives. Une étude récente (1999-2000)[13] ayant porté sur une « faille inverse » située dans le Nord de la mer du Nord (faille normale et restée longtemps inactive) montre que ce phénomène est actif. Cette étude a combiné des images sismiques détaillées à des mesures (in situ) de pression et de contraintes. Les auteurs concluent de leur analyse des données que cette faille est en cours de réactivation, pour trois raisons qui additionnent leurs effets :
Ces trois facteurs réunis, ont permis une reprise du glissement le long de la faille, et d’autre part une fuite de gaz le long de la section de faille, qui délimitait la faille et assurait l’étanchéité du réservoir.
Les auteurs affirment que l'accumulation de colonnes de gaz (CO2 par exemple) dans le voisinage de failles tectoniques peut contribuer à les remettre en mouvement.
Un aquifère salin de la Mer du Nord pourrait être utilisé dans le cadre de la séquestration géologique du dioxyde de carbone
[14], à des fins de décarbonation de l'économie.
Les eaux de la mer du Nord sont différentes des autres. Un système complexe de marées et de courants apporte les eaux riches de l'Atlantique par la Manche en créant des milieux variés qui nourrissent une grande diversité d'animaux. Trente espèces de cétacés y vivent, ainsi que six espèces de phoques dont deux se reproduisent sur ses côtes, le phoque gris et le veau marin.
Plus de 170 espèces de poissons, dont des requins, l'aiglefin, la morue, le maquereau, le hareng, le lançon et le sprat fréquentent les eaux grises[Quoi ?] de la mer du Nord. Les raies, les poissons plats, les anguilles et la baudroie sont tapis sur le fond.
Les fonds abritent également des myriades d'invertébrés tels que homards, éponges, oursins, crabes et poulpes. Sur les hauts fonds, les forêts de varech abritent balanes et moules.
Les écosystèmes de cette zone, notamment suivi par OSPAR montrent des signes de fortes transformations liées aux pressions de pêche, aux apports terrigènes en nutriments, au réchauffement climatique et à des phénomènes cycliques tels que l'oscillation nord-atlantique, variation climatique périodique naturelle à grande échelle spatiale mesurée par l'indice NAO qui semble par exemple bien corrélé aux variations périodiques de certaines communautés de microalgues en zone côtière belge (S.-E. de la mer du Nord)[15].
Le tableau suivant donne le nom de la mer du Nord dans les langues riveraines ; s'il n'y a pas de traduction, c'est que le terme signifie seulement « mer du nord ».
En 1958, des géologues ont découvert un gisement de gaz naturel dans Slochteren dans la province néerlandaise de Groningue et il était soupçonné qu'un plus grand nombre de domaines gisaient sous la mer du Nord. Toutefois, à ce stade, les droits à l'exploitation des ressources naturelles sur la haute mer étaient encore en litige.
Un test de forage a débuté en 1966 et, en 1969, Phillips Petroleum Company a découvert le gisement pétrolier Ekofisk (devenu norvégien), qui à ce moment-là était un des 20 plus importants au monde et s'est avéré être précieux par la faible teneur en soufre de son pétrole. L'exploitation commerciale a commencé en 1971 avec les navires-citernes et après 1975 par un gazoduc d'abord vers Cleveland, en Angleterre, puis un second après 1977 vers Emden, en Allemagne. Depuis la découverte de pétrole en mer du Nord dans les années 1970, des surnoms d'Aberdeen ont été capitale européenne du pétrole ou capitale européenne de l'énergie.[pas clair]
L'exploitation des réserves de pétrole de la mer du Nord a commencé juste avant la crise pétrolière de 1973, et la montée des prix internationaux du pétrole ont rendu les gros investissements nécessaires pour l'extraction beaucoup plus attrayants. Dans les années 1980 et 1990, de nouvelles découvertes de grands gisements de pétrole ont suivi. Bien que les coûts de production soient relativement élevés, la qualité du brut, la stabilité politique de la région, et la proximité de marchés importants en Europe occidentale ont fait de la mer du Nord une importante région productrice de pétrole. La plus grande catastrophe humaine dans l'industrie pétrolière en mer du Nord a été la destruction de la plate-forme pétrolière offshore Piper Alpha en 1988 lors de laquelle 167 personnes ont perdu la vie. Une grande éruption en 1977 dans le domaine Ekofisk a donné lieu à un écoulement de pétrole sans entrave à la mer pendant une semaine avant qu'il ne soit colmaté, les estimations de la quantité d'hydrocarbures rejetés dans l'environnement varient entre 86 000 et 202 380 barils (environ 10 000 à 30 000 tonnes, en fonction de la densité de l'huile). En revanche, les incendies sur le Piper Alpha ont brûlé la plupart des hydrocarbures à bord et libéré des puits perturbés.
Avec plus de 450 plates-formes pétrolières, la mer du Nord est la plus importante région du monde pour le forage au large. La partie britannique de la mer du Nord a le plus grand nombre de plates-formes, suivie par les norvégiens, néerlandais, et danois. Outre les champs de pétrole Ekofisk, le champ de pétrole Statfjord est aussi à noter comme étant à l'origine du premier gazoduc vers la Norvège. Le plus grand gisement de gaz naturel en mer du Nord, Troll, se trouve dans la fosse norvégienne à une profondeur de 345 mètres (1 100 pieds). Une plate-forme géante a été nécessaire pour y accéder. La section allemande a seulement deux plates-formes pétrolières, la plus grande des deux est le Mittelplate, et l'Allemagne est le pays riverain qui a le moins développé l'extraction.
En 1999, l'extraction a atteint un niveau record avec une production de près de 6 millions de barils (950 000 m3) de pétrole brut et 280 000 000 m3 de gaz naturel par jour. Toutes les grandes compagnies pétrolières ont été impliquées dans l'extraction. Mais ces dernières années, les grandes entreprises comme Shell et BP ont cessé l'extraction et, depuis 1999, les quantités extraites ont constamment diminué en raison de l'épuisement des réserves[16]. Total a néanmoins repris son activité sur la plateforme d'Elgin-Franklin en mars 2013[17].
Le prix du Brent Crude, l'un des premiers types de pétrole extraits de la mer du Nord, est utilisé aujourd'hui comme un standard de comparaison de prix pour le pétrole brut en provenance du reste du monde.
Le Sud de la mer du Nord n'a pas de marées ou vagues ou courants faciles à exploiter en raison de la nature des fonds et du fort trafic émergeant du pas de Calais. Les projets éoliens offshores qui ont émergé dans les années 1990 dans le Nord-Pas-de-Calais ont été retardés. De grands projets éoliens ont débuté plus au nord et au nord-ouest dès les années 1990 où les vents dominants sont forts et réguliers ; Angleterre et Danemark notamment ont utilisé les zones marines côtières pour produire de l'énergie.
La ferme éolienne offshore de Blyth (Royaume-Uni, 2000) a précédé celle de la côte danoise (2002, près de Horns Rev) et d'autres ont été mises en service (dont OWEZ et Scroby Sands) ou sont en projet (ex : 2 parcs prévus pour 2012 sous réserve de délivrance des concessions demandées par la belge Electrabel ; Blue4PowerI au nord du Bligh Bank (en) à 60 km du trait de côte et Blue4PowerII à 20 km au sud du précédent entre le Bligh Bank et le Bank zonder Naam).
Certains parcs offshore ont rencontré une résistance, par exemple en Allemagne, à propos des impacts environnementaux (collisions avec oiseaux, perturbations sous-marines lors de la pose des fondations…). La distance aux consommateurs conduit à des pertes énergétiques de transmission.
L'énergie marémotrice n'est pas oubliée : deux premières grandes turbines à eau profonde sont en commande pour Talisman Energy en Écosse, à 25 km (15 milles) en mer près du champ pétrolier Beatrice. (88 m (290 pieds) de haut et pales longues de 63 m, soit 210 pieds qui devraient produire 5 MW chacune, ce qui en fait le plus gros projet au monde. La côte norvégienne et l'intersection avec la mer d'Irlande pourraient être jugées aptes pour y exploiter l'énergie des vagues et/ou des courants marins.
Les premières tentatives de centrale d'électricité utilisant les vagues sont nées en 2003-2005 au Danemark. Le European Marine Energy Centre (EMEC) basé à Stromness (Orcades, Écosse) est un organe de recherche soutenu par le gouvernement. Il a construit un site de tests en mer à Billia Croo sur les îles Orcades et une station d'essai d'utilisation de l'énergie marémotrice sur l'île voisine d'Eday. Une petite installation-pilote pour la production d'énergie bleue existe à Trondheim (Norvège).
Pendant des siècles, la mer du Nord a nourri les peuples vivant sur ses littoraux (et bien au-delà via le commerce du poisson). Ces dernières décennies, ses ressources naturelles halieutiques ont été soumises à des pressions nouvelles, d'origine nettement anthropique. On observe des modifications importantes du réseau trophique et de certaines populations halieutiques ainsi que de la diversité des populations[18]. Pour partie, mais derrière la surpêche, le dérèglement climatique pourrait être en cause[18]. Ceci est connu depuis quelques décennies pour les poissons, mais depuis on commence à mieux suivre les communautés d'invertébrés benthiques, et à dresser des constats similaires. Certaines espèces sont bioindicatrices de changements importants environnementaux importants, notamment induits par le chalutage qui exerce une pression croissante sur certains fonds marins[18]. Une étude de l'épibenthos a montré qu'au tout début du XXe siècle, la diversité en espèces benthiques était plus faible dans le sud de la mer du Nord que dans les zones centre et du nord. Au contraire, il y avait plus d'espèces de poisson au sud[18]. les profondeurs de 50 m, 100 m et 200 m de profondeur définissent les limites de trois types différents de communautés benthiques (et de poissons). L'épibenthos de la mer du Nord méridionale était dominé par des espèces plus mobiles, alors qu'au nord, les espèces fixées dominaient. Un espace intermédiaire avec à peu près autant d'espèces sessiles que mobiles a été trouvé au large des côtes de Norfolk et de Flamborough s'étirant vers le Dogger Bank[18]. La température du fond, les courants et la nature des sédiments expliquent une part de la répartition de toutes ces espèces, mais la position des faisceaux de chalutage est également un facteur explicatif de la richesse ou pauvreté en certaines espèces[18].
La surpêche a notamment décimé les harengs et les morues. Faute de nourriture, les macareux disparaissent à leur tour[19]. En 1983, des quotas de pêche ont été définis, mais n'ont pas suffi à protéger la ressource qui est prélevée plus vite qu'elle ne peut se renouveler : les experts pensent aujourd'hui qu'il faudrait diviser ces quotas par deux voire plus.
Des poissons malades, parasités ou malformés forment une part croissante des captures. On pense que cela est dû à la pollution (métaux lourds dont mercure[20], hydrocarbures, aromatiques polycycliques, antifoolings et autres pesticides, etc). On a rejeté en Manche/mer du Nord de nombreux déchets marins, dont des munitions anciennes et des déchets radioactifs qui resteront dans l'écosystème pendant des siècles. Les hydrocarbures provenant du nettoyage des cuves en mer ou des accidents de navires pétroliers restent une menace grave pour la faune.
Chaque année, la Grande-Bretagne envoie 250 000 tonnes d'azote qui eutrophisent la mer du Nord, par voie aérienne. Mais la plus grosse pollution vient de la terre, apportée par les fleuves ou les égouts.
Les côtes de la mer du Nord sont très variées, des rivages rocheux aux marais salés et vasières des estuaires, aux longues plages bordées de dunes. Vasières et estuaires sont des aires de nutrition essentielle pour les limicoles comme le chevalier gambette et le bécasseau variable, ainsi que les canards et les oies. Lorsque la mer se retire avec la marée, tous viennent se nourrir de crustacés et mollusques cachés dans la vase.
Malgré des efforts de protection, ces biotopes restent vulnérables et menacés. Les marais fragiles sont fragmentés, drainés et localement densément construits de huttes et mares de chasse (sources durables de plomb et de saturnisme aviaire) eutrophisés ou affectés par des sédiments pollués, éventuellement d'origine portuaire. Les blooms planctoniques, dont de Phæocystis ou Pseudonitzchia delicatissima sont de plus en plus importants, étant un indicateur peut-être comparable à celui des marées vertes en Bretagne ou invasions de méduses en Méditerranée.
Depuis deux siècles, de nombreuses zones humides arrières littorales des côtes de la mer de Nord ont disparu, drainées et récupérées par l'agriculture, l'industrie ou le tourisme. Ont ainsi été transformés en terres agricoles 40 000 hectares de la mer des Wadden, zone qui accueille 1 000 espèces animales et végétales et d'importants sites de frai. Entre 1935 et 1982, 48 % des estuaires de la Tamise, de Medway et de Swale ont été mis en valeur et 90 % de l'estuaire de la Tees ont été transformés en zone industrielle. Celles qui restent se sont parfois fortement dégradées.
Toutes ces régions servaient de sites d'hivernage aux limicoles et anatidés. D'autres ont été drainées et détruites pour agrandir les ports, comme autour de Rotterdam.
Les oiseaux privés de ces habitats peuvent se déplacer mais sont alors confrontés à des niches écologiques déjà occupées, et ils ont peu de chance de retrouver d'autre sites favorables, ce qui les condamne à mourir ou à avoir peu de chances de pouvoir se reproduire.
Tous les trois ans, la conférence de la mer du Nord réunit les huit États riverains (Belgique, France, Royaume-Uni, Norvège, Suède, Allemagne, Danemark et les Pays-Bas), mais un grand nombre des objectifs n'ont jamais été atteints. Le Royaume-Uni est le pays le plus pollueur et, seul, continue de déverser ses déchets industriels malgré l'accord de 1987, applicable fin 1989.
En 1990, la conférence décidait que le rejet des boues d'égout devait progressivement cesser d'ici 1995 et l'incinération des déchets en bord de mer cesser en 1993. La pollution atmosphérique devait régresser de 50 %, tandis qu'étaient étudiés les effets de la pêche intensive.
L'organisation Greenpeace lança dans les années 1980 plusieurs campagnes de protestation contre le rejet de déchets toxiques en mer du Nord. Approuvée par la conférence de la mer du Nord et le Parlement européen, elle réclame que cesse toute pollution dès l'an 2000. Mais l'objectif ne sera pas atteint tant que l'industrie continuera de créer des sous-produits toxiques. Il faut passer à des méthodes de production « propres » pour sauver la mer du Nord et sa faune sauvage.
