Sarah Bernhardt, właśc. Henriette Rosine Bernardt (ur. 22 października 1844 w Paryżu, zm. 26 marca 1923 tamże) – francuska aktorka. Występowała głównie w teatrach Paryża, w latach 1872–1880 w Comédie-Française. Była znana z teatralnych ról dramatycznych.
Urodziła się w rodzinie żydowskiej[1] jako Rosine Bernardt. Jej matką była pochodząca z Amsterdamu Julie Bernardt (1821–1876), córka kupca Moritza Barucha Bernardta i jego żony Sary Hirsch. Aktorka zmieniła rodzinne nazwisko, modyfikując je w formę Bernhardt.
Sceniczna kariera Bernhardt zaczęła się w 1862, kiedy młoda aktorka studiowała jeszcze w Comédie-Française, najbardziej prestiżowym teatrze we Francji. W pewnym momencie zdecydowała się opuścić rodzinny kraj i przeniosła się do Belgii, gdzie została kochanką arystokraty Henriego de Ligne. W 1864 urodził się im syn Maurice. Po narodzinach dziecka książę zaproponował jej małżeństwo, ale jego rodzina sprzeciwiła się legalizacji związku i przekonała Bernhardt do odrzucenia oświadczyn i zakończenia związku[2].
Po wyrzuceniu z Comédie-Française za spoliczkowanie jednej z aktorek powróciła do życia kurtyzany, z którym zapoznała ją jej matka w bardzo młodym wieku. Dzięki temu w latach 1862–1865 zarobiła pokaźną sumę pieniędzy. W tym czasie nabyła swoją słynną trumnę, która często służyła jej za łóżko. Bernhardt twierdziła, że spanie w niej pomogło jej zrozumieć wiele tragicznych ról.
Potem Bernhardt powróciła do grania w teatrze, podpisując kontrakt z Théâtre de L’Odéon w 1866. Jej najsłynniejszą rolą z tamtego okresu była parodia florenckiego barda w sztuce Przechodzień François Coppé (styczeń 1869)[3]. Wraz z wybuchem wojny francusko-pruskiej (1870) przedstawienia zostały wstrzymane i Sarah przekształciła teatr w prowizoryczny szpital, gdzie opiekowała się rannymi żołnierzami[4]. Międzynarodową sławę zdobyła w latach 70. XIX wieku i szybko stała się najbardziej rozchwytywaną aktorką w Europie i Nowym Jorku. W przerwie między kolejnymi trasami Sarah wykupiła prawa najmu Théâtre de la Renaissance﻿(inne języki), gdzie zajęła stanowisko producentki, reżyserki oraz głównej aktorki w latach 1893-1899.[5]
W 1872 opuściła Odéon i wróciła do Comédie-Française. Jednym z jej największych sukcesów była główna rola w sztuce Voltaire’a Zair (1874). Sarah odbyła nawet podróż na Kubę, gdzie wystąpiła w teatrze Sauto w Matanzas w 1887 roku. Ponadto udzielała lekcji aktorstwa wielu młodym dziewczętom, w tym aktorce i kurtyzanie Liane de Pougy.
W 1880 otrzymała duński królewski Medal Nagrody na kokardzie Orderu Danebroga[6].
W 1899 Bernhardt przejęła były Théâtre des Nations﻿(inne języki) na Place du Châtelet w Paryżu i zmieniła jego nazwę na Théâtre Sarah-Bernhardt. 21 stycznia nastąpiło otwarcie, podczas którego Sarah stworzyła jedną ze swoich najbardziej docenianych kreacji, w spektaklu La Tosca Victoriena Sardou. Zaraz później wystawiono wznowienie Fedry Racine’a (24 stycznia), następnie Patron Bénic Gastona de Wailly’ego (14 marca), Samarytankę Octave’a Fauilleta (25 marca) oraz Damę kameliową Aleksandra Dumasa 9 kwietnia. 20 maja swój debiut miała najbardziej kontrowersyjna rola Bernhardt: zagrała Hamleta w sztuce Szekspira w adaptacji napisanej prozą przez Eugène’a Moranda i Marcela Schwoba na jej zamówienie. Sztuka została obsypana entuzjastycznymi recenzjami, pomimo że trwała cztery godziny bez przerwy. Bernhardt wyrobiła sobie opinię poważnej dramatycznej aktorki, zyskując tytuł Boskiej Sarah. Prawdopodobnie była najsłynniejszą aktorką XIX wieku[potrzebny przypis].
Bernhardt brała również udział w skandalizujących produkcjach, takich jak Judasz Johna Wesleya De Kaya. Spektakl był wystawiony w Globe Theatre w Nowym Jorku tylko raz, w grudniu 1910 roku, po czym został zakazany, tak samo jak w Bostonie i Filadelfii. Biorąc pod uwagę, jak w 1910 roku wyglądała scena teatralna Nowego Jorku, fabuła sztuki była oburzająca. Maria Magdalena, która najpierw była kochanką Poncjusza Piłata, a potem Judasza Iskarioty, związała się z Jezusem. Kiedy Judasz zorientował się, że Maria Magdalena oddała się Jezusowi, zdecydował się wydać przyjaciela Rzymianom. Dla nowojorskich miłośników teatru dopełniającym elementem prowokacji był fakt, że rolę Judasza grała ponętna Sarah Bernhardt[7].
Do śmierci zarządzała swoim Théâtre Sarah-Bernhardt. Potem to stanowisko zajął jej syn Maurice, który zmarł w 1928 roku. Teatr zachował swoją nazwę aż do niemieckiej okupacji w czasie II wojny światowej[8], kiedy został przemianowany na Théâtre de la Cité ze względu na żydowskie korzenie Bernhardt[9] .
Została pochowana na Cmentarzu Père-Lachaise.
W 1894 Alfons Mucha zaprojektował plakat do sztuki z jej udziałem pt. Gismonda i od tej pory projektował wszystkie plakaty do jej spektakli – stały się czołowymi dziełami secesji[9] .
Miała romans z belgijskim arystokratą Charlesem-Josephem Eugène’em Henrim, księciem de Ligne, z którym miała swoje jedyne dziecko – syna Maurice’a Bernhardta (ur. 1864), męża Polki Marii Jabłonowskiej (1863-1914).
W Londynie w 1882 poślubiła greckiego aktora Aristidesa Damalę (we Francji znanego jako Jacques Damala). Małżeństwo oficjalnie trwało do śmierci Damali w 1889 w wieku 34 lat, ale w praktyce zakończyło się szybciej z powodu uzależnienia aktora od morfiny[9] .
Michał Anioł (wł. Michelangelo), właśc. Michelangelo di Lodovico Buonarroti Simoni (ur. 6 marca 1475 w Caprese, zm. 18 lutego 1564 w Rzymie) – włoski rzeźbiarz, malarz, poeta i architekt epoki odrodzenia.
Zaliczany obok Leonarda da Vinci i Rafaela Santiego do trójki największych artystów epoki renesansu, a także uznawany za jednego z najwybitniejszych artystów w dziejach świata[2][3][4]. Autor między innymi fresków w kaplicy Sykstyńskiej. Wśród rzeźb najbardziej znane są Pietà i Dawid. Z dzieł architektonicznych – projekt kopuły bazyliki św. Piotra. Jego działalność artystyczna jest związana z Bolonią (1494–1495), Florencją (1501–1505, 1517–1534) a przede wszystkim z Rzymem (1496–1501, 1505–1517, 1534–1564), gdzie twórczość artysty objęta została mecenatem papieskim. Wykształcony we florenckim warsztacie malarza Domenico Ghirlandaio, a w dziedzinie rzeźby w pracowni Bertolda di Giovanniego. Wbrew obiegowym opiniom to nie Michał Anioł zaprojektował mundury Gwardii Szwajcarskiej[5].
Urodził się w rodzinie Buonarroti Simoni należącej do starych florenckich rodów mieszczańskich. Gdy miał zaledwie kilka tygodni, jego rodzina przeniosła się do Florencji. Jego ojciec Lodovico pełnił w tym czasie obowiązki burmistrza Chiusi i Caprese. Matka, Francesca di Neri di Miniato del Sera, zmarła 6 grudnia 1481 roku, kiedy artysta miał 6 lat. Z tego względu młody Michał Anioł był wychowywany przez spokrewnioną rodzinę kamieniarzy, co ukierunkowało jego późniejsze zainteresowania.
Od 1 kwietnia 1488 roku rozpoczął terminowanie w pracowni Domenica Ghirlandaia, uznanego florenckiego malarza. W okresie tym artysta poznał techniki malowania fresków, jednakże bardziej pociągała go rzeźba. Rok później przeszedł do pracowni rzeźbiarza Bertolda di Giovanni, który zarządzając zbiorem Medyceuszów, wprowadził Michała Anioła w otoczenie Wawrzyńca Wspaniałego, władcy Florencji. Przebywał na jego dworze w latach 1490–1492.
Z tego względu często odwiedzał ogrody pałacu Medyceuszów, tam też zwrócił na jego prace uwagę Lorenzo de’ Medici, który roztoczył opiekę nad młodym wówczas chłopcem zapewniając mu mieszkanie na terenie pałacu, wykształcenie oraz stałą pensję. Jego edukacją zajmowali się dwaj myśliciele i filozofowie: Marsilio Ficino, założyciel szkoły ateńskiej we Florencji, i hrabia Giovanni della Mirandola. W okresie tym powstały takie dzieła jak płaskorzeźby Madonna przy schodach (ok. 1490–1492 rok) i Bitwa centaurów (1491–1492 rok, obydwa dzieła znajdują się obecnie w Casa Buonarroti we Florencji).
W roku 1494 przebywając w Bolonii wyrzeźbił świecznik w formie klęczącego, nagiego anioła i posągi świętych: Petroniusza i Prokulusa w San Domenico.
Po śmierci Wawrzyńca Wspaniałego Michał Anioł opuścił pałac Medyceuszów i wrócił do rodzinnego domu. Pracował nad posągiem Herkulesa (dzieło zaginęło w XVIII wieku). W tym czasie studiował też anatomię, wykonując potajemnie sekcje zwłok w szpitalu przy klasztorze Santo Spirito za cichą zgodą przeora. W dowód wdzięczności wyrzeźbił dla klasztoru drewniany krucyfiks – jedyną w swoim dorobku polichromowaną rzeźbę w drewnie.
W roku 1495 kuzyn Wawrzyńca Wspaniałego zamówił wykonanie rzeźby śpiącego Kupidyna. Dzieło tak dokładnie oddawało antyczne kanony piękna, że kuzyn Wawrzyńca namówił Michała Anioła na spatynowanie rzeźby. Tak spreparowane dzieło zostało sprzedane handlarzom i dotarło do Rzymu, nabyte tam przez Rafaela Riario, znawcę sztuki i protektora artystów, tytularnego kardynała San Giorgio al Velabro. Jednakże szybko do uszu kardynała dotarła informacja, że dzieło zostało wykonane we Florencji. Dzięki temu zdarzeniu Michał Anioł pod koniec czerwca 1496 roku przyjechał do Rzymu, gdzie wykonał kilka zleceń (posąg Bachusa, inną wersję Kupidyna), między innymi Pietę Watykańską (1497–1500), która znajduje się w bazylice św. Piotra w Rzymie. W roku 1501 na prośbę ojca powrócił do Florencji, gdzie podjął się wyrzeźbienia posągu Dawida. Prace nad tym dziełem zakończył 25 stycznia 1504 roku. Posąg stanął przed pałacem florenckiej Signorii. W tym samym czasie powstała także Madonna dla kościoła Notre-Dame w Brugii.
We Florencji artysta zajął się wykonaniem na zamówienie dzieł do katedry florenckiej: posągów do Ołtarza Piccolominiego oraz 12 posągów apostołów. W roku 1504 otrzymał zamówienie na dekorację ściany w Palazzo Vecchio, naprzeciw bitwy zaczętej przez Leonarda da Vinci. Tematem obrazu było zaskoczenie kąpiących się florenckich żołnierzy przez nieprzyjaciół w bitwie pod Cascina. Nie wykończył kartonu do tego dzieła, fragmenty kompozycji znane są tylko z rycin Markantonia i A. Veneziana.
Michał Anioł został zaproszony w roku 1505 do Rzymu przez papieża Juliusza II. Był to pierwszy z siedmiu papieży, dla których artysta pracował w Rzymie. Buonarroti zaczął realizację papieskiego grobowca, który miał się składać z blisko 40 posągów. Pracę musiał jednak przerwać, by wykonać ogromny brązowy posąg papieża przed kościołem San Petronio w Bolonii, skończony w 1508 roku, a zniszczony w 1511.
Lata 1508–1512 poświęcił Michał Anioł freskom plafonowym w kaplicy Sykstyńskiej w Rzymie, przedstawiając w dziewięciu polach biblijną historię ludzkości od stworzenia świata, przez grzech pierworodny aż do potopu.
W następnych latach prace nad grobowcem Juliusza II były kontynuowane. Powstały rzeźby takie jak Mojżesz (1513–1516) oraz Rachela i Lea (1542). Równocześnie zaś z nimi artysta rozpoczął marmurowy posąg Chrystusa zmartwychwstałego dla kościoła Santa Maria sopra Minerva (1514). Zaczętej w roku 1516 z polecenia Leona X budowy nowej fasady dla kościoła San Lorenzo we Florencji Michał Anioł również nie skończył, pracując od 1520 przy kaplicy Medycejskiej tamże. W roku 1523 wykonał klatkę schodową w Biblioteca Laurenziana we Florencji. Nad kaplicą Medycejską pracował z przerwami do roku 1534. Zawiera ona grobowe pomniki Lorenza i Giuliana de Medici[6].
W roku 1529 Buonarroti udał się do Wenecji, gdzie wykonał dla księcia Ferrary obraz Leda z łabędziem, znany współcześnie tylko z kopii. Po powrocie do Rzymu w roku 1534 malował do 1541 Sąd Ostateczny na ścianie ołtarzowej kaplicy Sykstyńskiej. W tym samym czasie powstały rzeźby: Dzień, Świt, Zmierzch (1520–1535).
Od roku 1547 Michał Anioł prowadził budowę bazyliki św. Piotra w Rzymie, podczas której zmienił całkowicie plany swoich poprzedników – Bramantego i Antonia da Sangallo. Plan jego stwarza budowlę centralną z kopułą. Nad budową bazyliki pracował do końca życia, ale nie zdołał jej ukończyć. Dokończenie budowli nastąpiło dopiero przy końcu stulecia. Michał Anioł zajmował się też innymi budowlami w Rzymie, jak ukształtowaniem placu kapitolińskiego, budową kościoła S. Maria degli Angeli, Palazzo Farnese i fortyfikacjami, z których Porta Pia zaczęta była w 1564 według jego planów.
Z dzieł Michała Anioła wymienić jeszcze należy: figurę klęczącą Amora (Londyn), płaskorzeźbę Madonna z Chrystusem i św. Janem (we Florencji i podobną w Londynie), czterech jeńców z grobowca Juliusza II (Florencja, dwóch w Paryżu), grupę Zwycięzcy (Florencja), Dawida (Florencja) i okrągły obraz Św. Rodzina (Uffizi). Ostatnia Pieta jaką wykonał w latach (1550–1553) dla katedry we Florencji, miała zostać, według ostatniej woli twórcy, postawiona na jego grobie.
Główny temat rzeźb Michała Anioła stanowiła postać ludzka, w której artysta upatrywał głęboki sens. Pracował wyłącznie w marmurze, bez pomocy uczniów (nie licząc pracy przy sklepieniu kaplicy Sykstyńskiej, gdzie miał trzynastu pomocników). Zafascynowany kontrastem powierzchni surowej i wygładzonej dłutem, pozostawiał szereg prac niedokończonych.
Odkryto, że Michał Anioł miał w zwyczaju „podpisywać” się na swoich freskach, malując wśród innych postaci swój własny wizerunek. Można znaleźć go na fresku w kaplicy Sykstyńskiej, a latem 2009 roku odkryto podobny detal także na freskach w Cappella Paolina[7].
Michał Anioł może się poszczycić kilkoma osiągnięciami architektonicznymi, m.in.
Michał Anioł zmarł 18 lutego 1564 roku w Rzymie.
Z pogrzebem Michała Anioła związany był drobny incydent.
Nastąpiła przepychanka między rzeźbiarzami a malarzami, których pierwsi nie chcieli wpuścić na uroczystość pogrzebu[8]. Artysta został wybrany patronem powstałej we Florencji rok przed jego śmiercią Accademia del disegno (Akademii rysunku)[9]. Na nagrobku wzniesionym przez Akademię w kościele Santa Croce we Florencji, projektu Giorgio Vasariego, przedstawione są personifikacje trzech sztuk (od lewej Malarstwo, Rzeźba, Architektura). Ponad nimi umieszczone zostało popiersie artysty. Znak trzech nachodzących na siebie kół ilustruje tezę Michała Anioła, iż podstawą jego wszechstronności była umiejętność rysunku (disegno)[9].
Dzieła Michała Anioła odzwierciedlają jego bogatą osobowość. Kunszt widać we freskach kaplicy Sykstyńskiej, gdzie ruchy i formy ciała są dynamiczne. Przez ożywienie ruchem form architektonicznych stał się prekursorem sztuki barokowej. Miał kilku uczniów (Daniele da Volterra i Sebastiano del Piombo), jednak jego twórczość wpłynęła na sztukę włoską.
W swych pracach umiał wyrazić emocje. Jego prace charakteryzują się trójwymiarowością, bogactwem ruchów i gestów oraz dramatyzmem.
Michał Anioł na początku zajmował się malarstwem, później przez ostatnie 30 lat życia sławę zdobywał jako rzeźbiarz. Pod koniec życia zajmował się także poezją, wydając wiele sonetów i kazań. Temat jego utworów obraca się wokół spraw miłości i śmierci.
Sonety Michała Anioła zostały wydane po raz pierwszy przez jego siostrzeńca M. Buonarrotiego w roku 1623 i tłumaczone później na wiele języków (polskie tłum. m.in. Lucjana Siemieńskiego – zbiór „Poezye Michała – Anioła Buonarrotego” i Edwarda Porębowicza). Poezje Michała Anioła przełożył Leopold Staff (1922, ze wstępem i przypisami).
Miguel de Cervantes y Saavedra (ur. 29 września 1547 w Alcalá de Henares, zm. 23 kwietnia 1616 w Madrycie[1]) – renesansowy pisarz hiszpański, najlepiej znany jako autor powieści El ingenioso hidalgo Don Quijote de la Mancha  (Przemyślny szlachcic Don Kichote z Manczy).
Data jego urodzin nie jest poświadczona, jednak ze względu na imię, jakie mu nadano, przypuszczalnie urodził się 29 września, w dzień św. Michała Archanioła. Wiadomo, że ochrzczono go 9 października 1547 w Alcalá de Henares. Przyszły pisarz pochodził ze zubożałej, ale znanej rodziny szlacheckiej. Dziadkiem pisarza był pochodzący z Kordoby don Juan de Cervantes, prawnik z wykształcenia, który doszedł do stanowiska corregidora. Porzucił on swoją żonę i rodzinę. Ojcem Miguela był Rodrigo de Cervantes – medyk i cyrulik, a matką doṅa Leonor de Cortinas. Miguel był czwarty z siedmiorga rodzeństwa. W 1551 roku rodzina Miguela przeniosła się z Alcalá de Henares do Valladolid, a w 1553, po opuszczeniu przez Rodrigo więzienia, do którego trafił za niespłacone długi, rodzina przeniosła się do Kordoby. Miguel przez dwa lata uczył się w szkole prowadzonej przez Alonso de Vieras, a potem w kolegium jezuickim. W 1564 mieszkał w Sewilli, a od 1566 w Madrycie, pobierając nauki u znanego humanisty Juana Lopeza de Hoyos, który być może zaszczepił w Cervantesie miłość do literatury. W wyniku zranienia w zakazanym pojedynku Antonia de Sigura, skazany zaocznie Cervantes musiał uciekać z Madrytu. W 1569 r. dotarł do Rzymu.
W Rzymie Cervantes był szambelanem na dworze Acquavivy, wkrótce jednak postanowił wstąpić do hiszpańskiego regimentu stacjonującego w Neapolu (którym w owym czasie należał do korony hiszpańskiej). W słynnej bitwie w zatoce Lepanto (1571), w której walczył pod dowództwem don Juana de Austria, wykazał się dużą odwagą, stając do walki mimo dręczącej go gorączki. Za brawurę zapłacił dwiema ranami postrzałowymi, w pierś i strzaskaną lewą ręką, która pozostała nieruchoma do końca życia. Po bitwie szybko wrócił do czynnej służby. Przez dłuższy czas stacjonował w Neapolu, gdzie zapoznawał się z włoską literaturą.
Zwolniony ze służby wojskowej po czterech latach, w drodze do kraju, został wzięty do niewoli przez berberyjskich korsarzy, którzy sprzedali go bejowi Algieru. Ponieważ miał ze sobą listy polecające od księcia de Sessa i Don Juana, cena okupu za niego była wyższa niż za przeciętnego chrześcijanina, a tym samym czas oczekiwania na wykupienie wydłużał się. Jednocześnie niewykluczone, że listy tak ważnych osób uratowały mu życie, gdy kilkakrotnie próbował ucieczki. Niewola trwała pięć lat; o jego obecności w Algierze wspominają co najmniej dwa źródła z tamtego okresu. Wydaje się, że dość krnąbrny jako niewolnik Cervantes był przez swych kolejnych właścicieli traktowany z pobłażaniem, a jego talent, zarówno przywódczy, jak i artystyczny, zyskał mu uznanie współtowarzyszy niedoli. Wreszcie w 1580 rodzina Cervantesa uzbierała 500 eskudów i Miguel został wykupiony, niemal w ostatniej chwili, bo jego ostatni właściciel przenosił się właśnie do Konstantynopola i zamierzał zabrać ze sobą wszystkich niesprzedanych niewolników.
W pierwszym okresie po uwolnieniu znów zaciągnął się do wojska i stacjonował w Portugalii, świeżo podbitej przez Filipa II. Wkrótce opuścił armię na zawsze i po romansie (którego owocem była córka) z młodą mężatką o nazwisku Ana de Villafranca ożenił się z młodszą od siebie o 18 lat Cataliną de Salazar y Palacios, właścicielką niewielkiego majątku ziemskiego w Esquivias, niewielkiej wiosce w La Manchy. Po kilku bezowocnych próbach zatrudnienia się na dworze Filipa II zrezygnował i postanowił żyć wyłącznie z literatury, do czego zresztą został niejako zmuszony, ponieważ euforia wywołana bitwą w zatoce Lepanto już dawno przeminęła, a jego zasługi wojenne poszły w zapomnienie, nie dając gratyfikacji, na które być może liczył. Utrzymywanie się z pisarstwa okazało się rzeczą bardzo trudną. Wkrótce napisał pierwszą powieść, romans pasterski Galatea, wydany w 1585, który dedykował przyjacielowi Acquavivy, być może z nadzieją, że ten roztoczy nad nim patronat.
Był to okres bujnego rozkwitu hiszpańskiego teatru. Także Cervantes spróbował swych sił jako dramaturg. Z jego dzieł dramatycznych tego okresu zachowały się tylko dwie sztuki: El trato de Argel (Życie w Algierze) i tragedia z dziejów rzymsko-hiszpańskich Numancja (La Numancia). Jak twierdził, napisał w owym czasie kilkadziesiąt sztuk, w tym La confusa, którą uważał za najlepszą. Jednak jako autor dramatyczny Cervantes nie osiągnął powodzenia, choć, jak pisał po latach, jego sztuki zostały przyjęte przez publiczność „bez obrzucania aktorów warzywami”.
Nędza zmusiła go w końcu do podjęcia innej pracy; otrzymał urząd komisarza do spraw zaopatrzenia Wielkiej Armady. Nie wiązało się to z łatwym życiem, ale dawało pewne możliwości, m.in. podróży po Andaluzji, których reminiscencje znalazły oddźwięk w jego późniejszych utworach. Jednakże problemy ze zbilansowaniem ksiąg i procesy z przełożonymi, którzy zarzucali mu złą wolę i działanie na szkodę Armady, sprowadziły nań wielkie przykrości i we wrześniu 1592 roku znalazł się w więzieniu, na szczęście tylko na kilka dni. W 1594 roku wyjechał do Madrytu i tam otrzymał posadę poborcy podatkowego, która choć nobilitująca w stosunku do poprzedniej, to jednak wiązała się z równie nieprzyjemnymi obowiązkami. Po zaledwie dwóch latach Cervantes odrzucił ten zaszczyt i wyjechał do Sewilli, gdzie oddał się swej ulubionej sztuce: poezji.
Mając za sobą wygrany w 1595 roku konkurs poetycki w Saragossie, Cervantes oddał się z lubością swej ulubionej acz nadal mało płatnej pracy. Dał się poznać jako prześmiewca piętnujący ludzkie przywary i nie lękający się krytykować i odbrązawiać znane postaci, jak choćby księcia Medinę Sidonię, nieudolnego wodza Wielkiej Armady. Satyryczny charakter ma także sonet Na katafalk Filipa II w Sewilli, napisany po śmierci króla w 1598 r., w którym pochwała wspaniałych uroczystości pogrzebowych skrywa gorzką kpinę.
Wkrótce doszło do prawnego epilogu problemów buchalteryjnych sprzed czterech lat i Cervantes znalazł się ponownie w więzieniu, tym razem na dłużej. Być może tam powziął zamiar napisania Don Kichota.
W 1604 roku, po kilku latach podróży i publikowania niewielkich utworów, Cervantes odsprzedał prawa do druku swojego największego dzieła Francisco de Roblesowi, tym samym pozbawiając się ewentualnych tantiem z kolejnych wydań. Jego największe dzieło El ingenioso hidalgo Don Quixote de la Mancha ukazało się w 1605 roku, ale treść, przynajmniej w części, była znana już wcześniej wielkiemu współczesnemu Cervantesa – dramaturgowi Lope de Vega. Powieść okazała się prawdziwym sukcesem i wkrótce doczekała się kolejnych wydań w Madrycie, Brukseli, Mediolanie, nie przynosząc jednak autorowi żadnych korzyści materialnych, lecz tylko sławę. Część druga Segunda parte del ingenioso hidalgo Don Quixote de la Mancha ukazała się w 1615 roku, prawdopodobnie wskutek działań nieznanego pisarza kryjącego się pod pseudonimem de Avellanedy, który napisał „drugą część przygód Don Kichota z La Manchy”, ocenianą jako dobrą, lecz nie dorównującą oryginałowi, co więcej, zawierającą uszczypliwe uwagi wobec Cervantesa. Ten podjął rękawicę i nie tylko odpowiedział krytyką swego anonimowego adwersarza (zawartą w drugiej części rozdziału 59), ale wydał drukiem prawdziwą drugą część przygód rycerza z La Manchy, której popularność dorównała wkrótce części pierwszej.
Sukces Don Kichota sprawił, że wydawcy chętnym okiem spoglądali na utwory nieznanego wcześniej autora. W 1613 wyszły drukiem Nowele przykładne (Novelas ejemplares), zbiór nowel w stylu włoskim. W 1614 Cervantes opublikował Podróż na Parnas (Viaje del Parnaso), żartobliwy poemat opisujący obronę Parnasu przed kiepskimi poetami przez pisarzy wezwanych na pomoc przez Apolla. Poemat był okazją do pochwał i krytyk współczesnych i dawnych pisarzy; Cervantes włączył samego siebie w grono obrońców Parnasu, a w autoironicznej scenie, zaproszony przez Apolla, by „złożył swój płaszcz i usiadł na nim” u boku boga, tłumaczył, że nie może tego uczynić, bo jako poeta jest tak biedny, że nawet nie ma płaszcza. Cervantes tworzył także drobne humorystyczne utwory sceniczne z gatunku entremés[2].
Upragniony sukces wydawniczy obu części przygód Don Kichota przyszedł późno i nie zdążył zapewnić ich autorowi spokojnego i beztroskiego życia. Cervantes – pechowo dobierający sobie protektorów i nękany dziwnymi zbiegami okoliczności (jak choćby w 1605 roku krwawe zajście przed domem, w którym mieszkała jego rodzina, co zmusiło ją do wyjazdu do Madrytu) oraz nieustannymi procesami o odszkodowania, choćby za niemoralne prowadzenie się jedynej córki Isabel – umarł w Madrycie 23 kwietnia 1616 roku (niektóre źródła wspominają o 22 kwietnia jako o faktycznej dacie śmierci, dzień później miał on być jedynie pochowany), zaledwie 12 lat po opublikowaniu pierwszej części Don Kichota. Pochowano go następnego dnia w klasztorze trynitariuszy bosych przy dzisiejszej ulicy Lope de Vegi.
Ostatnie miesiące życia upłynęły Cervantesowi na pracy nad powieścią Niezwyczajne przygody Persilesa i Sigismundy (Los trabajos de Persiles y Segismunda). Zaledwie na cztery dni przed śmiercią, świadomy że jego życie się kończy (co podkreślał, cytując wiersz „Już nogę mając w strzemieniu”), Cervantes podpisał dedykację skierowaną do księcia de Lemos, w której pożegnał swego protektora. W napisanym nieco wcześniej „Prologu” umieścił natomiast melancholijne zdanie: „Adiós, żarty, adiós, drwiny, adiós, weseli przyjaciele...”[3] Powieść została opublikowana w 1617 r.
Kalidasa (skr. कालिदास, trl. Kālidāsa, "sługa Kali") – indyjski poeta i dramaturgiem z epoki klasycznego sanskrytu.
Trudno dokładnie określić, kiedy tworzył, najprawdopodobniej jednak w okresie imperium Guptów, w IV, V[1] lub VI wieku n.e. (mógł być poetą na dworze Ćandragupty II[2]). Jego sztuki i poezja opierały się przede wszystkim na indyjskiej mitologii i filozofii.
W literaturze sanskrytu zajmuje podobne miejsce jak William Szekspir w literaturze angielskiej. Sława spowodowała, że przypisywano mu też dzieła (ok. 30), których autorem pewnie nie był, takie jak Nālodaya i Śrutabodha. Badacze uważają obecnie, że te utwory napisali inni pisarze o takim samym imieniu - Kālidāsa. 
Jest autorem dzieł: 
Na język polski przełożono z oryginału Śākuntalę (Kalidasa, Siakuntala, przełożył Stanisław Schayer: Ossolineum 1957, Biblioteka Narodowa II:111, z obszernym wstępem; wcześniejsze wojenne wydanie: Budapeszt 1941) oraz Maghadutę (Joanna Sachse, na łamach Przeglądu Orientalistycznego, ok. 1980). Fragmenty dzieł w przekładzie Stanisława Schayera znaleźć można także w Wielkiej literaturze powszechnej, wydawca Trzaska, Evert i Michalski, Warszawa (ok. 1930), t. I, s. 115-226; a w przekładzie Juliana A. Święcickiego w Historyi literatury powszechnej w monografijach, Warszawa 1901, t. IV, s. 290-301. 
Wergiliusz, właściwie Publiusz Wergiliusz Maro (łac. Publius Vergilius Maro), niekiedy Publiusz Wergiliusz Maron, dawniej także Wirgiliusz (ur. 15 października 70 p.n.e., zm. 21 września 19 p.n.e.) – rzymski poeta, uważany za jedną z najważniejszych postaci w dziejach światowej literatury, autor Eneidy, Georgik, Bukolik﻿(inne języki) i Drobiazgów﻿(inne języki).
Wergiliusz pochodził z niezamożnej rodziny z Galii Cisalpejskiej. W młodości znajdował się pod wpływem innych rzymskich poetów, co było widoczne w jego młodzieńczych Drobiazgach. Swój własny język, styl i sposób wyrazu odnalazł w Bukolikach, które przyniosły mu sławę i tytuł największego poety Rzymu. Na talencie Wergiliusza poznali się Mecenas i August, którzy obdarzyli go opieką, majątkiem i przyjaźnią. Po opublikowaniu Georgik, przyjętych od razu jako arcydzieło, na prośbę Augusta rozpoczął pisać Eneidę, epopeję poświęconą historii Rzymu. Śmierć uniemożliwiła mu dokończenie tego poematu.
Dzieła Wergiliusza przynajmniej do początków XIX wieku znajdowały się w centrum kanonu literackiego, wywierając olbrzymi wpływ na europejską kulturę, naukę, a nawet politykę. Bezpośrednio lub pośrednio oddziaływały na całą późniejszą literaturę, a ich ślady są obecne w każdej epoce i wszystkich krajach, które czerpały wzorce z klasycznej łaciny. Powszechnie określano Wergiliusza, za Dantem, tytułem „najwyższego poety” (l’altissimo poeta). Przez współczesnych badaczy nazywany jest „Ojcem Zachodu”, gdyż stał się literackim pomostem pomiędzy tradycją pogańską a chrześcijańską – uważano go za wieszcza (vates), „duszę z natury chrześcijańską” (anima naturaliter christiana), a nawet proroka (propheta).
Większość współczesnych badaczy twierdzi, iż o Wergiliuszu zachowało się bardzo niewiele wiarygodnych informacji. Dzieła samego poety i współczesnych mu autorów przekazują zaledwie kilka faktów z jego życia. Dlatego podstawowymi, chociaż mało wiarygodnymi źródłami, za pomocą których można odtworzyć biografię Wergiliusza, są życiorysy sporządzone przez późniejszych autorów, zwane Vitae Vergilianae (Żywoty wergiliańskie)[1].
Najstarszy żywot Wergiliusza, który napisał Swetoniusz, nie zachował się. Opierał się na nim św. Hieronim, który poświęcił Wergiliuszowi rozdział w swoich Kronikach﻿(inne języki). Za podstawową biografię poety uważa się żywot, który napisał około połowy IV wieku Eliusz Donat. Dzieło Donata nie zachowało się w całości, ale część brakujących fragmentów tekstu zrekonstruowano z cytatów u innych pisarzy, przede wszystkim Serwiusza i Filargiriusza﻿(inne języki). Teksty Donata, Serwiusza i Filargiriusza stanowiły podstawę dla anonimowej biografii Wergiliusza, przypisywanej w średniowieczu Markowi Waleriuszowi Probusowi﻿(inne języki) (I wiek), ale napisanej prawdopodobnie w końcu V lub na początku VI stulecia. Podobne źródła ma wierszowana Vita Wergiliusza, którą w V wieku ułożył gramatyk Fokas[1].
Żywoty średniowieczne, które przeważnie opierały się na Donacie i Serwiuszu, dodawały do biografii Wergiliusza wiele nowych szczegółów. Najważniejsze z nich to Vita Noricensis (VIII w.), Vita Bernensis (IX w.), Vita Monacensis (X w.). Z XVI stulecia pochodzi anonimowy Donatus auctus – biografia Wergiliusza oparta na tekście Donata, ale poszerzona o liczne, nieznanego pochodzenia wstawki. Wszystkie średniowieczne i XVI-wieczne dodatkowe informacje o Wergiliuszu uważane są we współczesnej nauce za niewiarygodne[1].
Wergiliusz urodził się najprawdopodobniej w 70 roku przed Chrystusem, za pierwszego konsulatu Pompejusza i Krassusa, w Galii Cisalpińskiej. Dom rodzinny poety leżał nad rzeką Mincius, stąd używano wobec poety przydomku Minciades. Miał trzech młodszych braci, z których tylko jeden (przyrodni), Waleriusz Prokulus, przeżył go. Ojciec poety był według jednych źródeł garncarzem, według innych sługą woźnego Magiusa, którego córkę – Magię Pollę – poślubił. Dzięki pomocy teścia i własnej pracowitości ojciec poety zakupił niewielki majątek w Andes (według tradycji współcześnie Virgilio lub Pietole Vecchia) koło Mantui. Dlatego Wergiliusza nazywano również Mantuanus lub Andinus[2]. Zgodnie z późniejszą legendą, życie Wergiliusza od początku naznaczały cudowne wydarzenia. Jako niemowlę nie płakał, a gałąź topoli, włożona w ziemię z okazji jego urodzin, rozrosła się wkrótce do olbrzymich rozmiarów[3].
Uprawa roli, a także hodowla pszczół, zapewniły rodzinie Maronów źródło utrzymania pozwalające na wykształcenie dzieci. Ojciec wysłał młodego Wergiliusza do szkoły, jednak nie w pobliskiej Mantui, ale w odległej o około 70 kilometrów Cremonie, gdzie mógł uczyć się czytać i pisać pod okiem lepszych nauczycieli (litteratori). Po osiągnięciu pełnoletniości w roku 55 (jak twierdzą niektórzy biografowie – w dniu śmierci Lukrecjusza), Wergiliusz wyjechał do Mediolanu, gdzie kształcił się u gramatyków (grammatici)[2].
W roku 53 przeniósł się do Neapolu, gdzie u tamtejszych Greków pobierał lekcje retoryki i filozofii. Uczył się języka greckiego i studiował grecką literaturę. Donat przekazał dwa nazwiska nauczycieli poety. Pierwszym był nieznany bliżej Epidiusz – retor, u którego miał się również kształcić młodszy o siedem lat Oktawian. Drugim natomiast Siron, epikurejczyk z Neapolu, uczeń Filodemosa z Gadary. Niektórzy badacze uważają, że Wergiliusz studiował również u poety Partheniosa z Nicei[2].
Pod wpływem Syrona Wergiliusz stał się zwolennikiem nauk Epikura, co oddał w jednym z młodzieńczych wierszy[4]:
Nos ad beatos vela mittimus portus
magni petentes docta dicta Sironis,
vitamque ab omni vindicabimus cura[5].
„Łódź moja płynie do błogiej przystani,
Gdzie według wskazań wielkiego Syrona
Wyplączę życie z wszelkich trosk[6].”
Po kilku latach Wergiliusz porzucił jednak epikureizm na rzecz stoicyzmu. Przez całe życie wyrażał swoje zamiłowanie do filozofii – od młodości planował podróż do Grecji, chciał tam poświęcić się pracy naukowej[4].
Pod koniec 53 roku Wergiliusz przeniósł się z Neapolu do Rzymu, gdzie rozpoczął pracę jako prawnik. Okazało się, że nie miał talentu do retoryki, którą studiował w Neapolu. W Rzymie uważany był za mało skutecznego adwokata, który nie potrafił przekonać do swoich racji sądu. Do tego przed każdą publiczną mową spalała go trema[4].
W okresie, gdy pracował jako prawnik, Wergiliusz poznał i zaprzyjaźnił się z grupą rzymskich literatów, zwanych neoterykami. Szczególnie bliskie stosunki łączyły go z poetami Korneliuszem Gallusem i Azyniuszem Pollionem oraz prawnikiem z Cremony, Alfenusem Warusem[2]. Na spotkaniach w gronie przyjaciół, gdy zaczynał mówić wierszem, język Marona – jak przekazali świadkowie tych wydarzeń cytowani przez Donata – przeobrażał się w cudowną, uwodzicielską słodycz[4].
Podczas pobytu w stolicy powstały niewydane za życia poety utwory, zebrane po jego śmierci w zbiorze Drobiazgi[2]. Jednak po kilku nieudanych latach pracy w charakterze prawnika, z powodów finansowych i zdrowotnych Wergiliusz pożegnał się z przyjaciółmi i przeniósł z powrotem do Neapolu, gdzie osiadł na stałe. Część uczonych uważa, że utrzymywał się tam z pensji urzędnika[7].
W Neapolu, około połowy lat czterdziestych, Wergiliusz zaczął pisać utwory bukoliczne, które w latach trzydziestych zostały zebrane i wydane w jednym zbiorze jako poemat. Według Serwiusza i części współczesnych badaczy, pierwszym zaprezentowanym publicznie dziełem poety był wiersz, który później w zbiorze Bukolik został uszeregowany jako Ekloga VI[8]. Do tego utworu nawiązał Cyceron w swoich Rozmowach tuskulańskich i O znakach bożych﻿(inne języki), stąd wydarzenie, o którym napisał Serwiusz, datuje się najczęściej na rok 45. W ramach przerywnika muzycznego, podczas przedstawienia teatralnego w Rzymie, rozbrzmiała pieśń, której towarzyszyły występy taneczne. Wykonała ją najbardziej popularna wówczas aktorka, ogólnie uznana piękność Wolumnia – znana również pod pseudonimami Kyteris lub Lykoris. Pod tym ostatnim imieniem opiewał ją jej kochanek, poeta Korneliusz Gallus. Tamtego wieczoru specjalny utwór, składający się z osiemdziesięciu sześciu heksametrów, przygotował dla Wolumnii Wergiliusz[9].
Utwór był na owe czasy osobliwy, głównie za sprawą bogactwa poruszonych w nim, niesamowitych opowieści. Troje ludzi obezwładniło odsypiającego alkoholową libację starego satyra, Sylena. Satyr wykupił się intonując piosenkę, która oddziałała na całą otaczającą przyrodę. Śpiewał w niej o historii ludzkości, poruszając między innymi tematy powstania wszechświata z atomów, potopu, czy kradzieży ognia przez Prometeusza. Potem przeszedł do przykładów nieszczęśliwych miłości, które oddziałały na dalsze losy świata. W kulminacyjnym punkcie pieśni Sylen opiewał miłosną mękę królowej Krety, która bez wzajemności zakochała się w byku. W poruszających wersach opisywał zazdrość Kretenki o krowę. W końcowych fragmentach satyr opowiadał już o współczesnej słuchaczom, rzymskiej rzeczywistości – miłości Gallusa do Lykoris. Gdy pieśń dobiegła końca, Sylen śpiewał podobno dalej, aż do nadejścia nocy, ale Lykoris zakończyła swą kwestię już po kilku wersach[10].
Jak podkreślali rzymscy gramatycy, osobliwość utworu nie wynikała jednak jedynie z treści. Ich zdaniem wersy brzmiały zaskakująco miękko, a jednocześnie – mimo całej swojej harmoniczności – posiadały zarazem rzymską dostojność, jakiej w ciągu dwustu lat istnienia łacińskiej poezji dotąd nie słyszano[10]. Opiewanie zazdrości królowej Krety o krowę Wergiliusz rozpoczął wersem:
O virgo infelix, tu nunc in montibus erras[11].
„Nieszczęsna, teraz góry błędnym zwiedzasz krokiem[12].”
Wers o podobnej postaci był, według ówczesnych zasad poezji rzymskiej, niedoskonały – cztery stopy wierszowe składały się tylko z monotonnych sylab długich (spondejów): o vir–g(o)–infe–lix tu–nunc in... Poeta powinien też uniknąć wygłosowego, długiego „o” z następującym długim „i”: virg(o) infelix. Jednak dzięki temu Wergiliusz uzyskał niespodziewany – wzbudzający emocje wśród publiczności – efekt. Obecny w teatrze Cyceron był pod wielkim wrażeniem tego, co usłyszał i nazwał poetę magnae spes altera Romae („drugą nadzieją wielkiego Rzymu” – oczywiście po samym Cyceronie)[3].
Po bitwie pod Filippi w roku 42 weterani domagali się od Marka Antoniusza i Oktawiana, jako wynagrodzenie za trudy wojenne, przydziału ziemi w Italii. Ażeby zadośćuczynić ich żądaniom przystąpiono do rozdzielania między żołnierzy gruntów Cremony, która w czasie wojny opowiedziała się po stronie obrońców republiki i wystąpiła przeciwko triumwirom. Wywłaszczeniem została objęta również sąsiadująca z Cremoną Mantua. Do parcelacji na rzecz weteranów przeznaczone zostały między innymi grunty należące do rodziny Wergiliusza[13].
Według części badaczy, po wysiedleniu z Mantui, cała rodzina Maronów przybyła do Neapolu i zamieszkała w willi należącej do filozofa Syrona, leżącej niedaleko miasta. Wergiliusz wkrótce odzyskał jednak rodzinny majątek dzięki pomocy przyjaciół – Korneliusza Gallusa, Azyniusza Polliona oraz Alfenusa Warusa – pełniących na zlecenie Oktawiana obowiązki komisarzy przeprowadzających parcelację. Przyjacielem Wergiliusza był również główny mierniczy Oktawiusz Musa. Według kilku przekazów poeta miał osobiście starać się u Oktawiana o cofnięcie nakazu wysiedlenia[13].
Niektórzy uczeni uważają natomiast, że Wergiliuszowi dwa razy odbierano majątek. Początkowo wstrzymano parcelację, dzięki wstawiennictwu życzliwych komisarzy i decyzji Oktawiana. Jednak wybuch wojny peruzyńskiej między Oktawianem a Markiem Antoniuszem w roku 40, doprowadził w Gallii Cisalpińskiej do wznowienia wysiedleń oraz parcelacji gruntów. Rodzina Wergiliusza utraciła ostatecznie swój majątek i nie uzyskała żadnego odszkodowania[13].
Wkrótce po wysiedleniu zmarł ojciec poety. Wergiliusz nie powrócił już nigdy do Mantui. Zamieszkał na stałe w Neapolu, skąd często wyjeżdżał, ale prawie wyłącznie do Rzymu[13].
Azyniusz Pollion, po wyjeździe Wergiliusza z Rzymu i osiedleniu się poety w Neapolu, zachęcał Marona do dalszej twórczości. Jak wspomniał w jednym z wierszy sam Wergiliusz, Bukoliki były utworami pisanymi na życzenie przyjaciela (iussis carmina coepta tuis). Pollion był bowiem pod wrażeniem sukcesu, jaki odniosła pierwsza pieśń Wergiliusza zaprezentowana w teatrze[14].
Bukoliki otworzyły Wergiliuszowi drogę do domu Mecenasa, gdzie prawdopodobnie wprowadził go Azyniusz Pollion. Nie jest jasne, kiedy Wergiliusz poznał Mecenasa. Większość uczonych uważa, że w końcu lat czterdziestych, skoro z początkiem roku 38, z poetą Lucjuszem Wariuszem Rufusem, Wergiliusz przedstawił Mecenasowi Horacego. O tym wydarzeniu wspominał w swoich wierszach Horacy[13].
Pomoc przyjaciół, a potem hojność Mecenasa, uratowały Wergiliusza, w latach czterdziestych, od biedy. Mecenas podarował poecie dom w Rzymie – na Eskwilinie, obok swoich ogrodów. Natomiast w latach trzydziestych Wergiliusz otrzymał od Oktawiana kilka posiadłości ziemskich na Sycylii i w Kampanii (jedną z nich pod Neapolem) – być może tytułem spóźnionego odszkodowania za skonfiskowany majątek[13].
Wiosną 37 roku Wergiliusz znalazł się w orszaku Mecenasa (obok Horacego, Lucjusza Wariusza, Plocjusza Tukki﻿(inne języki), Marka Kokcejusza﻿(inne języki) i retora Heliodora), odbywającego na zlecenie Oktawiana misję dyplomatyczną do Brundyzjum dla uzyskania pomocy od Antoniusza przeciwko nękającemu wybrzeża Italii synowi Pompejusza, Sekstusowi Pompejuszowi[13].
Według świadectw starożytnych, uwolniony od trosk materialnych, Wergiliusz od roku 37 lub 36 pisał, w swojej posiadłości niedaleko Neapolu, Georgiki. Musiał je ukończyć najpóźniej w roku 29, gdyż Donat i Serwiusz podają, że tego roku Wergiliusz, na zmianę z Mecenasem, czytali poemat Augustowi, który przebywał na leczeniu gardła w Atelli﻿(inne języki) w Kampanii. Wergiliusz był wówczas również poważnie chory i nie miał siły przeczytać na głos całych Georgik[15].
Ostatnie dziesięć lat życia poświęcił Wergiliusz pracy nad Eneidą, której publikację zapowiedział w trzeciej księdze Georgik. Na napisanie epopei poświęconej historii Rzymu namówił go August. O tym, że Maron pracował w Neapolu nad Eneidą wiedziano w Rzymie, gdyż około roku 26 Propercjusz postanowił nie opiewać zwycięstwa Oktawiana pod Akcjum. Stwierdził, że dokona tego Wergiliusz, który zajmuje się orężnymi czynami trojańskiego Eneasza i opowiada o założeniu miasta na lawińskim wybrzeżu[16]. W roku 23 Wergiliusz czytał na dworze Augusta ukończone (choć może jeszcze nie wygładzone) księgi II, IV, VI. Podobno była już wtedy znana przyjaciołom poety księga I. W roku 22 księgę VIII prywatnie czytali Tibullus i Propercjusz[17].
W roku 19 Wergiliusz spełnił swoje marzenie i wyruszył do Grecji. Chciał podróżować śladami Homera i dokończyć Eneidę. Dotarł do Aten, gdzie unieruchomiła go choroba. Tam spotkał się z powracającym ze wschodnich prowincji Imperium Augustem, na którego prośbę przyłączył się do cesarskiego orszaku kierującego się do Rzymu. W trakcie podróży stan zdrowia Marona uległ dalszemu pogorszeniu z powodu upałów, które dokuczały mu w czasie zwiedzania leżącej na Przesmyku Korynckim Megary. Zmarł wkrótce po dopłynięciu do Italii, w Brundyzjum[18].
Zgodnie z jego ostatnią wolą, Wergiliusz został pochowany w Neapolu, przy drodze do Puteoli. Na grobie wyryto inskrypcję z dwuwierszem﻿(inne języki), które według jego biografów ułożył sam poeta[1]:
Mantua me genuit, Calabri rapuere, tenet nunc
Parthenope; cecini pascua, rura, duces.
„Mantui dłużnym życie, Kalabrom zgon, pośmiertną ostoję
Parthenopie; opiewałem pastwiska, role i boje.”
Pozostawił testament, w którym majątek (dom w Rzymie i posiadłości wiejskie) przekazał Augustowi, Mecenasowi, bratu przyrodniemu Waleriuszowi Prokulusowi oraz poetom Lucjuszowi Wariuszowi Rufusowi i Plocjuszowi Tukce﻿(inne języki). Polecił też nie wydawać Eneidy, którą określił dziełem niewykończonym i niegodnym publikacji. Ten ostatni punkt testamentu nie został wykonany, gdyż Wariusz i Tukka – na polecenie Augusta – zredagowali i wydali poemat z rękopisu autora[1]. Wkrótce narodziła się legenda, zgodnie z którą na łożu śmierci Wergiliusz próbował spalić jedyny egzemplarz nieukończonej Eneidy, ale August powstrzymał go w ostatniej chwili[19].
Już starożytni komentatorzy zauważyli, że dzieła Wergiliusza układają się w pewną całość. Pasterzy z Bukolik poeta porzucił w Georgikach na rzecz rolników, aby w Eneidzie snuć opowieści o królach i książętach. W tych trzech pracach – które powstały w sposób nie całkiem przypadkowy – odzwierciedlił Maron trzy etapy rozwoju ludzkości: od hodowli bydła, przez uprawę roli, aż do budowy miast i państw[20].
Również gatunki literackie, po które sięgał, były coraz bardziej znaczące i szlachetne. W Bukolikach opierał się na hellenistycznym, żyjącym około 200 lat przed nim Teokrycie, greckim poecie z sycylijskich Syrakuz. Za pomocą poematu o rolnictwie nawiązał do tworzącego ponad 700 lat przed nim beockiego klasyka Hezjoda. W końcu, wraz z Eneidą, wszedł w szranki z legendarnym Homerem, w jednym eposie liczącym dwanaście ksiąg łącząc i przekształcając wzorce z obu dzieł greckiego epika – Iliady i Odysei[21].
Dla późniejszych komentatorów poematy Wergiliusza były przykładami zastosowania trzech różnych stylów literackich w poezji – niskiego (Bukoliki), średniego (Georgiki) i wysokiego (Eneida)[22]. W XII wieku Jan z Salisbury upatrywał w twórczości Wergiliusza alegorię trzech stron życia – Bukoliki to życie kontemplacyjne, Georgiki zmysłowe a Eneida czynne[23].
Pod imieniem Wergiliusza został przekazany zbiór różnych utworów poetyckich z I wieku przed Chrystusem i I wieku po Chrystusie (wergiliana), zwany współcześnie Appendix Vergiliana﻿(inne języki) („Dodatek do dzieł Wergiliusza”). Już w późnej starożytności uważano ten zbiór za młodzieńczą twórczość Wergiliusza. Donat, obok dwuwiersza In Ballistam, którym miałby młody Wergiliusz rozpocząć twórczość poetycką, wymienia utwory: Catalepton, Priapea, Epigrammata, Dirae, Ciris, Culex oraz z pewnym zastrzeżeniem Aetna. Natomiast Serwiusz sporządził następującą listę: Ciris, Aetna, Culex, Priapea, Catalepton, Epigrammata, Copa, Dirae. Lista ta została poszerzona w wiekach średnich, kiedy różni pisarze zaczęli przypisywać Wergiliuszowi utwory Maecenas (elegia na śmierć Mecenasa, zmarłego 11 lat po Wergiliuszu) oraz powstałe niewątpliwie w średniowieczu: De viro bono, Est et non, De rosis nascentibus i kilka innych. Autor XVI-wiecznego Donatus auctus rozszerzył listę wergilianów o utwór Moretum[24].
Po odrzuceniu pozycji dodanych w okresie średniowiecza, współcześni uczeni przeanalizowali utwory z listy Serwiusza. Nieliczna grupa filologów uważa, że wszystkie są młodzieńczymi wierszami Wergiliusza. Niektórzy badacze kwestionują autentyczność całego zbioru. Przeważająca liczba uczonych jest zdania, że autentyczne są wiersze ze zbioru „Drobiazgi” (Catalepton)[24].
Drobiazgi (Catalepton) to zbiór osiemnastu krótkich wierszy. Nazwa nawiązuje do greckiego terminu κατὰ λεπτόν, którym Aratos z Soloj określił drobne utwory poetyckie. Tytuł ten nie pochodzi od zajmujących się – zgodnie z wolą poety – jego spuścizną Lucjusza Wariusza Rufusa i Plocjusza Tukki﻿(inne języki), ale został nadany prawdopodobnie przez wydawcę z czasów cesarza Tyberiusza. Wydawca ten w jedną księgę włączył różne wiersze[25].
Zbiór obejmuje trzy priapea oraz piętnaście epigramów o zróżnicowanej treści, napisanych w różnych miarach wierszowych. Część badaczy uważa priapea za nieautentyczne, natomiast epigramaty, według zdecydowanej większości filologów, można z dużą pewnością uznać za młodzieńcze utwory Wergiliusza[25].
Bukoliki﻿(inne języki) (Eclogae) to zbiór dziesięciu wierszy zebranych w jeden poemat. Chronologia powstania Bukolik jest sporna. Zdaniem części badaczy dzieło napisane zostało pomiędzy rokiem 42 a 39, jednak niektórzy przesuwają czas powstania najstarszej eklogi na rok 45 a ostatniej na rok 35. Istnieje też hipoteza, że zbiór miał dwie różne edycje, ale problem pozostał otwarty[26].
Wergiliusz opierał się na sielankach Teokryta i przyswoił ten gatunek literaturze łacińskiej. Od Teokryta przejął Wergiliusz niektóre tematy i motywy, greckie imiona pasterzy, pewne elementy sycylijskiego krajobrazu. Zachował też jego strukturę utworów, w której odśpiewywane są kolejne zwrotki przerywane refrenem. Nie wzorował się jednak na Teokrycie dosłownie, lecz wybierał niektóre elementy – łącząc je z rzymską tematyką społeczną, religijną i polityczną[27].
Tłem Bukolik Marona nie była Sycylia, ale baśniowy krajobraz Arkadii oraz realny Italii. Wergiliusz wyszedł poza grecką konwencję gatunku – w eklogach przeważają rysy nowe, oryginalne. Idylla Teokryta pod piórem Wergiliusza stała się rzymską eklogą[28]. Eklogi I i IX ukazują ból chłopa rzymskiego wyzutego ze swej ojcowizny. Ekloga IV mówi o marzeniach o powrocie złotego wieku Rzymu, zapanowaniu pokoju i odrodzeniu ludzkości, zawiera również tak zwaną mesjańską przepowiednię Wergiliusza. Z Bukolik historycy czerpią też wiedzę o skądinąd nieznanych wydarzeniach z historii Rzymu, na przykład Ekloga VI może stanowić źródło do poznania życia żołnierza i polityka Korneliusza Gallusa, o którym jest także mowa w Eklodze X. Jeżeli w założeniu eklogi miały być przebranymi w łacińską szatę słowną piosenkami sycylijskich pasterzy, ułożonymi dla zabawienia przyjaciół, to osobiste przeżycia poety i ówczesna rzeczywistość nie pozwoliły na zamknięcie się w tych ramach[27].
Dziesięć eklog Wergiliusza łączą nie tylko wspólne motywy i postacie, ale przede wszystkim wartości emocjonalne – wyrażane w nich gorące uczucia do ojczyzny, rodaków, rodziny, przyjaciół – podsumowane w jednym z wierszy słowami amor vincit omnia („miłość wszystko zwycięża”)[29].
Autora Bukolik ukształtowała zarówno lektura poetów greckich, jak i obcowanie z kołem neoteryków – Azyniuszem Pollionem, Korneliuszem Gallusem i Alfenusem Warusem. Od neoteryków Wergiliusz przejął zainteresowanie dla poezji aleksandryjskiej i Teokryta. Jednak język bukolik różni się od języka wcześniejszych poetów łacińskich, piszących sielanki, brakiem grecyzmów oraz szczególną czystością – chociaż przemawiają w nich pasterze, nie ma wyrażeń prowincjonalnych lub ludowych[30].
Eklogi zachwyciły już pierwszych słuchaczy i czytelników melodyjnością. W oszczędnym, powściągliwym języku zawarty był wielki ładunek uczuciowy, a poeta zachował – zadziwiającą także współczesnych krytyków – harmonię między wartościami emocjonalnymi i estetycznymi[30].
Georgiki (Georgica) to zawarty w czterech księgach poemat o rolnictwie, sadownictwie, hodowli bydła i pszczelarstwie. Kiedy August w roku 30 zawierał na wschodzie Imperium – po samobójstwach Marka Antoniusza i Kleopatry – pokój z Partami, Wergiliusz w swoim majątku pod Neapolem właśnie kończył dedykowany Mecenasowi utwór[31].
Według świadectw starożytnych Maron pisał poemat przez siedem lat – od roku 37 do 30 lub od 36 do 29. W roku 29 Wergiliusz, na zmianę z Mecenasem, czytał Georgiki Augustowi, gdy cesarz przebywał na leczeniu gardła w Atelli﻿(inne języki), niedaleko Neapolu[15].
Georgiki powstawały długo, gdyż kilka lat pracy zajęły Wergiliuszowi studia źródłowe oraz rozważania nad problemem kompozycji
i koncepcji poematu. Już w Bukolikach poeta przetworzył greckie wzory w nowe, dalekie od pierwowzorów kompozycje. Tą metodą, w sposób jeszcze doskonalszy, tworzył Georgiki. W poemacie znalazły się ślady dzieł Hezjoda, Arystotelesa, Teofrasta, Nikandra, Aratosa, Eratostenesa, Lukrecjusza, Warrona i innych – ale na ich podstawie powstał całkowicie nowy utwór. Kompozycję narzucił tradycyjny porządek wykładu: uprawa roli i drzew owocowych, hodowla bydła i pszczelarstwo. Te tematy wypełniły cztery kolejne księgi. Ale spod pióra Wergiliusza utwór nie tyle dydaktyczny, ile przede wszystkim poetycki i filozoficzny[32].
Wergiliusz określił swój poemat jako sielską pieśń, gdyż Georgiki nie były traktatem ani podręcznikiem obejmującym całokształt ówczesnej wiedzy z zakresu rolnictwa, naśladownictwem greckich dzieł agronomicznych. Był to poemat narodowy sławiący powrót do roli, do którego nawoływano w Rzymie już od czasu Grakchów. Zdaniem Wergiliusza nie tylko mieczem, ale i pługiem Rzymianie zdobyli Italię. Artystyczny punkt widzenia poety decydował o doborze wątków, o rozwinięciu jednych czy odrzuceniu drugich tematów, o wrażliwości na piękno natury, znajomości i umiłowania wsi[29].
Podczas gdy w Bukolikach motywem przewodnim była miłość, która wszystko zwycięża, to hasłem Georgik stało się: labor omnia vincit improbus („niezłomna praca wszystko pokona”). Georgiki były bowiem pochwałą fizycznej pracy jako dobroczynnego i uszlachetniającego czynnika, który wyzwala twórczą energię człowieka, rozwija zdolności, pobudza wynalazczość, jest źródłem bogactwa, dobrobytu, tężyzny fizycznej oraz postępu[29].
Eneida to licząca dwanaście ksiąg, nieukończona epopeja opowiadająca o powrocie trojańczyka Eneasza do ziemi praojca Dardanosa w Italii. Powrót utrudniała bohaterowi nieprzyjazna bogini Junona. Wskutek jej prześladowań uciekinierzy z Troi przeżyli wiele przygód i niebezpieczeństw. Już po przybyciu do Italii musieli stoczyć krwawe wojny, ażeby osiedlić się w Lacjum. Ostatecznie Junona pogodziła się z wyrokami losu, postawiła jednak – przyjęty przez Jowisza – warunek, że przybysze zlatynizują się, a ich dawne imię ulegnie zapomnieniu. W ten sposób w Italii powstała nowa Troja, która nie nosiła już dawnego imienia, lecz zwała się Rzymem – tak przedstawiał się zamysł poematu, którego śmierć nie pozwoliła dokończyć Wergiliuszowi[33].
Poemat poświęcony historii Rzymu Wergiliusz zaczął pisać na prośbę Augusta. Eneida nie została poddana końcowej redakcji. Wergiliusz chciał to zrobić po powrocie z Grecji. Lucjusz Wariusz Rufus, przygotowując do publikacji epopeję na polecenie Augusta (wbrew woli zmarłego poety), usunął z tekstu podwójną redakcję niektórych wersów. Pozostawił jednak różne sprzeczności oraz wersy nieukończone – hemistychy. Sprzeczności w fabule wynikały z faktu, że poemat był pisany częściami, a Wergiliusz zmieniał kilkakrotnie plan. Nie zdążył ostatecznie zredagować całości i dokonać korekty[17].
Wśród badaczy nie ma zgody co do tego, w jakiej kolejności powstawały poszczególne księgi. Jak podają starożytni biografowie opierając się na Swetoniuszu, Wergiliusz wpierw napisał prozą szkic przyszłego utworu, a następnie opracowywał w formie wierszowej poszczególne części. Dokonywał zmian, dopisywał poszczególne wersy, pozostawiał miejsca do uzupełnienia. Zwrócono na przykład uwagę, że księga III wyłamuje się z toku opowieści. Powstała więc hipoteza, że Wariusz przestawił pierwotny porządek ksiąg[17].
Eneida została napisana w latach 29–19. Badania nad czasem powstania poszczególnych ksiąg nie dały pewnych rezultatów. Według ustaleń niektórych uczonych Eneida mogła zostać napisana w następującym porządku: księga XI w latach 29–27; księgi III, IV, I, II, VII w latach 27–25; księgi X, VI w latach 25–23; księgi IX, VIII w roku 22; księgi V, XII w latach 21–19. Wergiliusz dokonywał w trakcie pisania licznych zmian, więc dokładne datowanie jest niemożliwe. Za fragment najpóźniejszej napisany część badaczy uważa wersy 626–731 z księgi VIII, czyli opis tarczy Eneasza, w którym wspomniano o potrójnym tryumfie Augusta z roku 20[34].
Ponieważ dzieło do ostatniej chwili życia autora było zmieniane, a sam poeta uważał, że nie jest ono jeszcze dostatecznie przygotowane do publikacji, większość uczonych twierdzi, że wszelkie dotychczasowe ustalenia o fazach pisania epopei przyniosły wątpliwe wyniki. Pewnym jest natomiast, że w roku 17 – dwa lata po śmierci poety – Eneida została opublikowana i była powszechnie czytana w Rzymie[34].
Od czasów Cesarstwa Rzymskiego aż do początków XIX wieku niemal każda nauka literatury zaczynała się od Eklogi I. Znajomość tego utworu – jak i całej twórczości Wergiliusza – jest, zdaniem historyków literatury, niezbędnym kluczem do zrozumienia literackiej tradycji europejskiej[35]:
Tityre, tu patulae recubans sub tegmine fagi
silvestrem tenui Musam meditaris avena;
nos patriae fines et dulcia linquimus arva,
nos patriam fugimus; tu, Tityre, lentus in umbra
formosam resonare doces Amaryllida silvas[36].
„Ty, Tytyrze, pod cieniem spoczywasz rozłożystego
Buka i leśną Muzą na wątłej fletni się bawisz;
My – porzucamy ojczyznę i słodkie niwy rodzinne,
My – wygnańcy z ojczyzny. A ty tu beztrosko imieniem
Amaryllidy nadobnej rozbudzasz echo leśne[37].”
Wergiliusz był niezwykle popularnym poetą już za życia. Autor dialogu De oratoribus, prawdopodobnie Tacyt, przekazał informację o gorącej owacji na cześć poety w teatrze – porównywalnej tylko z tą, jaką Rzymianie zgotowali Augustowi. Wychwalają Wergiliusza współcześni mu autorzy, zwłaszcza Horacy i Propercjusz[38]. Bukoliki i Georgiki weszły do kanonu lektur szkolnych i były szczegółowo analizowane jeszcze przed śmiercią poety. O popularności, jaką się cieszyły, świadczą odkryte przez archeologów w Pompejach liczne z nich cytaty, wymalowane na ścianach domów i różnych przedmiotach[39].
Styl Wergiliusza oddziałał na niemal wszystkich rzymskich poetów, ale także historyków (Liwiusz, Tacyt), epistolografów (Pliniusz, Fronton), retorów i gramatyków[40]. Dla Petroniusza twórczość Wergiliusza była ucieleśnieniem rzymskości, pierwszy też nadał on poecie tytuł Romanus Vergilius. Pliniusz przekazał, że
Silius Italikus﻿(inne języki), autor poematu Punica, w swojej posiadłości święcił rocznicę urodzin Wergiliusza uroczyściej niż własną, a grób poety pod Neapolem nawiedzał jak świątynię[38]. Układano z wersów Wergiliusza nowe utwory, tzw. centones, nie tylko poetyckie, lecz i dramatyczne, czego przykładem była tragedia Medea Hosidiusa Gety﻿(inne języki) ze schyłku II wieku[41].
Na kartach Historii Augusta zanotowano, że młody Hadrian, przygotowujący się do roli cesarza, niepewny uczuć, jakie dlań żywił przybrany ojciec Trajan, uciekł się do tzw. wróżb wergiliańskich (sortes Vergilianae) – otworzył bez namysłu Eneidę i odczytał pierwsze słowa na jakie padł jego wzrok. Tego rodzaju wróżby z poezji Wergiliusza były popularne nie tylko w starożytności, ale jeszcze w średniowieczu[41].
Popularność poezji Wergiliusza spowodowała, że powstawało sporo jej parodii i parafraz. Niejaki Numitorius stworzył w I wieku prześmiewcze Antybukoliki. Za panowania Nerona Kalpurniusz Sykulus﻿(inne języki) pisał eklogi wzorując się na Wergiliuszu. Juniusz Kolummella uzupełnił Georgiki w X księdze swojego dzieła o budownictwie, opisując pominięte przez Wergiliusza ogrodnictwo. Wergiliusz był też w starożytności jednym rzymskim poetą przekładanym na grekę. Bliżej nieznany Arrianos przełożył Georgiki; wiadomo też, że w II wieku było znane greckie tłumaczenie Eklogi IV[42].
Za rządów Augusta i jego następców powszechnie uważano Wergiliusza za wieszcza i piewcę potęgi Rzymu. Wizja złotego wieku w dziejach ludzkości, w powszechnej opinii spełniła się po bitwie pod Akcjum i zapanowaniu tak zwanego Pax Romana. W III wieku pojawiły się pierwsze chrześcijańskie interpretacje twórczości Wergiliusza. Cesarz Konstantyn Wielki w oficjalnym piśmie z roku 325 zadekretował, że przepowiednia Wergiliusza z Eklogi IV odnosiła się do Chrystusa i nadał tej interpretacji charakter oficjalny[43].
Niektórzy pisarze chrześcijańscy uważali Wergiliusza za pogańskiego proroka, który zapowiedział czasy mesjańskie, anima naturaliter christiana („duszę z natury chrześcijańską”)[44]. Jednak od końca IV wieku część Ojców Kościoła krytykowała – ich zdaniem przejęty z pogaństwa – kult poezji Wergiliusza. Uwielbienie dla pogańskiego poety potępił św. Hieronim, a św. Augustyn był przeciwny lekturze dzieł Wergiliusza. Pomimo tego literatura chrześcijańska nie mogła poniechać form kultury starożytnej i w V wieku poezja Wergiliusza była nadal częścią kanonu szkolnego[45].
Jako poeta najchętniej czytany i analizowany w rzymskich szkołach, Wergiliusz był w Cesarstwie Rzymskim powszechnie uznawany za nauczyciela i wychowawcę. Jego poezja nie tylko dostarczała wiedzy o literaturze, ale była również podręcznikiem gramatyki, retoryki czy filozofii. Rzymscy wykładowcy stosowali bowiem do twórczości Wergiliusza metodę alegorycznej interpretacji poezji, którą greccy uczeni wynaleźli wcześniej dla objaśniania dzieł Homera. Stało się to mniej więcej równolegle z narodzinami alegorycznej, chrześcijańskiej egzegezy Biblii. Alegoreza biblijna i wergiliańska zetknęły się ze sobą i zmieszały w późnej starożytności i średniowieczu[46].
W IV wieku Eliusz Donat napisał, nie zachowany do naszych czasów w całości, alegoryczny komentarz do poezji Wergiliusza. Na początku V wieku Serwiusz opublikował jeszcze bardziej szczegółowe komentarze, omawiające wers po wersie Bukoliki, Georgiki oraz Eneidę. Wergiliusz był dla Serwiusza mędrcem biegłym we wszystkich sztukach i naukach. Komentarz do VI księgi Eneidy rozpoczął Serwiusz od następującej uwagi: Chociaż cały Wergiliusz jest pełen wiedzy, to w tej właśnie księdze widać to szczególnie. Wiele powiedziano tam wprost, wiele zaczerpnięto z historii, lecz dużo także z głębokiej mądrości filozofów, teologów, Egipcjan[47].
Jednak to głównie dzięki pracom neoplatończyka Makrobiusza Wergiliusz stał się dla wieków późniejszych autorytetem nie tylko literackim, ale także filozoficznym i naukowym. W napisanym w V wieku komentarzu do Snu Scypiona Makrobiusz nazwał Wergiliusza ekspertem we wszelkich naukach (disciplinarum omnium peritissmus), który nie popełnił żadnego błędu (erroris ignarus). W Saturnaliach poeta był dla Makrobiusza mędrcem i najwyższym autorytetem we wszystkich dziedzinach wiedzy[48].
Według Makrobiusza poezja Wergiliusza była alegorycznym kostiumem dla pojęć filozoficznych – np. wersy o Saturnie, który zjada własne dzieci, aby następnie je zwymiotować, to dla Makrobiusza alegoria czasu, który pożera wszystkie rzeczy, aby je później stworzyć na nowo. Wergiliusz był dla Makrobiusza również teologiem. Gdy w ósmym wersie Eneidy napisał słowa quo numine laeso, odnoszące się do Junony, to wskazywał, że rozmaite numina są wyobrażeniem jednego Bóstwa. Bóstwo to zaś jest – zdaniem Makrobiusza – dwupłciowe, gdyż Wergiliusz napisał w drugiej księdze Eneidy o Wenus ducente deo[49].
Wergiliusz był dla średniowiecza nie tylko największym poetą rzymskim, mędrcem i prorokiem wśród pogan, lecz także potężnym czarodziejem, który mocą magiczną ochraniał ulubiony swój Neapol przed wężami i jadowitymi muchami, odganiał od niego choroby, otoczył też miasto murami. Opowiadano o czarodziejskich interwencjach poety w Rzymie i innych miastach Europy. Część uczonych przypisuje tym legendom genezę ludową, inni dopatrują się w nich tworu wczesnośredniowiecznych erudytów[50].
Autorytet Wergiliusza był również wykorzystywany w polityce, czego przykładem jest użycie jego poezji podczas ceremonii koronacji Karola Wielkiego. Koronacja cesarska z roku 800 oznaczała bowiem, w zamyśle jej organizatorów, nie tylko ciągłość imperium rzymskiego (translatio imperii), ale również ciągłość kultury (translatio studii), reprezentowanej przez poezję Wergiliusza[51].
Poezja Marona była czytana w szkołach i przepisywana w klasztorach przez całe średniowiecze. Poeci z kręgu Alkuina pisali listy w formie eklog. Mnich Notker Balbulus w X wieku przetłumaczył Bukoliki na język niemiecki. Remigiusz z Auxerre napisał komentarz do Bukolik a autorzy Romansu o róży znali i naśladowali Bukoliki, Georgiki oraz Eneidę[23].
W czasach karolińskich Wergiliusza nazwano jednym ze starych kierowników filozofii, kontynuując starożytną interpretację alegoryczną jego poezji. W Eneidzie wyróżniano dwie warstwy treści: prawdę filozoficzną i poetyckie zmyślenie (figmentum)[52]. W następnych wiekach jeden z najbardziej wartościowych tego typu komentarz napisał w duchu neoplatonizmu, około połowy XII wieku, Bernardus Silvestris﻿(inne języki)[53].
Wergiliusz, jako najwyższy poeta i mędrzec, pojawił się na kartach Boskiej komedii. Dla Dantego Wergiliusz jest mistrzem, najwspanialszym spośród wszystkich autorów – autorytetem biegłym we wszystkich dziedzinach wiedzy. Reprezentuje bowiem Maron encyklopedyczną sumę ludzkiej nauki[54]. W pierwszej księdze Boskiej komedii, gdy Dante, zabłąkany w ciemnym borze, nie widzi żadnej ścieżki, która mogłaby go wyprowadzić z gęstwiny pełnej groźnych bestii, nagle dostrzega Wergiliusza, którego prosi o pomoc[55]:
„Or se’ tu quel Virgilio e quella fonte
che spandi di parlar sì largo fiume?”,
rispuos’io lui con vergognosa fronte.
„O de li altri poeti onore e lume,
vagliami ’l lungo studio e ’l grande amore
che m’ ha fatto cercar lo tuo volume.
Tu se’ lo mio maestro e ’l mio autore,
tu se’ solo colui da cu’ io tolsi
lo bello stilo che m’ ha fatto onore.
Vedi la bestia per cu’ io mi volsi;
aiutami da lei, famoso saggio,
ch’ella mi fa tremar le vene e i polsi”[56].
„Więc Wergilego oglądam? Krynicę,
Skąd płynie Słowa strumień tak obficie?” –
Odezwałem się, zasromawszy lice.
„O ty, poetów światło i zaszczycie!
Niech mię zalecą miłość i uwaga,
Com z nią na myśli twojej szedł wykrycie.
Ty jesteś Mistrz mój, ty moja Powaga:
Przez cię jedynie styl mój nabył piętna
Sztuki, skąd dzisiaj moja cześć się wzmaga.
Płoszy mię bestia, przejściu memu wstrętna;
Dopomóż, mędrcze sławny! Oto ginę
Z trwogi i żywiej dygoczą mi tętna”[57].
Dante czerpał z poezji Marona, zwłaszcza Eneidy, postacie i obrazy. Wergiliusz Dantego nie był jednak jedynie klasykiem poezji, w przeciwieństwie do późniejszego Wergilego Tassa czy Miltona. Był on wyrazicielem doczesnego i wiecznego Rzymu, znawcą i przedstawicielem tamtego świata a jego imię można symbolicznie powiązać z rajem[58].
W wieku XII Gall Anonim, a w XIII Wincenty Kadłubek, cytują Wergiliusza jako pierwsi w Polsce. W XV stuleciu poezję Wergiliusza zaczęto wykładać na Uniwersytecie Krakowskim. Spisy wykładów świadczą, że zainteresowanie tym autorem było o wiele większe niż innymi poetami. Najpopularniejsze były Georgiki, z których czerpano praktyczne informacje o rolnictwie. Nieco mniejszym powodzeniem cieszyły się w XV i XVI wieku Bukoliki. Dopiero od połowy XVI stulecia odnotowano w Polsce większe zainteresowanie Eneidą[59]. Wtedy to Andrzej Kochanowski, młodszy brat czarnoleskiego poety Jana, przetłumaczył Eneidę. Przekład został wydany w 1590[60]. Jak pisze Michał Czerenkiewicz, Eneasz stał się wzorem osobowym dla sarmatów[61]. Szlachta czytała i cytowała Wergiliusza również w oryginale. Hetman Stanisław Żółkiewski w testamencie zapisał wskazówkę dla swojego syna wziętą z XII księgi Eneidy – Disce puer ex me virtutem verumque laborem, czyli Ucz się, chłopcze, ode mnie cnoty i prawdziwego trudu[61]. W krakowskim kolegium Nowodworskiego dzieła Wergiliusza czytano jako lekturę szkolną[61]. W XVIII stuleciu Eneidę przełożył Jacek Idzi Przybylski[62]. W XX wieku Eneidę przetłumaczył Tadeusz Karyłowski (wydanie w 1924 roku)[63] oraz Zygmunt Kubiak[64].
Édith Piaf, właściwie Édith Giovanna Gassion (ur. 19 grudnia 1915 w Paryżu, zm. 10 października 1963 w Grasse) – francuska piosenkarka. Uznawana za jedną z najwybitniejszych francuskich artystek XX wieku[1] oraz za ikonę francuskiej piosenki[2].
Napisano o niej wiele biografii, ale niektóre fakty z jej życia pozostały tajemnicą[3]. Urodziła się 19 grudnia 1915 jako Édith Giovanna Gassion[4] na paryskim osiedlu Quartier de Belleville, gdzie żyło wielu imigrantów. Przyszła na świat w szpitalu Tenon, w dzielnicy XX. Otrzymała imię po siostrze Edith Cavell, przełożonej szpitala wojskowego w Brukseli, brytyjskiej pielęgniarce, której bohaterstwo przyniosło sławę i miano męczennicy okresu I wojny światowej po rozstrzelaniu przez Niemców za pomoc brytyjskim żołnierzom w ucieczce z niewoli[5][6].
Piosenkarka miała pochodzenie francuskie, kabylskie, marokańskie i włoskie. Ojcem Édith Piaf był Louis Alphonse Gassion (ur. 10 maja 1881 w Castillon, Francja, zm. 3 marca 1944 w Paryżu), akrobata cyrkowy, matką – Annetta Maillard (ur. 4 sierpnia 1895 w Livorno, zm. 6 lutego 1945), śpiewaczka znana pod pseudonimem artystycznym „Line Marsa”. Louis Alphonse Gassion był synem pochodzącego z Normandii Victora Alphonse’a Gassion, stajennego cyrkowego, i Léontine Louise Descamp, znanej jako Maman Titine, właścicielki domu publicznego w Bernay w Normandii. Annetta Maillard była zaś córką Auguste’a Eugène’a Maillarda (1866–1912) i Emmy Saïd Ben Mohamed, znanej artystki kabaretowej (1876–1930 w Paryżu). Rodzicami Emmy Saïd Ben Mohamed byli Said Ben Mohammed (1827–1890), artysta cyrku marokańskiego, i Margueritte Bracco (1830–1898).
Rodzice Édith Piaf mieli jeszcze syna, Herberta Gassion (ur. 31 sierpnia 1918 w Marsylii, zm. 22 stycznia 1997). Porzucona przez matkę, krótko mieszkała z babką, Emmą (Aïcha) Saïd ben Mohamed (1876–1930). Ojciec po powrocie z frontu, zabrał ją do swojej matki, która w Normandii prowadziła dom publiczny. Tamtejsze prostytutki pomagały ją wychować[3]. Młodość spędziła na ulicach Paryża. W wieku 15 lat zajęła się śpiewaniem ulicznym. Została odkryta w 1935 przez impresaria Louisa Leplée i rozpoczęła występy w jego kabarecie „Le Gerny’s” przy Champs-Élysées, pod pseudonimem „La Môme Piaf” (mały wróbelek), który stał się później znany milionom wielbicieli jej talentu. W 1936 miała pierwsze nagranie dla wytwórni Polydor. W 1935 zmarła w wieku 2 lat jej jedyna córka. W czasie wojny występowała w lokalach, współpracując z francuskim ruchem oporu.
Słynęła z niebywałej ekspresji i dramatyzmu w wykonywaniu piosenek specjalnie dla niej pisanych[2]. Jej chropowaty i stosunkowo niski głos kontrastował z drobną sylwetką (147 cm), co fascynowało widzów m.in. w paryskiej Olympii, z którą była przez lata związana.
Śpiewała także w USA, odnosząc spore sukcesy. Oprócz jej talentu widzów przyciągała otaczająca Piaf legenda, wynikająca z jej przeszłości, a także nieudanych związków uczuciowych, które z jednej strony zwiększały dramatyzm jej recitali, a z drugiej – pogłębiały chorobę, z którą zmagała się heroicznie do końca życia.
Osiągnąwszy szczyty kariery zaczęła pomagać młodym piosenkarzom, ułatwiając im start artystyczny (wśród nich byli m.in. Yves Montand, z którym miała trwający kilka lat romans, Gilbert Bécaud i Charles Aznavour). Była dwukrotnie zamężna. Ze swym pierwszym mężem (Jacques Pills) wzięła ślub 20 września 1952, jednak po czterech latach rozwiedli się. W latach 1948–1949 miała romans z żonatym Marcelem Cerdanem, mistrzem świata w boksie. 9 października 1962 wyszła za młodszego od siebie o 21 lat Theophanisa Lamboukasa, znanego jako Théo Sarapo, któremu usiłowała pomóc w karierze piosenkarskiej. Pomimo osobliwości tej sytuacji, krytykowanej przez wielu jej przyjaciół, Lamboukas okazał się jej wiernym przyjacielem, opiekującym się troskliwie piosenkarką aż do jej śmierci.
Piaf zmarła na raka wątrobowokomórkowego 10 października 1963 w Grasse. Jest pochowana na cmentarzu Père Lachaise. Na jej pogrzebie zebrało się 40 tysięcy ludzi, a na jej grobie do dziś są składane kwiaty. Wiele osób inspirowało się jej muzyką i pisało własne piosenki. Twórczość artystyczna Piaf należy do klasyki piosenki francuskiej i pomimo całkowicie odmienionych dzisiaj standardów i gustów jest chętnie słuchana na całym świecie i wznawiana przez wydawnictwa muzyczne.
Przyjaźniła się z Jeanem Cocteau, który zmarł na atak serca 11 października tego samego roku. Wiele osób uważa, że miało to bezpośredni związek z otrzymaniem wiadomości o jej zgonie. Jednak Jean Marais – wieloletni przyjaciel, partner i aktor filmów Cocteau – wspomina, że pisarza nie łączyła zbyt wielka zażyłość z Édith i nie da się więc jednoznacznie łączyć tych dwóch zdarzeń.
W 1977 w Paryżu przy Crespin du Gast nr 5, powstało Muzeum Édith Piaf, w całości poświęcone artystce. Założycielem i właścicielem Muzeum jest Bernard Marchois, będący jednocześnie sekretarzem stowarzyszenia przyjaciół Édith Piaf – „Amis d’Édith Piaf”. Stowarzyszenie działa od 1967[7].
James Cook (ur. 27 października?/7 listopada 1728[a] w Marton koło Middlesbrough, Wielka Brytania, zm. 14 lutego 1779 na wybrzeżu Kealakekua, Hawaje) – angielski żeglarz i odkrywca, kartograf, astronom.
Organizator i kierownik trzech wypraw dookoła świata (1768–1780). W trakcie pierwszej (1768–1771) zdobył Wyspy Towarzystwa, spenetrował obszary Melanezji i Nowej Zelandii, dotarł do wybrzeży Australii (1770) i przepłynął cieśninę Torresa. Podczas drugiej (1772–1775) odkrył, między innymi: Nową Kaledonię, archipelag Fidżi, wyspy w Polinezji. Trzecia wyprawa (1776–1780) zorganizowana w celu odnalezienia przejścia morskiego z Oceanu Spokojnego na Ocean Atlantycki, dotarła do wybrzeży Alaski, na Morze Czukockie oraz na Hawaje, gdzie Cook zginął w potyczce z tubylcami.
Sporządzone przez Cooka mapy zmieniły wyobrażenie o zarysach lądów i mórz. Doniosłe znaczenie miało również stwierdzenie możliwości kolonizacji Australii. Laureat Medalu Copleya[1].
Cook urodził się 7 listopada 1728 roku w ubogiej rodzinie. Jego ojciec James najmował się do pracy w polu w Marton[2]. James Cook miał pięć sióstr (Mary, Mary, Jane, Christane i Margaret) i dwóch braci (Williama i Johna)[3]. Po awansie ojca na stanowisko zarządcy-ekonoma, ośmioletni Cook przeprowadził się z rodziną na farmę w Great Ayton, gdzie uczył się w miejscowej szkole[4]. Thomas Scottowe, właściciel ziemski zatrudniający ojca Cooka, dostrzegł spostrzegawczość młodego Cooka. Scottowe opłacił chłopcu edukację w szkole w Great Ayton[5]. W wieku 17 lat Cook opuścił dom, aby pracować jako pomocnik w sklepie Williama Sandersona w wiosce rybackiej Staithes[6].
Po kilkunastu miesiącach właściciel sklepu przedstawił młodego Jamesa Johnowi i Henry’emu Walkerom, miejscowym właścicielom statków do przewozu węgla. Cook został przyjęty na pokład jako młody czeladnik-praktykant w Grape Lane[7]. Pracował następnie na węglowcu „Freelove”, na którym spędził kilka lat, żeglując pomiędzy Whitby a Londynem[7]. Podczas pracy na węglowcu zdobył doświadczenie oraz podstawową wiedzę w dziedzinie matematyki, nawigacji i obsługi okrętów[8]. Za sprawą pracowitości i chęci do nauki Cook cieszył się szacunkiem Walkerów[9]. Walkerowie, będący kwakrami, mieli także duży wpływ na poglądy religijne Jamesa Cooka[10].
21 grudnia 1762 roku, w kościele pw. św. Małgorzaty w Barking Cook poślubił Elizabeth Bates, córkę jednego ze swoich mecenasów[11][12]. Miał z nią czworo[13] lub sześcioro[14] dzieci.
7 czerwca 1755 roku Cook zaciągnął się do marynarki wojennej jako starszy marynarz[15]. Pracował na okręcie „Eagle” dowodzonym przez kapitana Hamera (zastąpionego później przez Hugh Pallisera)[8][16]. Podczas wojny francusko-brytyjskiej o Kanadę w latach 1764–1766 roku dokonał pomiarów wybrzeży u ujścia Rzeki Świętego Wawrzyńca i Nowej Fundlandii[17]. Za sporządzenie dokładnych map otrzymał premię w wysokości 50 funtów[8]. 25 maja 1768 roku Cook awansował do rangi porucznika[18].
W 1768 roku Admiralicja brytyjska powierzyła mu dowództwo ekspedycji[17][19]. Na okręcie „Endeavour” oficjalnie został wysłany w celu zbadania przejścia Wenus przez tarczę słoneczną na Tahiti, które miało nastąpić 3 czerwca 1769 roku (miała to być druga próba, po niezbyt udanej obserwacji z 1766 roku). Dzięki dokonanym pomiarom miano umożliwić obliczenie odległości Ziemi od Wenus[8]. Ponadto Cook wraz z załogą mieli dokonać jak najwięcej odkryć w dziedzinie nawigacji i historii naturalnej[18]. Faktycznym zadaniem było odnalezienie Lądu Południowego (Terra Australis)[17]. Nowo odkryte ziemie miały być przyłączone do Imperium brytyjskiego za zgodą tubylców[18]. Wybór Cooka na kapitana wyprawy zbudził pewne kontrowersje. Royal Society finansujące wyprawę zaproponowało Alexandra Dalrymple’a, teoretyka i kartografa mającego za sobą wyprawę do Indii Wschodnich[20]. Admiralicja brytyjska wybrała jednak Cooka mającego już za sobą wyłącznie wyprawy na północnym Atlantyku. Cook nie posiadający formalnego wykształcenia i żonaty z córką właściciela portowej tawerny nie pasował do ówczesnych elit[21].
26 sierpnia 1768 roku dowodzony przez Cooka „Endeavour” wypłynął z Plymouth[22]. Statek zaopatrzono między innymi w: tony sucharów okrętowych, solonej wieprzowiny i solonej wołowiny[23], 1200 galonów piwa, 1600 galonów mocniejszych alkoholi (araku, brandy i rumu), 3032 galony wina z Madery. Dzienny przydział alkoholi na marynarza wynosił galon piwa lub pintę rumu rozcieńczonego wodą[24]. Co drugi dzień załoga zamiast mięsa jadła ser i owsiankę[23]. Na pokładzie znajdowały się 94 osoby[22]. Na pokładzie statku znalazł się także Joseph Banks, przyrodnik wykształcony w Harrow, Eton i na Oksfordzie, dwudziestopięcioletni dziedzic dużej fortuny, który za uczestnictwo w ekspedycji zapłacił dziesięć tysięcy funtów szterlingów[25].
Podążając szlakiem wytyczonym przez Ferdynanda Magellana, w 13 kwietnia 1769 roku ekspedycja dopłynęła do Zatoki Matavai przy Tahiti[17][26]. Na miejscu załoga „Endeavoura” odnalazła kilku członków załogi „Dolphina”, która dopłynęła do wyspy w 1767 roku[26]. Tubylcy wyszli na spotkanie z Cookiem i Banksem z zielonymi bukietami (symbolem pokoju). Kapitana statku i naukowca nazwano tiao (przyjaciele) i ofiarowano im perfumowane tkaniny. 14 kwietnia rozpoczęto budowę obozu, nazwanym przez Cooka Przylądkiem Wenus[27]. Jednym z większych problemów podczas pobytu na wyspie było rozpowszechnienie się syfilisu wśród załogi (obecnie przyjmuje się, że ta choroba nie była kiłą lecz framboezją, tropikalną chorobą skóry objawami przypominającą syfilis)[28].
Podczas pobytu na Tahiti Cook powierzył Banksowi nadzór nad handlem pomiędzy Brytyjczykami a Tahitańczykami[29].
W zachowanej relacji Cook opisał rytuał Tahitańczyków należącej do ariori, tamtejszej sekty: publiczny stosunek seksualny pomiędzy wysokim młodzieńcem a dziesięcioletnią (lub dwunastoletnia) dziewczynką. Kopulacja miała charakter ceremoniału, podczas którego starsze kobiety pouczały dziewczynkę, jak powinna się zachowywać podczas rytuału. Opis sceny wzbudził sensację w Wielkiej Brytanii i spotkał się z potępieniem ze strony anglikańskiego kleru[30]. Załoga „Endeavoura” była zszokowana i obrzydzona publiczną ceremonią[31].
3 czerwca 1769 roku Cook wraz z Solanderem i Greenem obserwował przejście Wenus przez tarczę słoneczną[32]. Wykonane pomiary okazały się niedokładne, co uniemożliwiło obliczenie odległości Wenus od Ziemi[33]. Cook zanotował, że pomimo czystego nieba zaobserwował „mglisty cień” rozmywający kontury planety[34]. Po zakończeniu obserwacji Cook i Banks prowadzili prace kartograficzne wśród sąsiednich wysp[17]. Podczas całej wyprawy Banks pozostawił wiele rysunków i szczegółowe opisy flory i fauny Polinezji[35]. Banks w swoim dzienniku Zachowanie i obyczaje mieszkańców wysp Mórz Południowych zrelacjonował liczne zwyczaje kulturowe mieszkańców wyspy, dziesiątki tahitańskich słów wraz z tłumaczeniem, teksty piosenek w języku tahitańskim[36].
13 lipca 1769 roku „Endeavour” odpłynął z Tahiti[32]. Do załogi dołączył Tupaia, wysoki kapłan opisywany przez Cooka jako bardzo inteligentnego człowieka, znającego okoliczne morza. Tupaia zabrał ze sobą służącego, chłopca o imieniu Taiata[37].
15 lipca „Endeavour” dotarł na Huahine, a 20 lipca do Raiatei[32]. Po wylądowaniu na Raiatei Cook wciągnął angielską flagę i wziął w posiadanie Raiateę i okoliczne wyspy[38]. Okoliczny archipelag Cook nazwał Wyspami Towarzystwa ze względu na bardzo bliskie położenie wysp[b]. Z Raiatei Cook skierował się na Bora-Bora (26 lipca), jednak nie udało mu się wylądować na wyspie[32][39].
8 października 1769 roku załoga Cooka dopłynęła do Wyspy Północnej Nowej Zelandii. Wyspę jako pierwszy dostrzegł dzień wcześniej dwunastoletni członek załogi Nick Young[40][32]. Kilka dni po dostrzeżeniu brzegu „Endeavour” rzucił kotwicę przy Poverty Bay (Zatoce Ubóstwa). Po wyjściu na brzeg załoga trafiła na grupę Maorysów tańczących haka (taniec wojenny)[41]. Spotkanie z Maorysami zakończyło się pojedynkiem[42]. Opuściwszy Poverty Bay, Cook skierował się na południe, ale widząc jałowe wybrzeże zwrócił się na północ, opływając wyspę w kierunku przeciwnym do ruchu wskazówek zegara[43]. Przez dłuższy czas załoga statku nie mogła wylądować na brzegu ze względu na agresywność Maorysów, jednak Brytyjczykom i tubylcom udało się podjąć współpracę handlową[44]. W dniach 23–29 października „Endeavour” przebywał nad Zatoką Cooka. Było to pierwsze miejsce w Nowej Zelandii, na którym udało się zatrzymać na dłużej[45]. Podczas wyprawy „Endeavour” przepłynął przez cieśninę, nazwaną imieniem Cooka[17].
„Endeavour” pozostawał na wodach Nowej Zelandii przez sześć miesięcy, ale jedynie niecałe dwa miesiące stał na kotwicy w pobliżu lądu. Podczas pobytu w okolicy wysp udało się sporządzić dokładną mapę Nowej Zelandii. Cookowi udało się dowieść, że Nowa Zelandia nie jest półwyspem legendarnego południowego kontynentu[46]. Obserwując rozproszenie plemiona Wysp Północnej i Południowej Cook zwrócił uwagę na podobieństwo dialektów i języka tahitańskiego[47]. Podczas pobytu w Nowej Zelandii Banks i Daniel Solander zebrali przykłady czterystu gatunków roślin, ptaków, owadów, ryb, muszli oraz kamieni[48].
Po odpłynięciu z Nowej Zelandii Cook zaczął przygotowywać się do powrotu do Wielkiej Brytanii[49]. 20 kwietnia 1770 roku „Endeavour” dotarł do okolic Point Hicks[50]. Nieznany ląd jako pierwszy zaobserwował porucznik Zachary Hicks. Cook od początku zakładał, że zaobserwowana ziemia to wschodnie wybrzeże Nowej Holandii[51]. Statek Cooka był pierwszym europejskim okrętem, który dopłynął do wschodniego wybrzeża Australii[50]. Wzburzone morze i przeciwne wiatry nie pozwoliły przybić do brzegu. Płynąc w górę wschodniego wybrzeża Australii Brytyjczycy znaleźli w nim drewno, nieznane wcześniej rośliny i ławice ryb. 29 kwietnia „Endeavour” dopłynął do Zatoki Botanicznej[52]. Wtedy po raz pierwszy Cook dostrzegł Aborygenów[53], którzy jednak okazali obojętność w stosunku do załogi statku[54].
Problemem stawało się coraz większe rozdrażnienie załogi, mającej dosyć wyprawy naukowej. Podczas pobytu w okolicach Australii kilku mężczyzn okaleczyło sekretarza Cooka[55]. 11 czerwca „Endeavour” dopłynął do Przylądka Udręki[55]. O zachodzie słońca „Endeavour” trafił na Wielką Rafę Koralową. Rafa koralowa przebiła dno statku. Przy odpompowywaniu wody pracowała cała załoga statku, łącznie z naukowcami[56]. 18 czerwca, tydzień po uderzeniu w rafę, „Endeavour” wpłynął do rzeki, nazwanej później Endeavour[57][51]. Znajdując się w okolicach dzisiejszego Cooktown naprawiono statek oraz zbadano okolice. 23 kwietnia Europejczycy po raz pierwszy zobaczyli kangury[32], a później także psy dingo, aligatory oraz nietoperze podobne do rudawki wielkiej[58]. Nazwa kanguru była zniekształconą formą nazwy gangurru pochodzącej z języka miejscowego ludu[51]. Lokalna nazwa zwierzęcia zachowała się dzięki liście stu pięćdziesięciu słów spisanej przez malarza Sydneya Parkinsona[59].
Napotkani w tym miejscu Aborygeni należeli do plemienia Guugu Yimidhirr[60]. Odbiór Aborygenów przez załogę statku był zróżnicowany. Cook wyrażał się o tubylcach życzliwie, z kolei Banks umiejscawiał rdzennych mieszkańców Australii pomiędzy ludźmi, a zwierzętami[61]. Według relacji Cooka załoga uznała tubylców generalnie za lud prymitywny, nie interesujący się europejskimi dobrami. Cook twierdził jednak, że za sprawą nieznajomości dóbr nie zaznają nierówności społecznych, co pozwala im żyć w spokoju[62].
22 sierpnia „Endeavour” dopłynął do półwyspu Jork[51]. Ponownie największym problemem podczas podróży było pokonanie rafy koralowej[63]. Kanał wiodący przez rafy Cook nazwał Kanałem Opatrzności[64]. Po przepłynięciu przez kanał „Endeavour” dotarł do Wyspy Possession. Na Wyspie Cook w imieniu króla Jerzego III objął w posiadanie ziemie nazwane Nową Południową Walią[65][17][51]. Nazwa ta utrzymała się przez trzydzieści lat. Nadana przez Matthew Flindersa nazwa Australia stopniowo wyparła Nową Południową Walię, która stała się nazwą stanu[66].
Po odpłynięciu z Nowej Południowej Walii „Endeavour” dotarł do Cieśniny Torresa[17]. Udowodniwszy, że Nowa Południowa Walia jest oddzielona od Nowej Gwinei morzem Cook zdecydował się na obranie kursu do Wielkiej Brytanii[67]. Następnie Cook zatrzymał się na Nowej Gwinei i wyspie Sawu w celu pilnego zaopatrzenia i naprawy okrętu. 5 października 1770 roku „Endeavour” przybył do Batawii (dzisiejszej Dżakarty)[68]. Podczas pobytu w Batawii prawa cała załoga zachorowała (najprawdopodobniej na dur brzuszny lub cholerę)[69]. W wyniku epidemii zmarło siedmiu członków załogi, w tym Tupaia (17 grudnia 1770 roku)[70]. Po trwającym dziesięć tygodni remoncie, „Endeavour” wyruszył w dalszą drogę. Poważnym problemem stała się epidemia dyzenterii. Wśród zmarłych na tę chorobę znaleźli się między innymi: astronom Charles Green, rysownik Sydney Parkinson oraz cieśla John Satterly[71]. Śmierć części załogi przygnębiła resztę i doprowadziła do postępującej anarchii[72]. Po jedenastu tygodniach od opłynięcia z Batawii „Endeavour” dopłynął do Kapsztadu, a następnie zatrzymał się na Wyspie Świętej Heleny[73][17].
13 lipca 1771 roku okręt dotarł do Downs[74]. Cook przekazał raport Admiralicji 17 lipca[32]. Relacje Banksa spotkały się z ogromnym zainteresowaniem, a sam botanik skupiał większą uwagę opinii publicznej niż Cook[74]. Pełniący funkcję kapitana Cook, będący de facto porucznikiem, po zakończeniu wyprawy awansował na stopień kapitana[75].
Podczas pierwszej wyprawy zmarło 36 członków statku, jeden zmarł wkrótce po powrocie do Anglii oraz jeden zdezerterował[73]. Podczas podróży Cook miał duże trudności z utrzymaniem dyscypliny wśród swoich marynarzy, którzy korzystali z wielkiego zainteresowania ze strony polinezyjskich kobiet. Cooka zapamiętano jako osobę wymagającą wiele od siebie i załogi, jednak będącą pobłażliwą wobec mieszkańców wysp oceanicznych. Twierdził, że Brytyjczycy, żądni bogactwa i stosunków seksualnych, zdemoralizują wyspiarzy. Surowo karał złodziei broni, narzędzi metalowych i gwoździ niezależnie od tego, czy złodziejem był członek załogi lub tubylec[35].
Pomimo braku odkrycia Lądu Południowego, pierwsza wyprawa Cooka wiele osiągnęła, a o ich skutkach przekonano się wkrótce po skolonizowaniu Australii, Nowej Zelandii i innych wysp Oceanii. W swoim sprawozdaniu Cook stwierdził, że tajemnicę Lądu Południowego można wyjaśnić w kolejnej wyprawie, która spenetrowałaby akweny rozciągające się pomiędzy 40 a 60° szerokością geograficzną południową. Admiralicja przekonała się do propozycji Cooka i powierzyła mu dowództwo kolejnej ekspedycji[76].
13 lipca 1772 roku wypłynęły dwa statki: „Resolution” (pod komendą Cooka) i „Adventure” (dowodzonej przez Tobiasa Furneauxa)[77]. W wyprawie wzięli udział między innymi: gdański przyrodnik Johann Reinhold Forster, jego syn Georg (późniejszy profesor Uniwersytetu Wileńskiego) oraz szwedzki przyrodnik Andreas Sparrman[76]. Forsterowie po wyprawie opublikowali dwa dzieła wraz z relacją i wynikami swoich obserwacji naukowych[78]. W drugiej wyprawie wzięło udział siedemnastu członków pierwszej ekspedycji. Miał w niej wziąć udział także Joseph Banks, jednak zrezygnował po nieuwzględnieniu jego wymagań co do udziału w misji badawczej[79].
Po uzupełnieniu zapasów w Maderze (2 sierpnia) Cook skierował się w rejs wokół Afryki[80]. 17 stycznia 1773 roku wyprawa jako pierwsza w dziejach odkryć geograficznych dotarła do koła podbiegunowego południowego[81]. Po zimie spędzonej w Polinezji w 1774 roku Cook podjął się drugiej próby wyprawy na południe. Dotarł do 71 równoleżnika, po czym utknął w polu lodowym[82]. Rok później Cook wyruszył w trzecią wyprawę w stronę Antarktydy[83].
2 października 1773 roku „Resolution” dopłynął do wysp Tonga[80]. Według relacji Cooka tubylcy wypłynęli czółnami na powitanie okrętu i weszli na pokład, a po wylądowaniu statku zgotowali załodze statku przyjęcie powitalne, złożone z kavy, bananów i kokosów[84]. Cook jako jedyny odważył się spróbować kavy[85]. Zdumienie wzbudził u niego kult tahama – świętego dziecka oraz stosunek Tongijczyków do króla[86]. Cook pozostał na wyspach przez kilka dni[87].
25 marca 1774 roku James Cook dopłynął do Nowej Zelandii, a 21 czerwca 1774 roku do Niue[80]. W dziennikach Cooka Niue jest nazywane Dziką wyspą, za sprawą agresywnych tubylców rzucających kamieniami i ciskających dzidy w Brytyjczyków[88]. Zaraz po dopłynięciu do wyspy botanicy zaczęli zbierać rośliny, a marynarze wciągnęli brytyjską flagę i objęli w posiadanie wyspę. Po kilku godzinach załoga statku zobaczyła mieszkańców wyspy pomalowanych w pasy na czarno, czerwono i biało. Po tym, jak jeden z Niueńczyków rzucił kawałkiem korala w Andersa Sparrmana, botanicy zaczęli strzelać z muszkietów w celu odstraszenia rdzennych mieszkańców Niue. Załoga wróciła do okrętu i popłynęła wzdłuż wybrzeża, szukając innego miejsca do wylądowania[89]. Według relacji Cooka, mieszkańcy wyspy mieli usta pomazane na czerwono[88]. Wroga postawa wobec Cooka prawdopodobnie wynikała z lęku Niueńczyków przed przewożonymi z zewnątrz chorobami. Raport Cooka zniechęcił innych podróżników do badania Niue[90]. Po opłynięciu z Niue Cook po raz drugi skierował się do wysp Tonga[87].
W listopadzie 1773 roku wyprawa ponownie przekroczyła krąg polarny, jednak zatory lodowe uniemożliwiły dalszą podróż na południe. Wyprawa odwiedziła także archipelag Juan Fernández, Wyspę Wielkanocą i Tahiti[76]. Na trasie dalszej żeglugi znalazły się Nowe Hebrydy (dzisiejsze Vanuatu)[91]. Badając Oceanię odkryto również wyspę, nazywaną przez tubylców Baladee (Wielka Ziemia). Brytyjczycy nazwali nowo odkrytą wyspę Nową Kaledonią, gdyż jej rzeźba przypominała żeglarzom krajobraz Szkocji[76]. W Nowej Kaledonii James Cook oraz Forsterowie zatruli się wątrobą tetraodona. Dzięki szybkiej pomocy medycznej (zażycie środków wymiotnych i napotnych) udało im się przeżyć[92].
W listopadzie 1774 roku Cook skierował się do Europy. Podczas powrotu przez przylądek Horn Cook bezowocnie próbował znaleźć Ląd Południowy między Nową Zelandią a Ameryką Południową. Podczas drogi powrotnej wyprawa zawijała kolejno: przy Przylądku Dobrej Nadziei, na wyspach Wniebowstąpienia, św. Heleny i Azorach, by ostatecznie 29 lipca 1775 roku wrócić do kraju. Podczas wyprawy z załogi „Resolution” ubyło czterech ludzi (jeden zmarł na gruźlicę, dwóch utonęło, jeden spadł do luku), a „Adventure” trzynaście osób. Po wyprawie Cooka uhonorowano Medalem Copleya za starania, by utrzymać marynarzy w dobrym zdrowiu[93].
W sierpniu 1775 roku Cook złożył podanie i otrzymał stanowisko kapitana Królewskiego Szpitala w Greenwich. Stanowisko to zapewniło Cookowi pensję w wysokości dwustu trzydziestu funtów rocznie oraz mieszkanie, drewno opałowe i dodatek na wyżywienie[94].
W lipcu 1776 roku na statku „Resolution” Cook podjął trzecią wyprawę badawczą. Towarzyszyli mu dowódcy drugiego statku „Discovery” Charles Clerke[76], George Vancouver (odkrywca wysp Snares i Chatham, a później też odkrywca wybrzeży Ameryki Północnej) i James Bruney (zastępca Forneaux z poprzedniej wyprawy)[95]. Ze względu na wcześniejsze konflikty, Cook nie zaproponował Forsterom udziału w trzeciej wyprawie[96].
Płynąc wokół Przylądka Dobrej Nadziei, Wyspy Mariona (gdzie Cook nazwał archipelag, w którą wchodzą owe wyspy, Wyspami Księcia Edwarda[97]) i zatrzymując się na Wyspie Kerguelena, 10 lutego 1777 roku Cook po raz trzeci dopłynął do Nowej Zelandii, a kilka miesięcy później po raz drugi do Wysp Przyjacielskich[98][99]. Na Wyspach Przyjacielskich Cook podarował tongijskiej rodzinie królewskiej żółwia promienistego Tu'i Malila przywiezionego z Madagaskaru[100]. Płynąc po wodach południowego Pacyfiku ekspedycja odkryła: niektóre wyspy w archipelagu Fidżi, archipelag (nazwany później Wyspami Cooka), niektóre wyspy Archipelagu Line (wśród nich największą wyspę archipelagu – Wyspę Bożego Narodzenia, nazywaną obecnie Kiritimati[95][101]).
Płynąc w pobliżu Tahiti Cook odstawił Tahitańczyka Omai, którego Tobias Furneaux z „Adventure” przywiózł do Anglii gdzie występował w roli atrakcji. Joseph Banks obwoził Tahitańczyka po Londynie w ubraniu z białej satyny i manczesterskiego aksamitu oraz przedstawił go królowi Jerzemu III Hanowerskiemu[102]. Będąc na Tahiti Cook uniósł się gniewem po kradzieży dwóch kóz. Wściekłość Cooka i chęć krwawej zemsty zaskoczyło załogę statku, znającego Cooka jako osobę spokojną, ale też i stanowczą[103]. Zmiana charakteru Cooka wynikała prawdopodobnie z przyczyn fizycznych. Sir James Watt, chirurg-admirał Marynarki Brytyjskiej, uznał, że Cook cierpiał na niedrożność jelit. Choroba wynikała z zarażenia glistą ludzką, co skutkowało niedoborem witaminy B. Brak tej witaminy objawia się zatwardzeniem, zmęczeniem, depresją, drażliwością, utratą inicjatywy i zainteresowania[104]. Inne możliwe przyczyny to: zachorowanie na kolkę żółciową, choroba wrzodowa, zapalenie pęcherzyka żółciowego, rwa kulszowa, uzależnienie od opiatów lub też wyczerpanie fizyczne i psychiczne[105].
18 stycznia 1778 roku ekspedycja dopłynęła na Hawaje, do północnej części archipelagu[106]. Według obserwacji Cooka, wyspiarze mówili językiem podobnym do języków używanych przez mieszkańców Oceanii[107]. Dopłynięcie statków przyjęte zostało ze zdumieniem ze strony Hawajczyków. Według relacji porucznika Jamesa Kinga, wyspiarze uznali załogę za istoty wyższe, a Cooka za boga[108]. Do dopłynięciu do wysp Cook natychmiast zakazał marynarzom obcowania z kobietami z obawy o rozpowszechnienie chorób wenerycznych. Zarażony marynarz, który złamał zakaz, był skazywany na dwadzieścia cztery baty chłosty. Pomimo zakazu członkowie załogi przebierali kobiety za mężczyzn i prowadzili je na pokład. Zakaz stosunków dotyczył także załogi „Discovery”. Krótko do dopłynięciu Cook nazwał archipelag wyspami Sandwich[109].
W grudniu 1777 roku po opuszczeniu Tahiti ekspedycja skierowała się na północ w celu zbadania wód północnego Pacyfiku i odnalezienia Przejścia Północno-Zachodniego. Dotychczasowe próby znalezienia przejścia kończyły się niepowodzeniem. Pokonawszy Ocean Spokojny, Cook dopłynął do Ameryki Północnej (do okolic dzisiejszego Oregonu) i ruszył do Kolumbii Brytyjskiej. Po postoju w Cieśninie Nootka, wiosną w 1778 roku wyruszył na północ do 65 równoleżnika. Zgodnie z rozkazami Admiralicji miał przeszukać wybrzeże, starając się odnaleźć fiordy i rzeki mogące być trasami prowadzącymi do Przejścia Północno-Zachodniego[110]. Cook znajdując się na Morzu Beringa do tego stopnia popełniał błędy nawigacyjne, że jedną z wysp trzykrotnie nazywał, uznając ją każdorazowo za nowe miejsce[103]. Napotkani ludzie wydawali się Brytyjczykom dziwakami, wkładającymi kości zdobione paciorkami przez duże otwory, przecięte pod dolną wargą. Cookowi spodobały się miejscowe ubrania: opończe z futer wydr i bobrów. Krajowcy nieświadomi rzeczywistej wartości przedmiotów w Europie oddawali za pojedynczy paciorek dziesięć skór (wartych kilkaset funtów szterlingów, tj. wówczas nawet dziesięciokrotności rocznych zarobków marynarza)[111].
27 czerwca 1778 roku Cook dopłynął do wyspy Unalaska[99]. Cook nadał wyspie nazwę Wyspy Opatrzności (ang. Providence Island), za sprawą uratowania statku przed rozbiciem o skały[112]. Przedostawszy się przez Cieśninę Beringa (nazwę nadał James Cook na cześć Vitusa Beringa, duńskiego żeglarza służącego w rosyjskiej marynarce[113]), ekspedycja dotarła do 71° szerokości geograficznej. Nie mogąc pokonać pola lodowego ekspedycja sporządziła jedynie mapy Półwyspu Czukockiego i Alaski[95]. Cookowi udało się określić dokładny zasięg lądów na Morzu Beringa, czego nie udało się wyprawie Vitusa Beringa podczas Wielkiej Ekspedycji Północnej z 1741 roku[114]. Nie mogąc pokonać pokrywy lodowej Cook zdecydował się zawrócić. Statki wycofały się na południe pod koniec sierpnia 1778 roku. Ze względu na uszkodzenia „Resolution”, Cook wrócił na kilka dni do Unalaski[115]. Płynąc wzdłuż wybrzeży Ameryki Północnej Cook dotarł do Alaski, a stamtąd popłynął najpierw ku Kamczatce, a potem na południe[95].
26 listopada 1778 roku ekspedycja wróciła na Hawaje, zatrzymując się w pobliżu wyspy Maui[99]. Cook postanowił opłynąć wyspę bez przybijania do brzegu, by móc powstrzymać marynarzy przed nadmierną wymianą handlową i nawiązaniem stosunków seksualnych z kobietami oraz ograniczył przydział alkoholu dla marynarzy[116]. Przed dopłynięciem na wyspy u niektórych marynarzy zauważono spuchnięte penisy i inne objawy chorób wenerycznych, które rozwinęły się w ciągu jedenastu miesięcy[117]. 1 grudnia okręty dotarły do wyspy O'why'he (błędnego fonetycznego zapisu Hawaiʻi)[118][99]. Statki przez sześć tygodni opływały wyspę, prowadząc handel z tubylcami na czółnach[118].
Statki przybiły do brzegu 16 stycznia 1779 roku, kiedy na pokładzie „Resolution” skończyły się zapasy wody[119]. Podobnie jak podczas pierwszej wizyty krajowcy potraktowali załogę z czcią. Na powitanie podpłynęło ponad tysiąc łodzi[120]. 17 stycznia Cook po raz ostatni dokonał zapisu w dzienniku. Dalsze informacje dotyczące pobytu ekspedycji na Hawajach pochodzą z relacji porucznika Jamesa Kinga[121].
Według Kinga, tubylcy powitali Cooka i kilku jego ludzi pieśnią Lono, padli na ręce i kolana, chyląc czoła ku ziemi. Cooka i jego ludzi zaprowadzono do świątyni nad wodą, na której znajdowała się drewniana platforma ozdobiona dwudziestoma ludzkimi czaszkami. Następnie kapłan o imieniu Koah przeprowadził Cooka przez szereg rytuałów, podczas których Cook pozostawał bierny. Kapitan wspiął się na podwyższenie, gdzie Koah owinął go czerwoną tkaniną. Koah wraz z Cookiem uderzyli czołem przed rzeźbionymi wizerunkami i ucałowali je. Następnie kapłan wraz z kapitanem odmawiali modlitwy, a Hawajczycy nadal śpiewali Lono. Pod koniec rytuału Cooka obdarowano trzciną cukrową, patatami, orzechami kokosowymi i innymi produktami spożywczymi oraz cuchnącą świnią[121]. Ceremonię zakończył inny kapłan, namaszczając ciało Cooka pulpą kokosową[122]. Kapitan „Discovery” Charles Clerke również był traktowany z czcią[123].
Nieznane jest znaczenie ceremonii. Dominuje pogląd, że Hawajczycy uznali Cooka za wcielenie Lobo – potężnego boga płodności oraz starodawnego króla, który według wierzeń udał się na wygnanie po zamordowaniu żony w ataku zazdrości. Co roku, w porze Mahahiki Hawajczycy celebrowali symboliczny powrót Lobo. W ramach kultu kapłani nieśli wizerunek Lobo i okrążali wyspę. Prawdopodobnie tubylcy uznali Cooka za wcielenie Lobo, który tym razem wrócił na wyspę w ciele człowieka[122]. Gananath Obeyesekere, antropolog z Uniwersytetu w Princeton stwierdził, że Hawajczycy byli zbyt racjonalnymi ludźmi i że uznali Cooka za postać zbliżoną do własnych najwyższych wodzów i traktowali go z taką samą czcią. Obeyesekere założył, że na Hawajach trwała bratobójcza walka pomiędzy członkami warstwy kapłańskiej a członkami klanu królewskiego i że oddając cześć Cookowi jedna ze stron chciała uzyskać pomoc od Anglików[124].
Pierwsze dni po przybyciu do zatoki Kealakekua upłynęły żeglarzom głównie na odpoczynku. Podczas pobytu na wyspie Charles Clerke jako pierwszy Europejczyk opisał surfowanie. Zgodnie z jego relacją Hawajczycy nazywali ten sport heʻe nalu. Z kolei według relacji spisanych na początku XIX wieku, Hawajczyków zachwycały u Brytyjczyków dziwne i pozorne magiczne czynności. Według mieszkańców wysp załoga statku miała luźną, pomarszczoną skórę (ubranie), „dziury na skarby w boku” (kieszenie), kanciaste głowy (trójgraniaste kapelusze) i wulkany w ustach (palenie tytoniu). Pismo było dla nich rodzajem dekoracji. Kobiety był zachwycone lusterkami, a mężczyźni metalowymi przedmiotami[125].
Dwa tygodnie po przybyciu na wyspę król i wodzowie zaczęli się pytać, kiedy Brytyjczycy odpłyną. Na Mauna Loa wyspiarze zgromadzili górę jedzenia i przedmiotów oraz zabawiali gości pokazami boksu i zapasów. 4 lutego 1779 roku „Resolution” i „Discovery” opuściły zatokę Kealakekua[126].
Przez kolejne trzy dni statki płynęły  na północ, ku cieśninie Alenuihaha między Maui i Hawaiʻi. Ze względu na zniszczenia „Resolution” podstawa fokmasztu na tyle przegniła, że zaczęła przesuwać się z płyty mocującej ją do kadłuba. Ponadto okręt przeciekał. Ze względu na pojawiające się sztormy Cook zdecydował się na powrót do zatoki Kealakekua[127]. Powrót statków 10 lutego wzbudził zdziwienie i niepokój u tubylców[128].
Wszelkie ślady gościnności minęły 13 lutego. Wyspiarze napadali na marynarzy, szydzili z żołnierzy stanowiących ochronę i okradali statki. Podczas jednego z ataków doszło do szamotaniny: jeden z marynarzy uderzył wodza wiosłem, tłum obrzucił żeglarzy kamieniami i dwóch z nich pobił. Po kradzieży szczypiec zbrojmistrza Cook wziął udział w chaotycznej pogoni za człowiekiem, którego o nią posądzono. Powrót na okręt zdenerwował Cooka do tego stopnia, że wydał rozkaz załadowania muszkietów ostrą amunicją, a nie śrutem[128].
14 lutego 1779 roku tubylcy ukradli kuter „Discovery” z miejsca, gdzie stał przycumowany. Cook nakazał blokadę zatoki i ostrzał czółen krajowców, gdyby próbowali dostać się do niej. O siódmej rano na pokład „Resolution” powrócił James King, który spędził noc na brzegu. Chwilę później Cook i dziesięciu żołnierzy piechoty morskiej popłynęli na brzeg. Na wodzie, w pobliżu brzegu, czekali uzbrojeni marynarze na pokładzie dwóch szalup „Resolution”. Cook zamierzał schwytać wodza i trzymać go dopóki nie odzyska wszystkich ukradzionych rzeczy[129]. Kapitan i żołnierze poszli do wioski wodzowskiej Kaʻawaloa i rozkazał porucznikowi Molesworthowi Phillipsowi wejść do chaty. Król wyspy zgodził się pójść na okręt. Gdy orszak zbliżał się do zatoki, nadciągnęli Hawajczycy. Na drugim końcu zatoki Brytyjczycy zabili jednego z wodzów, usiłującego sforsować blokadę. Przed wejściem na pokład jedna z żon króla oraz dwóch wodzów powstrzymało króla przed wejściem na pokład. W tym samym czasie Cooka i jego żołnierzy otoczył tłum, liczący kilka tysięcy ludzi[130].
Za zgodą króla żołnierze utworzyli nad wodą linię obronną. Tubylcy zaczęli zbierać kamienie i dzidy. W celu odwrócenia uwagi, Hawajczycy pozwolili żołnierzom przejść. Widząc znaczącą przewagę miejscowych, Cook porzucił plan wzięcia króla na zakładnika. Zgodnie z relacją Phillipsa Cook miał wydać rozkaz wejścia na łodzie, jednak jeden człowiek w tłumie zaczął machać żelaznym sztyletem i zagroził, że zaatakuje kapitana. Cook wystrzelił z pistoletu załadowanym śrutem[130]. Huk wystraszył Hawajczyków. Wyspiarze zaczęli rzucać w Anglików kamieniami i dziadami. Cook strzelił po raz drugi, zabijając jednego człowieka. Następnie rozkazał żołnierzom oddać salwę i uciec do łodzi. Żołnierze oddali w tłum salwę, ale zanim ponownie załadowali broń, Hawajczycy rzucili się do zatoki, by zmoczyć wodą bitewne maty. Do walki dołączyli żołnierze znajdujący się na łodziach. Cook próbował dostać się łodzi. Ludzie w jednej szalupie zaczęli wiosłować ku plaży, a drugiej (dowodzonej przez Johna Williamsona) oddaliła się w głąb zatoki. Będąc dziesięć metrów od szalupy został zaatakowany przez Hawajczyków i porwany w głąb lądu. Cook zdążyłby dostać się łódź, gdyby potrafił pływać[c][131]. Jeden z tubylców, o imieniu Kalanimano, skoczył na Cooka znienacka i dużą pałką uderzył go w tył głowy[132]. Według relacji spisanej na początku XIX wieku, wojownik był zaskoczony, że Cook krzyknął z bólu. Wtedy Kalanimano pomyślał, że to człowiek, nie bóg, więc można było zabić kapitana[133]. Po pierwszym ataku podbiegł kolejny mężczyzna i ugodził kapitana żelazem między łopatki. O ósmej rano, godzinę po opuszczeniu „Resolution”, kilka minut po wybuchu walk, Cook i czterech jego żołnierzy leżało martwych[134]. Cudem przeżył George Vancouver[14]. Zginęło również siedemnastu tubylców. Śmierć kapitana zszokowała załogę statku[134]. Śmierć Cooka była tym bardziej szokująca ze względu na wcześniejsze zachowanie Cooka, zabraniającego żołnierzom strzelania do nowo napotkanych ludzi[135].
Nieznany jest powód nagłego pogorszenia relacji pomiędzy załogami statków a Hawajczykami. Według antropologa Marshalla Shalinsa, Cook ponownie pojawiając się na wyspie (tym razem po porze Makahiki) zaburzył pogląd Hawajczyków na jego osobę. Makahiki kończyło się pozorowaną bitwą, w której król odbierał Lono ziemię. Dzień przed śmiercią kapitana wojownik z wyspy spytał Cooka, czy jest walczącym człowiekiem (kanaka koa). Cook w odpowiedzi pokazał rękę pokrytą bliznami. Zakłada się też, że wyspiarze zwątpili w boskość Cooka i jego ludzi, gdy zobaczyli śmierć jednego z marynarzy i uszkodzony „Resolution”[133]. Załoga statku twierdziła zaś, że pogorszenie relacji wynikało z nadużycia gościnności wyspy i dokonania świętokradztwa. James King napisał, że Hawajczycy uznawali Anglików za wygnańców z ziemi, na której panuje głód. W relacjach z początku XIX wieku zachowała się informacja, że wyspiarzy denerwowało upodobanie kobiet do angielskich marynarzy[136].
Dochodzenie nad śmiercią Cooka przeprowadził Charles Clerke, nowy dowódca „Resolution”. Według Clerke’a, całe wydarzenie stanowiło nieszczęśliwy zbieg okoliczności, zmierzający do tragicznego finału. Po śmierci Cooka załoga statku usiłowała odzyskać uszkodzony fokmaszt „Resolution”, przyrządy astronomiczne i szczątki zabitych Anglików[137]. Dzień po walce Hawajczycy drażnili Anglików, paradując po plaży w mundurach zabitych żołnierzy i pokazujących gołe pośladki. 16 lutego, pod osłoną ciemności, do Anglików podpłynął kapłan, przywożąc paczkę, zawierającą udo Cooka. Resztę ciało spalono, a kości rozdzielono między króla i wodzów[138]. 17 lutego jeden z Hawajczyków zaczął rzucać kamieniami w okręt i kręcić na ręce kapelusz Cooka, podczas gdy gromadzący się tłum zaczął śmiać się i szydzić z zabitego kapitana. Następnie wyspiarze obrzucili kamieniami marynarzy uzupełniających zapasy wody. Obrzucenie kamieniami wywołało wściekłość u Anglików, którzy zaczęli strzelać do tubylców i podpalać okoliczne domy. Gdy marynarze zabili bezbronnych ludzi, odcięli im głowy i przymocowali je do łodzi. Chorujący na gruźlicę Clerke nie był w stanie powstrzymać brutalności jego ludzi[139]. Podczas starcia zabito około 30 tubylców[106]. 20 lutego Hawajczycy przekazali załodze drugie zawiniątko zawierające spalone kończyny, skalp z uszami i obciętymi włosami oraz ponacinane i zasolone dwie dłonie[99][140].
Nie wiadomo, czy uroczyste spalenie ciała Cooka było symboliczną ofiarą, złożoną lokalnemu bogu wojny. Kości Cooka stały się czczonymi przedmiotami. W kolejnych latach kości kapitana noszono podczas procesji Lono. Według relacji z początku XIX wieku, szczątki Cooka trzymano w plecionym koszu, przykryte czerwonymi piórami[141]. Szczątki zwrócone Anglikowi włożono do trumny i wrzucono w morze[142]. Według relacji Charlesa Clerke’a, Cook nie został zjedzony, gdyż według wierzeń tubylców ciało tubylców poddaje się gotowaniu po to, by wyciągnąć z nich kości obdarzone boską mocą. Zachowują wyłącznie kości, a resztę ciała odrzucają[143].
Po śmierci Cooka, zgodnie ze zwyczajem okrętowym, Anglicy rozdzielili i sprzedali ubrania oraz inne przedmioty należące do kapitana. 21 lutego załoga „Resolution” opuściła flagę do połowy masztu, oddała salut z dziesięciu dział i uderzyła w dzwon[140].
29 kwietnia 1779 roku „Resolution” i „Discovery” dopłynęły do portu Pietropawłowsk na Kamczatce, skąd Clerke wysłał list do Admiralicji z informacją o śmierci Cooka i z zobowiązaniem do kontynuowania poszukiwań Przejścia Północno-Zachodniego[99]. Statki dopłynęły do 70° szerokości północnej[144]. W drodze powrotnej, 22 sierpnia 1779 roku Clerke zmarł na gruźlicę[145]. Został pochowany na Kamczatce pod wierzbami posadzonymi przez załogę. Dowództwo nad „Resolution” objął Amerykanin John Gore, a nad „Discovery” James King. 13 stycznia 1780 roku statki dotarły do Makau[99]. W międzyczasie do Wielkiej Brytanii dotarła informacja o śmierci Cooka[120]. W sierpniu 1780 roku statki dotarły do wybrzeża Wielkiej Brytanii[99].
Podróże Cooka i wykonane przezeń mapy zmieniły wyobrażenie o kształcie lądów i mórz. Cookowi udało się udowodnić, że Australia i Nowa Zelandia nadają się do kolonizacji[95]. Podczas wypraw Cook odkrył wiele wysp na Oceanie Spokojnym oraz ustalił zasięg Australii i Ameryki Północnej[101]. Nie mogąc przebić się przez antarktyczny krąg polarny Cook utwierdził się w przekonaniu, że Ląd Południowy nie istnieje. Błędna opinia spowodowała trwającą ponad 50 lat przerwę w poszukiwaniach południowego kontynentu[146]. Podobnie niepowodzenie podczas trzeciej wyprawy skłoniła Cooka do postawienia hipotezy, że Przejście Północno-Zachodnie nie istnieje. Po latach okazało, że Przejście istnieje, jednak osiemnastowieczne okręty nie były w stanie go pokonać. Pierwszą osobą, która pokonała Przejście, był Roald Amundsen (w 1906 roku). Pierwszy statek handlowy przepłynął przez Przejście w 1969 roku[147].
Botanicy uczestniczący w wyprawach zebrali tyle okazów flory, że liczba roślin znana w świecie zachodnim powiększyła się o jedną czwartą[148]. Podczas pierwszej wyprawy Parkinson stworzył ponad 1300 rysunków i obrazów (ponad 400 w Australii)[149][150].
Podczas wypraw James Cook narzucił marynarzom zmianę obyczajów higienicznych. Wprowadził regularne czyszczenie hamaków, ubrań i posłań, kąpiele w morzu, wietrzenie niższych pokładów (dzięki tunelom z płótna żaglowego), szorowanie pokładu octem[151]. Cook wprowadził do diety marynarskiej warzywa (głównie kapustę kiszoną) i owoce w celu opanowania plagi szkorbutu. Aby przekonać załogę do jedzenia kapusty, Cook rozpuścił plotkę, że kapustę podaje się codziennie oficerom i dżentelmenom. Załoga przekonana o elitarności potrawy dobrowolnie wprowadziła warzywo do swojej diety[152]. W rzeczywistości wprowadzone przez Cooka pomysły na walkę ze szkorbutem nie pomagały znacząco w walce z chorobą. Dieta marynarzy oprócz kapusty kiszonej składała się z brzeczki słodowej utrudniającej walkę ze szkorbutem. Chorobie udało się zapobiec przede wszystkim dzięki regularnemu zaopatrywaniu statku w świeże owoce i inne rośliny[153].
W książce A Voyage Towards the South Pole and Round the World (wydanej w 1773 roku w Londynie) zaprezentował informacje ze swojej wyprawy dookoła świata. Dzięki książce Cook spopularyzował słowo tatuaż. Ponadto dzięki Cookowi do zachodniego słownictwa weszły słowo tabu[148] i kangur[62].
Niektóre mapy sporządzone podczas wypraw Cooka były stosowane aż do lat 90. XX wieku[154]. Mapy Nowej Zelandii pozostawały w użyciu do 1994 roku, gdy Królewska Marynarka Nowej Zelandii uaktualniła mapy, dodając najnowsze pomiary głębokości i dane z satelity[46]. Podczas wypraw korzystał z nowo skonstruowanego chronometru Harrisona, pozwalającego na ustalenie dokładnego pomiaru czasu na statku, koniecznego do precyzyjnego określenia szerokości geograficznej[155].
Powszechny na Hawajach, Niue i Tonga nacjonalizm pogłębił negatywny stosunek do Cooka. Odkrywcę uważa się za winnego pojawienia się białych kolonizatorów odpowiedzialnych za rozpowszechnienie chorób i niszczenie miejscowych kultur[156].
Życzliwy stosunek Cooka do rdzennych mieszkańców Australii zachęcił Bawarczyka Johanna Flierla do wyruszenia w latach 80. XIX wieku do Cooktown. W pobliżu miasta Flierl założył misję luterańską, która stała się azylem dla Aborygenów prześladowanych przez poszukiwaczy złota[157].
Na podstawie dzienników ocenia się Cooka jako człowieka reprezentującego osiemnastowieczne Oświecenie. W raportach często używał słowa „przesądy”, w odniesieniu zarówno do wiar mieszkańców Oceanii jak i do religii zachodniej[158]. Nie był człowiekiem pobożnym, jednak zgodnie z Artykułami Wojennymi raz w tygodniu odprawiał nabożeństwo dla marynarzy[30]. Wpływ na poglądy Cooka miała też rodzina Walkerów, należąca do Religijnego Towarzystwa Przyjaciół[10]. Wychowanie wśród kwakrów mogło wykształcić u Cooka niechęć do zabijania tubylców, starając się strzelać do wrogo nastawionej ludności w ostateczności[158]. W stosunku do obyczajów seksualnych i kanibalizmu na Pacyfiku wykazywał przenikliwość i brak uprzedzeń[159].
Joseph Banks został pierwszym egzekutorem spuścizny Cooka. Jako przewodniczący Towarzystwa Królewskiego popierał kolonizację Australii i wiele wypraw (między innymi: Williama Bligha, George’a Vancouvera, Londyńskiego Towarzystwa Misyjnego). Nowe odkrycia rozszerzyły zasięg odkryć Cooka i przyczyniły się do budowy Imperium brytyjskiego. Banks do końca życia zbierał i opisywał rośliny znalezione zarówno przez niego, jak i przez kolejnych odkrywców. Przekształcił londyńskie Kew Gardens w ogród botaniczny, w którym sadzono rośliny sprowadzone z całego świata. Zmarły 19 czerwca 1820 roku Banks nie zdążył ukończyć wielotomowej pracy na temat roślin odkrytych podczas pierwszej wyprawy Cooka[160][161]. Dzieła Sydneya Parkinsona opublikowano w 1988 roku[162]. Przedmioty zebrane przez Georga i Reinholda Forsterów są wystawiane w muzeum Sammlung für Völkerkunde w Getyndze[163].
Wojna o niepodległość Stanów Zjednoczonych oraz wojny napoleońskie przesłoniły odkrycia Cooka. Zainteresowanie wyprawami miało miejsce dopiero w XIX wieku. Pierwsze biografie Cooka przedstawiały kapitana jako postać nudną i tępą, stanowiącą przeciwieństwo Horatio Nelsona[164].
Statek „Endeavour”, przemianowany na „Lorda Sandwicha”, przekształcono w statek wielorybniczy. Statek zatopiono w 1778 roku w okolicach Newport na Rhode Island. „Resolution” rozbił się u brzegów Newport w 1794 roku[165].
Stanley Kubrick (ur. 26 lipca 1928 w Nowym Jorku, zm. 7 marca 1999 w Harpenden) – amerykański reżyser, scenarzysta, montażysta i producent filmowy.
Jego filmy, które w dużej mierze są adaptacjami filmowymi obejmują szeroki zakres gatunków i wyróżniają się realizmem, czarnym humorem, charakterystyczną pracą kamery, rozbudowaną scenografią i wykorzystaniem muzyki poważnej.
Pochodził z rodziny aszkenazyjskich Żydów, która wywodziła się z Europy Środkowej; jego dziadek, Eliasz Kubrik przyszedł na świat 25 listopada 1877 w galicyjskiej Probużnie (obecnie Ukraina) i 25 lat później wyemigrował za ocean. Ojciec reżysera, Jakob Leonard Kubrik, znany także pod imionami Jack i Jacques, urodził się w Nowym Jorku 21 maja 1902; Eliasz i Róża Kubrikowie mieli także dwie córki, Hester Merel (ur. 12 czerwca 1904) i Lilly (ur. 11 sierpnia 1906). Nie wiadomo, kiedy nazwisko Kubrik przybrało zamerykanizowaną postać; na dyplomie ukończenia szkoły medycznej z roku 1927 ojciec reżysera figuruje już jako Kubrick, podobnie jak na akcie małżeństwa. Zawarł je w roku 1927 z Gertrudą Peveler, córką austriackich emigrantów. Ich pierwsze dziecko, Stanley Kubrick przyszedł na świat 26 lipca 1928 w klinicznym szpitalu położniczym Lying-In Hospital na Manhattanie[1][2]; niespełna sześć lat później urodziła się jego siostra, Barbara Mary[3].
Jego ojciec był lekarzem, którego pasją były szachy i fotografia. Przyszły reżyser rozpoczął naukę w roku 1934; nie był dobrym uczniem w szkole, sporo lekcji opuszczał, co w pewnym momencie nawet nasunęło podejrzenie o upośledzenie umysłowe; jednak stosowne testy wykazały bardzo wysoką inteligencję, zaś sam Stanley stwierdził, że w szkole nic nie jest w stanie go zainteresować, gdyż lekcje prowadzone są w nudny i mechaniczny sposób. Od ukończenia ośmiu lat był dodatkowo uczony przez prywatnego nauczyciela. Jack pozwalał synowi korzystać ze swojego profesjonalnego sprzętu fotograficznego, nauczył go też grać w szachy. Młodego Stanleya szybko zafascynował świat nieruchomych obrazów. Oprócz fotografowania, zajmował się również wywoływaniem i obróbką zdjęć. W szkolnym zespole jazzowym grał na perkusji.
Kubrick kontynuował edukację w szkole średniej William Howard Taft. Częściej jednak niż w szkolnej klasie (spośród wszystkich zajęć najczęściej przebywał na lekcjach języka angielskiego, prowadzonych przez Aarona Traistera; potem opowiadał z podziwem, jak Traister, zamiast – jak inni nauczyciele – nudno recytować banały o przerabianych lekturach, w teatralny, aktorski sposób odgrywał ich fragmenty przed klasą, wcielając się w różne postaci, oraz jak zachęcał klasę do dyskusji) można było go spotkać w parku Washington Square, gdzie obserwował rozgrywających zacięte pojedynki szachistów i sam wielokrotnie grał, także za pieniądze, oraz w miejscowym kinie, gdzie oglądał praktycznie każdy obraz, jaki trafił na ekran. Jak później opowiadał, znakomita większość tych filmów była zła lub bardzo zła, ale w pewnym momencie, oglądając te słabe filmy, doszedł do wniosku, że sam potrafiłby nakręcić lepsze. W tym czasie interesował go również jazz; w szkolnym combo swingowym grał na perkusji – jak wspominali jego rówieśnicy, radził sobie bardzo dobrze. W wieku 17 lat podjął pracę fotografa w magazynie „Look” (rozpoczął od zrobienia zdjęcia zasmuconego kioskarza otoczonego nagłówkami gazet, informujących o śmierci Franklina Delano Roosevelta – zdjęcie to ukazało się na pierwszej stronie magazynu „Look” 26 czerwca 1945; w kwietniu 1946 zrobił sesję zdjęciową Traistera odgrywającego przed klasą fragmenty Hamleta), sporo podróżował i dużo czytał. W szkole średniej poznał Alexandra Singera – również przyszłego reżysera, twórcy wielu filmów fabularnych i licznych odcinków seriali telewizyjnych, m.in. Posterunku przy Hill Street, Star Trek: Stacja kosmiczna, Star Trek: Następne pokolenie i Star Trek: Voyager – ich wspólne rozmowy ostatecznie zachęciły Stanleya do poświęcenia się w przyszłości reżyserii. Aby ukończyć szkołę średnią, musiał zdać odpowiednie egzaminy (tzw. major) – wybrał sztuki piękne pod kierunkiem nauczyciela Hermana Gettera. Getter (który zdobył sobie sympatię Kubricka tym, że uważał fotografię za formę sztuki, co było wówczas rzadkim poglądem) zapoznał go z podstawowymi technikami używanymi podczas kręcenia filmów – od strony teoretycznej, gdyż szkoła nie dysponowała kamerami filmowymi. W późniejszych latach Kubrick i Getter korespondowali ze sobą.
W styczniu 1946 Stanley Kubrick ukończył William Howard Taft; na 509 absolwentów zajął 414. lokatę, co skutecznie zamknęło mu drogę do college’u (w tym czasie wielu zdemobilizowanych po zakończeniu II wojny światowej młodych żołnierzy na mocy tzw. G.I. Bill wstępowało do college’ów, które zostały przepełnione studentami); wtedy poświęcił się całkowicie współpracy z „Look” (m.in. zrealizował intrygujący cykl fotografii dokumentujący jeden dzień z życia boksera Waltera Cartiera, w tym walkę na ringu z Tonym D’Amico). 29 maja 1948 ożenił się z Tobą Metz, młodszą o półtora roku koleżanką z klasy Gettera; nowożeńcy przeprowadzili się z Bronxu do artystycznej dzielnicy Greenwich Village. Był częstym gościem Museum of Modern Art i lokalnych kin. Podziwiał filmy Orsona Wellesa, Siergieja Eisensteina i Maxa Ophülsa.
Stanley i Alexander Singer utrzymywali ze sobą kontakt po ukończeniu szkoły średniej. Ambitny Singer planował realizację filmowej wersji Iliady, skontaktował się nawet w tej sprawie ze studiem Metro-Goldwyn-Mayer, jednak kierownictwo studia grzecznie odmówiło. Kubrick postanowił rozpocząć od realizacji krótkometrażowego obrazu dokumentalnego i w 1950 z pomocą Singera zrealizował 16-minutowy film dokumentalny Dzień walki (Day of the Fight), dokumentujący jeden dzień (dokładnie 17 kwietnia 1950) z życia boksera Waltera Cartiera, który walczył z Bobbym Jamesem w Laurel Gardens w Newark w stanie New Jersey, wygrywając przez nokaut w 2. rundzie (był to ten sam Walter Cartier, któremu dwa lata wcześniej Kubrick poświęcił cykl fotografii). Koszt realizacji filmu wyniósł około 3900 dolarów (Singer podawał później około 4500), dystrybutor RKO-Pathe, który prezentował go w kinach w cyklu krótkometrażówek This Is America i który dał Kubrickowi 1500 dolarów na jego realizację, wykupił go za 4000 dolarów. Oprócz zdjęć (zrealizowanych wspólnie z Singerem) Kubrick zmontował, wyprodukował i udźwiękowił film.
Zarobione na Dniu walki pieniądze zainwestował w kolejny krótkometrażowy dokument – Latający ojczulek (Flying Padre), opowiadający o Fredzie Stadtmuellerze, katolickim księdzu mieszkającym w Mesquero w hrabstwie Harding w północnej części stanu Nowy Meksyk, który używa małego samolotu o nazwie The Spirit of St. Joseph (Duch Św. Józefa), by przemieszczać się pomiędzy podległymi sobie jedenastoma kościołami, rozrzuconymi na obszarze ponad 4000 mil kwadratowych (ponad 10 880 km kw.). Podobnie jak poprzednio, Stanley odpowiadał za zdjęcia, montaż i dźwięk. Film ten (również wykupiony przez RKO-Pathe) zdradzał kolejną pasję Kubricka – lotnictwo; był również punktem zwrotnym jego kariery, gdyż właśnie wtedy Stanley Kubrick postanowił ostatecznie poświęcić się karierze reżysera.
W roku 1953 zrealizował ostatni z krótkometrażowych filmów dokumentalnych – Marynarze (The Seafarers), reklamówkę nakręconą na życzenie międzynarodowego związku zawodowego marynarzy. Była to pierwsza w karierze Kubricka praca na zlecenie; głównymi powodami, dla której podjął się jej, była możliwość pracy po raz pierwszy w karierze na taśmie kolorowej, jak również zdobycia środków na debiut fabularny, który również ujrzał światło dzienne w roku 1953.
Pracę nad fabularnym debiutem Stanley Kubrick rozpoczął w roku 1951. Scenariusz The Trap (Pułapka), alegorycznej opowieści o czterech zwykłych żołnierzach, uwięzionych za linią frontu na terenie wroga podczas nieokreślonej z nazwy wojny i próbujących wrócić do swoich kolegów, napisał Howard O. Sackler – znajomy Kubricka. Pierwszym dystrybutorem filmu miał być znany w owym czasie producent, Richard de Rochemont; ostatecznie obowiązki te spadły na Josepha Burstyna. Film nakręcono w okolicach Los Angeles; ponieważ wynajęcie profesjonalnego operatora byłoby zbyt kosztowne, Kubrick sam nakręcił film, posługując się wypożyczoną (za 25 dolarów za dobę) kamerą Mitchell, której obsługi nauczył go sprzedawca w sklepie z kamerami, Bert Zucker, na taśmie 35 mm. Film trafił na ekrany 31 marca 1953.
Sam Kubrick o Strachu i pożądaniu (Fear and Desire) – jak ostatecznie zatytułowano film – wypowiadał się zawsze negatywnie, uważając go za niewarty uwagi film amatorski; gdy jego kariera nabrała rozpędu, wstrzymał prezentacje swego fabularnego debiutu. Kiedy prawa autorskie na początku lat 90. wygasły i film można było bez zgody reżysera pokazywać i dystrybuować, Kubrick wykupił i zniszczył wszystkie kopie, do jakich udało mu się dotrzeć. Jedyna kopia w dobrym stanie przetrwała w prywatnej kolekcji i jest podstawą osiągalnych obecnie na rynku bootlegowych wersji DVD filmu.
Strach i pożądanie, pierwszy niezależny film fabularny w historii nowojorskiej sceny filmowej, wprowadza kilka tematów, które w twórczości Kubricka będą się przewijać prawie do końca. Okrutny fenomen wojny, szaleństwo i okrucieństwo jako stała, zawsze obecna część ludzkiej natury, jednostka ulegająca stłamszeniu przez otaczających go ludzi, fatalistyczne przeświadczenie, że człowiek w istocie nie ma wpływu na swój los – te motywy, które w różnych wersjach i odmianach będą powracać w kolejnych filmach Kubricka, po raz pierwszy zostały zaznaczone właśnie w Strachu i pożądaniu. Jest to zarazem pierwszy z dwóch filmów Kubricka, do których nie napisał on (bądź współtworzył) scenariusza.
W roku 1952, rok po rozwodzie z Tobą Metz, Stanley Kubrick poznał starszą o trzy lata austriacką tancerkę Ruth Sobotkę, która wyemigrowała do Stanów Zjednoczonych tuż po wybuchu II wojny światowej. Zamieszkali wspólnie, a pobrali się w roku 1954. Alexander Singer w tym czasie przebywał w Hollywood, gdzie poznał młodego producenta i reżysera – Jamesa B. Harrisa, którego wkrótce poznał z Kubrickiem.
W roku 1953, po ukończeniu prac nad Marynarzami, Kubrick – znów we współpracy z Sacklerem (obaj podpisali scenariusz filmu) – rozpoczął pracę nad kolejnym filmem fabularnym. Jako że pracując nad fotoreportażami i filmem dokumentalnym Dzień walki, Stanley poznał dokładnie środowisko bokserów, głównym bohaterem filmu uczynił on właśnie boksera na życiowym zakręcie, zakochanego w tancerce, którą z kolei uwodzi jej brutalny i chamski pracodawca. Aby film wyprodukować, Kubrick wraz z Harrisem założył własną spółkę producencką – Minotaur Productions.
Pocałunek mordercy (Killer’s Kiss) trafił na ekrany kin 28 września 1955. Utrzymany w poetyce filmu noir, był mroczną, ponurą historią kryminalną. Znów pojawiał się w nim wątek przypadku, determinującego ludzki los – gdy bohater oczekuje na ukochaną na ulicy, grupa pijanych wyrostków kradnie mu szalik; gdy oddala się z miejsca spotkania, by go odzyskać, zbiry nasłane przez brutalnego pracodawcę dziewczyny mordują zupełnie przypadkową osobę, która miała pecha się tam w tym momencie znaleźć. Miasto, w którym rozgrywa się akcja filmu, przedstawione jest zarazem bardzo realistycznie i w nieco odrealniony sposób: realistycznie przedstawione kawiarnie, ulice, place, zaułki przypominają dziwny, surrealistyczny, odhumanizowany labirynt.
Z uwagi na ograniczony budżet filmu wiele scen filmu nie dało się zainscenizować, lecz kręcono je ukrytą kamerą; utrwalone na taśmie reakcje przypadkowych widzów są całkowicie autentyczne.
Kolejnym, pierwszym pełnometrażowym dziełem Kubricka była adaptacja powieści Clean Break Lionela White’a (Pocałunek mordercy był ostatnim filmem reżysera opartym na oryginalnym pomyśle – wszystkie późniejsze dzieła Stanleya Kubricka były adaptacjami powieści lub opowiadań), historia skoku na pieniądze z zakładów bukmacherskich i jego konsekwencji. Harris osobiście doręczył scenariusz Jackowi Palance’owi, ten jednak nie zadał sobie nawet trudu przeczytania go (czego po latach żałował); ostatecznie główną rolę zagrał Sterling Hayden. Powstały film – zatytułowany ostatecznie The Killing (Zabójstwo) – trafił na ekrany kin 20 maja 1956.
Kubrick, pierwszy raz pracujący z zawodową ekipą filmową i zawodowymi aktorami, nieco zmienił wymowę powieści: główni bohaterowie nie są zatwardziałymi kryminalistami, lecz pechowcami, których na drogę przestępstwa popycha desperacja i brak możliwości znalezienia innego wyjścia z kłopotów, w jakie popadli. Film kręcił Kubrick niekonwencjonalnie, stosując np. szerokokątne obiektywy, używane do zdjęć plenerowych, do kręcenia scen we wnętrzu, nadając im niezwykłą ostrość i specyficzną perspektywę. Po raz pierwszy też dał o sobie znać perfekcjonizm reżysera, który bardzo dokładnie rozpisał wszystkie szczegóły techniczne, włącznie z użyciem odpowiednich obiektywów. Doprowadziło to do konfliktów z doświadczonym operatorem Lucienem Ballardem; gdy do kręcenia scen we wnętrzach Kubrick nakazał użyć szerokokątnego obiektywu, używanego do planów szerokich, Ballard użył zwykłego, uważając decyzję Kubricka za błąd niezbyt jeszcze doświadczonego reżysera, na co Kubrick zareagował od razu, każąc Ballardowi postąpić według jego wskazówek albo opuścić plan i już nie wracać. Ballard posłuchał i od tego momentu stosował się do poleceń Kubricka. Najwięcej trudności nastręczyła realizacja sceny skoku, zwłaszcza momentu, gdy rozpoczyna się wyścig i konie startują z boksów; nie mając pieniędzy na wynajęcie toru i sfilmowanie sceny, reżyser namówił Singera, by ten wtargnął z kamerą na tor podczas prawdziwego wyścigu i sfilmował start, zanim służby porządkowe go wyrzucą z toru. Udało się nakręcić tę scenę za pierwszą próbą.
Zabójstwo było, jak na ówczesny film kryminalny, nowatorskim eksperymentem formalnym: poszczególne zdarzenia nie były relacjonowane chronologicznie, lecz w sposób nieliniowy; choć ówcześni krytycy narzekali, że utrudnia to zrozumienie obrazu, ów eksperyment po latach znalazł licznych naśladowców – choćby Quentina Tarantino, który w swoim słynnym filmie Pulp Fiction również opowiada fabułę nieliniowo, achronologicznie.
W filmie pojawił się również – istotny dla Kubricka – motyw przypadku determinującego ludzkie losy: w finale plany bohaterów krzyżował bowiem mały piesek, który przypadkiem znalazł się w niewłaściwym miejscu.
Kolejnym filmem Kubricka była adaptacja powieści Paths Of Glory (Ścieżki chwały) Humphreya Cobba, historia trzech francuskich żołnierzy, którzy podczas I wojny światowej zostali niesłusznie oskarżeni o tchórzostwo (w wyniku chorej ambicji dowódcy wyznaczono im zdobycie ważnego, ale też zaciekle bronionego punktu oporu Niemców – Mrówczego Wzgórza; gdy natarcie załamuje się – dowództwo potrzebuje kozłów ofiarnych, by nie wyszło na jaw, że natarcie od początku nie miało szans powodzenia) i po karykaturalnym procesie skazani i rozstrzelani, by dać odstraszający przykład innym żołnierzom. Jak opowiadał sam Kubrick, powieść Cobba znalazł przypadkiem w poczekalni gabinetu lekarskiego ojca, gdzie zgubił ją jeden z pacjentów.
Długo kompletował obsadę: z uwagi na spore koszty realizacji scen batalistycznych, studio Metro-Goldwyn-Mayer zgodziło się sfinansować film tylko, jeżeli w głównej roli – adwokata trzech skazanych żołnierzy, pułkownika Daxa – wystąpi gwiazda. (Dodatkowo – po premierze filmu The Red Badge Of Courage – szefostwo studia nie miało ochoty realizować kolejnego ponurego, realistycznego filmu wojennego). Harris i Kubrick rozpoczęli pracę nad adaptacją powieści kryminalnej The Burning Secret Stefana Zweiga; MGM początkowo podpisała z trójką scenarzystów (oprócz Kubricka i współautora The Killing, Jima Thompsona, trzecim był młody pisarz Calder Willingham) kontrakt, anulowano go jednak, gdy prace nad scenariuszem zaczęły się przeciągać; wtedy Kubrick przekonał studio do realizacji Paths Of Glory. Gdy przygotowania do realizacji filmu utknęły w martwym punkcie, nagle filmem zainteresowała się gwiazda – i to gwiazda wtedy pierwszej wielkości. Scenariusz bowiem przypadkiem wpadł w ręce Kirka Douglasa.
Nazwisko i wsparcie Douglasa spowodowało, że Metro-Goldwyn-Mayer sfinansowało film; aby zmieścić się w narzuconych przez wytwórnię kosztach, Kubrick postanowił nakręcić film w Europie – wybór padł na rozległe nieużytki w podmonachijskim Geiselgasteig. Ekipa składała się w sporej mierze z Niemców; choć powodowało to problemy językowe, reżyser wysoko cenił ich poświęcenie pracy. Znów dał o sobie znać perfekcjonizm Kubricka: sceny batalistyczne kręcono z użyciem wielu statystów, zainscenizowane na potrzeby filmu Mrówcze Wzgórze – cel nieudanego ataku żołnierzy francuskich – podzielono na pięć, oznaczonych literami sektorów, i każdy statysta miał przypisany konkretny sektor, w którym miał widowiskowo „umrzeć”. Podczas kręcenia scen bitewnych zużyto takie ilości środków wybuchowych, że Kubrick musiał się postarać o specjalne zezwolenie Ministerstwa Spraw Wewnętrznych RFN, by taką ilość dostać. Scenę, w której trzej skazańcy dostają ostatni posiłek – smażoną kaczkę, powtarzano w sumie 68 razy; jeśli aktorzy zaczęli jeść – trzeba było sprowadzić kolejną kaczkę.
Douglas był zdania, że film warto zrobić, choć uważał, że nie przyniesie on zysków; Kubrick, aby zwiększyć komercyjny potencjał materiału, postanowił podczas filmowania zmienić zakończenie – w nowej wersji trzech żołnierzy w ostatniej chwili ułaskawiano. Ta zmiana rozsierdziła Douglasa, który zwymyślał reżysera na planie; Kubrick ze stoickim spokojem zgodził się powrócić do pierwotnego scenariusza.
Film wprowadzał kolejny wątek, który miał w dziełach Kubricka powracać szereg razy: ponury fenomen wojny, zinstytucjonalizowanego zabijania w imię wyższych celów. Również w tym filmie pojawił się motyw irracjonalnego przypadku, kaprysu losu: trzej skazani żołnierze byli bowiem wybrani przez dowódców swoich oddziałów – szeregowy Ferrol dlatego, że pod wpływem szoku bojowego załamał się, kapral Paris zaś dlatego, że był świadkiem, jak jego przełożony przez swoją głupotę spowodował śmierć innego francuskiego żołnierza, natomiast trzeci ze skazanych – szeregowy Arnaud – został wybrany drogą losowania, chociaż był jednym z najdzielniejszych żołnierzy w oddziale, odznaczonym za odwagę na polu bitwy. Film zaznaczył też wątek odkupienia, jaki przynosi kobieta: w finale pojmana niemiecka śpiewaczka, wykonując w kasynie wojskowym starą niemiecką pieśń, wzrusza żołnierzy francuskich do łez, przynosząc im chwilowe wytchnienie od koszmaru wojny. W rolę tę wcieliła się niemiecka aktorka Christiane Harlan, znana pod pseudonimem Suzanne Christian (jej dziadek Veit Harlan był twórcą nazistowskich filmów propagandowych, m.in. Żyda Süssa) – od roku 1959 Christiane Kubrick. (Kubrick i Ruth Sobotka rozwiedli się w roku 1957; 17 czerwca 1967 Sobotka zmarła śmiercią samobójczą.)
Douglas miał rację: film (który wszedł na ekrany kin 25 grudnia 1957) nie był kasowym przebojem, ale spotkał się z pozytywną reakcją krytyków – w Stanach Zjednoczonych, gdyż w Europie przyjęto go z mieszanymi uczuciami. Ze szczególnie negatywną reakcją film w momencie premiery spotkał się we Francji, gdzie uznano go wręcz za antyfrancuski i zakazano jego wyświetlania (zakaz ten zniesiono dopiero w połowie lat 70.); niechętnie przyjmowano go również w RFN, choć bardziej z kurtuazji, gdyż w tym czasie stosunki francusko-niemieckie, polepszające się od czasu zakończenia wojny, były wyjątkowo pozytywne i politycy niemieccy obawiali się, że prezentacja filmu uważanego za antyfrancuski może je pogorszyć. Armia francuska stanowczo twierdziła, że podczas I wojny światowej nie było popisowych, demonstracyjnych egzekucji mających odstraszać żołnierzy francuskich od dezercji, odmowy walki z nieprzyjacielem lub wycofywania się pod ogniem wroga, choć, jak udało się ustalić historykom, przynajmniej jedna taka pokazowa egzekucja miała miejsce (żołnierze zostali potem zrehabilitowani, a ich rodziny otrzymały od rządu francuskiego symboliczne odszkodowanie w wysokości 1 franka). Podmonachijskie Geiselgasteig, podówczas porzucone pole, szybko przekształcono w plan filmowy z prawdziwego zdarzenia, jeden z największych i najlepiej wyposażonych w Europie – Europa Film Studios (w latach 80. Wolfgang Petersen właśnie w tym studio nakręcił Okręt i Niekończącą się historię). Wysoce realistyczny, ponury, czarno-biały obraz był potem przywoływany jako główna inspiracja przez wielu twórców kina (m.in. Stevena Spielberga).
Po premierze Ścieżek chwały z Kubrickiem skontaktował się jeden z ulubionych aktorów reżysera – Marlon Brando, wówczas już wielka legenda i instytucja Hollywood. Brando planował nakręcić bardzo ambitny western, mający przebić wszystko, co do tej pory w tym gatunku stworzono – One-Eyed Jacks (Dwa oblicza zemsty). Rozpoczęto już nawet zdjęcia; jednak perfekcjonistyczny, autokratyczny reżyser i równie autokratyczna wielka gwiazda nie byli w stanie ze sobą pracować i ostatecznie po kilku miesiącach Brando zwolnił Kubricka, samemu przejmując obowiązki reżysera.
Stanley Kubrick niedługo pozostawał bez pracy. Szybko skontaktował się z nim Kirk Douglas, który w tym czasie, pod egidą swojej nowo stworzonej firmy Bryna Productions (od imienia matki Douglasa), rozpoczął pracę nad filmem poświęconym Spartakusowi i buntowi niewolników w starożytnym Rzymie. Rozpoczęto już zdjęcia do Spartakusa, jednak wybrany przez aktora reżyser Anthony Mann nie był w stanie poradzić sobie z wielką produkcją (choć wkrótce potem zrealizował epicki obraz El Cid) i został zwolniony po zrealizowaniu otwierającej film sceny w kamieniołomach. Z dnia na dzień, nie mając nawet możliwości zapoznania się ze scenariuszem czy dekoracjami (informację, że następnego dnia ma się zjawić na planie, otrzymał telefonicznie podczas wieczornej partii pokera ze znajomymi), jego miejsce zajął Kubrick.
Pod kierunkiem nowego reżysera praca nad filmem ruszyła naprzód, ale nie obywało się bez problemów. Kubrick chciał zmienić scenariusz, który uważał za miejscami naiwny i uproszczony; jego koncepcje (m.in. intrygującą ramę narracyjną, w której cała historia jest wizją konającego Spartakusa, ukrzyżowanego przy Via Appia, jak również scenę, zwięźle i celnie ukazującą zdeprawowanie i demoralizację rzymskiego patrycjatu, w której Krassus (Laurence Olivier) próbuje uwieść niewolnika i przyjaciela Spartakusa, Antoninusa (Tony Curtis), w wyrafinowany sposób porównując preferencje seksualne do preferencji kulinarnych, a moralność sprowadzając do kwestii wolnego wyboru) zostały odrzucone przez scenarzystę Daltona Trumbo i samego Douglasa. (W odrestaurowanej wersji filmu, przygotowanej na początku lat 90., scenę w łaźni z udziałem Krassusa i Antoninusa przywrócono, jednak okazało się, że ocalała tylko warstwa wizualna, bez dźwięku. Curtis ponownie nagrał swoje kwestie; zmarłego w lipcu 1989 Oliviera zastąpił inny ceniony aktor o szekspirowskim rodowodzie – Anthony Hopkins.) Znów dał o sobie znać perfekcjonizm reżysera: w widowiskowych scenach batalistycznych każdy z tysięcy statystów miał wyznaczone swoje miejsce, w scenach, gdzie pojmani buntownicy wiszą na krzyżach nad Via Appia, każdy z aktorów miał dokładnie wyznaczony moment, kiedy ma zajęczeć, do scen bitewnych Kubrick zaangażował statystów z amputowanymi kończynami, by można było wiarygodnie na ekranie przedstawić odcinanie kończyn mieczem podczas walki (nakręcono sporo takich ujęć, ale podczas próbnych pokazów widownia uznała je za zbyt szokujące i większość z nich wycięto). Zdjęcia batalistyczne realizowano w okolicach Madrytu w środku lata, z udziałem 8000 statystów; wielu spośród nich mdlało z gorąca.
Kubrick toczył ciągłe boje o sposób kadrowania, oświetlenia i filmowania poszczególnych scen i używane obiektywy z doświadczonym operatorem Russellem Mettym, który ciągle domagał się od Douglasa zdjęcia tego Żydka z Bronxu z dźwigu kamerowego (reżyser bowiem – co było zwyczajem Kubricka – samodzielnie nakręcił część ujęć). Kubrick zachowywał stoicki spokój: gdy podczas realizacji jednej ze scen we wnętrzu poprosił Metty’ego o zmianę światła, mówiąc, że nie widzi twarzy postaci, bo ich oświetlenie jest zbyt słabe, zdenerwowany operator kopnął jedną z lamp tak, że wylądowała na planie tuż przy postaciach, na co reżyser grzecznie poprosił o poprawienie światła, bo teraz z kolei twarze aktorów są zbyt mocno oświetlone. Współpraca tak dalece wykończyła nerwowo Metty’ego, że w pewnym momencie operator opuścił plan, oświadczając, że nie jest w stanie pracować z Kubrickiem; zgodził się kontynuować pracę dopiero po długiej rozmowie z Douglasem. (Ostatecznie tortury współpracy z autokratycznym, perfekcjonistycznym reżyserem się opłaciły: Russell Metty otrzymał za Spartakusa Oscara za najlepsze zdjęcia.) Przez cały czas pobytu na planie Kubrick chodził w jednym garniturze, którego nie czyścił; gdy zaczęło to przeszkadzać ekipie, zwróciła się do Kirka Douglasa, który przeprowadził rozmowę z reżyserem; Kubrick kupił wtedy nowy garnitur, który traktował identycznie jak poprzedni.
Kubrick był całkiem zadowolony z końcowego rezultatu swojej pracy (znów miał okazję zająć się jednym ze swoich stałych tematów: wojną, czy szerzej – fenomenem zinstytucjonalizowanego zabijania, jakiego ofiarą padają szykowani do krwawej walki na arenie gladiatorzy; z jego poglądami i historiozofią zgadzał się również przedstawiony w filmie obraz Spartakusa – wrażliwego, humanitarnego człowieka – który ponosi klęskę dlatego, że okazał ludzkie strony swojego charakteru; jego człowieczeństwo przegrywa z odhumanizowaną, zimną maszyną do zabijania, jaką jest rzymska armia, a Spartakus kończy życie w męczeński, upokarzający sposób – na krzyżu), jednak drażnił go fakt, że nie mógł wpływać na scenariusz, przez co Spartakusa oceniał jako film zbyt uproszczony i moralizatorski, nie podobało mu się również przedstawienie głównego bohatera jako jednostki pozbawionej wad i słabości, o co nieustannie kłócił się na planie z Douglasem (był to drugi i ostatni ich wspólny film). Po pewnym czasie Kubrick zaostrzył swoje stanowisko wobec Spartakusa, wyrzekając się tego filmu. Była to ostatnia praca na podstawie cudzego pomysłu i scenariusza, jakiej się w karierze podjął; od tej pory realizował wyłącznie autorskie pomysły i scenariusze.
Film okazał się dużym przebojem kasowym; oprócz Oscara za zdjęcia otrzymał też Nagrody Akademii za męską rolę drugoplanową (Peter Ustinov – Lentulus Batiatus, handlarz niewolnikami), scenografię i kostiumy. Spartakus przyczynił się również do ostatecznego zaniku tzw. black list – czarnej listy twórców filmowych, posądzanych o sympatie prokomunistyczne, którzy oficjalnie nie mogli pracować nad filmami, a jeśli to robili, musieli występować pod pseudonimami, lub ich pracę przypisywano innym osobom (Carl Wilson i Michael Thomas, scenarzyści filmu Most na rzece Kwai, nie mogli figurować w czołówce; jako scenarzysta został wymieniony Pierre Boulle, autor książki będącej podstawą filmu, on też odebrał Oscara za scenariusz). Scenarzystą Spartakusa był Dalton Trumbo, figurujący na czarnej liście, a ponieważ oficjalnie nie można było go wymienić, Kubrick zażądał, by to jego nazwisko firmowało scenariusz w czołówce filmu. To żądanie tak rozsierdziło Douglasa, że aktor autorytarnie zażądał, by jako scenarzysta został wymieniony Trumbo, i tak się stało. Był to pierwszy przypadek, że twórca oskarżany (w tym przypadku słusznie) o poglądy komunistyczne został mimo to oficjalnie usankcjonowany jako współtwórca wysokobudżetowego filmu hollywoodzkiego.
Po ukończeniu prac nad Spartakusem, w roku 1960, Kubrick rozpoczął prace nad kolejnym, tym razem w pełni autorskim projektem. Postanowił zaadaptować słynną powieść Vladimira Nabokova Lolita, której ukazanie się wywołało spory skandal. Z Nabokovem Kubrick szybko znalazł wspólny język – obaj byli znakomitymi, zapalonymi szachistami. Pierwszą wersję scenariusza napisał sam Nabokov; ponieważ w swej ukończonej postaci miał on około 400 stron (z reguły 1 strona scenariusza przekłada się na mniej więcej 1 minutę ukończonego filmu), otrzymany scenariusz dość istotnie przerobił sam reżyser wraz z Harrisem (choć w czołówce jako scenarzysta widnieje wyłącznie Nabokov).
Dobór aktorów okazał się trudnym zadaniem: znalezienie nastolatki do tytułowej roli trwało długo. Główną kandydatką była Hayley Mills; jej kandydaturę zablokował jednak jej ojciec John Mills pod naciskiem Walta Disneya, w którego filmach fabularnych Mills występowała; po obejrzeniu prawie 800 dziewcząt, w końcu Kubrick wybrał Sue Lyon. Długo szukał też aktora do roli profesora Humberta: głównym kandydatem był James Mason, jednak w owym czasie był on zaangażowany przez jeden z londyńskich teatrów i nie mógł wystąpić. Kubrick szukał dalej; po długich i bezowocnych poszukiwaniach (wielu znanych aktorów – m.in. Laurence Olivier, Cary Grant i David Niven – odmówiło wystąpienia w adaptacji książki Nabokova, obawiając się, że udział w produkcji opartej na tak skandalicznej książce zniszczy ich karierę) sztuka, w której grał w Londynie Mason, okazała się klapą i aktor był dyspozycyjny, co Kubrick natychmiast wykorzystał. Rolę tajemniczego nemezis Humberta, pisarza Clare’a Quilty’ego, otrzymał brytyjski aktor Peter Sellers; aktorska wszechstronność Sellersa zapadła w pamięć Kubricka, który postanowił, że jeszcze z aktorem będzie współpracował.
Reżyser szybko przekonał się, że zrealizowanie filmu, którego głównym bohaterem jest dojrzały mężczyzna pozostający w namiętnym związku z kilkunastoletnią dziewczyną, w Ameryce początku lat 60. będzie niezwykle trudne; książkowa Lolita/Dolores Haze miała 12 lat – Kubrick zmienił ją na 14-latkę, jednak producenci z góry zapowiedzieli, że mimo tego gotowy film może nie zostać dopuszczony do rozpowszechnienia przez komisję nadającą tzw. Code Seal – swoistą pieczęć, dającą filmowi przepustkę na ekrany w Stanach Zjednoczonych (choć pochodzący z roku 1956 film Baby Doll Elii Kazana, opowiadający historię swobodnej obyczajowo kilkunastolatki, taką zgodę dostał); ostatecznie reżyser był zmuszony podnieść wiek filmowej Lolity do 18 lat, co jednak i tak nie uchroniło go przez długimi sporami z producentami (w pewnym momencie rozważano włączenie do filmu sceny, w której Humbert i Haze pobierają się przed rozpoczęciem podróży po Ameryce, co uspokoiłoby cenzurę; po latach Kubrick przyznawał, że gdyby wiedział, na ile problemów natrafi, nigdy nie podjąłby się realizacji Lolity).
Ostatecznie powstał film nie oddający zmysłowej, perwersyjnej atmosfery powieści Nabokova, znacznie spłycający książkę, ale w zamian oferujący coś innego: Kubrickowi (częściowo mimowolnie) udało się nakreślić ciekawy obraz Ameryki czasu pomiędzy zakończeniem II wojny światowej a początkiem rewolucji obyczajowej lat 60., Ameryki ciągle pozostającej w ciasnym gorsecie obyczajowych ograniczeń, norm i zakazów; ciekawy obraz tego, jak ciasne, duszne było życie ograniczane takimi normami. Był to też ostatni film spółki Stanley Kubrick – James B.Harris; Harris postanowił rozpocząć samodzielną karierę reżysera i producenta.
Na przełomie lat 50. i 60. XX wieku Nowy Jork był postrzegany w Ameryce jako jeden z głównych celów radzieckiego ataku w razie wybuchu wojny nuklearnej. Mieszkający w Nowym Jorku Kubrick był żywo zainteresowany tematem konfliktu atomowego; w jego bibliotece było sporo książek na ten temat, między innymi thriller Red Alert (Czerwony alarm) Petera George’a, opowieść o szalonym generale, który na własną rękę postanawia wszcząć konflikt nuklearny pomiędzy Związkiem Radzieckim i Stanami Zjednoczonymi. Kubrick wykupił prawa do ekranizacji książki i przystąpił (z pomocą George’a) do pracy nad scenariuszem, uprzednio wnikliwie studiując ponad 50 książek poświęconych wojnie atomowej.
Adaptacja Red Alert była pierwotnie planowana jako poważny, ponury dreszczowiec; jednak w pewnym momencie Kubrick zauważył, że spora część scen, jaką stworzyli wspólnie z George’em i trzecim scenarzystą Terrym Southernem, w istocie była bardzo zabawna. Reżyser postanowił te sceny usunąć, bądź nadać im poważny wymiar; wtedy spostrzegł, że te sceny w istocie są bardzo ludzkie i bardzo prawdopodobne, jak również kluczowe dla przebiegu fabuły – postanowił więc przekształcić Red Alert w makabryczną, czarną komedię. Ostatecznie Dr Strangelove, Or How I Stopped Worrying And Love The Bomb (Dr. Strangelove, czyli jak przestałem się bać i pokochałem bombę) stał się właśnie satyryczną makabreską, przerażającą komedią o apokaliptycznym końcu świata.
Akcja filmu – jak widać przez moment w książce, gdzie radiooperator sprawdza otrzymany kod – rozgrywa się 13 września 1963 roku (jest to piątek trzynastego). Szalony generał Jack D. Ripper [dosłownie: Kuba Rozpruwacz], owładnięty wizją komunistycznego spisku, polegającego m.in. na potajemnej fluoryzacji wody w Ameryce (teorię taką lansowała w drugiej połowie lat 50. prawicowa, antykomunistyczna organizacja amerykańska – The John Birch Society), wydaje swoim załogom polecenie zrzucenia bomb atomowych na wybrane cele w Związku Radzieckim. W Białym Domu zbiera się sztab kryzysowy, z udziałem wojskowych i prezydenta USA Merkina Muffleya oraz tajemniczego naukowca niemieckiego pochodzenia, dr. Strangelove, który debatuje, co w tej sytuacji zrobić. Generał Buck Turgidson [dosłownie: Jurny Kozioł] proponuje wykorzystać sytuację i zaskoczenie Rosjan i kontynuować atak; jednak wezwany na posiedzenie sztabu rosyjski ambasador (który wywołuje poruszenie, gdyż próbuje małym aparatem fotografować Pokój Wojenny, gdzie sztab się zebrał) oświadcza, że jakikolwiek atak nuklearny na Związek Radziecki spowoduje uruchomienie nowej, tajnej broni sowieckiej – Machiny Zagłady (Doomsday Machine), która w momencie ataku włączy się automatycznie i nie będzie żadnej możliwości jej wyłączenia, a jej wybuch spowoduje utrzymujący się przez niemal sto lat opad promieniotwórczy, całkowicie niszczący życie na Ziemi. Machina Zagłady miała działać jako ostateczny środek zapobiegający atakowi na ZSRR, gdyż doprowadziłoby to do szybkiej zagłady ludzkości; jednak wiadomość o niej nie została jeszcze podana do publicznej wiadomości – miało to zostać ogłoszone na najbliższym Zjeździe Partii Komunistycznej, gdyż pierwszy sekretarz ZSRR bardzo lubi niespodzianki.
Zapada decyzja o zbrojnym ataku na bazę zarządzaną przez Rippera i przejęciu kontroli nad samolotami (od chwili wydania rozkazu są one w stanie ciszy radiowej; mogą przerwać ją tylko wtedy, jeśli wysłana do nich wiadomość będzie poprzedzona odpowiednim kodem, bo wszystkie inne wiadomości zostaną automatycznie odrzucone przez pokładowe radiostacje); choć podczas ataku generał Ripper popełnia samobójstwo – kod udaje się odnaleźć i w porę przekazać samolotom; niestety, jedna maszyna, dowodzona przez majora T.J. „King” Konga, ma uszkodzoną radiostację i kontynuuje atak, co kończy się zrzuceniem bomby (z trudnościami – ostatecznie major musi dokonać zrzutu ręcznie, co kończy się jego wypadnięciem z maszyny wraz z bombą – siedząc na niej okrakiem, zrywa z głowy kowbojski kapelusz, w którym przez cały czas paradował, i wydaje głośne okrzyki niczym kowboj na rodeo) na wyrzutnię rakiet w miejscowości Laputa, a więc w konsekwencji – uruchomieniem Maszyny. Turgidson i inni wojskowi nie przejmują się jednak specjalnie – jak sugeruje dr Strangelove (cierpi on na AHS – chorobę obcej ręki; jego prawa dłoń albo próbuje go udusić, albo podrywa się do góry w nazistowskim salucie; parokrotnie przez pomyłkę zdarza mu się zwrócić do prezydenta Muffleya mein Führer), w specjalnie przygotowanych szybach, w głębi Ziemi można do czasu całkowitej zagłady życia na Ziemi przygotować całkiem znośne warunki do życia dla grupy odpowiednio wybranych ludzi, by w momencie, gdy promieniotwórczość na opustoszałej Ziemi spadnie do akceptowalnego poziomu, odtworzyć na terenie Ameryki dawną demokrację. Pada propozycja, by wśród wybranych na jednego mężczyznę przypadało dziesięć odpowiednio dobranych kobiet – by zwiększyć możliwości prokreacyjne (oczywiście, każdy z mężczyzn będzie musiał uczestniczyć w tym obowiązku – trzeba się poświęcić dla dobra ludzkości). Przy akompaniamencie piosenki We’ll Meet Again Very Lynn świat – pośród potężnych detonacji nuklearnych – przestaje istnieć.
Kubrick postanowił ponownie zrealizować film w Europie; wybór padł na angielskie studia. Spora część akcji filmu rozgrywała się na pokładzie bombowca B-52; był on wtedy czarnym koniem amerykańskiej armii, nową, superpotężną bronią, której projekt był ściśle tajny. Kubrick i scenograf Ken Adam (w przyszłości twórca scenografii do licznych filmów bondowskich) odtworzyli z najdrobniejszymi detalami wnętrze Superfortecy, posiłkując się jednym jedynym opublikowanym zdjęciem wnętrza maszyny, ogólnym zarysem samolotu oraz powszechnie dostępnymi danymi starszego brata B-52 – bombowca B-29. Kubrick polecił Adamowi skrzętnie przechowywać wszelkie dane, na jakich się opierali; co okazało się ze wszech miar uzasadnione, gdyż zbudowana przez filmowców kopia B-52 okazała się wręcz idealną kopią prawdziwej maszyny, do tego stopnia, że funkcjonariusze CIA złożyli Adamowi wizytę, gdyż podejrzewali, że w nielegalny sposób wszedł on w posiadanie prawdziwych, ściśle tajnych planów B-52.
Sporo trudności nastręczył sam projekt Pokoju Wojennego; ogromna mapa świata na ścianie była olbrzymim rysunkiem na celuloidzie odpowiednio podświetlanym od tyłu przez ogromne lampy. Lampy te miały taką moc, że w pewnym momencie celuloid zaczął się topić; wtedy zainstalowano specjalny system chłodzący. Choć na ekranie tego nie widać (film jest czarno-biały – po raz ostatni w karierze Kubricka), stół, przy którym obradują politycy, jest pokryty zielonym suknem, jak stół do pokera w kasynie; sugestia, że oto politycy rozgrywają partię pokera, decydującą o losach świata. Do zaopatrzenia w prąd tej dekoracji zużyto łącznie 16 kilometrów przewodów elektrycznych.
Do roli generała Turgidsona Kubrick wybrał George’a C.Scotta; aktor był znany z wybuchowego charakteru i niezbyt chętnej współpracy z reżyserami, Kubrick utrzymał go jednak w ryzach w niezwykle prosty sposób: wiedząc, że Scott jest świetnym szachistą, na początku zdjęć wyzwał go na szereg partii szachów – i wszystkie wygrał, co wzbudziło taki podziw Scotta, że ten bez wahania spełniał wszystkie polecenia reżysera. Później jednak wypowiadał się o Kubricku z niechęcią: powierzoną sobie rolę Scott starał się zagrać poważnie, Kubrick namawiał go jednak, by, na próbę, zagrał poszczególne sceny w przerysowany, komiczny sposób – i właśnie te sceny włączył potem do filmu. Podobnie postępował z amerykańskim aktorem Slimem Pickensem, grającym rolę majora T.J. „King” Konga, dowódcy samolotu B-52; Kubrick do samego końca nie powiedział Pickensowi, że film ma być czarną komedią, utrzymywał go w przekonaniu, że jest to poważny dramat, dzięki czemu Pickens zagrał lotnika w bardzo poważny sposób, co dało niezwykle komiczne rezultaty. Rolę szalonego generała Rippera zagrał Sterling Hayden, znany z The Killing. Kubrick ponownie skorzystał również z usług Petera Sellersa; zagrał on w filmie trzy role (za każdym razem posługując się innym akcentem), tytułową, majora RAF Lionela Mandrake’a i prezydenta USA Merkina Muffleya. Tę ostatnią rolę Sellers grał początkowo w przerysowany sposób, posługując się sfeminizowanym, wysokim głosem i sfeminizowanymi gestami, Kubrick przekonał go jednak, by zagrał tę rolę na poważnie, czyniąc z prezydenta Muffleya oazę spokoju i rozsądku wśród szalonych naukowców i wojskowych. Popisy Sellersa na planie do tego stopnia przypadły reżyserowi do gustu, że zezwolił on aktorowi na improwizowanie swoich kwestii przed kamerą – co w przypadku perfekcjonisty Kubricka, wymagającego zawsze od aktorów i ekipy ścisłego trzymania się swoich wytycznych, było czymś zupełnie wyjątkowym. Gaża Sellersa – 1 milion dolarów – pochłonęła ostatecznie 55% budżetu filmu.
Oryginalnie, film miał się kończyć wielką bójką na torty i inne kulinarne przysmaki, w duchu najlepszych burlesek kina niemego (stąd stojący w Pokoju Wojennym wielki stół zastawiony najróżniejszymi smakołykami). Nakręcono stosowną scenę, jednak Kubrick podjął decyzję o nie włączaniu jej do filmu, gdyż jego zdaniem była zbyt farsowna. Decyzję tę przypieczętował zamach na prezydenta Kennedy’ego; w filmie prezydent Muffley pada po otrzymaniu ciosu tortem w twarz, na co generał Turgidson oświadcza: Panowie! Nasz dzielny młody prezydent właśnie poległ w chwale! (Pierwotnie premierę filmu wyznaczono właśnie na dzień wizyty Kennedy’ego w Dallas – 22 listopada 1963. Ostatecznie film na ekrany kin wszedł 23 stycznia 1964).
Choć początkowo film nie spotkał się ze zbyt przychylnym przyjęciem (po pierwszych pokazach uznano go za nie nadający się do prezentacji, za hańbę dla firmującej go wytwórni Columbia Pictures), szybko doceniono jego makabryczny, czarny humor. Na inspirację filmem, zwłaszcza sceną, gdy generał Turgidson doradza prezydentowi kontynuowanie ataku i rozpoczęcie wojny nuklearnej, gdyż, jak mówi: Panie Prezydencie, nie twierdzę, że nie oberwiemy po tyłku, ale szacunki wskazują, że stracimy jedynie 20 milionów obywateli; 30 milionów, w najgorszym wypadku! powoływał się Oliver Stone. To był pierwszy raz, gdy ktoś w filmie pokazał w taki sposób amerykańskich wojskowych i rząd; rząd, obojętny na losy swoich obywateli, rząd wrogi swoim obywatelom. To była niezwykle zapładniająca wizja. Choć w Białym Domu w istocie nie ma Pokoju Wojennego, jego obraz tak dalece zapadł w pamięć widzów, że prezydent Ronald Reagan, gdy po raz pierwszy oprowadzano go po Białym Domu, poprosił, by tenże Pokój Wojenny mu pokazano. W latach 90. Dr Strangelove trafił do Narodowej Biblioteki Kongresu USA, jako obraz o szczególnych wartościach artystycznych.
Film wzbudził również zainteresowanie CIA i Pentagonu; chodziło o łatwość, z jaką filmowcy perfekcyjnie odtworzyli ściśle tajny projekt samolotu opierając się na szczątkowych, powszechnie dostępnych danych, jak również o scenę, w której Mandrake próbuje dodzwonić się do Białego Domu, by przekazać ściśle tajny kod odwołujący natarcie nuklearne, ale nie ma drobnych do automatu telefonicznego, zaś podległy mu żołnierz odmawia odstrzelenia drzwi do automatu z Coca-Colą, w którym znajdują się drobne, gdyż jest to własność prywatna; wojskowi szczegółowo sprawdzali, czy rzeczywiście może zaistnieć sytuacja, w której superważna wiadomość nie dotrze na czas z tak błahych przyczyn jak brak drobnych do automatu telefonicznego. Wątek ten był też kolejnym w dorobku Kubricka studium wpływu absurdalnego przypadku na ludzkie losy.
Po zakończeniu realizacji filmu, Kubrickowie postanowili pozostać na stałe w Wielkiej Brytanii, gdzie nastrój był dalece odmienny od przepełniającej Stany Zjednoczone pierwszej połowy lat 60. nuklearnej paranoi. Jak wspominała Christiane Kubrick, w nowojorskim radiu dominowały informacje o zaopatrzeniu schronów przeciwnuklearnych, zachowaniu na wypadek ataku atomowego i sposobów jego ogłaszania, natomiast gdy przybyli do Wielkiej Brytanii, pierwszą rzeczą jaką usłyszeli w radio była porada, jakim rodzajem nawozu azotowego użyźniać glebę przy hodowli ozdobnych gatunków traw. Kubrickowie nabyli niewielką posiadłość w miejscowości Abbott’s Mead; w roku 1978 przeprowadzili się do Childwicksbury Manor w miejscowości Harpenden (ok. 40 km od Londynu), gdzie reżyser mieszkał do końca życia.
Po ukończeniu Dr.Strangelove zainteresowanie Kubricka zwróciło się w kierunku kina science fiction. Reżyser postanowił stworzyć film fantastyczno-naukowy, jakiego do tej pory nie widziano; obraz łączący realistyczne przedstawienie realiów kosmicznej podróży z filozoficzną podstawą.
Po obejrzeniu dziesiątków różnych filmów science-fiction i przeczytaniu wielkiej ilości opowiadań, noweli i książek popularnonaukowych, Kubrick wybrał niewielkie objętościowo opowiadanie Arthura C.Clarke’a The Sentinel (Strażnik), opowiadające o tajemniczym, pozaziemskim bycie, nadzorującym rozwój cywilizacji na Ziemi. Reżyser zaprosił Clarke’a do Londynu i wspólnie rozpoczęli pracę nad scenariuszem przyszłego filmu.
Gdy scenariusz był już ukończony – Kubrick rozpoczął w londyńskich studiach zdjęcia. Trwały one, co jak na ówczesne kino było czasem rekordowo długim, prawie trzy lata (jak głosi anegdota, jeden z szefów produkującej film wytwórni Metro-Goldwyn-Mayer w pewnym momencie spytał Kubricka, czy pojawiający się w tytule rok 2001 nie ma być przypadkiem rokiem premiery filmu); przyczynił się do tego znów perfekcjonizm reżysera, powtarzającego w nieskończoność nawet pozornie proste ujęcia, jak również spore techniczne trudności. Otwierającą film sekwencję The Dawn Of Man (Narodziny człowieka) filmowano na dużym placu prawie w centrum Londynu; kamera mogła wznieść się na ściśle określoną wysokość, powyżej której w polu jej widzenia znalazłyby się słynne londyńskie piętrowe czerwone autobusy. Ogromne trudności sprawiał najbardziej symboliczny obiekt w filmie: ogromny, czarny, prostopadłościenny monolit – musiał być on zrobiony z odpowiednio połyskliwego materiału, a do tego przy jego ustawianiu na planie ekipa musiała bardzo uważać, aby nie pozostawić na nim odcisku dłoni. Duże trudności sprawiła realizacja pozornie prostej sekwencji, w której astronauta Dave Bowman ćwiczy fizycznie na pokładzie statku Discovery, biegając wokół jego wnętrza po jego ścianach: dla potrzeb tej sceny zbudowano olbrzymie koło, wewnątrz którego umiejscowiono dekorację wnętrza statku: ogromne koło kręciło się, wprawiane z zewnątrz w ruch specjalnym silnikiem; grający Bowmana Keir Dullea biegał wewnątrz po obracającym się ze stałą prędkością ruchomym chodniku, ciągle znajdując się na samym dole koła, natomiast część dekoracji z inną prędkością obracała się wraz z kamerą wokół niego, co w końcowym efekcie dało oczekiwany przez Kubricka efekt, jakby to Bowman biegał po korytarzu i ścianach wokół wnętrza stacji. (Konstrukcja ta była uważana za niezwykle niebezpieczną: cały personel obsługujący wielkie koło był zobowiązany nosić przez cały czas kaski ochronne.) Sporo trudności nastręczyła też symulacja stanu nieważkości: w jednej z sekwencji kosmiczna stewardesa łapie dryfujące swobodnie w powietrzu wieczne pióro, zgubione przez jednego z pasażerów udającego się na Księżyc wahadłowca – ta prosta z pozoru scena okazała się bardzo trudna do sfilmowania, gdyż przez długi czas nikomu nie udało się wymyślić, jak sensownie umocować w pustej przestrzeni pióro, by stworzyć iluzję jego swobodnego unoszenia się – wszelkie druty i sznurki zarzucono, gdyż mimo różnych sztuczek były one zawsze widoczne na ekranie. Ostatecznie Kubrick wpadł na pomysł, by pióro to przymocować do przezroczystej płytki pleksiglasowej; uważne oko wypatrzy na ekranie lekki refleks światła na płytce.
Clarke i Kubrick zdecydowali się na tytuł 2001: Odyseja Kosmiczna; stwierdzili bowiem, że dla starożytnych Greków bezkres mórz stanowił taką samą zagadkę, co dla ludzi połowy lat 60. bezkresna czerń Kosmosu. W swej ostatecznej postaci film stanowił intrygujący, filozoficzny wykład o dziejach ludzkości, o niezmiennej, mimo technologicznego sztafażu, zwierzęcej naturze człowieka (surowe mięso, jakim żywią się przodkowie ludzi w otwierającej film sekwencji, wygląda równie apetycznie, co bezbarwna papka, jaką w Kosmosie żywią się astronauci; pierwszym narzędziem, jakie małpa, praprzodek Człowieka, wynajduje pod wpływem nagle pojawiającego się czarnego, prostopadłościennego monolitu, jest duża kość, za pomocą której można innej małpie roztrzaskać czaszkę), stanowił wizję intrygującego przeciwieństwa – zdehumanizowani Ludzie kontra uczłowieczona, odczuwająca Maszyna. Śmierć astronauty Poole’a – mimo że wcześniej widzowie są świadkami rodzinnego momentu, jakim jest złożenie Poole'owi życzeń urodzinowych przez jego rodziców poprzez wideotelefon– nie budzi u oglądających większych uczuć, podobnie jak śmierć zahibernowanych astronautów na pokładzie Discovery; natomiast „śmierć” wyłączanego HALa 9000 jest dla widza poruszająca; podobnie jak jego błędy – swoisty przejaw człowieczeństwa maszyny – budzą u widzów współczucie.
Film dzieli się na kilka części: pierwszą z nich jest Świt Człowieka – historia plemienia małp z afrykańskiej równiny, rywalizującego z innym plemieniem o dostęp do wodopoju. Pewnej nocy, nagle, przy siedzibie owych małp pojawia się tajemniczy czarny monolit, wzbudzając w małpach widoczne poruszenie; wkrótce potem, jedna z małp, grzebiąc przy leżącym na równinie szkielecie, doznaje olśnienia i wynajduje pierwsze narzędzie – maczugę, której wkrótce używa przy konflikcie o wodopój, rozbijając małpie z konkurencyjnego plemienia głowę; w ekstazie triumfu, małpa wyrzuca kość w niebo.
Spadająca kość nagle zmienia się w szybujący w kosmosie statek kosmiczny: przenosimy się o dziesiątki tysięcy lat naprzód, do świata regularnych podróży kosmicznych, w którym statki kosmiczne, poruszając się w kosmosie tańczą istny taniec w pustej przestrzeni (podkładem dźwiękowym tej sekwencji jest walc Nad pięknym modrym Dunajem Johanna Straussa). Jak się okazuje, naukowcy amerykańscy, badając Księżyc, odnaleźli pod jego powierzchnią nietypowy przedmiot – wielki, połyskujący, czarny prostopadłościenny monolit. Gdy przystępują do badań – monolit nagle emituje bardzo silny impuls radiowy. (W swojej książce 2001: Odyseja Kosmiczna Clarke wyjaśnia, że monolity na swój sposób śledzą rozwój cywilizacji: odkrycie monolitu pod powierzchnią Księżyca było dowodem, że cywilizacja osiągnęła etap umożliwiający jej opuszczenie planetarnej kolebki i rozpoczęcie kolonizacji innych planet.)
Kolejna sekwencja filmu poświęcona jest podróży statku Discovery w stronę Jowisza: grupę astronautów widzowie obserwują w czasie zwykłych, codziennych czynności kolejnej, rutynowej misji. Rutyna znika, gdy zarządzający statkiem superinteligentny komputer HAL 9000 (nazwa pochodzi od mechanizmu jego działania: Heurystyczny ALgorytmiczny; jak zawsze twierdzili Kubrick i Clarke, zupełnym przypadkiem było, że trzy kolejne litery w alfabecie to odpowiednio I, B, M) zaczyna wykazywać oznaki uszkodzenia, błędnie wykazując uszkodzenie pewnych elementów statku, choć ich badanie wskazuje, że są w pełni sprawne. (Uważny widz dostrzeże już wcześniej pewne oznaki błędnego funkcjonowania HALa: w jednej ze scen komputer rozgrywa z Bowmanem partię szachów. W pewnym momencie, po wykonaniu kolejnego posunięcia, HAL 9000 przedstawia Bowmanowi dalszą część partii, prowadzącą do mata w dwóch ruchach – jednak jedno z posunięć, jakie HAL wymienia, jest w tym konkretnym układzie figur posunięciem niedozwolonym i HAL może dać mata nie w dwóch, ale w trzech ruchach. Stanley Kubrick, jako znakomity i doświadczony szachista, nie mógłby popełnić takiego błędu – prawdopodobnie jest to dyskretna sugestia, że HAL 9000 nie funkcjonuje prawidłowo.) Astronauci Bowman i Frank Poole po naradzie postanawiają czasowo odłączyć komputer; HAL jednak odczytuje ich plany (nie może słyszeć, o czym mówią ukryci w dźwiękoszczelnej kapsule ratunkowej astronauci, ale potrafi czytać z ruchu ich warg), co kończy się śmiercią Poole’a pracującego poza statkiem i kilku przebywających w stanie hibernacji astronautów, zaś również przebywający poza Discovery Bowman jest zmuszony dostać się na statek w ryzykowny sposób, wymagający od niego przebywania przez kilkadziesiąt sekund w otwartym kosmosie bez kombinezonu próżniowego. Ryzykowna operacja udaje się i Bowman odłącza HALa; odczytuje wtedy zapisany w jego pamięci przekaz wideo, informujący o odkryciu monolitu na Księżycu i wysłanym przez niego przekazie radiowym w stronę Jowisza; właśnie znaczenie tego sygnału ma zbadać załoga Discovery, co miało zostać im powiedziane po dotarciu w okolice wielkiej planety.
W finałowej sekwencji filmu Discovery dociera w okolice Jowisza, gdzie w przestrzeni dryfuje ogromny czarny monolit. Bowman w małym statku wyrusza w jego stronę, przekraczając w pewnym momencie tajemnicze wrota, by po podróży przez niezwykłe, fantasmagoryczne pejzaże dotrzeć w końcu do małego pokoju w stylu wiktoriańskim, gdzie Bowman gwałtownie starzeje się i umiera, by odrodzić się jako embrion – Gwiezdne Dziecko.
Cechą charakterystyczną Odysei była olbrzymia dbałość o wierne oddanie realiów kosmicznej podróży: stacja kosmiczna wiruje wokół swojej osi z taką prędkością, by wytworzyć sztuczną grawitację równą ziemskiej, w otwartej przestrzeni kosmicznej nie ma dźwięku. Kubrick nie uniknął pewnych błędów, chwilami spowodowanych bardziej ograniczeniami technicznymi niż brakiem wiedzy: gdy kosmiczny lądownik osiada na powierzchni Księżyca, wzbudzony przez to księżycowy pył opada na powierzchnię z „ziemską” prędkością, choć na Ziemi grawitacja jest sześciokrotnie większa od księżycowej. Kubrick dobrze zdawał sobie sprawę z tego błędu, ale nie był w stanie go uniknąć technicznie. Podobnie Dave Bowman, próbując powrócić do Discovery bez skafandra próżniowego, co oznacza przebywanie przez pewien czas w otwartej próżni kosmosu (co wbrew obiegowym opiniom jest jak najbardziej przeżywalne dla człowieka – w badaniach małpy przeżywały w otwartym kosmosie 80-90 sekund, a po okresie przebywania w próżni około 60 sekund i ściągnięciu na powrót do statku kosmicznego zachowywały się dokładnie jak przed eksperymentem, nie wykazując żadnych ubytków fizycznych ani umysłowych), tuż przed zetknięciem się z próżnią kosmosu nabiera powietrza w płuca; byłoby to fatalne, gdyż spowodowałoby rozerwanie płuc, astronauta powinien raczej zrobić możliwie jak najpełniejszy wydech.
Sporo trudności nastręczyło stworzenie muzyki do filmu; Stanley Kubrick początkowo zwrócił się do znanego kompozytora muzyki filmowej, Alexa Northa; aby pomóc mu uzyskać odpowiedni nastrój, Kubrick stworzył zestaw znanych nagrań muzyki poważnej (m.in. Nad pięknym modrym Dunajem Johanna Straussa, Tako rzecze Zaratustra Richarda Straussa, Suita baletowa Gajane Arama Chaczaturiana) oraz współczesnej muzyki awangardowej (Requiem na sopran, mezzosopran, dwa mieszane chóry i orkiestrę, Przygody i Światło wiekuiste Györgya Ligetiego). North skomponował sporo muzyki, z której Kubrick wybrał część do użytku w filmie; jednak ostatecznie zdecydował się zrezygnować z muzyki Northa i wykorzystać właśnie zestawiony przez siebie wybór nagrań. Reżyser nie poinformował kompozytora o swojej decyzji; North dowiedział się o wszystkim, oglądając gotowy film, co było dla niego gorzkim rozczarowaniem. Nie był to koniec problemów ze ścieżką dźwiękową; jedną z wykorzystanych kompozycji Ligetiego – Przygody – Kubrick zmodyfikował dla potrzeb filmu, nie pytając o konieczną zgodę kompozytora, który pozwał reżysera do sądu i wywalczył spore odszkodowanie.
W pierwszej chwili film, którego premiera miała miejsce 9 kwietnia 1968, przyjęto z mieszanymi uczuciami (szczególnie miażdżąco skrytykowała film papieżyca amerykańskiej krytyki filmowej, Pauline Kael); widowni nie do końca podobała się otwarta forma filmu, pozwalająca każdemu widzowi na samodzielną, indywidualną interpretację przedstawionej fabuły. Szybko jednak doceniono nowatorstwo Kubricka, mnogość ukrytych w fabule filmu odniesień kulturalnych, filozoficznych i religijnych (układ planet w czołówce filmu tworzy symbol zoroastryjski; w finale Bowman, umierając, wypuszcza z ręki szklankę, która się tłucze – scenę tę kojarzono z żydowską ceremonią zaślubin, gdzie rozbijane szklane naczynie symbolizuje przejście z jednego życia do innego); dziś film uchodzi za jeden z najważniejszych obrazów w historii kina. Właśnie za 2001: Odyseję Kosmiczną Stanley Kubrick otrzymał jedynego w karierze Oscara – za opracowanie efektów wizualnych.
Bezpośrednio po ukończeniu prac nad 2001, Kubrick przystąpił do pracy nad filmem, który uważał za dzieło swego życia – biografię Napoleona I. Ściągnął ze Stanów Zjednoczonych i Europy setki różnych książek poświęconych Cesarzowi, nawiązał również współpracę ze słynnym badaczem dziejów Napoleona, profesorem Felixem Markhamem.
Główną rolę w filmie miał zagrać Jack Nicholson. Przygotowanie Kubricka do pracy nad scenariuszem było czymś niespotykanym w świecie kina: współpracownicy reżysera wspominają ogromną szafę, podzieloną na setki szuflad, w której szczegółowe informacje o życiu Napoleona były pogrupowane z podziałem na pojedyncze dni – tak że Kubrick mógł w każdej chwili sprawdzić, co Cesarz robił np. 12 września 1808. Współpracownicy Kubricka nawiązali współpracę z rządem rumuńskim, odnaleźli na terenie Rumunii odpowiednie plenery, zapewnili sobie wynajęcie tysięcy żołnierzy armii rumuńskiej jako statystów w scenach bitewnych (rząd Rumunii, aby móc dostarczyć potrzebną liczbę żołnierzy, zaplanował dodatkowy przymusowy pobór 8000 poborowych); Kubrick i jego współpracownicy rozmawiali również z lekarzami i firmami farmaceutycznymi, by zapewnić brytyjskiej części ekipy odpowiednie szczepionki i leki przed wyprawą na południe Europy. Szczegółowo zaplanowano również stronę produkcyjną filmu: aby zaoszczędzić na kosztach sporządzania dziesiątków tysięcy mundurów dla statystów, Kubrick wpadł na pomysł, by statyści widoczni w dalszym planie nosili specjalnie przygotowane kostiumy z papieru – dużo tańsze i szybsze w przygotowaniu niż zwykłe kostiumy, a na ekranie nieodróżnialne.
O tym, że pracę nad Napoleonem bezterminowo zawieszono, zadecydował przypadek. W tym czasie wszedł na ekrany film Siergieja Bondarczuka Waterloo, opowiadający historię legendarnej bitwy. Film ten mimo bardzo dobrej obsady (m.in. Rod Steiger jako Napoleon) okazał się klęską kasową, co spowodowało, że producenci Napoleona wycofali swoje fundusze w obawie przed kolejną klęską. Do końca życia Kubrick kilkukrotnie próbował wrócić do projektu, jednak bez powodzenia. Obecnie do wyreżyserowania Napoleona przymierza się rosyjski reżyser Aleksandr Sokurow; producencką pieczę nad tym projektem objął Martin Scorsese.
Po zawieszeniu prac nad Napoleonem, Kubrick rozglądał się za kolejnym projektem. Zdecydował się na ekranizację podarowanej mu kiedyś przez znajomego, pochodzącej z roku 1962 powieści Anthony’ego Burgessa Mechaniczna pomarańcza [taki tytuł powszechnie przyjął się w Polsce, choć lepszym tłumaczeniem oryginalnego tytułu A Clockwork Orange byłoby Sprężynowa pomarańcza lub Nakręcana pomarańcza, które to tytuły funkcjonują w niektórych polskich tłumaczeniach książki Burgessa].
Głównym bohaterem antyutopijnej powieści, rozgrywającej się w bliżej nieokreślonej przyszłości w Wielkiej Brytanii, jest nastolatek, Alex, wielki miłośnik Beethovena, który wraz z grupą podobnych sobie nastolatków (określa ich mianem droogów – slang, jakim się posługują, to swoista mieszanina języków angielskiego i rosyjskiego) dopuszcza się różnych aktów przemocy, m.in. zgwałcenia dwóch dziesięciolatek, ciężkiego pobicia znanego pisarza i brutalnego gwałtu na jego żonie. Jednak w pewnym momencie szczęście Alexa się kończy: jeden z napadów okazuje się zaplanowaną przez niechętnych przywództwu Alexa kompanów pułapką i chłopak trafia do więzienia, zwłaszcza gdy okazuje się, że ofiara napadu – Catlady – w jego efekcie zmarła.
Po pewnym czasie Alex dostaje szansę opuszczenia więzienia za cenę wzięcia udziału w nowatorskim eksperymencie, mającym człowieka pozbawić możliwości czynienia zła. Eksperyment polega na tym, że więźnia, będącego pod wpływem leków, zmusza się do oglądania pełnych przemocy scen (specjalne urządzenie uniemożliwia mu zamknięcie oczu), co powoduje wpojoną niechęć do przemocy.
Zresocjalizowany Alex wychodzi na wolność, ale to dopiero początek jego kłopotów. Po niespodziewanym powrocie do domu bynajmniej nie zostaje przyjęty przez rodziców z otwartymi ramionami; zostaje zaatakowany przez grupę swoich byłych ofiar, a wstręt do przemocy nie pozwala mu się bronić; przybyli na miejsce policjanci okazują się dawnymi kompanami Alexa, wywożą go do lasu i brutalnie katują. Ciężko pobity Alex trafia do domu, którego gospodarzem okazuje się niegdyś skatowany przez niego pisarz. Ten (poruszający się po napaści na wózku inwalidzkim; jego żona w następstwie gwałtu zmarła) w pierwszej chwili go nie rozpoznaje (Alex i jego kompani podczas swoich nocnych eskapad nosili wymyślne maski), jednak Alex przez swoje nieostrożne postępowanie zdradza swoją tożsamość. Oszalały z bólu po utracie ukochanej żony pisarz postanawia się zemścić. Upija Alexa winem zmieszanym ze środkami nasennymi, zamyka go na strychu domu i zaczyna odtwarzać na pełny regulator Beethovena (Alex zdradził się, że ubocznym efektem terapii jest wstręt do muzyki tego kompozytora – jego utwory towarzyszyły brutalnym filmom, jakie musiał w czasie eksperymentu oglądać). Cierpiący Alex nie potrafi znieść fizycznych mąk, jakie powoduje w nim muzyka Beethovena, i w rozpaczy rzuca się z okna.
Gdy odzyskuje przytomność w szpitalu, okazuje się, że pisarz został aresztowany, a eksperyment, jakiemu poddano Alexa, spotkał się z potępieniem, co prowadzi do zmian w rządzie. Alex odzyskuje zdolność do czynienia zła i słuchania Beethovena; po pewnym czasie decyduje się jednak porzucić dawny styl życia i ustatkować się.
Do roli Alexa Kubrick wybrał Malcolma McDowella – wtedy po sukcesie filmu Lindsaya Andersona If… (Jeżeli…) (1968). Rolę pisarza zagrał Patrick Magee, w rolę jego żony – po rezygnacji pierwotnie wybranej aktorki, która nie wytrzymała kręcenia przez cały dzień sceny brutalnego gwałtu – wcieliła się Adrienne Corri. Do roli opiekuna poruszającego się na wózku pisarza Kubrick wybrał kulturystę; odgrywający tę rolę David Prowse kilka lat później odtwarzał fizyczną postać Lorda Dartha Vadera (podkładający głos Vadera czarnoskóry aktor James Earl Jones również wystąpił w epizodzie w filmie Kubricka – zagrał członka załogi bombowca w Dr. Strangelove).
Kubrick zdecydował się napisać scenariusz na podstawie amerykańskiej edycji powieści, okrojonej o ostatnią część, w której Alex postanawia się ustatkować. Zdjęcia zajęły pół roku; nakręcono je głównie w Londynie. Były one szczególnie trudne dla McDowella: w scenie eksperymentu Ludovica został przywiązany do krzesła, a jego powieki unieruchomiono specjalnymi zaciskami – aby jego oczy nie wyschły, regularnie nawilżano je solą fizjologiczną. Pewnego razu jeden z lekarzy przypadkiem zadrasnął mu rogówkę, co spowodowało bardzo duży ból; na przeraźliwy krzyk aktora Kubrick zareagował stoickim: Nie przejmuj się. Będę oszczędzał twoje drugie oko. (od czasu pracy nad Mechaniczną pomarańczą McDowell cierpi na lęk przed stosowaniem kropli do oczu.) Scenę seksu z dwoma nastolatkami (są starsze niż w powieści, a seks z nimi jest dobrowolny) nakręcono w jednym, prawie czterdziestominutowym ujęciu, które potem znacznie przyspieszono. Sporo trudności przysporzyło też nakręcenie sceny pobicia pisarza i gwałtu na jego żonie; mimo licznych powtórek, reżyser wciąż uważał, że scena jest zbyt statyczna i zwyczajna. W pewnym momencie Kubrick zapytał McDowella, czy ten umie tańczyć; gdy aktor zaprzeczył, reżyser spytał, czy umie śpiewać. Na odpowiedź twierdzącą Kubrick polecił McDowellowi podczas sceny gwałtu zaśpiewać jakąś piosenkę; Malcolm zaśpiewał jedyną piosenkę, jakiej słowa znał. Powstała w ten sposób scena, gdy podczas katowania bezbronnego, leżącego na podłodze pisarza Alex śpiewa tytułową piosenkę z filmu Singin’ In The Rain (Deszczowa piosenka), weszła do klasyki kina. Scenę gwałtu nakręcono z szeregiem pornograficznych szczegółów, które Kubrick w ostatecznym montażu wyrzucił; niewykorzystane ujęcia polecił po ukończeniu montażu zniszczyć.
Muzykę do filmu skomponował amerykański kompozytor, z wykształcenia fizyk i muzyk, Walter Carlos (obecnie, po operacji korekty płci, Wendy Carlos), który zyskał sławę pod koniec lat 60. albumami z adaptacjami muzyki poważnej, m.in. Johanna Sebastiana Bacha, nagranymi przy użyciu pierwszych syntezatorów; na potrzeby filmu stworzył szereg podobnych adaptacji muzyki Beethovena i Rossiniego (adaptacja uwertury do Wilhelma Tella w scenie seksu McDowella z nastolatkami); Kubrick wykorzystał również oryginalną muzykę obu kompozytorów oraz kilka ckliwych przebojów z przełomu lat 60. i 70. (I Wanna Marry A Lighthouse Keeper, Overture To The Sun). Kubrick planował wykorzystać w filmie również fragmenty suity Atom Heart Mother grupy Pink Floyd, ponieważ jednak zamierzał dość istotnie zmodyfikować te fragmenty, lider zespołu, Roger Waters, nie wyraził zgody. Zemściło się to na nim dwadzieścia lat później: w utworze Perfect Sense Part I, pochodzącym z wydanej we wrześniu 1992 płyty Amused to Death, Waters pragnął wykorzystać zsamplowany głos HALa 9000 z 2001: Odysei Kosmicznej, Kubrick jednak nie wyraził zgody, uzasadniając to tym, że w ten sposób stworzyłby precedens, prowadzący do niezliczonych próśb o zgodę na wykorzystanie fragmentów ścieżek dźwiękowych swoich filmów – choć parę lat wcześniej hiphopowy zespół The 2 Live Crew, pragnący w nagraniu Me So Horny z płyty As Nasty As They Wanna Be wykorzystać głos wietnamskiej prostytutki z innego filmu Kubricka, Full Metal Jacket, taką zgodę bez problemu otrzymał. Rozgoryczony Waters zamieścił na płycie nagrany od tyłu sarkastyczny komentarz do całej sytuacji – ironiczne „podziękowanie” dla reżysera. Po śmierci Kubricka wdowa po reżyserze, Christiane Kubrick, zgodziła się na użycie przez muzyka stosownych sampli i na płycie koncertowej Watersa In The Flesh – Live pochodzącej z 2000 roku głos HALa 9000 się znalazł. Sama zaś Atom Heart Mother w filmie się pojawiła: w scenie, w której Alex uwodzi dwie nastolatki w sklepie muzycznym, wyraźnie widać na jednej z półek charakterystyczną okładkę z krową.
Gdy Stanley Kubrick przedstawił ukończony film komisji klasyfikacyjnej, okazało się, że z uwagi na scenę seksu Alexa z dwiema dziewczynami film otrzyma w Stanach Zjednoczonych kategorię X – z reguły do tej kategorii przypisywano filmy pornograficzne (choć kategorię X otrzymał też nagrodzony Oscarem Midnight Cowboy (Nocny kowboj) Johna Schlesingera). Kubrick był wściekły, gdyż właśnie w celu uniknięcia tej cieszącej się złą sławą i poważnie ograniczającej dystrybucję filmu kategorii znacznie przyspieszył na ekranie stosowną scenę, jednak szef komisji klasyfikacyjnej obawiał się precedensu, gdyż każdy twórca filmu pornograficznego mógłby nieznacznie zmienić prędkość odtwarzania sceny erotycznej i domagać się przyporządkowania swego dzieła do niższej kategorii, umożliwiającej szeroką dystrybucję. Ostatecznie Mechaniczna pomarańcza dostała kategorię X.
Film wszedł na ekrany 19 grudnia 1971 i natychmiast wywołał gorące dyskusje, zarzucono Kubrickowi estetyzowanie przemocy (najbardziej brutalne sceny filmowane są w odrealniony sposób, jakby stanowiły jakiś dziwny balet, ilustruje je muzyka poważna – np. uwertura do opery Sroka złodziejka Rossiniego), epatowanie brutalnością i gwałtem. Gdy w Wielkiej Brytanii kilkoro grup młodocianych przestępców zaczęło się stylizować na bandę droogów z filmu, Kubrick podjął decyzję o wycofaniu obrazu z kin i zakazie wyświetlania, który zniesiono dopiero po jego śmierci; przypadki nielegalnego wyświetlania filmu zawsze spotykały się z gwałtowną reakcją reżysera, egzekwującego swój zakaz na drodze sądowej.
Zasadniczym motywem filmu jest jedna z fundamentalnych kwestii kina Kubricka: pytanie, czy dobro i zło można komuś narzucić; czy zło może być przez człowieka odrzucone, czy też stanowi trwałą część jego natury, którego pozbyć się nie sposób. Kubrick stawia tezę, że zło stanowi tak trwałą część ludzkiej natury, że możliwość świadomego wyboru zła jest w istocie miarą człowieczeństwa; że człowiek pozbawiony tej możliwości staje się mechanizmem, tytułową nakręcaną pomarańczą, czymś z pozoru żywym, a w istocie mechanicznym, sterowanym bez udziału swojej woli. (Oryginalny tytuł jest nieprzetłumaczalną na język polski, dwujęzyczną grą słów: orange to po angielsku pomarańcza, orang to w języku malajskim – który poliglota Burgess znał dobrze, gdyż przez wiele lat mieszkał na Malajach; scena brutalnego gwałtu na żonie pisarza ma źródło w malajskich przeżyciach Burgessa – człowiek; tak więc tytuł można w istocie przetłumaczyć jako nakręcany człowiek.) W filmie doszukiwano się również innych aluzji, m.in. do nazizmu, filozofii nietzscheańskiej, niektórzy zaś krytycy widzieli w filmie po prostu jadowity obraz Wielkiej Brytanii pod rządami socjalistów.
Film otrzymał cztery nominacje do Oscara, z czego trzy przypadły samemu Kubrickowi: za najlepszy film (jako producentowi), scenariusz adaptowany i reżyserię – w każdej z tych kategorii zwyciężył Francuski łącznik. Sam Anthony Burgess do filmu podchodził z mieszanymi uczuciami: nie podobało mu się, że dzięki filmowi, z całego jego obszernego dorobku beletrystycznego największą sławę zyskała powieść, którą sam uważał za niezbyt warte uwagi dzieło poboczne, sprzeciwiał się decyzji Kubricka, by scenariusz oprzeć na amerykańskiej, skróconej edycji, irytowały go spore zmiany (jak zauważyli krytycy, filmowy i książkowy Burgess pod wieloma względami są zupełnie różni). W późniejszych swoich dziełach kilkukrotnie w zawoalowany sposób drwił z reżysera.
Widoczny w domu reżysera obraz został namalowany przez Christiane Kubrick; po zakończeniu zdjęć trafił on do salonu domu Kubricka.
Po ukończeniu prac nad Mechaniczną pomarańczą, kierownictwo firmy Warner Bros. zaproponowało Kubrickowi wyreżyserowanie adaptacji bestsellerowej powieści Williama Petera Blatty’ego Egzorcysta według scenariusza samego autora. Kubrick był projektem bardzo zainteresowany, jednak studio, obawiając się legendarnego już wtedy perfekcjonizmu reżysera i bardzo długiego czasu realizacji, ostatecznie postawiło na opromienionego sławą Francuskiego łącznika Williama Friedkina (który okazał się zresztą podobnym perfekcjonistą i kręcił film przez 226 dni zdjęciowych w czasie jednego roku). Kubrick postanowił wtedy wykorzystać ogromną wiedzę o realiach epoki Oświecenia, jaką zdobył przygotowując się do realizacji filmu o Napoleonie. Przymierzał się do realizacji adaptacji powieści Williama Makepeace Thackeraya Targowisko próżności, jednak ostatecznie uznał, że nie uda mu się sensownie przykroić tej powieści do ram trzygodzinnego widowiska. Wtedy postanowił zaadaptować na film inną powieść Thackeraya – Dole i niedole Szanownego Pana Barry’ego Lyndona.
Ponieważ prawa autorskie do powieści Thackeraya wygasły, praktycznie każdy mógł zaadaptować ją na film. Aby uniknąć powtórzenia się sytuacji, że konkurencyjna produkcja uniemożliwi zrealizowanie jego filmu, Kubrick postanowił możliwie jak najdłużej trzymać w tajemnicy, jaki film zamierza nakręcić. Warner Bros. zgodziło się na sfinansowanie filmu o nieznanej treści, stawiając jedynie warunek, by główną rolę zagrał jeden z listy dziesięciu najbardziej kasowych aktorów roku 1973; po odrzuceniu propozycji przez numer 1 z tej listy – Roberta Redforda – Kubrick wybrał znanego z filmu Love Story Ryana O’Neala (był to jedyny rok, w którym O’Neal na liście dziesięciu najbardziej kasowych aktorów się pojawił). Rolę lady Lyndon otrzymała Marisa Berenson (jej siostra, Berry Berenson, ceniona fotograf, przez kilkanaście lat była żoną Anthony’ego Perkinsa; zginęła 11 września 2001.) W roli lorda Bullingdona wystąpił przyjaciel Kubricka, Leon Vitali (wystąpi potem jeszcze w roli mistrza ceremonii w filmie Oczy szeroko zamknięte; po roli w Barrym Lyndonie zrezygnował z aktorstwa, poświęcając się asystenturze – był asystentem Kubricka przy pracy nad wszystkimi późniejszymi dziełami reżysera). W filmie pojawiło się też kilku aktorów znanych z wcześniejszych dzieł Kubricka: odtwarzający postać kapitana Quinna Leonard Rossiter grał w Odysei Kosmicznej naukowca, pojawiający się w epizodycznej roli lorda Ludda Steven Berkoff pojawił się w Mechanicznej pomarańczy jako policjant przesłuchujący na komisariacie aresztowanego Alexa, natomiast Patrick Magee (kawaler de Balibari) to nikt inny jak pisarz z tegoż filmu.
Film, zatytułowany ostatecznie Barry Lyndon, był realizowany w naturalnych plenerach – starych, XVIII – wiecznych zamkach i posiadłościach na terenie Wielkiej Brytanii i Irlandii. W niektórych posiadłościach ekipa filmowa miała wolną rękę i nieograniczony czas filmowania; w innych, przekształconych tymczasem w muzea, Kubrick i jego ekipa mogli kręcić tylko wtedy, jeśli nie było akurat żadnych zwiedzających. Po pewnym czasie – gdy pojawiła się informacja, jakoby Irlandzka Armia Republikańska przygotowywała zamach na ekipę – Kubrick i jego ludzie powrócili na stałe do Anglii. Żeby możliwie najdoskonalej oddać klimat XVIII-wiecznej Europy, Kubrick postanowił, że planu nie będzie oświetlał światłem elektrycznym, lecz będzie filmował ujęcia we wnętrzach przy świetle świec i naturalnym świetle słonecznym (ostatecznie niektóre ujęcia oświetlano światłem elektrycznym – olbrzymie reflektory ustawiano za oknami budynków, by imitować światło słoneczne; podczas sceny pojedynku Barry’ego z lordem Bullingdonem można zauważyć, że padające z zewnątrz światło ma lekko niebieskawy odcień, którego nie ma światło słoneczne). Ponieważ nigdy wcześniej żaden reżyser nie odważył się na filmowanie wyłącznie przy świetle świec, Kubrick potrzebował specjalnych obiektywów, umożliwiających filmowanie przy tak niskim natężeniu światła; ostatecznie za około 100 tysięcy dolarów zakupił w firmie Carl Zeiss Oberkochen optykę, jaka była wykonana na zamówienie NASA do fotografowania powierzchni niewidocznej strony Księżyca. Były to trzy obiektywy Zeiss Planar o ogniskowej 50 mm i wartości przysłony f/0,7 – najjaśniejsza optyka kiedykolwiek zastosowana w kinematografii. Obiektywy musiały zostać przystosowane do używanych przez Kubricka kamer Mitchell BNC, czego – nie bez pewnych oporów ze względu na techniczne trudności – podjęła się firma Cinema Products Corporation. Jeden z obiektywów został również wyposażony w konwerter szerokokątny, który pozwalał na uzyskanie ogniskowej 36,5 mm przy zachowaniu tej samej jasności[4][5].
Sporo problemów nastręczyło przygotowanie kostiumów: kostiumografki Milena Canonero i Ulla-Britt Soederlund kupiły lub wypożyczyły szereg różnych ubrań z epoki, jednak okazało się, że szyto je dla ludzi o innych niż dwudziestowieczne proporcjach ciała, w dodatku wyraźnie niższych. Wszystkie szwy w kostiumach metodycznie rozpruwano, każdy osobny element ubrania przerysowywano na papier i sporządzano drugi rysunek, proporcjonalnie powiększony, po czym na podstawie tych rysunków sporządzano kopię stroju pasującą na nieco wyższą osobę niż oryginalnie, a oryginał ponownie starannie zszywano. Kubrick długo zastanawiał się nad muzyką, pierwotnie chciał zilustrować Barry’ego Lyndona muzyką graną przez Ennio Morricone na gitarze klasycznej; ostatecznie powierzył skomponowanie i zaaranżowanie muzyki Leonardowi Rosenmanowi, osobiście wybierając szereg kompozycji z epoki; na stałe z filmem związała się Sarabanda Georga Friedricha Händla, wielokrotnie powracająca w czasie filmu, ale w różnych instrumentacjach (m.in. w scenie pojedynku Barry’ego z Bullingdonem w podkładzie wykorzystano jedynie linię basso continuo z tego utworu). Filmowanie i postprodukcja zajęły łącznie dwa lata (odpowiednie zmontowanie samej sceny pojedynku Barry’ego i Bullingdona zajęło sześć tygodni). Film ostatecznie miał premierę 18 grudnia 1975.
Barry Lyndon był pod wieloma względami inną wersją filmu o Napoleonie; oryginalny scenariusz Napoleona był ilustracją Kubrickowskiego fatalizmu, przeświadczenia, że człowiek nie ma wpływu na swój los, jest tylko zabawką w rękach kapryśnego Przypadku; był ironiczną parabolą ludzkiego losu, historią człowieka, który zaczyna od niczego, by własną pracą, ambicją i wolą walki wejść na sam szczyt – tylko po to, by następnie stracić wszystko, co udało mu się uzyskać, i powrócić do punktu wyjścia. Taki był w scenariuszu Napoleon Bonaparte, który przez karierę wojskową wdrapał się mozolnie na sam szczyt, by następnie, krok po kroku, spaść na dno i zakończyć życie jako ubogi wygnaniec. Taki był też bohater Barry’ego Lyndona, Irlandczyk Redmond Barry, który własnym sprytem, odwagą, przedsiębiorczością, a czasem i szczęśliwym przypadkiem zdobył tytuł szlachecki, możnych przyjaciół, wysoką pozycję w społeczeństwie, szlachetnie urodzoną żonę i doczekał się ukochanego syna, by wszystko krok po kroku stracić i zakończyć życie jako samotny, kaleki szuler. Także tu pojawiał się wątek odkupienia poprzez postać kobiety: na swojej drodze Redmond Barry napotyka bowiem Lischen, młodą Prusaczkę z małym dzieckiem, która składa mu propozycję pozostania z nią (Kubrick nieco zmodyfikował tu powieść, w której Lischen była prezentowana jako postać dość lekkich obyczajów; zresztą Barry Lyndon też jest przedstawiony nieco cieplej niż w powieści), jednak pozostawia ją i wyrusza, by dalej szukać przygód. Innym z Kubrickowskich toposów był motyw wojny, zorganizowanego zabijania w imię tajemniczych „wyższych celów”, jako że Redmond Barry bierze udział w wojnie siedmioletniej najpierw ochotniczo po stronie angielskiej, następnie zaś, przymusowo wcielony do armii po zdemaskowaniu jako brytyjski dezerter – po stronie pruskiej; jednak zmiana ogranicza się jedynie do koloru munduru, gdyż po obu stronach jedyne, co może spotkać szeregowego żołnierza, to śmierć na polu bitwy jako anonimowe „mięso armatnie”.
Powstał film niezwykle barwny, plastyczny, malarski (inspiracją dla wielu scen było malarstwo m.in. Daniela Chodowieckiego i Thomasa Gainsborougha); ten malarski rodowód filmu widać w specyficznej realizacji wielu scen, gdzie kamera na początek skupia się na niewielkim fragmencie sceny, po czym następuje powolny odjazd kamery, aż do ukazania całości; zupełnie jakby widz oglądał najpierw mały fragment obrazu, by potem powoli ogarnąć jego całość. Film nie był przebojem kasowym, jednak spotkał się z pozytywną reakcją krytyki (Pauline Kael pisała o czasie, zatopionym w tym filmie jak komar w bursztynie) i zdobył cztery Oscary (zdjęcia, scenografia, muzyka, kostiumy); Kubrick ponownie był nominowany za najlepszy film, reżyserię i scenariusz – tym razem lepszy okazał się Lot nad kukułczym gniazdem.
W poszukiwaniu kolejnego projektu Kubrick ponownie sięgnął do literatury; jego sekretarka wspominała, że przywiózł ze sobą do swego biura ogromne pudło z zakupionymi okazyjnie książkami, siedział na podłodze i czytał kolejno losowo wybrane książki; gdy ta, którą aktualnie czytał, nie podobała mu się – ciskał nią o ścianę i brał na chybił trafił kolejną. Gdy przez długi czas nie było słychać dźwięku rzucanej o ścianę książki, sekretarka weszła do gabinetu Kubricka i zastała go głęboko pogrążonego w lekturze powieści Stephena Kinga Lśnienie.
Scenariusz po raz pierwszy od czasu 2001 powstawał we współpracy: Kubrick wybrał literaturoznawczynię, Diane Johnson, autorkę detektywistycznej powieści The Shadow Knows (którą Kubrick również rozważał jako obiekt ekranizacji). Wspólnie czytali książkę, następnie na podstawie każdego fragmentu Kubrick i Johnson osobno pisali fragment scenariusza, po czym Kubrick wybierał lepszy jego zdaniem fragment, lub łączył części obu fragmentów w jedną całość i tę włączał do scenariusza.
Bodźcem, który ostatecznie pchnął Kubricka do realizacji Lśnienia, był krótki film, jaki otrzymał w roku 1977; zawierał on szereg niezwykle płynnych, wirtuozerskich, uważanych za niezwykle trudne bądź niemożliwe do zrealizowania ujęć. Kubrick skontaktował się z twórcą filmu, Garrettem Brownem; okazało się, że ujęcia te Brown zrealizował za pomocą wynalezionej przez siebie specjalnej platformy przymocowanej do ciała operatora kamery, która odpowiednio amortyzowała ruchy operatora, zapewniając ogromną płynność ujęcia. Kubrick zaprosił Browna wraz z jego Steadicamem – tak bowiem platforma się nazywała – na plan filmu. Wbrew pojawiającym się czasem informacjom, Lśnienie nie było pierwszym filmem, w którym zastosowano Steadicam; wykorzystano go do realizacji niektórych scen w filmie Rocky (1976).
Zdaniem niektórych krytyków, możliwość korzystania ze Steadicamu była dla Kubricka głównym powodem realizacji filmu; charakterystyczne dla Barry’ego Lyndona odjazdy kamery zastąpił ciągły, obsesyjny ruch naprzód, widoczny choćby, gdy kamera płynnie podąża za przemierzającym bezkresne korytarze hotelu Dannym na trójkołowym rowerku. Aby dodatkowo zapewnić mobilność Steadicamu, zamontowano go na odpowiednio zaadaptowanym wózku inwalidzkim.
Do głównej roli początkowo Kubrick przymierzał Roberta De Niro; ostatecznie uznał, że aktor jest zbyt mało psychotyczny jak na Jacka Torrance’a. Kolejnym kandydatem był Robin Williams; przesłuchanie jednak zaszokowało reżysera, który stwierdził, że Williams jest wręcz zbyt psychotyczny jak na Torrance’a. Rolę tę ostatecznie Kubrick powierzył niedoszłemu Napoleonowi Jackowi Nicholsonowi. Otwierającą film sekwencję sfilmowano z powietrza w parku krajobrazowym w stanie Montana; tam również odnaleziono hotel, który następnie na podstawie dokumentacji fotograficznej odbudowano w studiach EMI Elstree pod Londynem jako miejsce akcji filmu.
Film opowiada historię Jacka Torrance’a, niespełnionego pisarza – byłego alkoholika, który w poszukiwaniu weny twórczej przyjmuje wraz z żoną Wendy i synem Dannym posadę stróża w odciętym przez całą zimę od świata górskim hotelu Overlook (Panorama), by tam w spokoju pracować nad dziełem. Poczucie izolacji może być niebezpieczne: poprzednik Jacka, Delbert Grady, w pewnym momencie dostał szału i porąbał siekierą żonę i dwie córeczki, po czym zastrzelił się; Jack jednak niespecjalnie przejmuje się ostrzeżeniami.
Gdy personel hotelu wyjeżdża na zimę, Danny nawiązuje przyjaźń z czarnoskórym kucharzem, Dickiem Halloranem; okazuje się, że obaj mają zdolność komunikacji telepatycznej, którą Dick nazywa lśnieniem, zdolność ta również powoduje, że obaj są w stanie widzieć przeszłe zdarzenia, przed czym Halloran ostrzega Danny’ego, mówiąc, że obrazy, które może widzieć, są jedynie wspomnieniem przeszłości, jak fotografia, która zdaje się realna, ale przedstawia jedynie to, co kiedyś się zdarzyło.
Osamotnienie w hotelu staje się coraz bardziej dokuczliwe dla rodziny: Danny, przemierzając korytarze Overlook na swoim trójkołowym rowerku, napotyka w pewnym momencie na dwie dziewczynki – zamordowane córeczki Grady’ego, które wzywają go, by pozostał z nimi na zawsze. Problemy ma też Jack, który nie potrafi się skupić na pisaniu i całe dnie spędza, odbijając machinalnie piłkę tenisową o ściany hotelu.
W pewnym momencie Danny zostaje skuszony do wejścia do pokoju 237, przed czym przestrzegał go Dick, mówiąc, że tam właśnie wspomnienia przeszłości są bardzo silne. Gdy zszokowany Danny wraca do rodziców ze śladami duszenia na szyi; Wendy oskarża Jacka, że ten zaatakował syna, na co Jack reaguje ze zdumieniem. Zdenerwowany oskarżeniami, udaje się do pustej sali balowej hotelu i tam przy barze nawiązuje konwersację z barmanem Lloydem (ten pojawia się znikąd, gdy Jack stwierdza, że oddałby duszę za łyk alkoholu); jak wynika z rozmowy, Jack kiedyś przypadkiem złamał synowi rękę, gdy ten rozrzucił jego papiery na biurku.
Rozmowę przerywa Wendy, która wbiega do sali balowej (Lloyd wtedy nagle znika, tak jak się pojawił), mówiąc Jackowi, że ktoś jeszcze jest w hotelu – Danny’ego w pokoju 237 zaatakowała jakaś kobieta. Jack udaje się do pokoju, gdzie znajduje piękną, nagą dziewczynę w wannie; gdy ta jednak się do niego przytula, Jack z przerażeniem dostrzega w lustrze, że przytula rozkładającego się trupa. Przerażony ucieka z pokoju. Wstrząśnięty rozwojem sytuacji Danny telepatycznie woła na pomoc przebywającego na Florydzie Dicka Hallorana.
Gdy Jack ponownie trafia do sali balowej, ta nagle jest pełna ludzi w strojach z lat 20., a w tle orkiestra gra jazzowe standardy Midnight The Stars And You i It’s All Forgotten Now. Za barem znów krząta się Lloyd; gdy Jack próbuje zapłacić za drinka, ten odmawia przyjęcia zapłaty, mówiąc, że pieniądze Jacka nie są tu ważne. Torrance zostaje przypadkiem oblany ajerkoniakiem przez innego barmana; gdy ten w łazience czyści ubranie Jacka, przedstawia się jako Delbert Grady. Na reakcję Jacka, który przypomina sobie nazwisko, Grady odpowiada: Nie, pan się myli sir. Mnie tu wcześniej nie było; pan tu był. Zawsze pan tu był. Grady opowiada również o swoich córeczkach i żonie, które przeszkadzały mu w pracy, więc je skorygował. Ostrzega też Jacka przed niebezpieczeństwem z zewnątrz – czarnuchem.
Wendy, uzbrojona w kij baseballowy, przemierza korytarze hotelu; gdy dochodzi do biurka Jacka, odkrywa, że sterty zapisanych na maszynie kartek zawierają w istocie jedno zdanie: All work and no play makes Jack a dull boy (Tylko praca, bez zabawy zdechnie z nudów Jack niebawem). [Kartki te sporządził osobiście Stanley Kubrick. Przygotował też podobne kartki w innych wersjach językowych z myślą o międzynarodowej dystrybucji filmu.] Wtedy zaskakuje ją Jack, agresywny, szalony; Wendy w ostatniej chwili ogłusza go uderzeniem pałką, po czym zamyka go w hotelowej spiżarni, obiecując wezwać pomoc, czego jednak nie jest w stanie zrobić: Torrance unieruchomił pojazd śnieżny i zniszczył radiostację.
Torrance’a w spiżarni odwiedza Grady, mocno krytykując go za nieudolne zajęcie się żoną i synem; gdy Jack obiecuje poprawę, Grady wypuszcza go i Jack rozpoczyna pościg za swymi bliskimi z toporem strażackim w ręku. Danny wymyka się na zewnątrz, jednak Wendy nie jest w stanie przecisnąć się przez okno; od śmierci ratuje ją przybycie Hallorana. Kucharz zostaje zabity przez Jacka; Wendy po długim przemierzaniu bezkresnych korytarzy (również ona widzi wtedy niesamowite obrazy: gościa hotelowego z postrzałem głowy i mężczyznę, uprawiającego w pokoju seks oralny z innym, przebranym w strój psa; jest to sugestia, że również Wendy w pewnym zakresie posiada zdolność „lśnienia”) udaje się uciec na zewnątrz; w tym czasie Danny ucieka przed ojcem w otaczającym hotel ogrodowym labiryncie (w powieści go nie ma, są za to drzewa przycięte w kształt różnych zwierząt, które ożywają i atakują chłopca; ponieważ Kubrick uznał, że technicznie taka scena jest nie do zrealizowania, zmienił zwierzęta w skomplikowany labirynt); udaje mu się oszukać oszalałego Torrance’a i wymknąć się z labiryntu, by wraz z matką uciec na pojeździe śnieżnym Hallorana; Jack gubi się w labiryncie i zamarza na śmierć.
Sam Stephen King odnosił się zawsze do Lśnienia z niechęcią, narzekając na spore skróty i zmianę wymowy filmu; w oryginalnej powieści hotel Overlook wypełniają duchy i upiory, natomiast u Kubricka zło pochodzi z wnętrza ludzi zamieszkujących hotel, jest właściwością wyłącznie ludzką. Zmiany wobec wyjściowej książki są tak duże, że polska filmoznawczyni, profesor Alicja Helman, pisze, że w istocie jest to nie tyle adaptacja powieści Kinga, co raczej film autonomiczny wobec książki, niezależny od niej. W powieści duchy zamieszkujące hotel są realne – u Kubricka zdają się być wytworem wyobraźni Jacka: zawsze gdy Torrance widzi ducha i rozmawia z nim – rozmawia w istocie z lustrem; w jedynej scenie, gdzie upiora nie widać, w scenie, gdzie duch Delberta Grady’ego uwalnia Jacka ze spiżarni – wyswobodzenie się oszalałego Torrance’a można łatwo uzasadnić logicznie, dokładnie oglądając scenę, w której Wendy zamyka ogłuszonego męża, można bowiem zauważyć, że po prostu zamyka ona spiżarnię niedokładnie. Wizja Wendy, w której mężczyzna przebrany za psa zaspokaja drugiego mężczyznę, ma uzasadnienie w akcji powieści, w filmie jest jedynie szokującym obrazem z przeszłości. Kubrick zmienił całe zakończenie powieści: w książce Halloran przeżywa atak szaleńca, zostaje zaatakowany kijem do roque’a, nie toporem, a Jack ginie w eksplozji kotła parowego, która niszczy cały hotel.
Kluczowym ujęciem w Lśnieniu jest ujęcie finałowe: najazd kamery na zdjęcie w holu hotelowym, czarno-białą fotografię z balu z okazji Dnia Niepodległości, 4 lipca 1921. Na pierwszym planie fotografii widać wyraźnie Jacka Nicholsona – Jacka Torrance’a; Torrance był już w hotelu Overlook w roku 1921, jak wynika ze wspomnień „upiorów”, z którymi „rozmawia” Jack, był w latach 40., był w okresie, kiedy rozgrywa się akcja filmu – i pojawi się w tym hotelu jeszcze wiele razy. Jack Torrance jest ucieleśnieniem zła, będącego immanentną, stałą częścią ludzkiej natury, zła pochodzącego z wnętrza człowieka; Torrance będzie powracał do hotelu Overlook, dopóki człowiek będzie istniał – zło jest wieczne, tak jak nieskończoną ilość razy pojawiało się w historii ludzkości – tak nieskończoną ilość razy jeszcze powróci.
Różnie interpretowano Lśnienie: krytycy widzieli w tym filmie studium rozpadu i atrofii uczuć i więzi rodzinnych, poetycką, metaforyczną wizję kryzysu twórczego artysty, porównywalną z Godziną wilka Ingmara Bergmana. Minę Jacka tuż przed ujrzeniem po raz pierwszy barmana-upiora porównywano do obrazu Goi o Saturnie pożerającym własne dzieci. Zwracano uwagę na odwrócenie typowych motywów horroru: zło czai się w jasno oświetlonych korytarzach hotelu Overlook i w nieskończonej, śnieżnej bieli ogrodowego labiryntu, w finale Wendy, uciekając wraz z synem na pojeździe śnieżnym, szuka schronienia w bezkresnej ciemności. Wykrywano w filmie aluzje do nazizmu: podczas rozmowy w łazience Grady nakazuje Jackowi zamordowanie najbliższych, ale nie używa ani razu słowa zabić, mówi zamiast tego o skorygowaniu swojej rodziny, podobnie jak naziści w odniesieniu do Holocaustu używali określeń ostateczne rozwiązanie czy ewakuacja, nigdy nie mówiąc wprost o eksterminacji czy mordowaniu.
Zdjęcia w podlondyńskich studiach Elstree zajęły okrągły rok, od kwietnia 1978 do kwietnia 1979; wiele ujęć było powtarzanych dziesiątki razy. 70 razy powtarzano scenę, w której Dick Hallorann nawiązuje telepatyczny kontakt z uwięzionym w hotelu Dannym, co doprowadziło grającego tę rolę Scatmana Crothersa do załamania nerwowego. 127 razy powtarzano scenę, w której broniąca się kijem baseballowym Wendy wycofuje się po schodach przed oszalałym Jackiem, choć Garrett Brown twierdził, że ta scena była po prostu bardzo trudna technicznie do sfilmowania. Choć Kubrick nie oszczędzał swoich aktorów – był wyjątkowo opiekuńczy wobec grającego małego Danny’ego 5-letniego Danny’ego Lloyda; Lloyd (dziś nauczyciel w szkole podstawowej) dopiero jako nastolatek dowiedział się od rówieśników, że występował w horrorze, gdyż pracę na planie wspominał jako niesamowitą frajdę. Kubrick ponownie skorzystał z usług aktorów, którzy już u niego występowali: Delberta Grady’ego gra Phillip Stone, ojciec Alexa z Mechanicznej pomarańczy i lekarz z Barry’ego Lyndona, wcielający się w rolę barmana Lloyda Joe Turkel był w Ścieżkach chwały skazanym na rozstrzelanie szeregowym Arnaudem, wcześniej wystąpił także w Zabójstwie.
Sam scenariusz jeszcze w toku zdjęć podlegał nieustannym zmianom; film już po wejściu na ekrany był przez Kubricka przemontowywany i skracany, ostatecznie reżyser z ukończonego już filmu usunął dwie sceny: badania małego Danny’ego przez dziecięcą psycholog oraz wypożyczania skutera śnieżnego przez Hallorana w bazie należącej do Larry’ego Durkina. Grająca psycholożkę Anne Jackson i występujący w roli Durkina Tony Burton w finałowej wersji filmu nie pojawiają się w ogóle na ekranie, ale ich nazwiska są w czołówce. Muzykę do filmu, podobnie jak w przypadku Mechanicznej pomarańczy, skomponował Walter, czy raczej wtedy już Wendy Carlos we współpracy z Rachel Elkind; elektroniczne kompozycje uzupełnił wybór nagrań awangardowej muzyki poważnej (m.in. Jutrznia Krzysztofa Pendereckiego) i standardy swingowe z lat 30. Film promował dość niezwykły zwiastun kinowy, zawierający jedną scenę z filmu, przerażającą wizję Danny’ego, którą w finale dostrzega też Wendy: scenę, w której z szybu hotelowej windy wylewa się potokami krew. Realizacja tej jednej sceny trwała przez cały czas zdjęć, gdyż Kubrick za każdym razem uznawał, że rozbryzgujący się płyn nie przypomina na ekranie krwi; ostatecznie po roku pracy udało się uzyskać oczekiwany przez reżysera efekt. Zwiastun napotkał na problemy z rozpowszechnianiem, gdyż wtedy pokazywanie w zwiastunie krwi było zabronione; ostatecznie Kubrick wmówił członkom stosownej komisji, że to po prostu woda zmieszana z rdzą.
Film miał premierę 23 maja 1980 roku i znów spotkał się z mieszanymi reakcjami krytyki (choć wielu krytyków – m.in. Roger Ebert – zweryfikowało potem swoje poglądy), stał się za to przebojem kasowym. Stał się też zachętą do rozmaitych prób interpretacji, którym świadectwo daje zrealizowany w 2012 roku film dokumentalny Pokój 237 w reżyserii Rodneya Aschera, zestawiający najbardziej radykalne teorie na temat tego horroru, oparte nie tylko na konwencjonalnych analizach języka i struktury dzieła filmowego, lecz także sięgające po tak nieoczywiste zabiegi, jak jednoczesne odtwarzanie filmu od początku i od końca albo poszukiwanie ukrytego w obrazie przekazu podprogowego[6].
Fabularną podstawą kolejnego filmu Stanleya Kubricka została powieść The Short-Timers (Krótkoterminowcy) Gustava Hasforda, który był korespondentem wojennym podczas wojny w Wietnamie. Powieść ta – wstrząsająca w swej dokumentalnej zwięzłości – opowiada o losach młodego żołnierza piechoty morskiej USA o pseudonimie Joker (jest on alter ego Hasforda, korespondentem wojennym), od szkolenia w ośrodku na wyspie Parris w Karolinie Południowej po udział w krwawych walkach. Jako współtwórcę scenariusza Kubrick wybrał innego korespondenta wojennego – Michaela Herra, wcześniej współpracującego przy powstawaniu bodaj najsłynniejszego filmu o wojnie w Wietnamie – Apocalypse Now (Czasu apokalipsy) Francisa Forda Coppoli (1979).
W przeciwieństwie do innych filmów opowiadających o wojnie w Wietnamie, Kubrick postanowił zrealizować swój film w typowo miejskim pejzażu; odpowiednio fotogeniczne ruiny wyszukał na terenie londyńskiej dzielnicy, przeznaczonej do wyburzenia, Isle Of Dogs, oraz wśród pozostałości wyburzonej gazowni w Beckton (gdzie sześć lat wcześniej Alan Parker nakręcił fragmenty filmu Pink Floyd The Wall), na których teren sprowadził drogą morską z Południowo-Wschodniej Azji dziesiątki specjalnie dobranych żywych drzew palmowych. Zamiast kolejnego zestawu walk w tropikalnej dżungli, przedstawił walkę w dżungli miejskiej, konkretnie – starcie o południowowietnamskie miasto Huế. Również ośrodek szkoleniowy Parris Island odbudowano na planie w Wielkiej Brytanii.
Długo potrwało kompletowanie obsady; wybrany do głównej roli Anthony Michael Hall został wyrzucony z planu za ignorowanie poleceń reżysera, w ostatniej chwili zastąpił go – znany z innego obrazu o wojnie w Wietnamie, Birdy (Ptasiek) Alana Parkera (1984) – Matthew Modine. Do roli kompanijnej ofiary, otyłego i niezbyt rozgarniętego Gomera Pyle’a, Kubrick wybrał nowojorskiego aktora teatru niezależnego, Vincenta D’Onofrio; musiał on do tej roli znacznie przytyć, co skończyło się dla niego nieciekawie, gdyż podczas kręcenia jednej ze scen uszkodził sobie kostkę, co skończyło się paromiesięczną przerwą w zdjęciach. Po zakończeniu zdjęć, D’Onofrio potrzebował roku wytrwałych ćwiczeń, by powrócić do formy. Do roli bezwzględnego sierżanta szkoleniowego Hartmana planowany był najpierw Bill McKinney, grający w filmie Uwolnienie (Deliverance) (1972) Johna Boormana człowieka z Appalachów – zabójcę, jednak Kubrick, porażony jego kreacją w tym filmie, uznał, że nie wytrzymałby psychicznie pobytu na planie w jego obecności. Następnym kandydatem był Tim Colceri; po pewnym czasie nadzorujący jego przygotowanie do roli Ronald Lee Ermey, były żołnierz piechoty morskiej (który został zaangażowany do filmu jako konsultant techniczny, bo przysłał Kubrickowi kasetę VHS, na której przez kwadrans przeklinał, nie powtarzając się ani razu ani nie zacinając się, mimo że w tym czasie Leon Vitali, asystent reżysera, obrzucał Ermeya pomarańczami), uznał, że ani Colceri, ani żaden inny z proponowanych przez Kubricka aktorów nie nadaje się do tej roli, i zażądał od reżysera, by powierzył tę rolę jemu. Gdy siedzący na swoim krześle Kubrick odmówił, Ermey zwymyślał go, żądając, by zwracając się do oficera, stał na baczność; ton jego głosu spowodował, że Kubrick automatycznie stanął na baczność i bał się w ogóle odezwać do Ermeya, który z miejsca dostał rolę Hartmana. Tim Colceri na pocieszenie dostał epizodyczną rolę psychopatycznego strzelca pokładowego, mordującego seriami ze śmigłowca bezbronnych Wietnamczyków.
Kreacja Ermeya jako Hartmana do tego stopnia przypadła Kubrickowi do gustu, że uczynił dla aktora wyjątek i zgodził się, by Ermey improwizował swoje kwestie, co było dla reżysera bardzo wyjątkowe (jedynym innym aktorem, któremu Kubrick na to pozwolił, był Peter Sellers w Dr. Strangelove). Full Metal Jacket – bo taki tytuł ostatecznie otrzymał film (jest to określenie typu pocisku, w którym ołowiany rdzeń zawarty jest w wykonanym z miedzi korpusie, co zwiększa celność trafienia; w Polsce film był znany na rynku wideo jako Pełny magazynek, pod takim też, błędnym tytułem bywa wymieniany w niektórych polskich leksykonach filmowych) – był też jedynym filmem, w którym Kubrick był obecny fizycznie na ekranie: głos oficera, z którym przez radio rozmawia Kowboj w sekwencji ze snajperką w finale filmu, należy bowiem właśnie do Stanleya Kubricka.
Full Metal Jacket fabularnie dzieli się na dwie części; pierwsza z nich, Czy to ty, Johnie Wayne? Czy to ja? [tytuły obu części filmu pojawiają się w scenariuszu, ale nie ma ich w gotowym filmie], to szczegółowo przedstawione szkolenie młodych chłopców, przygotowanie ich do roli bezwzględnych zabójców. Osią fabularną tej części filmu jest konflikt pomiędzy oficerem szkoleniowym, sierżantem Hartmanem, a kompanijną ofiarą, Gomerem Pyle’em. Gdy Hartmanowi nie udaje się zmienić Pyle’a, postanawia, że za jego błędy będzie ponosić konsekwencje cały oddział. Doprowadza to do wybuchu zorganizowanej przemocy wobec nieudolnego żołnierza, któremu reszta oddziału wymierza w nocy tzw. kocówę – zbiorowe bicie zawiniętymi w ręczniki kostkami mydła (prawdopodobnie zresztą było to głównym powodem decyzji Hartmana – grupa działa zorganizowanie i jednomyślnie, czego wpojenie jest przecież jednym z celów szkolenia). Ten akt przemocy zmienia Pyle’a, który powoli popada w obłęd, co kończy się tragicznie: w ostatnią noc w ośrodku Pyle najpierw zabija Hartmana, po czym popełnia samobójstwo.
Druga część filmu – Zapach piekącego się ciała jest powszechnie akceptowalnym aromatem – w całości rozgrywa się w Indochinach. Ta część filmu ma budowę bardziej epizodyczną, stanowiąc szereg perypetii Jokera, który styka się z różnymi objawami okrucieństwa obu stron konfliktu: psychopatyczny członek załogi śmigłowca seriami z działka pokładowego morduje dziesiątkami bezbronnych Wietnamczyków, w tym kobiety i dzieci (Łatwizna! Wolniej biegają, więc nie trzeba tak dokładnie celować. Czyż wojna nie jest piekłem?), zaś żołnierze Wietkongu wymordowują dziesiątki mieszkańców miasta Hue, oskarżonych o sympatyzowanie z Amerykanami ( – Zginęli za dobrą sprawę. – Niby za jaką? – Za wolność. – Chyba ci wyprali mózg, młody. Myślisz, że tu jeszcze w ogóle o coś chodzi? To tylko rzeź.). W finale dochodzi do krwawego starcia oddziału Jokera z ukrytym w ruinach starej fabryki bezwzględnym snajperem, którym okazuje się młoda, śliczna dziewczyna.
Film był w dorobku Kubricka najpełniejszym ujęciem jednego ze stałych toposów reżysera: wojny i zorganizowanego, zinstytucjonalizowanego zabijania. Zwłaszcza pierwsza, czterdziestominutowa część Full Metal Jacket, przedstawiająca ze szczegółami transformację młodych chłopaków w bezwzględne maszyny do zabijania, jest w historii kina sekwencją wyjątkową, szczegółową wiwisekcją zinstytucjonalizowanej, zorganizowanej przemocy, w której nawet energia libidalna młodych żołnierzy jest ukierunkowywana na zabijanie (żołnierze śpią ze swoimi karabinami w jednym łóżku polowym, mają też rozkaz nadać swojej broni kobiece imiona). Kubrick przedstawia przemoc tak dalece zorganizowaną i usankcjonowaną, że zabójstwo – dobicie ciężko rannej snajperki przez Jokera, by w ten sposób oszczędzić jej konania w mękach – staje się w istocie aktem łaski, aktem miłosierdzia – przejawem człowieczeństwa.
Oryginalną muzykę do filmu, sygnowaną przez Abigail Mead, stworzyła w rzeczywistości najstarsza córka Kubricka – Vivian Kubrick [autorka krótkiego filmu dokumentalnego o realizacji Lśnienia, załączanego jako materiał dodatkowy do edycji DVD tegoż filmu.] Reżyser ponownie chciał pracować z operatorem Johnem Alcottem, swoim stałym współpracownikiem od czasu Mechanicznej pomarańczy, ten jednak był zajęty innymi projektami i musiał odmówić; podczas wakacji w Hiszpanii w sierpniu 1986 zmarł nagle na atak serca. Ostatecznie za kamerą stanął brytyjski operator Douglas Milsome. Dość trudne w realizacji okazały się sceny w ośrodku na wyspie Parris: aby podkreślić, że wszyscy z prawie identycznie wyglądających, ogolonych prawie na zero, identycznie ubranych szeregowców są jednakowo istotni, czy raczej – wszyscy w jednakowym stopniu są tylko mięsem armatnim, Kubrick zażądał, by wszyscy byli widoczni w kadrze z jednakową ostrością, co okazało się trudne w realizacji. W kulminacyjnej scenie ze snajperką w płonących ruinach fabryki migawka w kamerze była niezsynchronizowana z szybkością przesuwu taśmy filmowej, co dało dość surrealistyczny efekt, jakby płomienie wpełzały na taśmę filmową.
Gdy film był już praktycznie ukończony – dał o sobie znów znać cechujący Kubricka pech: na ekrany amerykańskich kin pół roku przed jego premierą 23 czerwca 1987 trafił Pluton Olivera Stone’a, również podejmujący temat rozrachunku z wojną wietnamską, choć w innym ujęciu. Choć fakt ten nie miał już wpływu na produkcję Full Metal Jacket, miał wpływ na losy tego filmu na ekranach kin: po filmie Stone’a znaczna część publiczności odniosła się do dzieła Kubricka z niechęcią, nie mając ochoty oglądać kolejnego ponurego filmu o wojnie w Wietnamie, w efekcie czego film ten odniósł znacznie mniejszy sukces komercyjny, niż oczekiwał tego Stanley Kubrick i Warner Bros.
Kubrick był nominowany do Oscara za scenariusz filmu; skończyło się znów na nominacji.
Ukończywszy pracę nad Full Metal Jacket Kubrick rozpoczął przygotowywać scenariusz do filmu Aryjskie papiery według powieści Wojenne kłamstwa (Wartime Lies) Louisa Begleya, opowiadającej o wojennych przeżyciach młodego chłopca w ogarniętej Holocaustem Polsce czasów II wojny światowej. Długo szukał odpowiednich plenerów, między innymi w Polsce (w latach 70. Kubrick nałożył na Polskę swoiste embargo, zrywając kontakty z Polską po tym, jak wypożyczona na dwa tygodnie kopia Mechanicznej pomarańczy wróciła po czterech miesiącach w strzępach); ostatecznie zdecydował się na duńskie miasto Arhus i jego okolice – postanowił sporządzić ogromną dokumentację fotograficzną tych rejonów, by potem odpowiednio odtworzyć je w Anglii. Do głównej roli wybrał Josepha Mazzello; gdy młody aktor został zaangażowany do filmu Park Jurajski – Kubrick osobiście zwrócił się do Stevena Spielberga z prośbą o nie ruszanie włosów Mazzello.
O zarzuceniu pracy nad Wojennymi kłamstwami znów zadecydowała konkurencja; w tym czasie bowiem Steven Spielberg przystąpił do pracy nad Listą Schindlera. Na wieść o tym Kubrick postanowił wstrzymać swój projekt, aby uniknąć sytuacji, w której producenci wycofają się z finansowania projektu (jak stało się z Napoleonem), bądź gotowy film poniesie porażkę kasową, bo widzowie po zobaczeniu jednego filmu o Holocauście nie będą chcieli oglądać kolejnego (który to los spotkał kilka lat wcześniej Full Metal Jacket). Współpracownicy Kubricka mówią też o wątpliwościach, jakie Kubrick od początku żywił wobec całego projektu; reżyser miał duże wątpliwości, czy odpowiednie opisanie tak upiornego w swej masowej skali i technicznej doskonałości zjawiska jak Holocaust mieści się w ogóle w możliwościach kinematografii.
Następnym projektem, nad którym zaczął pracować Kubrick, była adaptacja opowiadania Briana Aldissa Supertoys Last All Summer Long, opowiadającego o przyjaźni chłopca i androida w świecie przyszłości. Wizja zdehumanizowanej ludzkości przeciwstawionej uczłowieczonym maszynom była Kubrickowi bliska od dawna (pojawiała się choćby w Dr.Strangelove i 2001: Odysei Kosmicznej); tym razem na przeszkodzie w realizacji filmu stanęło niewystarczające zdaniem reżysera zaawansowanie cyfrowych efektów wizualnych; Kubrick postanowił zaczekać z realizacją filmu, aż możliwości techniczne zezwolą na wytworzenie takiej wizji, jaką planował.
Ostatecznie reżyser postanowił ukończyć pracę nad projektem, nad którym pracował już od lat 60.: adaptacją noweli wiedeńskiego pisarza i psychologa Arthura Schnitzlera Traumnovelle, opowiadającą historię pokus i dziwnych zdarzeń, jakich podczas jednej, niezwykłej nocy zaznaje młode małżeństwo; zdarzeń, które staną się próbą dla ich związku, które podadzą w wątpliwość podstawowe wartości, na jakich opiera się uczuciowy związek dwojga ludzi.
Schintzler akcję swojej powieści osadził w Wiedniu epoki fin de siecle’u, Kubrick postanowił przenieść akcję filmu do czasów współczesnych. Ostatecznie wraz ze współscenarzystą Fredericem Raphaelem osadził akcję w Nowym Jorku u schyłku XX wieku. Bohaterami filmu są doktor William Harford (nazwisko jest aluzją do osoby aktora, który pierwotnie miał zagrać tę rolę – Harrisona Forda) i jego żona Alice (w rolę pary małżonków wcieliła się para aktorów będących wtedy jeszcze małżeństwem Tom Cruise i Nicole Kidman). Po wystawnym przyjęciu, w którym oboje uczestniczą, pod wpływem marihuany para nawiązuje zaciekłą dyskusję o ich małżeństwie, roli wierności we współczesnym świecie i czyhających pokusach. Alice wyznaje wówczas mężowi, że kiedyś miała pokusę zdradzić go z przystojnym oficerem marynarki. Zaszokowany tą wiadomością, William wyrusza na nocną eskapadę po Nowym Jorku; eskapada ta stanie się próbą jego wierności, siły ich związku, zdolności stawienia czoła pokusom i niebezpieczeństwom. Znów swą rolę odegra przypadek: tylko przypadek bowiem powstrzyma Harforda przez zbliżeniem z prostytutką, która potem okaże się nosicielką wirusa HIV. Kulminacyjną sceną filmu była tajemnicza, wyuzdana uroczystość w położonej na uboczu posiadłości, na którą trafia Harford, swoisty niby-religijny rytuał, którego uczestnicy skrywają twarze pod kunsztownymi maskami. Gdy Harford zostaje zdemaskowany przez uczestników uroczystości jako obcy, ratuje go tajemnicza uczestniczka orgii, ofiarowując się zamiast niego; wkrótce potem William, przeglądając gazetę, znajduje jej nekrolog – dziewczyna zmarła na skutek przedawkowania narkotyków. Ostatecznie, po całonocnej, fantasmagorycznej wędrówce, Harford znajduje wreszcie ukojenie – u boku ukochanej żony.
Choć akcja filmu Eyes Wide Shut (Oczy szeroko zamknięte) rozgrywa się na Manhattanie, swoim zwyczajem Kubrick odtworzył ulice Nowego Jorku w brytyjskim studio (posunął się nawet do wysyłania współpracowników za ocean, by przywozili mu śmieci z ulicznych koszy na Manhattanie). Filmowanie trwało ostatecznie 400 dni zdjęciowych, na skutek czego kilku aktorów wybranych pierwotnie przez Kubricka musiało zrezygnować na rzecz innych zobowiązań i zostać zastąpionych innymi aktorami; np. Harvey Keitel, grający milionera Zieglera, musiał po pewnym czasie wrócić do Stanów, na plan innego filmu; zastąpił go znany reżyser i przyjaciel Kubricka, Sydney Pollack. Tytuł filmu wywodzi się z koncepcji psychoanalitycznych, jakich liczne tropy w gotowym dziele można odnaleźć: oznacza, że film jest w istocie próbą oddania „pejzażu wewnętrznego”, zaś poszczególne przygody, jakie spotykają Harforda w czasie nocnej eskapady, niekoniecznie muszą być realne, mogą być wytworem jego podświadomości.
Po ukończeniu pracy nad Oczami szeroko zamkniętymi Stanley Kubrick planował wznowić pracę nad adaptacją Supertoys Last All Summer Long, którą planował zatytułować A.I.: Artificial Intelligence. Nim jednak zdołał ukończyć pracę nad Eyes Wide Shut – po ukończeniu pierwszego montażu filmu, w cztery dni po pierwszej, prywatnej projekcji – 7 marca 1999 zmarł we śnie na atak serca, w swoim domu w Harpenden.
Wersja filmu, jaka ostatecznie trafiła na ekrany 16 lipca 1999 była właśnie pierwszą wersją. Wytwórnia Warner Bros zapewniała, że była to też wersja ostateczna i montaż filmu został przez reżysera zakończony[7], nie jest jednak wykluczone, że Kubrick pracowałby nad nim nadal, przemontowywał go i poprawiał (co zdarzało się już w przypadku jego poprzednich produkcji). Niektórzy są również zdania, że słowo przedstawicieli Warner Bros, w których interesie leżało szybkie dostarczenie filmu do kin, nie jest miarodajne i Kubrick – uginając się pod naciskami szefów wytwórni – przedstawił jedynie jego roboczą wersję[8]. Obraz przyjęto z dość mieszanymi uczuciami: dla niektórych krytyków był jedynie rozbuchaną erotyczną fantazją starszego pana, inni dostrzegli w dziele intrygujący i nietypowy jak na reżysera wątek rodzinny, motyw odkupienia, jakie przynosi miłość najbliższej osoby (warto zauważyć, że po raz kolejny w filmie Kubricka postać kobieca przynosi odkupienie: przed coraz poważniejszymi groźbami tajemniczego, zamaskowanego towarzystwa ratuje głównego bohatera młoda dziewczyna, zgadzając się ofiarować zamiast niego). Odnotowywano różne kulturowe odniesienia ukryte w filmie: aby wejść na tajemniczą uroczystość w odludnej willi, Harford musi założyć ozdobną pelerynę i maskę i podać hasło – hasłem tym jest Fidelio, tytuł opery Beethovena, której głównym bohaterem jest kobieta, zakładająca męskie odzienie, by tak zamaskowana mogła uratować swojego męża od grożącego mu niebezpieczeństwa. W Stanach Zjednoczonych scena orgii została cyfrowo ocenzurowana: aby nie były widoczne uprawiające seks postaci, zasłonięto je cyfrowo stworzonymi i wstawionymi w obraz postaciami.
W dorobku Kubricka obok dzieł ukończonych przez reżysera można znaleźć też filmy, których ukończyć się z różnych przyczyn nie udało, czy też ściślej: nie udało się doprowadzić do rozpoczęcia ich realizacji.
W filmach Kubricka można wyróżnić kilka dominujących wątków: przekonanie, że człowiek jest z gruntu zły; że w istocie człowiek ma niewielki wpływ na swój los, pozostając zabawką w rękach kapryśnego losu; że zło pochodzi z wnętrza człowieka, a zdolność do świadomego, dobrowolnego wybierania zła jest miarą człowieczeństwa.
Charakterystyczną cechą Kubricka podczas pracy nad filmem była niezwykła dbałość o każdy detal: wymagał od aktorów, by ściśle trzymali się wytycznych scenariusza (wyjątkami byli Sellers i Ermey), ściśle pilnował, by każdy detal – rodzaj użytych obiektywów i soczewek, sposób i siła oświetlenia planu, gesty i mimika aktorów, użyta muzyka – dokładnie pasował do tego, co sobie zaplanował.
W filmach Kubricka muzyka pełni wyjątkowo ważną rolę: „Stanley Kubrick należał do nielicznych reżyserów traktujących muzykę jako pełnoprawny i decydujący współczynnik formy”[9]. Twórcą muzyki do jego wczesnych filmów był przyjaciel z czasów szkolnych Gerald Fried. Wyjątkowo ciekawy efekt uzyskali w Ścieżkach chwały, gdzie ścieżkę dźwiękową zdominowała perkusja. Była to pierwsza w historii filmu oryginalna partytura przeznaczona wyłącznie na perkusję[10]. Poczynając od Odysei kosmicznej, są to przede wszystkim cytowane bądź adaptowane utwory klasyczne (od Haendla poprzez Beethovena, Schuberta i Johanna Straussa aż po Ryszarda Straussa i Szostakowicza) oraz awangardowe (Ligeti, Penderecki). Mając do dyspozycji tak bogaty repertuar symfoniczny, współczesny oraz awangardowy, nie bardzo widzę sens angażowania kompozytora, który może być nawet znakomity, ale przecież nigdy nie dorówna ani Mozartowi, ani Beethovenowi – tłumaczył swoją decyzję reżyser. – Postępowanie takie pozwala też na eksperymentowanie z muzyką na wczesnym etapie montażu, czasem nawet na przycinanie scen do muzyki. Przy normalnym trybie pracy [czyli zamawiając muzykę u kompozytora na ostatnim etapie produkcji filmu – DG] nie da się tego zrobić równie łatwo[11].
Kubrick był trzykrotnie żonaty, Dwa pierwsze małżeństwa, z Tobą Metz i Ruth Sobotką, zakończyły się rozwodami po kilku latach. Z trzecią żoną, Christiane Harlan, reżyser przeżył 40 lat i doczekał się dwóch córek: Anyi (1959-2009) i Vivian (ur. 5 sierpnia 1960). (Kubrickowie wychowywali też córkę Harlan z wcześniejszego związku, Katharine). Rodzice wychowali go w duchu religii judaistycznej, lecz on sam nigdy nie czuł specjalnej potrzeby uczestniczenia w religijnych ceremoniach.
Powszechnie znanym faktem była niechęć reżysera do uczestniczenia w życiu publicznym. Czas, kiedy nie pracował nad kolejnym projektem, Kubrick zawsze spędzał z rodziną w swojej posiadłości Childwickbury Manor w Harpenden w hrabstwie Hertfordshire. Fakt ten spowodował, że bardzo niewiele osób wiedziało, jak reżyser naprawdę wygląda; wielu reporterów, przybywających do Harpenden w nadziei przeprowadzenia wywiadu, było witanych u bramy posiadłości osobiście przez Kubricka, który grzecznie informował przybyłych, że reżyser aktualnie przebywa na planie filmu – ponoć żaden z reporterów nigdy nie rozpoznał w witającym Kubricka. To izolowanie się reżysera miało swoje konsekwencje: funkcjonował szereg plotek o Kubricku, dotyczących jego zachowania wobec dziennikarzy i fanów (według jednej z nich, Kubrick najpierw postrzelił przybyłego fana za karę, że ten zawraca mu głowę, po czym postrzelił go jeszcze raz – tym razem za karę, że intruz zakrwawił mu idealnie wypielęgnowany trawnik), pojawiła się też spora grupa osób podających się za reżysera i oszukująca dzięki temu ludzi, nierzadko na spore sumy (o jednym z takich oszustów opowiadał film Być jak Stanley Kubrick). Do dziś funkcjonuje też – nigdy ostatecznie nie potwierdzona – informacja, jakoby reżyser cierpiał na zespół Aspergera.
W młodości Kubrick pasjonował się lotnictwem, zdobył nawet licencję pilota na samoloty jednosilnikowe i często latał. Pewnego razu, podczas startu z lotniska w Anglii, omal nie rozbił jednak maszyny, gdyż, jak się później okazało, źle ustawił konfigurację klap. Od tej pory starał się latać jak najrzadziej, gdyż prześladowała go myśl, że skoro on – wtedy już całkiem doświadczony pilot – popełnił tak banalny błąd, zawodowi piloci pracujący dla linii lotniczych również mogą takie błędy popełnić i doprowadzić do katastrofy. (Komisja badająca przyczyny katastrofy lotniczej w Madrycie ustaliła, że powodem rozbicia się maszyny podczas startu, co doprowadziło do śmierci 154 osób, było błędne ustawienie konfiguracji klap.)
Dawni współpracownicy różnie wypowiadali się o Kubricku; dość niechętnie wypowiadał się o nim George C. Scott, który miał reżyserowi za złe, że ten do filmu Dr.Strangelove wybrał niezbyt udane, przerysowane sceny z jego udziałem. Według Jacka Nicholsona, Kubrick do końca życia nie potrafił mu wybaczyć, że zarobił na filmie Lśnienie mniej niż Nicholson. Podczas kręcenia Mechanicznej pomarańczy Malcolm McDowell zaprzyjaźnił się z reżyserem, z którym namiętnie na planie grał w tenis stołowy; potem się okazało, że godziny spędzone na grze Kubrick potrącił McDowellowi z wynagrodzenia. McDowell i Kubrick spędzili też wiele godzin, słuchając na radiu krótkofalowym rozmów pilotów z wieżami kontrolnymi londyńskich lotnisk; rozmowy te wywołały u aktora lęk przed lataniem (McDowell wspomina realizację filmu Błękitny grom, w którym grał pilota śmigłowca, jako prawdziwy koszmar). Po zakończeniu prac nad Mechaniczną pomarańczą Kubrick bez słowa zerwał kontakty z McDowellem. Wielu aktorów Kubricka wypowiadało się o nim z podziwem; choć psychiczny ciężar i emocjonalny stres spowodowany pracą nad Oczami szeroko zamkniętymi był jednym z głównych czynników, które doprowadziły do rozpadu małżeństwa Toma Cruise’a z Nicole Kidman, oboje wypowiadali się o reżyserze w samych superlatywach, podobnie jak Scatman Crothers, który pracę nad filmem Lśnienie przypłacił załamaniem nerwowym.
Oprócz szachów i fotografii, Kubrick był też zapalonym miłośnikiem tenisa stołowego, interesował go również baseball i futbol amerykański – podczas przebywania w Europie Kubrick kazał swoim amerykańskim znajomym nagrywać w telewizji mecze National Football League, które potem w swoim angielskim domu godzinami oglądał i analizował.
Stanley Kubrick został pochowany na terenie swojej rezydencji w Harpenden.
Leonhard Euler (wym. niem. MAF: [ˈleːɔnhaʁt ˈɔɪ̯lɐ],  posłuchaj i; ur. 15 kwietnia 1707 w Bazylei, zm. 18 września 1783 w Petersburgu) – szwajcarski matematyk i fizyk; był pionierem w wielu obszarach obu tych nauk. Większą część życia spędził w Rosji i Prusach. Jest uważany za jednego z najbardziej płodnych matematyków w historii[1][2].
Dokonał licznych odkryć w tak różnych gałęziach matematyki jak rachunek różniczkowy i całkowy oraz teoria grafów. Wniósł duży wkład w rozwój terminologii i notacji matematycznej, szczególnie trwały w dziedzinie analizy matematycznej. Jako pierwszy w historii użył na przykład pojęcia i oznaczenia funkcji[3]. Opublikował wiele ważnych prac z zakresu mechaniki, optyki i astronomii.
Euler jest uważany za czołowego matematyka XVIII wieku i jednego z najwybitniejszych w całej historii. Oto przypisywane Laplace’owi zdanie wyrażające wpływ Eulera na matematykę:
Czytajcie Eulera, czytajcie go – jest mistrzem nas wszystkich[4].
Uczony ten należy do grona najbardziej twórczych – jego dzieła zapełniłyby 60-80 woluminów kwarto[5] (czwarta część arkusza drukarskiego).
Podobizna Eulera widnieje na szwajcarskim banknocie[6] 10-frankowym szóstej serii; uczonego uwieczniono też na wielu szwajcarskich, niemieckich i rosyjskich (radzieckich) znaczkach pocztowych. Na jego cześć jedna z asteroid zyskała miano „(2002) Euler”. 
Leonhard Euler urodził się w Bazylei w rodzinie Paula Eulera, pastora Kościoła Reformowanego[7] (patrz: kalwinizm), i Marguerite Brucker, której ojciec był także pastorem. Miał dwie młodsze siostry: Annę Marię i Marię Magdalenę. Wkrótce po przyjściu syna na świat Eulerowie przenieśli się z Bazylei do pobliskiej miejscowości Riehen, gdzie Leonhard spędził większą część dzieciństwa. Podstawową wiedzę, w tym matematyczną, przekazał swojemu synowi Paul Euler, który szykował Leonarda do stanu duchownego. Pastor Euler był przyjacielem rodziny Bernoullich, m.in. Johanna Bernoulliego, wówczas czołowego szwajcarskiego matematyka (wywarł on znaczący wpływ na życie Leonharda). Formalna edukacja Eulera rozpoczęła się w Bazylei, w tamtejszym gimnazjum łacińskim.
20 października 1720 roku, w wieku trzynastu lat, rozpoczął studia na Uniwersytecie Bazylejskim. Sobotnimi popołudniami brał prywatne lekcje u Johanna Bernoulliego, który szybko odkrył wielki talent matematyczny swojego ucznia[8]. W roku 1723 młody Euler, przedłożywszy końcową rozprawę, w której porównywał systemy filozoficzne Kartezjusza i Newtona, otrzymał stopień magistra filozofii.
Od tego momentu, za namową ojca, który nadal widział w Leonhardzie przyszłego pastora, rozpoczął studia nad teologią, greką i językiem hebrajskim. Pastor Euler zmienił zdanie co do zawodu Leonharda wskutek interwencji Johanna Bernoulliego; uczony przekonał go, że przeznaczeniem jego syna jest stać się wielkim matematykiem. W roku 1726 Euler ukończył swoją rozprawę doktorską na temat rozchodzenia się dźwięku, De Sono[9]. W roku następnym stanął do corocznego konkursu o prestiżową, ufundowaną przez Francuską Akademię Nauk Wielką Nagrodę Akademii Paryskiej (ang. Paris Academy Prize Problem), ze swoim opracowaniem zagadnienia optymalnego rozmieszczenia okrętowych masztów. Zajął drugie miejsce, przegrywając jedynie z Piotrem Bouguerem znanym jako „ojciec nauki o architekturze okrętów”. W ciągu życia Euler wygrywał tę doroczną nagrodę dwunastokrotnie[10].
Kiedy Euler kończył studia, dwaj synowie Johanna Bernoulliego – Daniel i Mikołaj (II) – pracowali dla Petersburskiej Akademii Nauk. Gdy w lipcu 1726 roku Mikołaj po roku spędzonym w Rosji zmarł na zapalenie wyrostka robaczkowego, Daniel, objąwszy po bracie funkcję na wydziale matematyczno-fizycznym, zarekomendował Eulera na wakujące po swoim odejściu stanowisko na fizjologii. W listopadzie 1726 roku Euler zaakceptował tę ofertę, wstrzymał się jednak z wyjazdem do Rosji do wiosny roku następnego – starał się w tym czasie bez powodzenia o objęcie katedry fizyki na Uniwersytecie w Bazylei[11].
Do stolicy Rosji Euler przybył 17 maja 1727 roku. Z posady na wydziale medycznym został awansowany na stanowisko na odpowiedniejszym dla niego wydziale matematycznym. Zamieszkał razem z Danielem Bernoullim, z którym poza tym często ściśle współpracował. Euler doskonalił swoją znajomość języka rosyjskiego i osiadł na dobre w petersburskim życiu; dodatkowo zaangażował się do pracy w służbie medycznej rosyjskiej marynarki wojennej[12].
Petersburski Uniwersytet Państwowy, założony przez cara Piotra I (zwanego później Wielkim), stawiał sobie za zadanie poprawienie stanu edukacji w Rosji i zmniejszenie dystansu jaki w nauce dzielił ten kraj od zachodniej Europy. W związku z powyższym celem petersburska uczelnia stwarzała cudzoziemskim uczonym takim jak Euler szczególnie atrakcyjne warunki; źródła finansowania akademii były zasobne, jej biblioteka zaś – złożona z księgozbiorów samego Piotra I i rosyjskiej arystokracji – wszechstronnie zaopatrzona. Studentów na uczelni nie było zbyt wielu, uczeni nieobciążeni pracą dydaktyczną mogli się skupić na badaniach naukowych. Okolicznością im sprzyjającą była także zasada całkowitej swobody badań[13].
W dniu przybycia Eulera do Petersburga zmarła dobrodziejka Akademii, wdowa po Piotrze I, jego następczyni na tronie – cesarzowa Katarzyna I, która usiłowała kontynuować nowoczesne dzieło swego zmarłego męża. Z chwilą jej śmierci i nastaniem panowania dwunastoletniego Piotra II wzrosło znaczenie rosyjskiej arystokracji – wielce podejrzliwej w stosunku do cudzoziemskich naukowców; wydarzenia te były przyczyną cięć funduszy przeznaczonych dla akademii i licznych trudności, wobec których stanął Euler i jego koledzy z uczelni.
Warunki poprawiły się nieco po śmierci Piotra II; Euler, przechodząc różne uczelniane stanowiska, szybko awansował, by w 1731 roku zostać profesorem fizyki. Dwa lata później Daniel Bernoulli, który miał dosyć panującej w Rosji cenzury i wrogości, z którą spotykał się w Petersburgu, wrócił do Bazylei. Euler objął po nim wydział matematyki[14].
7 stycznia 1734 ożenił się z Katarzyną Gsell, córką artysty malarza z petersburskiego gimnazjum, pochodzącą podobnie jak Euler z rodziny szwajcarskiej. Młodzi małżonkowie kupili dom nad Newą. Doczekali się w sumie trzynaściorga potomstwa, z których tylko pięcioro przeżyło lata dziecięce[15].
Euler zaniepokojony przeciągającym się w Rosji wrzeniem rozważał opuszczenie Petersburga. W końcu zaakceptował złożoną jakiś czas wcześniej przez Fryderyka II Hohenzollerna propozycję przeniesienia się do Berlina i objęcia stanowiska w Pruskiej Akademii Nauk. Wyjechał z Petersburga 19 czerwca 1741 roku i przez 25 lat mieszkał w Berlinie. Tam napisał ponad 380 artykułów i opublikował dwie spośród swoich prac, które w największym stopniu rozsławiły jego imię. Były to: praca poświęcona funkcjom – Introductio in analysin infinitorum i dzieło dotyczące rachunku różniczkowego – Institutiones calculi differentialis[16].
W czasie pobytu w Berlinie Euler został poproszony o udzielanie prywatnych lekcji księżniczce Anhalt-Dessau, siostrzenicy Fryderyka. Napisał do niej ponad 200 listów, co zaowocowało późniejszym wydaniem ich w formie drukowanej pod postacią bardzo dobrze się sprzedających Listów do księżniczki niemieckiej. Zawierają one przystępne Eulerowskie naświetlenie różnych tematów związanych z matematyką i fizyką, dają także – wartościowe z dzisiejszego punktu widzenia – wyobrażenie o osobowości uczonego i o jego poglądach na wiarę. Książka ta, czytana częściej niż którekolwiek z dzieł matematycznych Eulera, stała się – według dzisiejszej terminologii – bestsellerem; była szeroko znana – opublikowano ją w Europie i Stanach Zjednoczonych. Popularność Listów świadczy o rzadkiej wśród naukowców zajmujących się badaniami zdolności Eulera do przekazywania treści naukowych laikom.
Mimo ogromnego udziału Eulera w znacznym ówcześnie prestiżu Akademii, uczony został w końcu zmuszony do opuszczenia Berlina. Przyczyną był, przynajmniej częściowo, osobisty konflikt Eulera z Fryderykiem. Fryderyk z upływem czasu zaczął uważać Eulera za człowieka prostego, zwłaszcza gdy porównywał go do uczonych z kręgu filozofów, których on – król Prus – sprowadził do Akademii. Był wśród nich Wolter i to on cieszył się uprzywilejowaną pozycją w towarzystwie skupionym wokół króla. Euler, zwyczajny religijny człowiek, niewolniczo oddany swojej pracy, był bardzo konwencjonalny w swoich przekonaniach i gustach. Pod wieloma względami stanowił wprost przeciwieństwo Woltera. Jego przygotowanie retoryczne było bardzo ograniczone, przy tym miał skłonność do brania udziału w dyskusjach na tematy, w których nie był zbyt biegły, co czyniło z niego częsty cel dowcipów Woltera. Poza tym Fryderyk wyrażał niezadowolenie z praktycznych zdolności inżynierskich Eulera:
Chciałem mieć w moim ogrodzie silny strumień wody: Euler obliczył moc kół niezbędną do wzniesienia wody na poziom rezerwuaru, z którego miała się rozpływać kanałami, wytryskając w końcu w Sanssouci. Młyn został skonstruowany zgodnie z geometrycznymi planami, a nie był w stanie podnieść nawet kropli wody pięćdziesiąt kroków od zbiornika. Marność nad marnościami! Marność geometrii![17]
Trzy lata po przejściu niemal śmiertelnej gorączki, która dotknęła go w roku 1735 Euler prawie całkowicie stracił wzrok w prawym oku, ale obarczał winą za ten stan rzeczy swoją drobiazgową pracę kartografa, którą wykonywał dla Akademii Petersburskiej. Wzrok w tym oku pogorszył się Eulerowi w ciągu jego pobytu w Niemczech tak bardzo, że Fryderyk mawiał o nim „Cyklop”. W późniejszym okresie Euler cierpiał na kataraktę w drugim, dotychczas zdrowym oku; doprowadziła go ona już w kilka tygodni po jej odkryciu do niemal całkowitej ślepoty. Mimo tych kłopotów zdrowotnych wydajność Eulera w jego pracy spadła tylko w niewielkim stopniu – kłopoty ze wzrokiem kompensował swoją fotograficzną pamięcią i umiejętnościami dokonywania obliczeń pamięciowych. Był na przykład zdolny do powtórzenia bez najmniejszego wahania słowo w słowo Eneidy Wergiliusza, co więcej: był w stanie wskazać jakim wersem zaczyna się i jakim kończy dowolna stronica tej książki[5].
Sytuacja w Rosji poprawiła się po objęciu tronu przez Katarzynę II (1762) i w 1766 roku Euler zaakceptował zaproszenie do powrotu do Akademii w Petersburgu i w mieście tym spędził resztę życia. Na jego drugim pobycie w stolicy Rosji zaciążyły tragiczne wydarzenia; w 1771 roku pożar, który wybuchł w Petersburgu, strawił dom Eulera i jego samego kosztował niemal życie. Dwa lata później stracił żonę, z którą przeżył prawie 40 lat. Trzy lata po jej śmierci ożenił się ponownie.
18 września 1783 roku w Petersburgu Euler zmarł, doznawszy wylewu krwi do mózgu; pochowano go w ławrze Aleksandra Newskiego. Dorobek życia Eulera, wraz z listą jego prac, został opisany przez jego zięcia, Nicolausa von Fussa, sekretarza Cesarskiej Akademii Nauk i Sztuk Pięknych w Petersburgu. Pośmiertną mowę pochwalną na cześć Eulera skierowaną do Akademii Francuskiej napisał matematyk i filozof francuski markiz de Condorcet, który takimi słowami skomentował fakt śmierci Eulera:
…il cessa de calculer et de vivre – …przestał liczyć i przestał żyć[19]
Euler wniósł wkład do niemal wszystkich ówczesnych dziedzin matematyki – geometrii, rachunku różniczkowego i całkowego, trygonometrii, algebry, teorii liczb. Zajmował się m.in. fizyką ośrodków ciągłych i teorią ruchów Księżyca﻿(inne języki)[20]. Waga jego dokonań w matematyce nie może być przeceniona: gdyby wydać drukiem wszystkie jego dzieła, z których wiele ma fundamentalne znaczenie, zajęłyby od 60 do 80 woluminów oprawionych in quarto[5] (czwarta część arkusza). Za jedynego matematyka równie płodnego może być uważany XX-wieczny uczony węgierski, Paul Erdős.
Euler, dzięki swoim licznym i szeroko rozpowszechnionym podręcznikom, zainicjował i spopularyzował kilka konwencji zapisu; w szczególności, wprowadził pojęcie funkcji i jako pierwszy zastosował zapis  dla oznaczenia funkcji  argumentu [3]. Był też autorem nowoczesnego oznaczania funkcji trygonometrycznych, litery  jako podstawy logarytmu naturalnego (obecnie znanej także jako liczba Eulera), zastosowania greckiej litery Σ dla oznaczania sumy i litery  do wyrażenia jednostki urojonej. Użycie greckiej litery π dla oznaczenia stosunku obwodu okręgu do jego średnicy nie było wprawdzie jego autorstwa, ale zostało przez niego rozpropagowane[21].
Rozwój rachunku różniczkowego był jednym z najważniejszych prądów badawczych matematyki XVIII wieku i to dzięki Bernoullim – rodzinie zaprzyjaźnionej z Eulerami – dokonał się na tym polu wielki postęp. Ich wpływ spowodował, że analiza znalazła się w centrum zainteresowań Eulera. Chociaż niektóre z przeprowadzonych przez niego dowodów są nie do zaakceptowania z punktu widzenia obecnych rygorów dowodzenia twierdzeń[22], idee Eulera zapoczątkowały wielki postęp w tej dziedzinie.
Jest bardzo dobrze znany z częstego używania w analizie szeregów potęgowych; przyczynił się do ich znacznego rozwoju; był m.in. prekursorem wyrażania rozmaitych funkcji jako sumy nieskończenie wielu składników szeregu potęgowego, np.:
W szczególności odkrył rozwinięcie za pomocą tego rodzaju szeregu funkcji arcus tangens i liczby . Jego odważne – choć, według obecnych standardów, technicznie niepoprawne – użycie szeregów potęgowych, umożliwiło mu w 1735 roku rozwiązanie słynnego problemu bazylejskiego[23]:
Euler zaczął używać w dowodach analitycznych funkcji wykładniczej i logarytmów. Znalazł sposób ujmowania różnych funkcji logarytmicznych w postaci szeregów potęgowych; udało mu się również zdefiniować logarytm dla argumentów ujemnych i zespolonych, co znacznie rozszerzyło zakres ich zastosowania w matematyce[24]. Euler zdefiniował też funkcję wykładniczą dla liczb zespolonych i odkrył relacje łączące ją z funkcjami trygonometrycznymi. Równość Eulera stwierdza, że dla dowolnej liczby rzeczywistej φ zespolona funkcja wykładnicza daje się wyrazić w postaci:
Szczególnym przypadkiem powyższej równości jest tożsamość:
nazwana przez Richarda Feynmana „najniezwyklejszym wzorem w matematyce”, który łączy w sobie znaki: dodawania, mnożenia, potęgowania, równości, a przy tym angażuje pięć najważniejszych stałych matematycznych: 0, 1, ,  oraz [25].
Euler opracował też teorię funkcji specjalnych, wprowadzając funkcję Г; zaproponował także nową metodę rozwiązywania równań czwartego stopnia. Znalazł sposób obliczania całek o granicach zespolonych, zapoczątkowując tym rozwój nowoczesnej analizy zespolonej. Dał początki rachunkowi wariacyjnemu z najbardziej znanym wynikiem tych rozważań – równaniem Eulera-Lagrange’a.
Euler był też pionierem użycia metod analitycznych w rozwiązywaniu problemów teorii liczb. Tym sposobem połączył w jedną nową dziedzinę analitycznej teorii liczb dwie zasadniczo odmienne gałęzie matematyki. Niejako przy okazji stworzył podwaliny pod teorię szeregów hipergeometrycznych, funkcji hiperbolicznych i analityczną teorię ułamków łańcuchowych. A oto przykład zastosowania przez niego metod analizy w teorii liczb: używając twierdzenia o rozbieżności szeregu harmonicznego, dowiódł nie tylko, że liczb pierwszych jest nieskończenie wiele, ale także, że szereg ich odwrotności jest rozbieżny; użył też metod analizy dla zrozumienia w jaki sposób rozmieszczone są liczby pierwsze. Prace Eulera w tej dziedzinie doprowadziły do rozwoju twierdzenia o liczbach pierwszych[26].
W wielkim zainteresowaniu, jakim Euler darzył teorię liczb, zauważalny jest wpływ znanego mu z akademii petersburskiej Krystiana Goldbacha. Wiele z wczesnych prac Eulera w tej dziedzinie matematyki bazowało na dokonaniach Piotra Fermata; między innymi, obalił hipotezę Fermata o pierwszości liczb Fermata  rozkładając 
Euler odkrył związek między teorią liczb i analizą matematyczną. Udowodnił, że suma odwrotności liczb pierwszych jest rozbieżna, przez sformułowanie wzoru Eulera, dla wprowadzonej przez siebie (dla argumentów rzeczywistych) funkcji dzeta Riemanna, w terminach liczb pierwszych.
Euler udowodnił tożsamości Newtona (wzory rekurencyjne wiążące sumy potęg wszystkich pierwiastków wielomianu z jego współczynnikami), małe twierdzenie Fermata, twierdzenie Fermata o sumie dwóch kwadratów i wniósł znaczący wkład do twierdzenia Lagrange’a o sumie czterech kwadratów. Odkrył też funkcję φ (zw. funkcją φ Eulera), przyporządkowującą każdej dodatniej liczbie całkowitej  liczbę mniejszą od  która informuje ile jest liczb względnie pierwszych z  Korzystając z właściwości tej funkcji, Euler uogólnił małe twierdzenie Fermata w postaci znanej obecnie pod nazwą twierdzenia Eulera o liczbach względnie pierwszych. Uzyskał znaczne wyniki o liczbach doskonałych, które fascynowały ludzi od niepamiętnych czasów, a co najmniej matematyków od czasów Euklidesa – podstawowe twierdzenie o parzystych liczbach doskonałych jest pewną równoważnością, dowiedzioną w jedną stronę przez Euklidesa, a w drugą przez Eulera. Duży postęp poczynił w kierunku twierdzenia o liczbach pierwszych i domyślał się istnienia prawa wzajemności reszt kwadratowych. Te dwa ostatnie pomysły są uważane za fundamentalne dla teorii liczb; utorowały drogę przyszłym pracom Karola Gaussa[27].
Przed rokiem 1772 Euler dowiódł, że liczba  jest liczbą pierwszą Mersenne’a. Pozostawała ona największą znaną liczbą pierwszą do roku 1867[28].
W roku 1736 Euler rozwiązał problem znany jako zagadnienie mostów królewieckich[29]. Królewiec w Prusach (obecnie w Rosji) leży nad rzeką Pregołą na dwóch dużych wyspach, które w czasach Eulera były połączone – wzajemnie ze sobą i ze stałym lądem – siedmioma mostami. Pytanie brzmiało: Czy można przejść kolejno przez wszystkie mosty tak, żeby każdy przekroczyć tylko raz? Odpowiedź, którą znalazł Euler, brzmi: nie; Euler dowiódł, że tzw. ścieżka Eulera przebiegająca raz i tylko raz przez wszystkie krawędzie grafu istnieje tylko wtedy, gdy liczba węzłów o nieparzystej liczbie krawędzi jest równa 0 lub 2. Rozwiązanie tego problemu jest uważane za pierwsze twierdzenie teorii grafów[29]. Euler wprowadził też oznaczenie znane dziś pod nazwą charakterystyki Eulera powierzchni i dodatkowo wzór pokazujący, że dla dowolnego wielościanu wypukłego wynosi ona 2 (twierdzenie Eulera dla wielościanów). Studia nad tym wzorem – zwłaszcza Cauchy’ego[30] i L’Huilliera[31] – i jego uogólnienie to początki topologii.
Niektóre z największych sukcesów Eulera wiążą się z użyciem metod analizy matematycznej w rozwiązywaniu problemów realnego świata; opisał liczne zastosowania: liczb Bernoulliego, szeregów Fouriera[potrzebny przypis], liczb Eulera (związanych z rozwinięciem we wzór Taylora funkcji sekans i sekans hiperboliczny), stałych – e i π, ułamków łańcuchowych i całek. Zintegrował rachunek różniczkowy Leibniza z metodą fluksji Newtona i rozwinął narzędzia, które ułatwiły obliczenia fizyczne. Uczynił wiele dla rozwoju całkowania numerycznego, odkrywając metodę znaną dzisiaj pod postacią aproksymacji Eulera. Godnymi uwagi w dziedzinie aproksymacji są: metoda Eulera i wzór Eulera-Maclaurina. Ułatwił też używanie równań różniczkowych, zwłaszcza przez wprowadzenie stałej Eulera-Mascheroniego (γ):
Jednym z najniezwyklejszych zainteresowań Eulera było stosowanie idei matematycznych w muzyce. W roku 1739 napisał Tentamen novae theoriae musicae, mając nadzieję zespolić teorię muzyki z matematyką. Ta część jego pracy nie spotkała się z nazbyt wielką uwagą – wręcz ujęcie to zostało nazwane kiedyś zbyt matematycznym dla muzyków i zbyt muzycznym dla matematyków[32].
Euler rozwinął model belki nazwany później modelem Bernoulliego-Eulera; stanowił on kamień węgielny nowoczesnej myśli inżynierskiej. Obok pomyślnego stosowania własnych narzędzi analitycznych w rozwiązywaniu problemów mechaniki klasycznej, Euler używał ich do rozwiązywania problemów astronomii sferycznej. Jego praca w dziedzinie astronomii znajdowała uznanie w paryskiej akademii, której nagrody wielokrotnie otrzymywał. Oto niektóre z astronomicznych osiągnięć Eulera: określanie z wielką dokładnością orbit komet i innych ciał niebieskich, wyjaśnienie natury komet i wyliczenie paralaksy Słońca. Jego obliczenia przyczyniły się do zwiększenia dokładności tabel długości geograficznej[33].
Euler wniósł też ważny wkład do rozwoju optyki. W swojej pracy Optyka nie zgadzał się z ówcześnie obowiązującą korpuskularną teorią światła Newtona. Jego praca z roku 1740 w dziedzinie optyki sprawiła, iż falowa teoria światła zaproponowana przez Huygensa stała się dominującym paradygmatem aż do czasu rozwinięcia kwantowej teorii światła, łączącej obydwa te podejścia[34].
Euler jest uznawany za autora użycia krzywych zamkniętych dla zilustrowania sylogistycznego rozumowania (1768). Diagramy te stały się znane pod nazwą diagramów Eulera[35].
Euler, tak jak Isaac Newton i Samuel Clarke, wierzył w absolutny czas i przestrzeń. Tym samym krytykował relacyjną koncepcję przestrzeni i czasu zaproponowaną przez Leibniza. Szwajcarski matematyk wierzył, że absolutny czas i przestrzeń są konieczne do tego, żeby prawa fizyki były wieczne i powszechne. Ten pogląd później przejął od Eulera Immanuel Kant[36].
Euler i Daniel Bernoulli byli oponentami leibnizowskiego monadyzmu i filozofii Krystiana Wolffa. Euler niezmiennie trwał przy poglądzie, że wiedza jest oparta na fundamencie określonych, ścisłych ilościowych praw – był to pogląd, którego ani monadyzm, ani rozumowanie Wolffa nie przewidywały. Wpływ na taki stosunek Eulera do doktryny Wolffa miały zapewne sympatie religijne tego pierwszego; posunął się on do określenia pomysłów Wolffa jako „pogańskich i ateistycznych”[37].
Wiele z poglądów Eulera na religię można wydedukować z Listów do księżniczki niemieckiej i jednej z jego wczesnych prac, Rettung der Göttlichen Offenbahrung Gegen die Einwürfe der Freygeister (w wolnym tłumaczeniu: Obrona Objawienia Bożego przed zarzutami wolnomyślicieli). Obie te pozycje prezentują Eulera jako zagorzałego chrześcijanina, zwolennika dosłownego traktowania tekstu Biblii (Rettung... na przykład było pierwotnie używane jako argument za boskim pochodzeniem Pisma Świętego)[38].
Do sporów Eulera w kwestiach religii toczonych ze świeckimi filozofami nawiązuje anegdota z okresu drugiego pobytu Eulera w Petersburgu. Zdarzyło się, że w tym samym czasie przebywał w Rosji na zaproszenie Katarzyny II francuski filozof Denis Diderot. Cesarzowa zaalarmowana, że argumenty jej gościa za ateizmem wpływają na członków jej dworu, poprosiła Eulera o stanięcie do konfrontacji z Diderotem. Francuz został poinformowany, że uczony matematyk opracował dowód na istnienie Boga; Diderot zgodził się na publiczne zaprezentowanie tego dowodu przez Eulera przed cesarskim dworem. W ustalonym czasie Euler przybył, skierował swe kroki ku Francuzowi i, stanąwszy przed nim, tonem całkowitej pewności siebie oznajmił: „Panie,  a więc – Bóg istnieje. Replikuj!”. Diderot, dla którego cała matematyka była bzdurą (tak w każdym razie mówi ta historia), stał osłupiały, aż salwy śmiechu wybuchnęły wśród całego dworu. Zażenowany, poprosił o pozwolenie opuszczenia Rosji, na co cesarzowa łaskawie się zgodziła. W prawdziwość tej anegdoty należy wątpić, biorąc pod uwagę fakt, że Diderot był w rzeczywistości sprawnym matematykiem[39].
Lista prac Eulera jest bardzo obszerna; poniżej niektóre z jego dzieł:
Pełna kolekcja dzieł Eulera zatytułowana Opera Omnia jest publikowana od roku 1911 przez Komisję Eulera[41] Szwajcarskiej Akademii Nauk.
Lista artykułów dotyczących pojęć o nazwach związanych z nazwiskiem Eulera.
Gottfried Wilhelm Leibniz, znany także pod nazwiskiem Leibnitz (ur. 21 czerwca?/1 lipca 1646 w Lipsku, zm. 14 listopada 1716 w Hanowerze)[1] – niemiecki polihistor: prawnik, dyplomata, historyk[2] i bibliotekarz, zajmujący się też filozofią, matematyką, fizyką teoretyczną i inżynierią mechaniczną; doktor prawa i filozofii, przez większość kariery zatrudniony na dworze Księstwa Hanoweru[3].
Leibniz jest znany głównie jako filozof i matematyk – klasyk racjonalizmu oraz jeden z pionierów analizy matematycznej, tworzący niezależnie od Isaaca Newtona.
Założył również Elektorskie Brandenburskie Towarzystwo Naukowe, potem znane pod wieloma innymi nazwami. Jedna z nich to „Pruska Akademia Nauk”; pod koniec XX wieku stała się znana jako Berlińsko-Brandenburska Akademia Nauk. Leibniz został pierwszym prezesem tego towarzystwa. Bywa zaliczany do przedstawicieli epoki oświecenia[4].
Urodził się 1 lipca 1646 r. w Lipsku[5]. Był synem profesora Uniwersytetu Lipskiego[6] Friedricha Leibniza i Catharine Schmuck. W dniu jego narodzin, ojciec zapisał w dzienniku następujące słowa:
(rozbieżność dat wynika z faktu zmiany kalendarza w Europie na przestrzeni czasu od XVI do XX wieku).
Ochrzczony został 3 lipca tego samego roku w kościele im. Mikołaja w Lipsku. Jego ojcem chrzestnym został luterański teolog Martin Geier. Ojciec Leibniza zmarł, gdy ten miał 6 lat i od tamtej pory był wychowywany jedynie przez matkę.
Friedrich Leibniz był profesorem filozofii na Uniwersytecie w Lipsku, a jego syn odziedziczył po nim całą kolekcję książek, składającą się na pokaźnych rozmiarów bibliotekę. Dzięki temu od najmłodszych lat mógł zgłębiać wysublimowaną wiedzę i czytać o wybitnych jednostkach filozoficznych. Również dzięki oczytaniu Leibniz nauczył się biegle języka łacińskiego już w wieku 12 lat, ponieważ większość publikacji była napisana w tym języku.
Jego ojciec umarł w 1652, zostawiając mu bogatą bibliotekę gdzie rozpoczął swoją naukę[5]. Wstępując na uniwersytet miał zaledwie 14 lat[7]. W grudniu 1662 ukończył studia z filozofii broniąc swojej pracy pod tytułem: Disputatio Metaphysica de Principio Individui. Tytuł magistra filozofii Leibniz otrzymał 7 lutego dwa lata później obroną Specimen Quaestionum Philosophicarum ex Jure collectarum. Następnie zdecydował się na studia prawnicze i 28 września 1665 roku otrzymał tytuł licencjata[8]. Na początku 1666 roku Leibniz napisał swoją pierwszą książkę De Arte Combinatoria, której pierwsza część stanowiła jego pracę habilitacyjną z filozofii obronioną w marcu 1666 roku. Po tym osiągnięciu zdecydował się na kontynuację studiów prawniczych, a jego celem było uzyskanie stopnia doktora, jednak Uniwersytet w Lipsku odrzucił jego wniosek i odmówił przyznania mu doktoratu z prawa, najprawdopodobniej ze względu na jego młody wiek[9].
Wyjechał zatem z rodzinnego miasta i zapisał się na Uniwersytet w Altdorfie, gdzie w listopadzie 1666 roku otrzymał doktorat z prawa. Odrzucił natomiast propozycję mianowania akademickiego, tłumacząc: „moje myśli skierowały się w zupełnie innym kierunku”[10].
Jego pierwszym etatowym stanowiskiem był sekretarz towarzystwa alchemicznego w Norymberdze[11]. Następnie Johann Christian von Boyneburg zatrudnił go jako swojego asystenta. W 1669 roku Leibniz został mianowany asesorem Sądu Apelacyjnego. Pełnił tę funkcję do roku 1674.
Po powrocie do rodzinnego Lipska, przez jakiś czas pracował na uniwersytecie, ale praca ta nie satysfakcjonowała go. Został wysłany na dwór Ludwika XIV (którego usiłował skłonić do wyprawy do Egiptu oraz do Londynu). Swój pobyt w Paryżu (1672-1676) wykorzystał także dla spraw naukowych, nawiązując kontakty z francuskimi uczonymi. Spotkał wtedy Christiaana Huygensa, dzięki któremu, uświadomił sobie, ze jego wiedza z matematyki i fizyki mogłaby być na wyższym poziomie. Na ten czas przypada wynalezienie przez niego rachunku różniczkowego i całkowego, ogłoszonych w rozprawach: „Acta eroditorum” (1684)[5] i „De geometria recondita et analysi indivisibilium atque infinitorum” (1686).
Gdy książę Hanoweru, Jerzy Ludwik, rozpisał konkurs na swojego osobistego asystenta, Leibniz zdecydował się wziąć w nim udział. Po jego wygraniu stał się wieloletnim asystentem księcia, którą to funkcję sprawował aż do śmierci. Od roku 1677 sprawował obowiązki bibliotekarza, doradcy prawnego i historiografa przy dworze hanowerskim. W tym okresie napisał: „Codex iuris gentium diplomaticus” (1693–1700), „Accesiones historicae” (1698–1700), „Scriptores rerum Brunsvicensium illustrationi inservientes” (1707–1711). Pełnił też rolę nauczyciela dzieci księcia oraz jeździł po całej Europie z tajnymi misjami dyplomatycznymi. Dzięki licznym podróżom i wizytom na dworach całej Europy miał okazję poznać większość ważniejszych filozofów i naukowców swoich czasów.
Wolny czas poświęcał na samokształcenie i doskonalenie swojej wiedzy z zakresu matematyki, logiki, filozofii, rachunku różniczkowego, historii, teologii i dyplomacji.
Przyjaźnił się z Baruchem Spinozą. Jego wielkimi adwersarzami byli między innymi:
Stosunki z misjami jezuickimi wykorzystał dla studiów etymologicznych („Collectanea etymologica”, 1717). Pracował również nad pojednaniem Kościoła katolickiego i protestanckiego („Systema theologicum”, napis. 1686, wyd. 1820). Wpływu swego na królową Zofię Karolinę użył przy zakładaniu Pruskiej Akademii Nauk (1700), której był pierwszym prezesem. Według jego planów Piotr Wielki założył w 1724 roku podobną Akademię w Sankt Petersburgu. Następnie Leibniz przebywał w Wiedniu, gdzie dla księcia Eugeniusza napisał „Principes de la Nature et de la Grâce fondés en Raison – Monadologie”. Następnie powrócił do Hanoweru.
Swej filozofii nie ujął w jednym systematycznym dziele. Z jego dzieł filozoficznych najważniejsze są: „Essai de theodicée sur la bonté de Dieu, la liberté de l’homme et l’origine du mal” (1710), „Monadologie” (1721) i „Nouveaux essais sur l’entendement humain” (1704).
Zmarł w Hanowerze w 1716 roku[5], przeżywszy 70 lat. Na jego pogrzeb nie przybył żaden dworzanin, poza jego osobistym sekretarzem[5]. Mimo iż Leibniz był członkiem Towarzystwa Królewskiego oraz Berlińskiej Akademii Nauk, żadna organizacja nie uznała za stosowne, aby uhonorować jego pogrzeb. Grób Leibniza był nieoznakowany przez ponad 50 lat. Swój życiowy dorobek zostawił swojemu jedynemu spadkobiercy – pasierbowi swojej siostry, ponieważ nigdy się nie ożenił, a także nie miał swoich dzieci.
W dziedzinie filozofii Leibniz zajmował się zarówno:
Jest jednym z czołowych przedstawicieli XVII-wiecznego racjonalizmu[12], kontynuującym prace Kartezjusza i Spinozy, jak również wcześniejszą tradycję scholastyczną – przez systematyczne stosowanie pierwszych zasad. Jego system filozoficzny – zwany czasem monadologią – był jednym z filarów późniejszej filozofii Kanta, zalążkiem XIX-wiecznej logiki matematycznej i późniejszej logiki modalnej, a także wzorcem dla XX-wiecznej filozofii analitycznej i programu Hilberta. Jego filozofia umysłu bywa uznawana za panpsychiczną. W filozofii religii Leibniz zajmował się między innymi teodyceą – próbą rozwiązania teologicznego problemu zła. Nie tylko bronił koncepcji Stwórcy przed tym kontrargumentem, ale również próbował ją udowodnić przez aprioryczne argumenty jak ontologiczny i kosmologiczny[potrzebny przypis], zadając słynne pytanie: dlaczego istnieje raczej coś niż nic? Był również prekursorem ruchu ekumenicznego – jako luteranin popierał zbliżenie do katolicyzmu.
Filozofia Leibniza stoi w ścisłym związku z naukowymi badaniami jego czasów.
Jego myślenie filozoficzne wydaje się być fragmentaryczne, ponieważ składa się z wielu pojedynczych artykułów i tekstów, a nie jednego spójnego dzieła. Napisał jedynie dwa pełnowymiarowe traktaty filozoficzne, z czego tylko Teodycea z 1710 roku została opublikowana za jego życia.
W przeciwieństwie do Spinozy, u którego jednostka rozpływa się w jednej boskiej substancji, ogarniającej cały świat, Leibniz jest przeniknięty poczuciem odrębności i wartości jednostki. Świat jest dla niego zbiorowiskiem „monad”, to jest indywidualnych jednostek siły. Jest ich nieskończenie wiele i różnią się między sobą stopniem doskonałości, tak, że nie ma dwóch monad jednakowych. Organizmy są zbiorami monad (np. nasze ciało), a dusza jest tylko monadą centralną.
Leibniz, pomimo wielkiego podziwu dla Spinozy, był przerażony niektórymi jego wnioskami, szczególnie gdy te nie były zgodne z doktryną chrześcijańską.
Monady są niezależne od zewnętrznych wpływów (jak powiada Leibniz, nie mają drzwi ani okien) i na siebie nawzajem nie oddziałują, zaś zgodność pomiędzy poszczególnymi monadami jest wynikiem harmonii przedustawnej (harmonia praestabilita), wprowadzonej przez Boga. Tak jak dwa zegary idą zgodnie, choć na siebie nie oddziałują, bo tak wyregulował je zegarmistrz, tak też przedstawienia i wrażenia (bo na nich polega życie monady) odpowiadają przedstawieniom innych monad nie dlatego, że te na nie oddziałują, ale że Bóg tak urządził, że w chwili, gdy monada A ma przedstawienie a, to monada A1, zna odpowiednie przedstawienie a1.
Świat ten w ten sposób urządzony jest najlepszy z możliwych. Gdyby był możliwy świat lepszy, to Bóg wiedziałby o tym dzięki swej mądrości, a jego dobroć skłoniłaby go do urzeczywistnienia tego lepszego świata.
Leibniz odwoływał się także do siedmiu fundamentalnych zasad filozoficznych:
Leibniz zastanawiał się nad tym, jak poprawnie zdefiniować prawdę. Popularna, intuicyjna definicja głosi, że wypowiedź jest prawdziwa, gdy jej zawartość jest adekwatna do rzeczywistości. Adekwatność jest w istocie rodzajem logicznej relacji między wypowiedzią a stanem rzeczywistości. Skoro jest to relacja logiczna, to tak jak wszystkie inne relacje logiczne powinna ona być niezmienna i zależna tylko od zawartości wypowiedzi i „zawartości” rzeczywistości.
Jeśli przyjmie się to założenie, to relacja „adekwatności” staje się klarowna. Dana wypowiedź jest prawdziwa (adekwatna), jeśli w przedmiocie (obiekcie) tego zdania jest zawarte jego orzeczenie (jakaś cecha lub zdarzenie związane z obiektem). Leibniz nazwał tę zasadę regułą koniecznej przyczyny prawdy. Zdaniem Leibniza odrzucenie reguły koniecznej przyczyny prawdy prowadzi do nieuniknionych sprzeczności logicznych.
Tak samo jak Kartezjusz i Spinoza, Leibniz rozwija swoją teorię bytu w formie teorii substancji[16]. Z reguły koniecznej przyczyny wynika, że każdy poszczególny byt musi przez cały czas zawierać całą prawdę o sobie, a zatem jest w sensie absolutnym niezmienny. Nie można z zewnątrz wpłynąć na jego strukturę ani na jego dzieje, bo gdyby można było, nie zawierałby on w którymś momencie całej prawdy o sobie. A zatem świat składa się z bliżej nieustalonej, ale ogromnej liczby całkowicie od siebie odseparowanych i nie wpływających na siebie bezpośrednio bytów, z których każdy jest „całym światem dla siebie samego”. Te poszczególne byty Leibniz nazwał monadami. Dla Leibniza monadą było automatycznie wszystko, co dawało się wyróżnić jako osobny byt[17]. Leibniz stwierdził: „Nie ma nic oprócz monad, albo inaczej – wszystko co istnieje musi być monadą, czyli osobnym bytem, który zawiera w sobie całą prawdę o sobie”.
Monady są zatem „ostatecznymi jednostkami istnienia w przyrodzie”. Mogą się one zmieniać z czasem i wykazywać inne niż na początku cechy, jednak każda monada jest niezmiennie wyjątkowa. Są one ośrodkiem siły, podczas gdy przestrzeń, materia i ruch są jedynie zjawiskami. W przeciwieństwie do atomów, monady nie mają charakteru przestrzennego ani materialnego, są niezależne, a interakcje między nimi są jedynie pozorne.
Wpływ na Leibnitza miały poglądy Anne Conway, angielskiej filozofki racjonalistycznej[18][19].
Na pierwszy rzut oka koncepcja monad stoi w ostrej sprzeczności z naszym codziennym doświadczeniem, w którym obserwujemy, że jedne rzeczy wpływają na drugie i następują ciągłe zmiany. Leibniz uważał, że te obserwacje są swoistym złudzeniem. Złudzenie to powstaje na skutek tego, że monady nie tworzą przypadkowej mozaiki bytów, lecz istnieje rodzaj „praporządku”, ogólnej struktury wszystkich monad.
Aby to wyjaśnić, Leibniz podawał przykład dwóch dobrze wyregulowanych zegarów stojących w jednym pokoju, które pokazują czas przesunięty o ułamek sekundy. Ktoś nie znający zasady działania zegara, obserwując, że cały czas gdy sekundnik pierwszego zegara wykona jednosekundowe drgnięcie wskazówki, to zaraz za nim drugi zegar robi to samo. Mógłby wyciągnąć z tego wniosek, że ruch pierwszego zegara jest przyczyną – powoduje ruch drugiego. W rzeczywistości jednak oba zegary mają zupełnie niezależne mechanizmy, tyle że ich działanie ktoś dobrze skoordynował. Tak samo jest ze wszystkimi monadami – działają one wspólnie i wydaje się, że wpływają one na siebie wzajemnie, bo ktoś je idealnie „wyregulował” umieszczając w każdej komplementarną do innych monad prawdę o nich samych, warunkujących ich działanie.
Dla Leibniza to rozumowanie było swoistym dowodem na istnienie Boga. Skoro monady są tak ze sobą świetnie skoordynowane, że u każdej z nich wywołuje to wrażenie ciągłości i logiczności wszelkich zdarzeń, to nie może to być przypadkowe i ktoś to musiał celowo uczynić. Tym kimś jest Bóg. Bóg też jest monadą – ale monadą szczególną – będącą ostateczną przyczyną i celem istnienia pozostałych monad.
Dowód na istnienie Boga Leibniz zawarł w Théodicée[20], gdzie doszedł do wniosku, że pierwszą racją wszystkich rzeczy jest Bóg, którego nazwał monas monadum.
Z koncepcji praporządku świata wynika najsłynniejsza teza Leibniza. Skoro monady są preuporządkowane przez Boga, to wszystkie zależności między nimi są koniecznościami wynikającymi z tego preuporządkowania. Poszczególne monady mogą mieć złudzenie, że wpływają na swój los, „robiąc” to lub owo, lecz w rzeczywistości są one „zaprogramowane” do tych działań i nie mogą uczynić nic innego. A zatem istniejący świat jest jedynym możliwym, wynikającym z boskiego preuporządkowania. Każda monada zawiera w sobie instrukcje, które kierują nią i pozwalają na robienie czegoś w określony sposób.
Można sobie co prawda wyobrazić inaczej preuporządkowany świat, ale z pewnością nie byłby on już tak doskonały, gdyż Bóg jako monada „naczelna”, wszechmocna i wszechwiedząca, na pewno preuporządkował świat najlepiej, jak było można. A zatem nasz świat jest jednocześnie jedynym możliwym i najdoskonalszym ze wszystkich światów do pomyślenia, ponieważ Bóg nie stworzyłby świata niedoskonałego.
Leibniz twierdził, że prawdy filozoficzne i religijne nie mogą sobie zaprzeczać, ponieważ rozum i wiara, są darami Bożymi, także ich wzajemne wykluczanie byłoby oznaką, ze Bóg walczy sam przeciwko sobie. Zatem, ponieważ te dwa elementy musza być zawsze ze sobą zgodne, każda zasada wiary, której rozum nie może obronić, powinna zostać odrzucona. Następstwem takiego podejścia, było odniesienie się do jednego z głównych zarzutów wobec chrześcijaństwa – dlaczego skoro Bóg jest wszechmogący i najwspanialszy, to jakim cudem zło przyszło na świat? Leibniz na to pytanie odpowiadał jednoznacznie: Bóg jest oczywiście nieograniczony w swej mocy i sile, jednak stworzone przez niego byty ludzkie takiej siły nie posiadają i są ograniczone z wielu stron, toteż maja skłonność do fałszywych osądów i podejmowania błędnych decyzji. Bóg nie zadaje ludziom cierpienia, jednak dopuszcza zarówno zło moralne (grzech), jak i zło fizyczne (ból) jako konieczne konsekwencje zła metafizycznego (niedoskonałości). Jest to ośrodek monitorowania i sprawdzania przez ludzi czy ich poczynania są dobre czy złe, dzięki czemu mogą korygować i naprawiać swoje błędy. W ten sposób Leibniz łączy swoją wiedzę filozoficzną z wiarą chrześcijańską, tworząc byt nierozerwalny. Takie podejście było motywowane przede wszystkim głęboką wiarą. Twierdził on także, że natura ludzka jest doskonała sama w sobie, gdyż stworzona została przez Najwyższego.
Ponadto, chociaż ludzkie działania wynikają z uprzednich przyczyn, które ostatecznie powstają w Bogu i dlatego są znane Bogu jako metafizyczne pewniki, wolna wola jednostki funkcjonuje w ramach praw naturalnych, gdzie wybory są jedynie przypadkowym elementem, o których decyduje „cudowna spontaniczność”, która zapewnia jednostkom ucieczkę od rygorystycznej predestynacji.
Konsekwencją teorii monad było też to, że czas i przestrzeń nie istnieją w sensie absolutnym, lecz są złudzeniami, jakie miewają poszczególne monady – a ściśle biorąc, są tylko formą niezmiennych, logicznych zależności, jakie występują między monadami. Skoro bowiem każda z monad zawiera w sobie całą prawdę o sobie samej, to w sensie absolutnym są one niezmienne i w każdej dowolnej chwili jednakowe.
Skoro istnieją tylko monady i są one w każdej chwili takie same, to poszczególne chwile nie różnią się między sobą. Następuje tylko zmiana logicznych relacji między monadami, wynikająca z ich wewnętrznej, preuporządkowanej dynamiki, która tworzy wewnątrz monad świadomych swojego istnienia złudzenie występowania logicznego ciągu zdarzeń nazywanego czasem.
Podobnie jest z przestrzenią – istnieją tylko monady, między którymi istnieją logiczne zależności wynikające z ich preuporządkowania. Między monadami niczego nie ma, bo i być nie może – przestrzeń to tylko złudzenie tworzone przez szczególny rodzaj preuporządkowania – mianowicie preuporządkowanie geometryczne.
Innymi słowy Leibniz uznawał relacyjną koncepcję przestrzeni i czasu, krytykując poglądy Newtona[21].
Myśl symboliczna
Leibniz uważał, że ludzkie postępowanie można sprowadzić do pewnego rodzaju działania matematycznego i w ten sposób rozwiązać wiele problemów:
Uważał, ze symbole są ważne dla ludzkiego zrozumienia. Zamiłowanie Leibniza do symboli oraz przekonanie, że są one niezbędne do funkcjonowania logiki i matematyki, uczyniły z niego prekursora semiotyki[22].
Logika
Został uznany za jednego z najważniejszych logików między Arystotelesem, a Gottlobem Frege[23]. Leibniz przedstawił główne założenia, między innymi tego, co obecnie nazywamy koniunkcją, negacją czy tożsamością. Zasady logiki według niego sprowadzają się przede wszystkim do dwóch punktów:
Za jego życia nie został opublikowany żaden tekst na temat logiki. Większość z notatek to jedynie brudnopisy i wersje robocze.
Wiele tekstów Leibniza jest obecnie komentowana i ponownie analizowana, ze względu na jego możliwe odkrycia, które nie zostały udowodnione wcześniej.
W matematyce wsławił się nie tylko przez projekt matematyzacji logiki. To przede wszystkim pionier analizy – konkretniej rachunku różniczkowo-całkowego – tworzący równolegle do Newtona i prawdopodobnie niezależnie od niego. Perspektywa Leibniza na analizę była inspiracją dla Abrahama Robinsona do stworzenia analizy niestandardowej opartej na liczbach hiperrzeczywistych[24]. Miał też wkład do algebry i geometrii[potrzebny przypis].
Jako inżynier i wynalazca był twórcą jednego z pierwszych kalkulatorów mechanicznych – prototypu późniejszych arytmometrów, zwanego ławą liczącą. Leibniza można uznać za jednego z wizjonerów i inspiratorów informatyki – wierzył, że można zautomatyzować nie tylko obliczenia arytmetyczne, ale dzięki odpowiedniemu językowi (kodowaniu) także przetwarzanie innych informacji, inne wnioskowania i rozstrzyganie ogólnych problemów, nie tylko matematycznych.
W fizyce zajmował się mechaniką klasyczną, antycypując pojęcie energii kinetycznej, zasadę zachowania energii i wariacyjną zasadę stacjonarnego działania[potrzebny przypis]. Jego relacyjna koncepcja przestrzeni i czasu jest ideą filozoficzną, jednak znalazła odzwierciedlenie w ściśle fizycznych postulatach jak zasada Macha. 
Przyczynił się w znacznym stopniu do dyskusji na temat statyki i dynamiki, nie zgadzając się z Kartezjuszem i Newtonem. Opracował nową teorie ruchu opartą na energii kinetycznej i energii potencjalnej, która zakładała, że przestrzeń jest względna, podczas gdy Newton był głęboko przekonany, że przestrzeń jest absolutna. Ważnym przykładem zaangażowania Leibniza w zakres fizyki jest jego Specimen Dynamicum (1695).
Twierdząc, że Ziemia ma stopniowy rdzeń, przewidział współczesne teorie geologii.
Wydaje się niedocenionym pionierem psychologii[25]. W swoich rozważaniach podejmował tematy, które obecnie stanowią ważny element nauk psychologicznych. Pisał między innymi o: uwadze i świadomości, pamięci, uczeniu się, kojarzeniu, motywacji, indywidualności.
W dziedzinie zdrowia publicznego postulował utworzenie medycznego organu administracyjnego, mającego kompetencje z zakresu epidemiologii i weterynarii. W polityce gospodarczej proponował reformy podatkowe i narodowy program ubezpieczeń. W socjologii przyczynił się po części do późniejszej teorii komunikacji.
W pracy Wybór króla w Polsce[26] (inny tytuł Wzorzec dowodów politycznych) (1659) rozważał problematykę właściwych rządów w Rzeczypospolitej Obojga Narodów[27]. 
Leibniz bardzo sumiennie studiował języki. Interesowało go słownictwo i gramatyka. Obalił przekonanie, szeroko rozpowszechnione w jego czasach przez chrześcijańskich uczonych, że hebrajski był pierwotnym językiem rodzaju ludzkiego. Zastanawiał się także nad pochodzeniem języków słowiańskich. Interesował się tradycyjnym językiem chińskim. Był znawcą języka sanskryckiego.
Ze współczesnych Leibnizowi filozofów największymi jego krytykami byli Wolter oraz Isaac Newton. Mimo to sporo słabo zrozumianych za jego życia koncepcji zawartych w teorii monad było potem rozszerzanych i pogłębianych przez późniejszych filozofów. David Hume rozwinął wątek niemożności rozróżnienia zachodzenia relacji przyczynowo-skutkowych od przypadkowych następstw czasowych zdarzeń, zaś Immanuel Kant rozwinął teorię, według której czas i przestrzeń nie są obiektywnymi cechami świata materialnego, tylko wewnętrzną formą poznania.
Z Leibnizem korespondował m.in. polski uczony Adam Adamandy Kochański. Dyskutowali zarówno sprawy naukowe, jak i polityczne[28].
Polskojęzyczne
Anglojęzyczne
 Artykuły na Internet Encyclopedia of Philosophy (ang.) [dostęp 2018-08-07]:
 Artykuły na Stanford Encyclopedia of Philosophy (ang.) [dostęp 2018-08-07]:
Konfucjusz (chiń. 孔子 / 孔夫子; pinyin Kǒng Zǐ / Kǒng Fūzǐ; dosł. „Mistrz Kong”; ur. 551 p.n.e. w Qufu, zm. 479 p.n.e. tamże) – chiński filozof, twórca konfucjanizmu.
Konfucjusz nie pozostawił pism – jego nauczanie zebrano w księdze Dialogi konfucjańskie. Głównym tematem rozważań było codzienne życie ludzi i stabilność społeczeństwa. Jego zdaniem podstawowymi cnotami są: humanitaryzm, praworządność, poprawność, mądrość i lojalność. Cnoty te realizują się w pięciu powinnościach, czy też fundamentalnych relacjach społecznych: między panującym i urzędnikiem, ojcem i synem, starszym i młodszym bratem, mężem i żoną oraz między przyjaciółmi. Moralność jednostki i podstawowa rola rodziny jest fundamentem i gwarantem stabilności państwa. Konfucjusz dużą rolę w kształtowaniu ładu społecznego przypisywał wykształceniu, które obejmowało także kwestie ducha i serca.
Doktryna Konfucjusza (konfucjanizm) była rozwijana początkowo przez uczniów, a później przez filozofów-następców i komentatorów (Mencjusz, Xunzi, neokonfucjanizm). Myśl konfucjańska objęła zasięgiem obszar Chin, Japonii, Korei i Wietnamu.
Ród, z którego wywodził się Konfucjusz był według tradycji spauperyzowaną gałęzią rodu dawnych władców Shang. Jego ojciec, Shuliang He (叔梁紇), nosił jednak pospolite nazwisko Kong i służył jako żołnierz oraz urzędnik wojskowy. Konfucjusz urodził się jako dziecko z drugiego małżeństwa, zawartego przez Shulianga He w wieku ponad pięćdziesięciu lat w celu spłodzenia zdrowego, męskiego potomka (z pierwszego małżeństwa miał wyłącznie dziewięć córek i jednego kalekiego syna). Ojciec Konfucjusza umarł kiedy syn miał trzy lata. Kontrowersyjny charakter jego związku skazał matkę przyszłego filozofa oraz samego Konfucjusza na życie w odcięciu od rodziny[1].
Biografia Konfucjusza zawarta jest w 77. rozdziale „Shiji” (Zapiski Historyczne). Na imię miał Qiu (丘), czyli „pagórek”, prawdopodobnie z powodu wypukłego czoła, według wierzeń Chińczyków, znamionującego inteligencję. Zwany był też Zhongni (仲尼). Urodził się w państwie Lu (obecnie prowincja Shandong). Wychowywała go samotnie matka, poznał biedę i trudy życia. Dzieciństwo spędził w Qufu pod Yanzhou, gdzie od małego musiał fizyczną pracą zarabiać na siebie i swoją matkę.
Mając 15 lat podjął naukę, którą sam opłacał wytężoną pracą. Posiadając początkowo jedynie umiejętność strzelania z łuku, był nadzorcą stad owiec i wołów. W wieku 19 lat ożenił się i rozpoczął karierę urzędniczą. Zdobył sławę człowieka niezwykle uczonego, miał znać na pamięć klasyczną „Księgę Pieśni”. Był kolejno naczelnikiem spichlerzy, nadzorcą pól książęcych i ludzi doglądających zwierząt ofiarnych.
W 530 p.n.e. założył szkołę, w której uczono pisma, zasad zachowania się i podstawowej wiedzy. Około 517 p.n.e. spotkał się z Laozi. Po wybuchu buntu w państwie Lu musiał uciekać do sąsiedniego państwa Qi. Po powrocie do Lu został zarządcą u następnego władcy, a później jego bliskim doradcą. Naprawianie państwa rozpoczął od reformy uciążliwego systemu podatkowego (był zwolennikiem niskich podatków). Domagał się powierzenia przywództwa „ludziom szlachetnym i uczonym”, bez względu na ich pochodzenie.
Od 498 p.n.e. jego uczniowie zaczęli obejmować najwyższe stanowiska w rządzie. Około 497 p.n.e. wprowadził politykę burzenia murów miast buntowniczych rodów, co wywołało ponowną rewoltę i zmusiło go do opuszczenia księstwa. W towarzystwie kilku uczniów przez 12 lub 14 lat błąkał się po sąsiednich państwach, starając się o posadę u różnych władców, nie znajdując jednak zrozumienia. Nie był zręcznym politykiem, a rozmówców ponoć zniechęcał wrodzoną wyniosłością. W tym czasie często wręcz przymierał głodem.
W 483 p.n.e. wrócił do Lu i zadowolił się pozycją nauczyciela. Prawdopodobnie nie zgromadził za życia wielu uczniów, choć niektóre źródła konfucjańskie wyolbrzymiają ich liczbę nawet do 3000. Według legendy gdy umarł uczniowie opłakiwali go przez 3 lata, a najwierniejszy, Zigong, spędził na grobie mistrza 6 lat, twierdząc:
Pochowany został w Qufu, gdzie do dziś znajduje się jego grób. Rośnie w tamtym miejscu las pokrywający obszar gruntu o średnicy ponad 8 kilometrów, który według legendy wyrósł z kilku drzew zasadzonych przez uczniów Konfucjusza w hołdzie jego pamięci[2]. Proste, niedogmatyczne maksymy Konfucjusza są wciąż popularne w Chinach i innych krajach Dalekiego Wschodu. Twierdził, że zmysł moralny u człowieka jest odpowiednikiem kosmicznego porządku natury. W swej nauce w ogóle nie odwoływał się do sił nadprzyrodzonych, jako jedyny z twórców wielkiej doktryny nie twierdził, że pochodzi ona z jakiegokolwiek boskiego źródła czy też natchnienia niedostępnego dla innych ludzi. Konfucjanizm w pierwotnej wersji nie jest właściwie religią, lecz systemem etycznym, nauką tego, jak być dobrym, społecznie wartościowym człowiekiem, aczkolwiek od samego początku zawiera w sobie wiele pierwiastków metafizycznych. Sam Konfucjusz podejrzliwie odnosił się do religii, gdyż widział w niej niebezpieczny zabobon. Gdy był bliski śmierci nie pozwolił, by się za niego modlono. Urzekał ludzi swą uczciwością, prawością i nauczycielskim entuzjazmem.
Ponad 70 jego bezpośrednich uczniów zostało wybitnymi uczonymi chińskimi. Przypisuje mu się zredagowanie wielu klasycznych ksiąg, które składają się na tzw. „klasykę konfucjańską”: „Pięcioksiąg konfucjański” (do którego należy też "Księga Przemian": Yijing) oraz „Czteroksiąg konfucjański” (do którego należy również „Księga Mencjusza” ). Zbiór tych dzieł do 1905 roku stanowił podstawę egzaminów dla kandydatów do służby państwowej w Chinach. Starsze źródła upatrywały w nim również autora Yijing lub komentarzy do tej księgi „Dziesięć Skrzydeł”.
Nie sposób dziś ustalić, jaki miał rzeczywiście wpływ na ostateczny kształt tych tekstów. Niektórzy wręcz twierdzą, że nigdy osobiście niczego nie napisał. Choć za życia jako polityk i reformator poniósł porażkę, kolejne pokolenia jego zwolenników rozpropagowały jego idee w całych Chinach. Ostatecznie w II wieku n.e. konfucjanizm został uznany w Chinach za ideologię państwową, a jego samego uhonorowano tytułem Największego Mędrca-Nauczyciela. Ok. 175 roku n.e. jego maksymy wykuto na kamiennych tablicach, których fragmenty zachowały się do dziś. W 555 wydano dekret cesarski nakazujący wybudowanie świątyni ku czci filozofa w każdym mieście, będącym siedzibą władz okręgu. W 637 rozpoczęto umieszczanie jego wizerunków w szkołach. Za panowania dynastii Tang do wyżej wspomnianego tytułu Największego Mędrca-Nauczyciela dodano tytuły Najwyższego Mistrza (665), Pierwszego Świętego (1013), a także honorowy tytuł królewski (739). Kolejnym ważnym wydarzeniem w historii budowania jego kultu było nadanie mu, za panowania dynastii Song, honorowego tytułu cesarskiego (1048). Panowanie dynastii Yuan i Ming przyniosło proklamowanie go Wielkim Nauczycielem Narodu oraz Najdoskonalszym, Najprzenikliwszym, Najcnotliwszym Nauczycielem.
Zajmował się zagadnieniami etycznymi, społecznymi, a nawet politycznymi. 
Był tradycjonalistą stojącym na straży szeroko pojętej tradycji.
Nauczał nienagannych manier i użyteczności dla państwa i społeczeństwa.
Uważał, iż z każdej nazwy wynika coś, co tworzy istotę rzeczy, do której dana nazwa się odnosi. Rzeczy więc powinny pozostawać w zgodzie z tą idealną istotą. Czyli rzeczywistość powinna odpowiadać nazwie ją określającą.
Z każdej nazwy, określającej stosunki społeczne, wynikają konkretne obowiązki i odpowiedzialność. Władca, minister, ojciec i syn - osoby noszące te nazwy, muszą spełniać obowiązki wynikające z nazwy.
W zakresie cnót podkreślał:
Konfucjusz często nauczał o człowieku ren (humanitarnym), który posiada wiele cnót połączonych w sobie, wtedy można humanitarność rozumieć jako cnotę doskonałą. W praktyce humanitarności muszą być względy dla innych.
W praktyce humanitarności Konfucjusz wyróżnia:
Z idei prawości Konfucjusz wyprowadza zasadę działania bez nagrody, ponieważ każdy człowiek ma obowiązki, które musi wypełniać. Wartość wynikająca z działań nie leży w efekcie, lecz w podjętym wewnętrznym wysiłku.
Należy działać wypełniając obowiązki, a efekty powierzyć woli Niebios, losowi, przeznaczeniu (ming). Rozumieć ming znaczy rozumieć nieuniknioność świata i w związku z tym nie zwracać uwagi na zewnętrzne porażki lub powodzenia, a działać i wypełniać swoje obowiązki.
Efektem takiego działania jest szczęście, ponieważ jesteśmy wolni od niepokoju o powodzenie naszych działań i od lęku przed porażką.
Karl Marx (pol. Karol Marks[a]; ur. 5 maja 1818 w Trewirze, zm. 14 marca 1883 w Londynie) – niemiecki filozof pochodzenia żydowskiego, socjolog, ekonomista, historyk, dziennikarz i działacz rewolucyjny. Twórca socjalizmu naukowego, współzałożyciel I Międzynarodówki.
Pochodził ze zasymilowanej rodziny żydowskiej, pochodzenia aszkenazyjskiego. Jego dziadek Meier Halewi Marx był rabinem w Holandii, pełniąc tę funkcję od 1723 roku[4]. Jako pierwszy z rodziny świecką edukację otrzymał Heinrich Marx, ojciec Karla. Heinrich, aby uciec od antysemickich ograniczeń prawnych, z judaizmu przeszedł na dominujący w Niemczech luteranizm, zmieniając wówczas imię z pochodzącego z języka jidysz Herschela na niemieckiego Heinricha[5].
Ojciec Karla popierał liberalizm klasyczny i zainteresował się ideami filozofów Imanuela Kanta oraz Woltera. Brał udział w agitacji na rzecz wprowadzenia w Prusach konstytucji i reform, które ograniczyłyby monarchię absolutną[6]. W 1815 roku rozpoczął karierę adwokacką, a w 1819 roku wraz z rodziną wprowadził się do dziesięciopokojowego domu przy Simeongasse (obecnie Simeonstrasse 8) w pobliżu Porta Nigra[7]. Jego żona, Henrietta Pressburg była holenderską Żydówką, która pozostała przy judaizmie. Henrietta była bardzo opiekuńcza i religijna, co nie pozostało bez wpływu na jej dzieci[8].
Karl urodził się 5 maja 1818 roku w Trewirze, w budynku 664 przy Brückergasse[9]. W 1928 roku lokal został zakupiony przez Socjaldemokratyczną Partię Niemiec, a współcześnie mieści się tam muzeum poświęcone Marksowi[10].
Niewiele wiadomo o dzieciństwie Karla[11]. Był trzecim z dziewięciorga dzieci, najstarszym synem stał się, gdy w 1819 roku zmarł jego brat Moritz[12]. Karl w sierpniu 1824 roku został ochrzczony w Kościele luterańskim wraz z rodzeństwem: Sophie, Hermannem, Henriette, Louise, Emilie i Karoline[13]. Do 1830 roku miał prywatnego nauczyciela, kiedy rozpoczął naukę w nowo utworzonej szkole gimnazjalnej, której dyrektorem był przyjaciel jego ojca Hugo Wyttenbach. Zatrudniał on jako nauczycieli wielu liberalnych humanistów, co rozgniewało konserwatywny rząd. W 1832 roku policja zorganizowała nalot na szkołę i znalazła na miejscu literaturę głoszącą liberalizm. Władze zreorganizowały szkołę i wymieniły kilku pracowników[14].
W wieku 17 lat, w październiku 1835 roku, rozpoczął studia na Uniwersytecie w Bonn. Studiował filozofię i literaturę (choć jego ojciec nalegał na Karla, aby ten rozpoczął studia prawnicze)[15]. Kontynuował naukę, gdy udało mu się uniknąć służby wojskowej[16]. W Bonn dołączył do Klubu Poetów, grupy monitorowanej przez policję ze względu na radykalne poglądy polityczne niektórych uczestników[17]. Młody Marx dołączył też do klubu Landsmannschaft der Treveraner, którego został współprzewodniczącym[18]. W sierpniu 1836 roku odbył pojedynek z członkiem Korpusu Borussii[19]. Początkowo jego oceny na Uniwersytecie były dobre, ale stopniowo się pogarszały. Jego ojciec postanowił przenieść go na cieszący się lepszą renomą Uniwersytet Berliński[20].
W lecie 1836 roku zaręczył się z Jenny von Westphalen, którą znał od wczesnej młodości. Odtąd spoważniał i bardziej odpowiedzialnie podchodził do nauki i pracy[21]. Ich związek budził pewne kontrowersje ze względu na różnice pochodzenia religijnego i klasowego, Marx został jednak zaakceptowany przez ojca Jenny, liberalnego arystokratę Ludwiga von Westphalena[22].
W październiku 1836 roku przybył do Berlina, gdzie wynajął pokój przy Mittelstraße. W czasie studiów w tym mieście zainteresował się filozofią niedawno zmarłego Georga Wilhelma Freidricha Hegla. W 1837 roku, w czasie rekonwalescencji w Stralau, dołączył do Klubu Doktora (Doktorklub), którego członkowie dyskutowali filozofię Hegla, związał się z radykalnym nurtem interpretacji Hegla, tzw. młodoheglistami. Najważniejszymi jego przedstawicielami byli m.in. Ludwig Feuerbach, Bruno Bauer i Adolf Rutenberg, którego Marx poznał osobiście i z którym się zaprzyjaźnił. Młodohegliści odrzucili metafizyczne założenia Hegla, lecz przyjęli jego metodę dialektyczną, którą zastosowali w krytyce społeczeństwa, polityki i religii z lewicowego punktu widzenia[23]. W maju 1838 zmarł ojciec Marxa, z którym łączył go silny związek emocjonalny[24][25].
W 1837 roku zakończył pisanie krótkiej powieści komediowej Skorpion i Felix, napisał też kilka wierszy miłosnych poświęconych Jenny, które zostały opublikowane[26]. Porzucił pisarstwo i zajął się nauką języka angielskiego i włoskiego, studiami historii sztuki i tłumaczeniem klasyków łacińskich[27]. Napisał rozprawę doktorską pod nazwą Różnice między demokrytejską a epikurejską filozofią przyrody. Pracę nad rozprawą ukończył w 1841 roku[28]. Wzbudziła ona pewne kontrowersje w kręgach konserwatywnych profesów Uniwersytetu w Berlinie. Marx zdecydował się zaprezentować ją na bardziej liberalnym Uniwersytecie w Jenie, który w kwietniu 1841 roku nadał mu doktorat[29]. W 1840 roku rozpoczął współpracę z wojującym ateistą Bruno Bauerem, w marcu 1841 roku zaczęli planować wydawanie pisma „Archiv des Atheismus” („Archiwum Ateizmu”), pomysł ten jednak nigdy nie został zrealizowany.
Kariera akademicka została zniwelowana przez silną opozycję rządu wobec klasycznego liberalizmu i młodoheglistów[30]. W 1842 roku Marx przeprowadził się do Kolonii, gdzie pracował jako dziennikarz, pisząc dla radykalnej gazety „Rheinische Zeitung” („Gazeta Reńska”), wyrażając w niej swoje wczesne poglądy na temat socjalizmu i ekonomii. Krytykował zarówno prawicowe rządy europejskie, jak i ruchy liberalne oraz socjalistyczne, które uważał za szkodliwe lub nieskuteczne[31]. Gazeta zwróciła uwagę cenzury rządu pruskiego, która przed drukowaniem materiału sprawdzała wydania pisma. Po tym, gdy w 1843 roku, „Rheinische Zeitung” opublikowała artykuł mocno krytyczny wobec monarchii rosyjskiej i cara Mikołaja I, rząd rozpoczął nagonkę na gazetę. Represje spowodowane były demokratyczno-radykalnym charakterem pisma. Doprowadziły do ustąpienia redaktora naczelnego i zamknięcia pisma w marcu 1843 roku[32][33]. 19 czerwca 1843 roku Karl wziął ślub z Jenny w kościele protestanckim w Bad Kreuznach[34].
Po ślubie Marx przeniósł się do Paryża. We Francji miał wraz z Arnoldem Ruge wydawać radykalne pismo emigracyjne, jednak ze względu na rozbieżności ideowe redaktorów, jak i problemy z kolportażem, wyszedł tylko jeden podwójny numer pisma zatytułowanego „Roczniki Niemiecko-Francuskie”. Marx opublikował w nim dwa eseje: Przyczynek do krytyki heglowskiej filozofii prawa i W kwestii żydowskiej, w których ostatecznie porzucił idealizm i po raz pierwszy ujawnił się jako rewolucjonista i internacjonalista, odwołujący się do Mas i Proletariatu. W 1844 stworzył Rękopisy ekonomiczno-filozoficzne, którymi rozpoczął swoją krytykę ekonomii politycznej i filozofii heglowskiej. Rozwijał w nich również teorię humanizmu socjalistycznego.
We wrześniu 1844 poznał Fryderyka Engelsa, przyszłego przyjaciela i głównego współpracownika. Obaj uczestniczyli aktywnie w życiu politycznym Paryża i wspólnie opracowywali idee socjalizmu proletariackiego, czyli komunizmu. Ich pierwszą wspólną pracą była Święta rodzina, czyli krytyka krytycznej krytyki, analizująca krytycznie poglądy heglistów, ze szczególnym uwzględnieniem Bruno Bauera.
W 1845 roku został na skutek działań rządu pruskiego wydalony z Paryża i wyjechał do Brukseli. W latach 1845–1846 powstawał pierwszy wykład tez materializmu historycznego autorstwa obu teoretyków, zawarty w Ideologii niemieckiej. W latach 1845–1847 Marx pisał do gazet, wydawanych zarówno we Francji, jak i w Niemczech. Jego artykuły można było znaleźć w „Vor-warts, Deutsche-Brusseler Zeitung, Das Westphalische Dampfboot” i „Der Gesellschaftsspiegel”. W roku 1847 wydał Nędzę filozofii; dzieło, w którym poddał krytyce poglądy Proudhona.
Marx zaangażował się w tworzenie Związku Komunistów, którego członkiem został wraz z Engelsem wiosną 1847. W listopadzie 1847 obaj brali udział w II Zjeździe tego związku w Londynie. Ich czynny udział w związku przejawiał się również w tym, iż obaj są autorami jego programu. Stanowił go wydany w 1848 roku Manifest partii komunistycznej, który zawierał idee rozwiniętego materializmu, dialektyki, walki klas i przypisywał proletariatowi potencjał rewolucyjny. Był to pierwszy program oparty w całości na zasadach socjalizmu naukowego. Manifest, według A.C. Suttona, był plagiatem wydanego wcześniej dzieła Democracy Manifesto, napisanego przez francuskiego fourierystę, Victora Prospera Consideranta i wydanego w 1843 roku (Principe du socialisme: Manifeste de la démocratie au XIXe siècle, publié en 1843). Marx prowadził w tym czasie działalność publicystyczną. W 1848 wyszło jego Przemówienie w sprawie wolnego handlu. Po wybuchu rewolucji lutowej 1848, Marx został wydalony z Belgii. Wrócił do Paryża, a stamtąd po rewolucji marcowej udał się do Kolonii. Został tam redaktorem naczelnym wychodzącej od 1 czerwca 1848 do 19 maja 1849 „Nowej Gazety Reńskiej”. Marx publikował w niej m.in. artykuły Praca najemna a kapitał i Liberałowie przy władzy.
Został oskarżony o przestępstwa prasowe i nawoływanie do stawiania oporu zbrojnego rządowi, stanął przed sądem, który go uniewinnił 9 lutego 1849. 16 maja 1849 został wydalony z Królestwa Prus. Wyjechał do Paryża, skąd po demonstracji 13 czerwca 1849 udał się do Londynu, w którym spędził resztę życia. W Anglii, borykając się z dużymi problemami finansowymi, utrzymywał się z działalności publicystycznej oraz z bezzwrotnych pożyczek od Engelsa. W Londynie podjął ostrą polemikę z krytykami swojej teorii i osoby. Od 1851 do 1862 był stałym współpracownikiem „New York Tribune”, gdzie część jego artykułów była publikowana jako materiały redakcyjne. W piśmie tym opublikował m.in. Rewolucja i kontrrewolucja w Niemczech oraz Rewelacje z dziejów dyplomacji XVIII w. Współpracował również z „Neue Oder-Zeitung” oraz wiedeńskim „Presse”.
Badał ekonomię polityczną i historię oraz tworzył kolejne dzieła. Podsumował wyniki rewolucji (1848–1851) w Osiemnasty brumaire’a Ludwika Bonaparte wydanym w 1852. Pierwszym efektem studiów ekonomicznych był Przyczynek do krytyki ekonomii politycznej﻿(inne języki). Najpełniejszy wyraz praca Marksa na tym polu znalazła w wydanym w 1867 I tomie Kapitału. Opisał w nim prawa rządzące ekonomią państw kapitalistycznych. W 1871 poddał analizie okres Komuny Paryskiej w Wojnie domowej we Francji, zaś w Krytyce programu gotajskiego skrytykował Socjaldemokratyczną Partię Niemiec.
W 1864 Marx współtworzył Międzynarodowe Stowarzyszenie Robotników, tzw. Pierwszą Międzynarodówkę, będąc jej sekretarzem, autorem pierwszej Odezwy i innych fundamentalnych pism. W 1869 jego przyjaciel i uczeń Wilhelm Liebknecht założył socjalno-demokratyczną partię robotniczą, która połączywszy się z radykalnymi zwolennikami Ferdinanda Lassale’a, stworzyła Socjaldemokratyczną Partię Robotniczą Niemiec. Doprowadził również do przeniesienia Rady Generalnej Międzynarodówki do Nowego Jorku po kongresie w Hadze w 1872. Równocześnie zajmował się redagowaniem Kapitału, w czym przeszkodziła mu choroba. Dzieło zostało ukończone przez Fryderyka Engelsa na podstawie zgromadzonych zapisków i wydane po śmierci Karola Marxa.
2 grudnia 1881 zmarła żona Marxa, zaś w 1883 on sam. Oboje zostali pochowani na Highgate Cemetery w Londynie[35]. W chwili śmierci Marx był bezpaństwowcem[36]. W 1956 dzięki funduszom Komunistycznej Partii Wielkiej Brytanii dokonano przeniesienia szczątków Marxa i jego rodziny do nowego grobu we wschodniej części cmentarza. Autorem projektu nowego nagrobka był Laurence Bradshaw.
Karl Marx uchodzi za założyciela tzw. socjalizmu naukowego, który, wyrósłszy z jego poglądów filozoficznych, tylko w związku z nimi staje się zrozumiały. Marx pozostawał stale pod wpływem filozofii Hegla, zachował też jego sposób dociekania, jego dialektyczną metodę i przekonanie o identyczności „bytu” i „myślenia”. Szczytowym zakończeniem filozoficznego systemu Marksa jest jego materialistyczne pojmowanie dziejów, oparte na historycznym materializmie.
Pod względem ekonomicznym Karl Marx jest uczniem Davida Ricarda, na którego teorii o wartości pracy oparł swój własny system, zwłaszcza teorię wyzysku. Podobnie jak Ricardo, Marx zapatruje się bardzo pesymistycznie na położenie klasy pracującej i przepowiada upadek kapitalistycznego ustroju wskutek ciągłego wzrostu klasy robotniczej i coraz bardziej zaostrzających się kryzysów (teoria katastrof).
Marx w Przyczynku do krytyki heglowskiej filozofii prawa określił religię mianem opium ludu:
Nędza religijna jest jednocześnie wyrazem rzeczywistej nędzy i protestem przeciw nędzy rzeczywistej. Religia jest westchnieniem uciśnionego stworzenia, sercem nieczułego świata, jest duszą bezdusznych stosunków. Religia jest opium ludu[37].
Był też jednym z twórców pojęcia alienacji religijnej[38].
Amerykański ekonomista Paul Sweezy w swojej pracy Teoria rozwoju kapitalizmu[39] stwierdza, że dla Marksa w odróżnieniu od Adama Smitha wymiana nie jest czymś bardziej pierwotnym niż praca i jej podział. Towar jest czymś co zaspokaja potrzeby ludzkie, elementarną jednostką w kapitalistycznej formie produkcji. Towar posiada jednocześnie dwie formy wartości: użytkową i wymienną. Towar może mieć wartość użytkową (materialna podstawa wartości) wynikającą z tego czym on jest (jego własności). Urzeczywistnia się ona przez użycie lub spożycie danego towaru. Poza tym ma też wartość wymienną (wielkość wartości) opartą na relacji wartości użytkowych wymienianych towarów. Każda jednostkowa proporcja wymiany towaru x na towar y jest określoną wartością wymienną, aby znaleźć wspólny mianownik dla wymiany towarów konieczny jest ekwiwalent, czyli towar za pośrednictwem którego można dokonywać wymiany, pieniądz stanowi taki uniwersalny ekwiwalent.
Choć wartość wymienna określa relacje między produktami, to w istocie wyraża stosunki między producentami. Nie tylko ilościowy stosunek wartości wymiennych wyrażonych w pieniądzu (cena), ale i szereg społecznych stosunków między członkiniami i członkami społeczeństwa kapitalistycznego. Stosunki między produkującymi i wymieniającymi towary ludźmi otrzymują swoją symboliczną materializację w postaci towarów:
Jednakowość różnych prac ludzkich otrzymuje rzeczową postać jednakowej wartości przedmiotowej produktów pracy; czas jako miernik wydatkowania ludzkiej siły roboczej przybiera postać wielkości wartości produktów pracy; wreszcie, stosunki wzajemne wytwórców, w których urzeczywistniają się owe społeczne określenia ich prac, przybierają formę społecznego stosunku między produktami pracy[40].Wartość towarów (w tym samej pracy), to jakie towary są produkowane, jakie konsumowane, wydaje się rządzić społeczeństwem kapitalistycznym. Martwe przedmioty wydają się mieć olbrzymie znaczenie i kierować poczynaniami ludzi, podczas gdy w rzeczywistości sytuacja jest odwrotna: towary są wyrazem i odzwierciedleniem społecznych stosunków. Fakt, że towary można nabyć na rynku sprawia, że powstaje pozór jakoby można było wraz z towarem nabyć same stosunki społeczne, które one reprezentują. Przykładowo, towar wyprodukowany w przemyśle, stojącym wysoko w hierarchii globalnego podziału pracy, taki jak ekskluzywna limuzyna, powinien stanowić symbol wysokiego statusu właściciela. Według Marksa, sytuacja jest zupełnie odwrotna: to wysoki status społeczny i władza, którą dysponuje jednostka zostaje przypisana towarowi, jakim jest ekskluzywna limuzyna. Pozór fetyszyzmu towarowego nie jest jednak czystą mrzonką. Jest raczej specyficzną dla gospodarki towarowej formą doświadczania życia społecznego. W odmiennych społeczeństwach taką podstawową formą może być dar. Uznaje się symboliczną wartość towarów jako nośników relacji społecznych, przez co, choć są one tylko nośnikami i środkami, to zdają się być źródłem wartości. Ludzie wierzą, że limuzyna jest źródłem prestiżu, podczas gdy jest jedynie jego nośnikiem i jako taka jest w równym stopniu wytworem techniki, co wytworem kulturowym społeczeństwa.
Praca stanowi podstawę analizy Marksa, ponieważ dzięki niej jako kategorię można rozpatrywać towar „siłę roboczą”, jako nośnik stosunków społecznych i wartości wymiennej. Tylko w ten sposób możliwe jest rozpatrywanie towaru nie jako przedmiotu konsumpcji, tylko jako przedmiotu produkcji (produkcji jako wyniku pracy). Sama praca jest w kapitalizmie towarem i jako taki da się rozłożyć na pracę użyteczną (czyli taką, która wytwarza wartość użytkową jej produktu) oraz pracę abstrakcyjną (wydatkowaną siłę roboczą). Praca nie jest albo użyteczna albo abstrakcyjna. Jest zawsze jednocześnie użyteczna i abstrakcyjna, tak jak towar posiada zawsze wartość użytkową i wymienną. O ile praca użyteczna jest po prostu zestawem fizycznych czynności, o tyle praca abstrakcyjna jest właśnie sposobem w jaki ta praca jest w kapitalizmie waloryzowana. Nie chodzi tu o prostą ocenę wartości np. 15 zł za godzinę pracy, ale zarówno o warunki które określają wartość pracy (nie tylko ekonomicznych, ale i społecznych oraz politycznych), jak i proces, w którym praca i jej wytwory (towary) przybierają taką formę (w innych społeczeństwach wymiana i społeczny podział pracy może przybrać zupełnie inną postać). Jak twierdzi Sweezy:
[...] towar ma coś wspólnego z wszystkimi innymi towarami (tj. że wszystkie one są wartościami), a mianowicie to, że wchłania część całkowitej rozporządzalnej w społeczeństwie siły roboczej (tj. że wszystkie towary są zmaterializowaną abstrakcyjną pracą). Ta właśnie cecha charakterystyczna towarów (która zakłada wartość użytkową i przejawia się sama w wartości wymiennej) czyni z „towaru” punkt wyjścia i centralną kategorię ekonomii politycznej współczesnej epoki[39].
Pracę jako towar można ująć od strony ilościowej, co Marks ujmuje jako czas pracy. Innym sposobem jest mierzenie pracy jako towaru na rynku, czyli ceny siły roboczej, jest to sposób badania przejawiania się pracy, a nie badania pracy jako źródła wartości. Badanie ilościowego aspektu wartości to badanie praw rządzących podziałem siły roboczej do różnych działów (dziś powiedzielibyśmy sektorów) produkcji w społeczeństwie kapitalistycznym, zatem teoria wartości powinna wyjaśniać: a) stosunki wymienne między towarami b) ilość wytwarzanych towarów c) przydział siły roboczej do działów produkcji.
Marks miał wyraziste i zdecydowane poglądy w sprawie Polski, której niepodległość w pełni popierał. Jego zainteresowanie Polską można datować od powstania krakowskiego (1846). Marx czytał prace Joachima Lelewela poświęcone historii Rzeczypospolitej i uważał Polskę za zaporę dla rosyjskiego imperializmu, dążącego do dominacji m.in. w Europie. Krytykował także brak pomocy dla Polaków ze strony mocarstw, który w jego ocenie mógł doprowadzić do złamania ducha oporu wobec Rosji. Znaczący wzrost jego zainteresowania sprawą polską miał miejsce w okresie powstania styczniowego (1863–1864), upadek którego Marx uznał za jedno z dwóch najważniejszych wydarzeń w Europie po 1815 r. W ocenie Marksa upadek powstania miał zniechęcić Polaków do walki o suwerenność i umocnić pozycję Rosji[41].
Marx organizował zbiórki pieniędzy dla polskich emigrantów wbrew oporowi prasy i parlamentarzystów, występował na wiecach upamiętniających powstanie i protestował przeciw wydawaniu rosyjskim władzom powstańców przebywających na Zachodzie. Gmina Centralna Londyńskiego Zjednoczenia Emigracji Polskiej imiennie podziękowała Marksowi za jedno z jego przemówień. Krytykę braku reakcji na podbój Polski przez Rosję Marx zawarł w manifeście I Międzynarodówki. Córka Marksa Jenny nosiła polski krzyż powstańczy, co Marx osobiście popierał[41].
Jedna tylko alternatywa pozostała Europie. Albo barbarzyństwo azjatyckie pod moskiewskim przywództwem zaleje ją jak lawina, albo musi ona odbudować Polskę tak, aby między nią a Azją stanęło dwadzieścia milionów bohaterów i by dzięki temu zyskała ona na czasie na dokonanie swego społecznego odrodzenia.
Aleksander III Macedoński (stgr. Ἀλέξανδρος ὁ Τρίτος ὁ Μακεδών Aleksandros ho Tritos ho Makedon) zwany też Aleksandrem Wielkim (Ἀλέξανδρος ὁ Μέγας Aleksandros ho Megas) i niezwyciężonym (άνίκητος) (ur. 19–20 lipca 356 p.n.e.[1] w Pelli, zm. 10 czerwca 323 p.n.e. w Babilonie) – król Macedonii z dynastii Argeadów w latach 336–323 p.n.e. Jest powszechnie uznawany za wybitnego stratega i jednego z największych zdobywców w historii ludzkości. Okres panowania Aleksandra wyznacza granicę między dwiema epokami historii starożytnej: okresem klasycznym i epoką hellenistyczną.
Jego ojciec Filip II zreformował państwo macedońskie i zdołał podporządkować sobie większość Grecji. W 336 p.n.e. Aleksander odziedziczył silne państwo oraz dobrze zorganizowaną i doświadczoną armię. Po ujarzmieniu zbuntowanych greckich poleis kontynuował plany ekspansji pozostawione przez Filipa – w 334 p.n.e. zaatakował rządzoną przez Persów Azję Mniejszą, rozpoczynając serię kampanii trwającą 10 lat.
Złamał potęgę imperium perskiego w wyniku serii zwycięstw (najsłynniejsze odniósł pod Issos i Gaugamelą) i do 327 p.n.e. opanował całe państwo. W tym samym roku ruszył na Indie, lecz pomimo odnoszonych zwycięstw został zmuszony do odwrotu z powodu niezadowolenia w armii. Zmarł w wieku 32 lat w Babilonie[2][3] w trakcie przygotowań do kolejnych wypraw wojennych, pozostawiając imperium, którego rozpiętość ze wschodu na zachód wynosiła 5 tys. km. Po jego śmierci rozgorzały walki pomiędzy dowódcami macedońskiej armii, tzw. diadochami, które doprowadziły do podziału ogromnego państwa na kilka królestw.
Tocząc zwycięskie bitwy z wojskami Traków, Ilirów, Persów i Hindusów, opierał się na zdyscyplinowanej piechocie – falandze i ciężkiej macedońskiej jeździe – hetajrach.
Aleksander urodził się 6 dnia ateńskiego miesiąca Hekatombajon, co w naszym kalendarzu odpowiada mniej więcej 19–20 lipca, w 356 r. p.n.e.[4] Był synem króla Macedonii i twórcy potęgi tego państwa, Filipa II, i Olimpias, córki króla Epiru Neoptolemosa I[5]. Filip miał kilka żon i kochanek, ale Olimpias, z racji powicia uznanego za następcę tronu Aleksandra, miała wśród nich pozycję uprzywilejowaną[6].
Jak w przypadku większości osób, które w starożytności stały się sławne, narodziny Aleksandra otoczono później różnymi legendami i cudownymi zdarzeniami[7]. Zgodnie z tradycją synchronizowania dat ważnych wydarzeń przez autorów greckich, podawano, że Aleksander narodził się tego samego dnia, którego spłonęła świątynia Artemidy w Efezie, jeden z siedmiu cudów świata[8]. Spekulowano, że sama Artemida nie mogła uchronić swojego przybytku przed pożarem, gdyż była zajęta asystowaniem Olimpias przy porodzie[8]. Odczytywano to jako znak przyszłego podboju Azji przez Aleksandra. Zarówno Filip, jak i Olimpias mieli doświadczyć tego dnia wizji, które rzekomo zwiastowały wielkie czyny przyszłego króla[7]. Przed narodzinami syna Filip miał śnić o zapieczętowanym sygnecie z lwem w brzuchu ciężarnej Olimpias, z kolei ona o uderzającym w jej łono piorunie. Inna legenda mówiła o podglądaniu przez Filipa przez dziurkę od klucza Olimpias spółkującej z wężem, pod postacią którego kryło się bóstwo, prawdopodobnie Amon z Siwy. Popularność tej opowieści brała się po części z wiary w kontakty bóstw z ludźmi, a także z jej prawdopodobieństwa dla starożytnych – Olimpias znana była z hodowania węży oraz udziału w przerażających Greków kultach z Epiru[9]. Innymi boskimi zdarzeniami były również trzy wspaniałe wieści, które jednego dnia miał otrzymać Filip oblegający Potidaję: armia macedońska pod dowództwem Parmeniona pokonała Illirów, jego koń wygrał zawody konne w Olimpii i Olimpias urodziła mu syna[10].
Wkrótce po urodzeniu Aleksander został przekazany pod opiekę mamki – Lanike (siostry Klejtosa Czarnego, późniejszego towarzysza króla)[11]. Olimpias mianowała jego wychowawcą swego krewnego, pochodzącego z Akarnanii Leonidasa, którego wspomagał Lizymach i inni nauczyciele m.in. muzyki, gramatyki, retoryki[11]. Leonidas był surowym wychowawcą i nie wzbraniał się ganić Aleksandra[12]. Z kolei Lizymach prawdopodobnie zainteresował młodego księcia twórczością Homera, ulubionego autora Aleksandra, który później często starał się dorównać bohaterom Iliady i Odysei, a zwłaszcza Achillesowi[12]. Aleksander był też dobrze zaznajomiony z resztą literatury greckiej[12]. Niezbędnym elementem wychowania każdego macedońskiego arystokraty były również nauka jazdy konnej i posługiwania się bronią[11].
Filip miał jeszcze drugiego syna Filipa Arridajosa, niemal równolatka Aleksandra[12]. Uwaga jaką przywiązywano do wychowania Aleksandra wskazuje jednak, że to on od początku traktowany był jako następca tronu[12]. Być może już wtedy widoczne stało się opóźnienie umysłowe Arridajosa[12]. Aleksander miał także o kilka lat młodszą siostrę – Kleopatrę[11].
Źródła antyczne nie przekazują zbyt wielu konkretnych informacji na temat dzieciństwa Aleksandra, są to głównie różnego rodzaju anegdoty, które miały za zadanie pokazywać cechy charakteru i cnoty przyszłego władcy[13]. Przedstawiają one młodego księcia jako osobę ambitną, ciekawą świata, dojrzałą emocjonalnie i przekonaną o swojej wartości[14]. Mimo wielkiej sprawności fizycznej miał mieć lekceważący stosunek do sportu, przedkładając nad zmagania atletyczne nauki i sztukę[15]. O wcześnie podjętych planach podboju świata przekonywać ma opowieść o wizycie posłów Wielkiego Króla, przyjętych pod nieobecność Filipa przez Aleksanda, który zamiast o bogactwa i inne cudowności perskiego imperium wypytywał gości o sieć dróg i sprawy wojskowe[16]. Najbardziej znana z anegdot o dzieciństwie Aleksandra dotyczy poskromienia Bucefała[16]. Już jako 12-letnie (lub 8-letnie według innych źródeł)[17] dziecko Aleksander miał ujarzmić wierzchowca, z którym nie radzili sobie najlepsi jeźdźcy[16]. Zauważył, że koń boi się własnego cienia i wykorzystał tę informację do uspokojenia i ujeżdżenia zwierzęcia[16]. Bucefał (gr. Boukephalos, „głowa wołu”) – tak młody książę nazwał rumaka – towarzyszył Aleksandrowi i służył mu w bitwach przez wiele lat[16], sam władca zaś uznawał go za jednego ze swych najbliższych przyjaciół[potrzebny przypis]. Jedno z miast, jakie wzniósł nad Hydaspesem w Indiach, nazwał Bukefalą[18].
W 343/342 p.n.e. Filip zatrudnił dla swego syna nowego nauczyciela – Arystotelesa (o tę posadę starało się wielu reprezentantów ówczesnej nauki greckiej m.in. Isokrates), którego ojciec był związany z dworem i rodziną panującą w Macedonii[19]. Kształcenie odbywało się w mieście Mieza na południu kraju, w ogrodzie poświęconym Nimfom[19]. Zetknięcie wielkiego filozofa z przyszłym królem i zdobywcą stanowiło już dla autorów starożytnych fascynujący temat do spekulacji o wzajemnych wpływach tak silnych osobowości[19][20]. Trudno jednak powiedzieć na ten temat coś konkretnego, Arystoteles w tych latach nie osiągnął jeszcze sławy, jaka otaczała jego imię w przyszłości, poza tym on i Aleksander pochodzili z różnych światów: filozof wywodził się z kręgu greckiej polis i raczej nie mógł przygotować księcia do roli władcy wielkiego imperium[21][20]. Arystoteles uczył Aleksandra etyki, polityki, medycyny i greckiej literatury, być może także dialektyki i erystyki[22]. Późniejsze działania macedońskiego władcy nie wskazują na to, że przyjął poglądy polityczne Arystotelesa czy też jego przekonanie o niższości barbarzyńców (w tym mieszkańców imperium perskiego), choć z drugiej strony zgadzał się z filozofem co do wyższości kultury greckiej[23]. Być może Arystoteles rozbudził w młodym księciu ciekawość intelektualną, o czym może świadczyć zabranie przez Aleksandra na wyprawę do Azji wielu uczonych[23][20].
Podczas edukacji w Miezie Aleksandrowi towarzyszyła grupa młodych arystokratów macedońskich, zarówno przyjaciele z dzieciństwa, jak i starsi chłopcy wybrani przez jego ojca[24]. Z tego grona pochodziło wielu najbliższych towarzyszy i współpracowników późniejszego króla, mogli wśród nich być m.in. jego najbliższy przyjaciel Hefajstion, Leonnatos, Perdikkas, Klejtos Czarny oraz Kassander, syn jednego z dowódców Filipa – Antypatra[24].
W latach 342–340 p.n.e. Filip kontynuował podboje, głównie na północy i wschodzie, w Tracji, na północnym wybrzeżu Morza Egejskiego i w rejonie Propontydy, mając za przeciwnika m.in. Ateny i ich sojuszników[25]. Aleksander, który w 340 p.n.e. skończył 16 lat, otrzymał wtedy pierwsze zadania wagi państwowej: ojciec na czas swojej nieobecności powierzył mu regencję w Macedonii[26]. Działania młodego regenta nadzorował i wspierał radą Antypater[27]. W tym czasie Aleksander odniósł swoje pierwsze zwycięstwo militarne – pokonał zbuntowane przeciw Macedonii trackie plemię Majdów[27]. Zwyciężonych wypędził z ich stolicy, zasiedlił ją nowymi osadnikami i nazwał miasto Aleksandropolis[27]. Później następca tronu towarzyszył ojcu w wyprawie na północ przeciw Scytom, których armia macedońska pokonała na terenie dzisiejszej Dobrudży[27]. W drodze powrotnej zwycięskie wojska wdały się w konflikt z plemieniem Tryballów – w bitwie utracono scytyjskie łupy, a Filip został ranny w udo[28].
W 339 p.n.e. w Grecji Środkowej wybuchła IV wojna święta i jedna ze stron – Tesalowie – poprosiła o pomoc Filipa[28]. Ten skorzystał z okazji i wkroczył do Fokidy nieopodal Beocji, co z kolei skłoniło Teby, największe miasto regionu, do sojuszu z Atenami[28]. Latem 338 p.n.e. doszło do bitwy między armią macedońską a liczniejszymi siłami tebańsko-ateńskimi, stoczonej na niewielkiej równinie[29]. Filip kierował prawym skrzydłem, złożonym z piechoty i stojącym naprzeciw Ateńczyków[29]. Aleksandrowi powierzono dowodzenie (przy wsparciu doradców) lewym skrzydłem i konnicą, mającymi za przeciwników Beotów[29]. Piechota dowodzona przez Filipa skutecznie walczyła z ateńskimi hoplitami, ale to szarża jazdy pod dowództwem następcy tronu rozstrzygnęła bitwę, przełamując szyki greckie w najsilniejszym miejscu i zmuszając żołnierzy do ucieczki[30]. Na miejscu pozostał tylko elitarny tebański „Święty Zastęp”, którego członkowie walczyli do śmierci z żołnierzami Aleksandra[31]. Taktyka, w której piechota wiąże przeważające siły wroga, a ciężka konnica kierowana przez władcę zadaje decydujące uderzenia, była później często wykorzystywana przez Aleksandra w kampanii przeciwko imperium perskiemu[31].
Filip narzucił Tebom ciężkie warunki pokoju, natomiast dla przygotowanych do długiej obrony Ateńczyków był znacznie łagodniejszy[32]. Poselstwo złożone z dostojników macedońskich i Aleksandra zawiozło do Aten prochy poległych pod Cheroneą żołnierzy ateńskich i wynegocjowało pokój kończący wojnę Macedonii z Atenami[33]. Po zakończeniu poselstwa Aleksander udał się na północ, gdzie zwycięsko walczył z Illirami[34].
Tymczasem Filip ze swoją armią wkroczył na Peloponez, gdzie wiele polis przeszło na jego stronę[35]. Później zaatakował Lakonię, jednak samą Spartę pozostawił nietkniętą[35]. W 337 p.n.e. zwołał delegatów ze wszystkich polis Grecji kontynentalnej i kilku wyspiarskich do Koryntu, gdzie ogłoszono powstanie Związku nazywanego dzisiaj Korynckim[34]. Członkowie zgodzili się nie prowadzić wojen między sobą a wszelkie konflikty miała rozwiązywać rada (synedrion) z decydującym głosem króla Macedonii[34]. Związek był narzędziem kontroli macedońskiej nad Grecją (dodatkowo utwierdzały ją macedońskie garnizony w strategicznych miejscach – Korynt i Chalkis[36]), jednak nie dawał jej władzy absolutnej[37].
Jesienią 337 p.n.e. Związek wypowiedział imperium perskiemu wojnę, którą Filip planował już od jakiegoś czasu[38]. Do Azji Mniejszej wysłano armię liczącą ok. 10 tys. żołnierzy, która początkowo odnosiła sukcesy, zajmując tereny na zachodzie półwyspu[39]. Jednak w 336 p.n.e. Memnon z Rodos na czele oddziału greckich najemników odzyskał dla króla perskiego większość utraconych obszarów i zepchnął korpus macedoński nad Hellespont[40].
Plutarch opisał konflikt, który miał miejsce między Filipem i Aleksandrem prawdopodobnie w 337 p.n.e.[41] Piksodaros, satrapa Karii i Likii w południowej Azji Mniejszej, zaproponował Filipowi przymierze, które miało zostać przypieczętowane przez małżeństwo między Arridajosem i córką Piksodarosa – Adą[42]. Aleksander dowiedział się o negocjacjach i obawiając się, że nowe małżeństwo może uczynić jego brata preferowanym następcą tronu, wysłał własnych posłów, oferując satrapie siebie za zięcia[42]. Filip odkrywszy działania syna, zwymyślał go i wygnał doradców Aleksandra, którzy posłużyli mu niewłaściwą, zdaniem króla, radą[41]. Zdarzenie to może sugerować, że Aleksander miał w tym czasie obawy co do swojej pozycji jako następcy tronu[43].
Do poważniejszego konfliktu między ojcem i synem doszło w 337 p.n.e. podczas kolejnego ślubu Filipa, tym razem z Kleopatrą, bratanicą Attalosa, arystokraty macedońskiego, jednego z dowódców korpusu wysłanego do Azji Mniejszej[44]. Podczas wesela Attalos wzniósł toast, życząc Filipowi, by ze związku z Kleopatrą narodzili się przyszli królowie, czystej krwi Macedończycy[44]. Obecny na przyjęciu Aleksander odczytał to jako obelgę wskazującą na obce pochodzenie jego matki[45]. Wywiązała się gwałtowna kłótnia, a w obronie Attalosa stanął mocno już pijany Filip, który swój gniew skierował na syna[45]. Aleksander i Olimpias szybko opuścili Pellę, udając się do Epiru, skąd następca tronu trafił na dwór bliżej nieokreślonego w źródłach królestwa iliryjskiego[46]. Wygnanie Aleksandra nie trwało długo, Filip pozwolił mu wkrótce powrócić do Macedonii, ale Olimpias pozostała na dworze swojego brata w Epirze[47].
Władzę objął w roku 336 p.n.e. po zamordowaniu Filipa II przez byłego kochanka króla, Pauzaniasza. Według oficjalnej wersji mordercę, który został zabity podczas ucieczki, przekupił król Persji Dariusz III. Podejrzenie o spisek padło jednak na Aleksandra i Olimpias, których pozycja była zagrożona przez nowe małżeństwo Filipa. Po ojcu Aleksander odziedziczył silną Macedonię, kontrolującą od niedawna znaczną część Grecji, dobrze zorganizowaną armię oraz ideę podboju Persji, która od dwóch stuleci była największym zagrożeniem dla Grecji.
Na wieść o śmierci Filipa i udziale Aleksandra w walce z Trakami na północy (wkrótce po objęciu przez niego władzy), zbuntowało się greckie miasto Teby (335 p.n.e.), które młody król błyskawicznie zdobył. Około sześciu tysięcy mieszkańców zostało wymordowanych, a 30 tysięcy sprzedano w niewolę. Miasto zrównano z ziemią. Ów pokaz siły schłodził buntownicze zamiary innych greckich państw.
W roku 334 p.n.e. Aleksander zostawił w Macedonii część wojsk pod wodzą regenta Antypatra i wyruszył na podbój Persji na czele 49 100 żołnierzy – Macedończyków, Greków, Traków i Ilirów. Wyprawa według oficjalnej propagandy miała na celu wyzwolić greckie miasta w Anatolii oraz pomścić krzywdy, jakich doznała Grecja podczas perskich najazdów na początku V w. p.n.e. Pierwsza bitwa z perskimi wojskami miała miejsce nad rzeką Granik. Król Macedonii zwyciężył. Po bitwie wysłał do pracy w kopalniach dwa tysiące wziętych do niewoli greckich najemników, którzy służyli w perskiej armii. Według niektórych źródeł wielu najemników zostało zamordowanych zaraz po bitwie.
Przez kilkanaście kolejnych miesięcy opanował Anatolię, zajmując bez walki wiele polis. Poważniejszy opór stawiły jedynie Milet i Halikarnas, w których broniły się perskie garnizony wspierane przez mieszkańców. Z reguły zabraniał swojej armii rabunków, od miast żądał umiarkowanych danin i pozostawiał na zdobytych terenach niezmienioną administrację. W trakcie podboju Anatolii Aleksander zawitał do miasta Gordion, gdzie rozsupłał legendarny węzeł. Zrobił to bardzo prosto: wyjął miecz i rozciął go. Wedle legendy ten, kto tego dokona, miał zostać panem Azji.
Po opanowaniu dzisiejszej zachodniej Turcji Aleksander skierował się ze swoją armią do Fenicji (dzisiejszy Liban). Chciał zająć wszystkie perskie porty nad Morzem Śródziemnym, by uniemożliwić wrogom inwazję na Grecję. Przeciwko niemu ruszył z Mezopotamii król Persji Dariusz III, prowadząc potężną armię, w której służyło między innymi 30 tys. greckich najemników. Obie armie spotkały się w pobliżu Issos. Wyrównaną bitwę rozstrzygnęła brawurowa szarża macedońskiej jazdy prowadzona przez samego Aleksandra, która zmusiła perskiego władcę do ucieczki. Pozbawiona dowódcy perska armia rozpadła się. Korzystając z chaosu, Macedończycy zabili tysiące swych wrogów (starożytni mówili o stu tysiącach, ale niechybnie mocno przesadzili). Wojska Aleksandra miały stracić jedynie 302 zabitych. W zdobytym obozie wroga Aleksander pojmał między innymi: matkę Dariusza – królową Sysygambis, jego żonę – Statejrę I, oraz córki: Statejrę II i Drypetis, które – co podkreślali starożytni pisarze – Aleksander traktował bardzo dobrze.
Po bitwie macedoński władca przeszedł przez Fenicję i dotarł do jej największego miasta – położonego na przybrzeżnej wyspie Tyru, który nie otworzył przed nim swoich bram. Rozpoczęło się oblężenie, trwające aż osiem miesięcy. Przełom nastąpił, dopiero gdy na stronę Aleksandra przeszły walczące dotychczas po stronie Persji okręty z Fenicji i Cypru. Po zdobyciu miasta 8 tys. jego mieszkańców zostało zamordowanych, a 30 tys. sprzedano w niewolę.
Następnie Aleksander musiał zdobyć bronioną przez perski garnizon Gazę, po czym dotarł do Egiptu.
W roku 333 p.n.e. wyrocznia w Egipcie ujawniła Aleksandrowi, że jest synem samego Zeusa. Aleksander z czasem zaczął wierzyć, że jest półbogiem, a pod koniec swego życia nabrał przekonania, że jest wręcz bogiem i wysłał list nakazujący greckim miastom uznanie jego boskości. Te uczyniły to, ale nie obyło się bez drwin pod adresem króla.
Od 525 roku p.n.e., kiedy to król perski Kambyzes II pokonał Psametyka III, Egiptem rządzili Persowie. Po zwycięstwie i zdobyciu Tyru i Gazy (jesienią 332 roku p.n.e.) Aleksander przekroczył w Peluzjum granice Egiptu, gdzie entuzjastycznie witany był jako wyzwoliciel. Następnie Aleksander wraz z wojskami skierował się w górę Nilu, w kierunku Memfis. Perski satrapa Egiptu, Sabakes, poległ pod Issos, a jego następca Mazakes, rezydujący w Memfis, po krótkich negocjacjach poddał miasto bez walki. Oddał Aleksandrowi pałac i wypłacił ze skarbca sumę około 800 talentów. W świątyni Ptaha, w Memfis, kapłani prawdopodobnie wprowadzili Aleksandra na tron faraonów, nadając mu tytuł Króla Dolnego i Górnego Egiptu.
Następnie na śródziemnomorskim wybrzeżu, na zachód od kanopijskiego ujścia Nilu, Aleksander założył najsłynniejsze z wielu miast nazwanych swoim imieniem – Aleksandrię. Egipska Aleksandria stała się później siedzibą władców z dynastii Ptolemeuszy oraz jednym z najważniejszych miast starożytnego świata. Na wiosnę 331 roku p.n.e. Aleksander opuścił Egipt, powierzając administrację państwem dwóm byłym nomarchom – Doloaspisowi i Petisisowi, pozostawiając dowództwo sił wojskowych Macedończykom. Był to pierwszy w historii rozdział władzy cywilnej i wojskowej[potrzebny przypis].
W roku 331 p.n.e. na czele 40 tys. piechoty i 7 tys. jazdy Aleksander wyruszył w głąb Azji. Pod Gaugamelą w Asyrii na jego drodze stanął Dariusz III w asyście 40 tys. konnych oraz 16 tys. ciężkich piechurów wspartych słoniami, rydwanami i ogromnymi rzeszami lekkozbrojnej piechoty. Według ostrożnych szacunków armia perska liczyła około 200 tysięcy ludzi. Bitwę raz jeszcze rozstrzygnęła szarża ciężkiej macedońskiej jazdy – hetajrów – która wdarła się w lukę między perskimi oddziałami i ruszyła prosto na Dariusza. Ten już po raz drugi musiał ratować się ucieczką. Armia perska znowu poszła w rozsypkę. Po tym zwycięstwie Aleksander zajął Babilon i zdobył skarbiec perski w Suzie zagarniając bajeczne skarby perskiego władcy.
Kolejna armia perska przegrała z macedońskimi zdobywcami w 330 p.n.e., gdy próbowała zablokować drogę do stolicy Persji, Persepolis. Po zdobyciu miasta 1 października zostało ono splądrowane i spalone. W pościgu za uciekającym Dariuszem armia Aleksandra dotarła w okolice dzisiejszego Teheranu, a później ruszyła w kierunku Afganistanu. W końcu Aleksander dogonił Dariusza, konającego od ran zadanych mu przez zdradzieckiego satrapę Baktrii, Bessosa, który ogłosił się nowym królem Persji. Nie zakończyło to jednak kampanii. Aleksander postanowił pojmać morderców Dariusza i wyruszył ich śladem na wschód, gdzie Bessos rozpoczął rządy pod imieniem Artakserksesa V.
Przez kolejne lata Aleksander prowadził ciężkie walki w Baktrii i Sogdianie (obecne tereny Uzbekistanu i Afganistanu). Pojmał w końcu Bessosa i innych zabójców Dariusza. By zdławić ciągłe bunty, założył w tych prowincjach liczne Aleksandrie, w których osiedlał weteranów i kolonistów z Grecji. Ostatecznie podporządkował sobie te prowincje zawierając małżeństwo z Roksaną, księżniczką baktryjską. W tym samym czasie doszło do pierwszych konfliktów z częścią macedońskich arystokratów, którym nie podobało się przejmowanie przez Aleksandra perskich obyczajów; oburzała ich też coraz większa rola Persów w otoczeniu macedońskiego króla. Ofiarą gniewu Aleksandra padł m.in. Parmenion, najważniejsza osoba w armii po Aleksandrze, a zarazem jego najwierniejszy przyjaciel, który został oskarżony o zdradę.
Po zakończeniu podboju Sogdiany żądny dalszych zwycięstw Aleksander ruszył w roku 327 p.n.e. na Indie. W wyprawie uczestniczyło już wtedy około 120 tysięcy ludzi – macedońskich, greckich i azjatyckich żołnierzy oraz kupców, tragarzy, niewolników, kurtyzan i żon. Była to najcięższa część kampanii. Mieszkańcy ziem nad Indusem okazali się twardym przeciwnikiem, a klimat (m.in. częste deszcze) dawał się we znaki wojskom Aleksandra. Rozdrażnieni żołnierze urządzali rzezie miejscowej ludności. Nad rzeką Hydaspes macedoński król pokonał wojska indyjskiego księcia Porosa, który miał aż 200 słoni bojowych. Było to już ostatnie wielkie zwycięstwo Aleksandra. W Indiach Północnych Aleksander wywarł takie wrażenie na Hindusach, że przez pewien czas był czczony jako bóstwo. Zamieszkujący Himalaje Gurkhowie wywodzą pochodzenie swoich noży kukri od greckich mieczy kopis, w które uzbrojeni byli żołnierze Aleksandra. Zmęczone niekończącą się wojną wojsko odmówiło dalszego marszu. Wielki zdobywca musiał wrócić na zachód. W drodze powrotnej podczas szturmu na miasto Mallów w Indiach Aleksander sam rzucił się na mury, by pociągnąć niechętnych wojaczce żołnierzy do szturmu. Swojej brawury o mało nie przypłacił życiem, gdy indyjska strzała zraniła go w płuco. Maszerując z Indii do Persji ruszył z częścią sił przez pustynną Gedrozję. Marsz zakończył się śmiercią tysięcy ludzi z braku wody i jedzenia.
W 324 p.n.e. Aleksander zorganizował w Suzie olbrzymie wesele: nakłonił wielu dostojników macedońskich do małżeństwa z przedstawicielkami arystokracji perskiej. Sam poślubił dodatkowo dwie córki królów perskich: Parysatis II (córkę Artakserksesa III) i Statejrę II (córkę Dariusza III); miał już 3 żony. Hefajstion poślubił Drypetis, siostrę Statejry II. Wokół tej uroczystości powstało mnóstwo teorii w nowożytnej historiografii, starającej się zgłębić motywy i cele macedońskiego króla. W oczach wielu historyków Aleksander coraz szybciej przekształcał się w despotę i tracił kontakt z rzeczywistością. W świetle najnowszych badań ten sąd wydaje się jednak zbyt pochopny. Podczas wyprawy Aleksandra do Baktrii, Sogdiany i Indii wielu satrapów, sądząc, że nigdy już stamtąd nie powróci, nadużywało władzy przez niesubordynację, zły zarząd, a nawet grabież królewskich pieniędzy (jak Harpalos). Po powrocie króla ze wschodu wielu satrapów podejrzanych o spisek (np. niedostarczenie zapasów dla maszerującej przez Gedrozję armii) zostało skazanych na śmierć lub też zdjętych ze stanowiska, co być może było jedynym sposobem, by opanować poważny kryzys imperium, jaki wówczas nastąpił.
Stan Aleksandra pogorszył się latem 324 p.n.e. w Ekbatanie, po śmierci Hefajstiona, jego najbliższego przyjaciela, współpracownika, a według części źródeł, także kochanka. Na pogrzeb i grobowiec wydał gigantyczną fortunę i zaczął jeszcze bardziej nadużywać alkoholu. Innym wieloletnim ukochanym Aleksandra miał być, podarowany mu w hołdzie przez perskiego generała, eunuch Bagoas[52]. Te wieloletnie homoseksualne związki romantyczne Aleksandra miały być powodem troski o możliwość spłodzenia następcy. Pisał o tym już w starożytności rzymski historyk Kwintus Kurcjusz Rufus (Quintus Curtius)[53]. Obawy te okazały się bezzasadne, gdyż Aleksander miał być ojcem pogrobowca Aleksandra IV. Synem Aleksandra ze związku z kochanką Barsine był być może Herakles (ur. ok. 327 p.n.e.).
Po kilkunastu dniach choroby Aleksander zmarł. Nie można wykluczyć, że został otruty przez obawiających się kolejnej fali czystek dowódców jego armii (trucizna wlana do wina). Kilka miesięcy po jego śmierci Roksana urodziła mu syna Aleksandra IV. 
Inne teorie mówią, że przyczyną śmierci Aleksandra mogło być niezamierzone przedawkowanie stosowanej wówczas jako lek białej ciemiężycy, która w mniejszych dawkach działa jako lekarstwo, lecz w większych zabija. 
W 2018 roku opublikowana została jeszcze inna hipoteza, wg której Aleksander cierpiał na zespół Guillaina-Barrégo, schorzenie autoimmunologiczne, wskutek którego pod koniec życia doznał całkowitego paraliżu, który w połączeniu ze skrajnie spłyconym i spowolnionym oddechem i zanikiem reakcji na bodźce uznany mógł zostać przez starożytnych, nie dysponujących dokładniejszymi metodami, za objaw śmierci. Na poparcie tej tezy autorka opracowania wskazuje m.in. fakt, że przez sześć dni po stwierdzeniu zgonu, a przed rozpoczęciem balsamowania, pomimo wysokiej temperatury panującej w czerwcu w Babilonie, jego ciało (jak opisywał Plutarch) pozostawało "czyste i świeże"[54][55][56].
Nie jest znane miejsce pochówku Aleksandra Macedońskiego[57].
Mimo że nie rządził swoim wielkim imperium zbyt długo, efekty jego podbojów były ogromne. Zakładane przez niego greckie kolonie spowodowały rozprzestrzenienie się helleńskiej kultury na ogromnych obszarach, co spowodowało powstanie kultury hellenistycznej. W kilka lat po jego śmierci imperium zostało podzielone w wyniku wojen diadochów – dowódców armii Aleksandra Wielkiego. Założone przez nich królestwa na wiele lat zdominowały Bliski Wschód.
Dzieje Aleksandra są nam znane dzięki przekazom Flawiusza Arriana, Plutarcha, Diodora Sycylijskiego, Pompejusza Trogusa za pośrednictwem epitomy Marka Junianusa Justynusa oraz Kwintusa Kurcjusza Rufusa.
O Aleksandrze Macedońskim wspomina też Biblia, w 1 Księdze Machabejskiej: Aleksander, syn Filipa Macedońskiego, wyruszył z kraju Kittim i po zwycięskich walkach pokonał Dariusza, króla Persów i Medów, i w jego miejsce objął panowanie najpierw nad Helladą. Prowadził on wiele wojen: zdobył [wiele] umocnionych miejscowości i wytracił królów [rządzących na swych] ziemiach. Przeszedł on aż na krańce świata, a nabrał łupów od wielu narodów. Cała ziemia przed nim zamilkła, jego zaś serce wbiło się w pychę. Zebrał bowiem bardzo liczne wojsko i rządził państwami, narodami i [ich] władcami tak, że płacili mu daniny.
Imieniem króla nazwano krater Alexander na Księżycu[59].
Opracowania:
Źródła:
Elżbieta I Tudor (ur. 7 września 1533 w Greenwich, zm. 24 marca 1603[a] w Richmond) – królowa Anglii i Irlandii (panowała od 17 listopada 1558 do śmierci w 1603, czyli 44 lata), córka Henryka VIII i jego drugiej żony Anny Boleyn. Ostatnia z rodu Tudorów. Nazywana Glorianą[2] lub Królową-Dziewicą (the Virgin Queen)[b].
Elżbieta była jedynym dzieckiem z drugiego małżeństwa Henryka VIII Tudora z Anną Boleyn, które dożyło wieku dojrzałego. Henryk i Anna byli ze sobą związani prawdopodobnie już od roku 1527[3], ale w wyniku sprzeciwu papieża mogli się pobrać dopiero w końcu 1532 r., gdy dzięki odłączeniu się Kościoła angielskiego od papiestwa rozwód Henryka VIII z poprzednią żoną, Katarzyną Aragońską, stał się możliwy.
Anna została koronowana w lecie 1533 r., kiedy była już w ciąży z pierwszym dzieckiem. Noworodkowi nadano imię Elżbieta na cześć jej dwóch babć: Elżbiety York i lady Elżbiety Boleyn. Henryk wolał, by urodził się syn, ale na razie Elżbieta była jedyną osobą godną dziedziczenia, jako że rozwód wyeliminował Marię, córkę Henryka z pierwszego małżeństwa.
Elżbieta miała dwie ciotki ze strony ojca. Pierwszą była Małgorzata Tudor, późniejsza królowa Szkocji, i młodsza siostra Henryka VIII Maria Tudor, późniejsza księżna Suffolk; lady Maria Boleyn, ciotka ze strony matki, była królewską metresą na kilka lat przed poznaniem Anny przez Henryka. Wujem ze strony matki był George Boleyn, wicehrabia Rochford, gorący zwolennik religii protestanckiej.
Pierwsze lata Elżbiety upływały w luksusie, szczególnie, że matka była wobec niej bardzo pobłażliwa. Szczęście to zakończyło się wraz z trzecimi urodzinami, bo 19 maja 1536 r. królowa Anna została ścięta, będąc oskarżoną o zdradę, cudzołóstwo i uprawianie czarów[4][5]. Wiadomym jest, że oskarżenia te były bezpodstawne[6]. Henryk nie okazywał żadnego smutku z powodu śmierci żony. 11 dni po egzekucji poślubił jedną z jej dam dworu – Jane Seymour. Także wuj Elżbiety został skazany na śmierć i ścięty na dwa dni przed śmiercią Anny, oskarżony o cudzołóstwo i kazirodztwo. Elżbieta została wtedy uznana za niegodną dziedziczenia, utraciła tytuł księżniczki i była tytułowana jedynie lady. Gdy jej ojciec poślubiał kolejne żony, żyła na wygnaniu.
Jane Seymour umarła w 1537 r. w wyniku gorączki połogowej po urodzeniu syna – późniejszego Edwarda VI. Trzy lata później jej miejsce zajęła niemiecka księżniczka Anna z Kleve, z którą Henryk szybko wziął rozwód. Następna była lady Katarzyna Howard, pochodząca z arystokratycznego i katolickiego rodu. Katarzyna jednak już po dwóch latach małżeństwa również skończyła na szafocie. Elżbieta miała wówczas 8 lat. Słysząc o śmierci Katarzyny, miała powiedzieć podobno: I will never marry (Nigdy nie wyjdę za mąż).
Pierwszą guwernantką Elżbiety była lady Margaret Bryan, baronowa, którą Elżbieta nazywała Muggie. W wieku 4 lat Elżbieta miała nową opiekunkę Katarzynę Champernowne, która, znana jako Kat. Champernowne, nawiązała bliskie i przyjazne stosunki z małą dziewczynką i została jej zaufaną i wieloletnią przyjaciółką. Matthew Parker, ulubiony kapelan jej matki, zajmował się wychowaniem religijnym Elżbiety. Parker został arcybiskupem Canterbury, gdy jego podopieczna została królową. Innym towarzyszem jej młodych lat był Irlandczyk Thomas Butler, późniejszy trzeci hrabia Ormonde, który był związany z rodziną Boleynów.
Przyszła królowa odziedziczyła cechy zarówno ojca, jak i matki. Była więc osobą charyzmatyczną, urzekającą, lubiącą flirtować. Cechy te pasowały zarówno do Henryka, który w młodości był notorycznym podrywaczem, jak i do Anny, znanej z piękna i wdzięku. Elżbieta odziedziczyła po matce delikatną budowę ciała, czarne oczy i zgrabną figurę. Po ojcu zaś – rude włosy, chęć do nauki oraz intelekt i zdolność do zdobywania sympatii ludzi.
Ostatnia żona Henryka VIII Katarzyna Parr pomogła mu się pogodzić z Elżbietą. Zgodnie z Aktem Sukcesyjnym z 1544 r. Maria (córka Katarzyny Aragońskiej) i Elżbieta zostały włączone do dziedziczenia tronu zaraz po księciu Edwardzie.
Henryk VIII Tudor umarł w 1547 r., a tron po nim objął Edward VI. Świeżo owdowiała Katarzyna Parr poślubiła Thomasa Seymoura, pierwszego barona Sudeley. Tomasz i Katarzyna zaopiekowali się Elżbietą i zabrali ją do swojego domu. Tam też otrzymała wykształcenie pod kierunkiem Rogera Aschama. Udało jej się posiąść umiejętność władania siedmioma językami. Oprócz ojczystego angielskiego znała: szkocki, francuski, włoski, hiszpański, grecki i łacinę. Posiadała też zdumiewającą inteligencję. Pod wpływem swoich opiekunów Katarzyny i Aschama Elżbieta została wychowana jako protestantka.
Tak długo, jak jej przyrodni brat – protestant – pozostawał na tronie, ona sama była bezpieczna. Edward jednak od urodzenia był dzieckiem bardzo chorowitym, co nie dawało mu perspektyw na długie rządzenie krajem. Umarł w 1553 roku w wieku 15 lat i od tego momentu zaczęły się trudne dni dla młodej Elżbiety.
Niezgodnie z Aktem sukcesyjnym zarówno Maria, jak i Elżbieta zostały wykluczone z dziedziczenia, a Jane Grey ogłoszona dziedziczką tronu. Zawiązał się w tym czasie spisek z udziałem Johna Dudleya, 1. księcia Northumberland, którego syn Guildford był żonaty z Lady Jane. Spisek mający na celu wsparcie młodej królowej został jednak wykryty, a sama Jane usunięta z tronu 9 dni później i ścięta. Sprawczynią tego zamieszania była Maria, prawowita następczyni tronu, która po tych wydarzeniach wjechała triumfalnie do Londynu razem ze swoją przyrodnią siostrą Elżbietą.
Maria I Tudor doprowadziła do zawarcia przez siebie małżeństwa z królem Hiszpanii – Filipem II, mając nadzieję na wzmocnienie katolicyzmu w Anglii. Powstanie Wyatta (Wyatt's Rebellion), które wybuchło w 1554 r., miało na celu zapobieżenie temu małżeństwu. Powstanie skończyło się jednak porażką, a zamieszana w nie Elżbieta została uwięziona w Tower.
Mimo żądań, by Elżbieta została skazana na śmierć, jej siostra Maria sprzeciwiała się, argumentując, że byłby to niebezpieczny precedens dla królobójstwa. Maria podejmowała próby, aby Parlament wykluczył jej siostrę z dziedziczenia, skończyły się one jednak niepowodzeniem. Po dwóch miesiącach pobytu w Tower Elżbieta została zwolniona, ale dalej przebywała w areszcie domowym pod nadzorem Henryka Bedingfielda. Pod koniec roku, kiedy krążyły plotki o rzekomej ciąży Marii, na rozkaz Filipa pozwolono Elżbiecie wrócić na dwór królewski.
Filip wolał, żeby Elżbieta dziedziczyła tron po Marii, niżby miała to być Maria I Stuart, następna w linii dziedziczenia, a to ze względu na otwartą wrogość szkockiej królowej wobec Hiszpanii, spowodowaną sojuszem z Francją. Maria Tudor jako gorliwa katoliczka naznaczyła swoje rządy prześladowaniem protestantów, których uznawała za heretyków. Wśród swoich wrogów zaczęła być znana jako „Krwawa Maria” (ang. Bloody Mary).
W 1558 r. umarła Maria I Tudor. Zgodnie z porządkiem dziedziczenia Elżbieta objęła tron. W czasie jej pochodu ulicami stolicy była serdecznie witana przez zwykłych mieszkańców Londynu, którzy na jej cześć tworzyli przedstawienia i sztuki opiewające jej piękno i grację. Została ukoronowana 15 stycznia 1559 r. Przy ceremonii nie był jednak obecny arcybiskup Canterbury – ostatnio piastujący to stanowisko Reginald Pole, będący katolikiem, zmarł kilka godzin po królowej Marii. Ponieważ ważniejsi w hierarchii biskupi odmawiali zgody na uczestniczenie w koronacji (Elżbieta była niegodna dziedziczenia zarówno według prawa kanonicznego, jak i państwowego, a na jej niekorzyść przemawiało to, że była protestantką), wyznaczono do koronacji mniej ważnego biskupa Carlisle Owena Oglethorpe’a, który zgodził się ją namaścić na królową.
Komunia została udzielona przez osobistego kapelana królowej, aby uniknąć przepisów katolickich. Koronacja Elżbiety była ostatnią, która odbyła się według obrządku łacińskiego; następne były już prowadzone w obrządku anglikańskim. Później Elżbieta przekonała osobistego kapelana swojej matki, Matthew Parkera, aby został arcybiskupem Canterbury.
Jednym z najważniejszych problemów w okresie panowania młodej królowej były sprawy religijne. Na początku polegała ona w tym zakresie głównie na sir Williamie Cecilu, późniejszym lordzie Burghley. Jej pierwszym krokiem było wydanie Act of Uniformity (Aktu o ujednoliceniu), który nakazywał używanie protestanckiego modlitewnika we wszystkich kościołach (ang. The Book of Common Prayer). Papieska kontrola nad kościołem angielskim, przywrócona przez Marię, została przez Elżbietę zniesiona. Królowa przyjęła tytuł „Najwyższego Zwierzchnika Kościoła Anglii” (ang. Supreme Governor of the Church of England) zamiast „Najwyższej Głowy”, głównie ze względu na powszechny pogląd, że kobieta nie może być głową kościoła. Tzw. Akt o Zwierzchnictwie (ang. Act of Supremacy), wydany w 1559 r., nakazywał wszystkim urzędnikom złożenie przysięgi, uznającej królewską zwierzchność nad kościołem; w przeciwnym razie mogło im grozić oskarżenie o zdradę i egzekucja.
Wielu biskupów było zrażonych do nowej polityki religijnej. Zostali więc usunięci ze swoich urzędów i zastąpieni ludźmi wiernymi królowej. Elżbieta wybrała także nową Radę Przyboczną, usuwając z niej przy okazji wszystkich katolików. Głównymi doradcami Elżbiety byli sir William Cecil jako sekretarz stanu i sir Nicholas Bacon jako strażnik Wielkiej Pieczęci (ang. Lord Keeper of the Great Seal).
Zmniejszone zostały także wpływy hiszpańskie w Anglii. Elżbieta pozostawała niezależna od Filipa II w polityce zagranicznej. Jej mottem była zasada „Anglia dla Anglików” (ang. England for the English). Na takiej polityce cierpiała oczywiście Irlandia, podbita przez Anglię. Znalazło się tam wielu przeciwników nowej królowej, jako że wyspa była prawie w całości katolicka.
Wkrótce po wstąpieniu Elżbiety na tron dworzanie rozpoczęli dyskusję, kogo poślubi królowa. Powodów, dla których miałaby pozostać w stanie panieńskim, było wiele. Przede wszystkim do instytucji małżeństwa zraził ją jej ojciec Henryk, który traktował swoje kolejne żony bardzo instrumentalnie. Także śmierć matki nie była dla niej miłym wspomnieniem.
Ówczesna plotka głosiła, że bała się ukazywać swoje ciało, co miało być spowodowane jej ułomnością fizyczną po przebytej ospie. Niektórzy twierdzili, że jej wybrankiem był Robert Dudley, 1. hrabia Leicester, w którym miała być zakochana, a na przeszkodzie stała jedynie jej Rada Przyboczna, która sprzeciwiała się temu małżeństwu. Było to spowodowane udziałem rodziny Dudleya w sprawie Jane Grey i tajemniczą śmiercią jego pierwszej żony.
Przyszły mąż mógł być także przedmiotem rozgrywek pomiędzy poszczególnymi frakcjami istniejącymi na dworze. Panieństwo królowej okazało się atutem w dyplomacji: swą rękę mogła wykorzystywać jako przynętę dla innych władców europejskich. Małżeństwo mogło także kosztować ją utratę niezależności oraz uszczuplić jej stan majątkowy. Mimo wielu starających się (m.in. Filip II – król Hiszpanii, Iwan IV Groźny – car Rosji czy Duc d'Alençon – książę francuski) Elżbieta nigdy nie zdecydowała się na małżeństwo.
Rywalką Elżbiety w tym okresie stała się władczyni Szkocji, katoliczka Maria Stuart, zamężna z królem Francji, Franciszkiem II. W 1559 r. Maria uznała się królową Anglii i zyskała poparcie Francji. W Szkocji matka Marii, Maria de Guise, próbowała umocnić wpływy francuskie, sprowadzając z Francji wojsko i budując fortyfikacje, które miały posłużyć do inwazji na Anglię. Grupa szkockich lordów, sprzymierzona z Elżbietą, usunęła jednak Marię de Guise i pod naciskiem Anglii Maria podpisała Traktat Edynburski, w którym zobowiązała się do wycofania wojsk francuskich. Po tym wydarzeniu wpływy francuskie w Szkocji bardzo osłabły.
Po śmierci swojego męża Franciszka Maria Stuart wróciła do Szkocji. W międzyczasie we Francji wybuchł konflikt pomiędzy katolikami i hugenotami. Elżbieta potajemnie udzielała pomocy protestantom. W 1564 r. podpisała z Francją traktat pokojowy, zrzekając się jednocześnie praw do jedynej posiadłości angielskiej na kontynencie, miasta Calais. Miasto to faktycznie zostało utracone przez Anglię już w czasie panowania Marii I, po klęsce ekspedycji angielskiej w Hawrze. Nie zrzekła się jednak roszczeń do korony francuskiej, które wywodziły się jeszcze z czasów Edwarda III (patrz: wojna stuletnia), a ustały dopiero w XVIII w. za czasów Jerzego III.
W końcu 1562 r. Elżbieta zachorowała na ospę, ale została wyleczona przez Burkota – lekarza pochodzenia śląskiego[7]. W 1563 r. parlament, zaalarmowany jej chorobą, zażądał, aby poślubiła kogoś lub mianowała swojego dziedzica, zapobiegając tym samym wojnie domowej po swojej śmierci. Elżbieta odrzuciła obydwie prośby i zawiesiła funkcjonowanie parlamentu, który nie zbierał się aż do 1566 r., kiedy potrzebna była jego zgoda na podniesienie podatków. Izba Gmin zagroziła, że wstrzyma finansowanie skarbu królewskiego, dopóki Elżbieta nie rozstrzygnie sprawy swego następstwa. Ona jednak ciągle odmawiała.
Pod rozwagę były brane różne linie dziedziczenia. Jedna wywodziła się od Małgorzaty Tudor, starszej siostry Henryka VIII, i prowadziła do Marii Stuart. Alternatywą była linia wywodząca się od młodszej siostry Henryka VIII, Marii Tudor, księżnej Suffolk. Z linii tej pochodziła Lady Katarzyna Grey i jej siostra. Dalszym możliwym sukcesorem był Henryk Hastings, trzeci hrabia Huntingdon, który mógł się powoływać jedynie na pochodzenie od Edwarda III. Każda z tych możliwości miała swoje wady i zalety: Maria Stuart była katoliczką, Lady Katarzyna Grey wyszła za mąż bez zgody królowej, purytański Lord Huntingdon nie chciał przyjąć korony.
Maria Stuart miała własne kłopoty w Szkocji. Elżbieta próbowała ją wyswatać z protestantem Robertem Dudleyem, co miało być warunkiem koniecznym do uznania jej praw do tronu po Elżbiecie. Maria jednak odmówiła. Po licznych perypetiach jej szkoccy poddani zbuntowali się, uwięzili ją i zmusili do abdykacji na rzecz syna.
Problem sukcesji został poruszony w parlamencie, który powołał specjalny komitet do jego rozwiązania. 19 października 1566 sir Robert Bell przekonywał królową do zabrania głosu w tej sprawie. Nie spotkał się jednak z zadowalającym odzewem. W 1568 zmarła jedna z możliwych dziedziczek, Katarzyna Grey, pozostawiając jedynie syna z nieprawego łoża.
Spadkobierczynią Katarzyny była jej siostra Maria Grey, która miała niezbyt przyjemną prezencję – była garbatą karlicą. W grze pozostawała zatem jedynie królowa szkocka, która tymczasem uciekła z więzienia do Anglii i została pojmana przez wojska Elżbiety. Powstał problem: odesłanie jej z powrotem do Szkocji nie służyłoby interesom Anglii, wysłanie do Francji spowodowałoby, że Francuzi uzyskaliby nowe narzędzie przeciw Elżbiecie, zaś uwięzienie w Anglii sprawiłoby, że Maria stałaby się inspiracją dla spiskowania przeciw Elżbiecie. Królowa wybrała to ostatnie rozwiązanie, które według niej miało być najmniejszym złem.
W 1569 r. wybuchło tzw. powstanie północne, inspirowane przez Thomasa Howarda, 4. księcia Norfolk, Charlesa Neville’a, 6. hrabiego Westmoreland i Thomasa Percy’ego, 7. hrabiego Northumberlandu. W kwietniu 1570 roku papież Pius V promulgował bullę Regnans in Excelsis ekskomunikującą Elżbietę, a także każdego z jej poddanych, który wykonywał jej rozkazy.
Elżbieta miała teraz nowego wroga w swoim byłym szwagrze Filipie II, królu Hiszpanii. Kiedy Filip nakazał atakować angielskie statki kaperskie, Francis Drake i John Hawkins otrzymali zgodę Elżbiety na zatrzymanie skarbów zdobytych na hiszpańskich statkach. Filip był już jednak uwikłany w tłumienie powstania w Niderlandach i to uchroniło Anglię przed wypowiedzeniem jej wojny przez Hiszpanię.
Filip brał udział w kilku spiskach, mających na celu usunięcie Elżbiety. Norfolk był uwikłany w pierwszy z nich, tzw. spisek Ridolfiego w 1571 r. Książę planował poślubić Marię Stuart i obalić Elżbietę. Po wykryciu całej sprawy książę Norfolk został stracony, a Maria poddana ostrzejszemu nadzorowi.
W 1571 r. sir William Cecil został mianowany Lordem Burghley. Ten przebiegły dyplomata pozostawał doradcą królowej aż do swojej śmierci w 1598 r. W 1572 r. został awansowany na stanowisko sekretarza do spraw skarbu. Stanowisko sekretarza stanu objął po nim sir Francis Walsingham.
W 1572 r. doszło także do zawarcia sojuszu z Francją. Masakra protestantów podczas tzw. „nocy św. Bartłomieja” nadwerężyła to porozumienie, ale nie doprowadziła do jego złamania. Elżbieta prowadziła też rozmowy z Franciszkiem Walezjuszem, księciem Évreux, Alençon i Andegawenii (bratem Henryka III, króla Polski i Francji) w sprawie zawarcia małżeństwa. Anjou przyjechał do Anglii w 1581 r., jakkolwiek znane były jego skłonności do płci męskiej i z negocjacji nic nie wyszło.
W 1580 r. papież Grzegorz XIII wysłał pomoc wojskową dla powstania Desmonda w Irlandii. W 1583 r. powstańcy musieli jednak się poddać po rzezi, jakiej dokonały oddziały angielskie. Straszne zniszczenia wśród Irlandczyków poczynił też panujący na skutek powstania głód.
W tym samym roku Filip II przejął we władanie Portugalię, zwiększając tym samym swoją potęgę na morzu. Po zamordowaniu Wilhelma I Orańskiego Elżbieta stanęła otwarcie po stronie zbuntowanych prowincji niderlandzkich. To, łącznie z przyczynami ekonomicznymi i angielskim piractwem (doszło nawet do sojuszu Anglii z islamskim Marokiem przeciwko Hiszpanii), doprowadziło do wybuchu wojny pomiędzy Hiszpanią i Anglią w 1585 r. W 1586 r. hiszpański ambasador został wydalony za udział w spisku przeciw Elżbiecie.
Obawiając się spisku przeciw królowej, parlament w 1584 r. wydał nawet specjalną ustawę, zgodnie z którą każdy, kto był zamieszany w spisek na życie suwerena, miał być wyłączony od dziedziczenia tronu angielskiego. Późniejszy spisek Babingtona został wykryty dzięki siatce szpiegowskiej, jaką uruchomił sir Francis Walsingham. Zamieszana w spisek była Maria Stuart. Elżbieta pozwoliła na jej proces, a następnie podpisała wyrok skazujący ją na śmierć. Został on wykonany 8 lutego 1587 r.
W swoim testamencie Maria przekazała swoje roszczenia do tronu Filipowi II, który w tym czasie przygotowywał plan inwazji na Anglię. W lipcu 1588 r. hiszpańska „Wielka Armada”, licząca 130 okrętów i 30 tys. ludzi, wyruszyła z Hiszpanii, aby spotkać się z księciem Parmy, dowodzącym wojskami hiszpańskimi w Niderlandach. Do spotkania jednak nie doszło, bo udaremniła je flota angielska pod dowództwem Charlesa Howarda i Francisa Drake’a, której sprzyjała zła pogoda, pomagając rozproszyć i zatopić większość sił hiszpańskich. Armada była zmuszona wracać do Hiszpanii, opływając Wyspy Brytyjskie od północy i tracąc przy okazji większość swoich okrętów na skałach Szkocji i Irlandii. Zwycięstwo to bardzo przyczyniło się do wzrostu popularności Elżbiety w kraju.
Starcie jednak nie było decydujące i wojna toczyła się nadal, tyle że w Niderlandach. Anglia zaangażowała się także w konflikt we Francji, gdzie wspierała protestanckiego pretendenta do tronu, Henryka Burbona (późniejszego Henryka IV). W 1596 r. Anglia wycofała się ostatecznie z Francji po tym, jak Henryk Burbon zdobył władzę. W tym samym czasie siły hiszpańskie wylądowały w Bretanii i pokonały stacjonujących tam Anglików. Elżbieta wysłała 2 tys. ludzi do Francji wkrótce po tym, jak Hiszpanie zdobyli Calais. Niepowodzeniem zakończyła się także próba zdobycia Azorów w 1597 r. Ostatecznie w 1598 r. Hiszpania i Francja podpisały pokój.
Wojna angielsko-hiszpańska utkwiła tymczasem w martwym punkcie po śmierci Filipa II. Zakończył ją dopiero traktat pokojowy podpisany już przez następcę Elżbiety Jakuba I w 1604 r.
W 1598 r. zmarł główny doradca królowej, Lord Burghley. Talenty polityczne odziedziczył po nim jego syn Robert Cecil, który został mianowany sekretarzem stanu w 1590 r. W tym czasie popularność Elżbiety spadała, co było spowodowane głównie jej praktyką przyznawania monopoli królewskich, których zniesienia domagał się parlament. Wkrótce Elżbieta obiecała przeprowadzenie reform. 12 monopoli królewskich zostało zniesionych przez proklamację królewską. Reformy były jednak powierzchowne, a monopole przyznawano nadal.
W czasie wojny angielsko-hiszpańskiej miało miejsce następne powstanie w Irlandii, tzw. irlandzka wojna dziewięcioletnia. Przedstawiciel królowej w Irlandii, drugi hrabia Tyrone, Hugh O’Neill, który stanął na czele powstańców, został uznany za zdrajcę. Elżbieta, chcąc zaprowadzić porządek jak najmniejszym kosztem, zawierała z nim szereg rozejmów. Hiszpania próbowała także w tym czasie dwukrotnie wysyłać posiłki dla Irlandczyków, które jednak nie dotarły ze względu na złe warunki pogodowe. W czasie rozejmu z Anglikami Tyrone wyszkolił i dozbroił swoje wojska. Po tych przygotowaniach zadał Anglikom druzgocącą klęskę pod Yellow Ford.
W 1599 r. Robert Devereux, 2. hrabia Essex, został wysłany do Irlandii, aby w końcu zdławić powstanie. Dostał największą armię, jaką kiedykolwiek wysłano do Irlandii. Essex wkrótce jednak roztrwonił swoje siły. Kiedy bez pozwolenia królowej wrócił do kraju, został pozbawiony wszelkich przywilejów i monopoli, swojego głównego źródła dochodów.
W 1601 r. Essex stanął na czele rewolty przeciw królowej, jednak nie zyskała ona szerokiego poparcia, a dotychczasowy ulubieniec tłumów został stracony. Na miejsce Essexa do Irlandii został wysłany Charles Blount, 8. baron Mountjoy. Mountjoy próbował zwyciężyć Irlandczyków głodem. W 1601 r. Hiszpanie przysłali posiłki w liczbie 3,5 tys. ludzi, usprawiedliwiając to tym, że Elżbieta także udzielała pomocy buntownikom w Niderlandach. Po długotrwałym zimowym oblężeniu Mountjoy pokonał Hiszpanów i Irlandczyków w bitwie pod Kinsale. Tyrone poddał się w kilka dni po śmierci Elżbiety w 1603 r.
Podobno Elżbieta miała powiedzieć: Wysłałam wilki, a nie pasterzy, aby rządziły Irlandią, bo nie zostawili mi [tam] niczego do rządzenia poza popiołami i trupami (ang. I sent wolves, not shepherds, to govern Ireland, for they have left me nothing to govern over but ashes and carcasses).
Po straceniu Essexa królowa popadła w neurastenię. Wcześniej już cierpiała na reumatyzm i żółtaczkę, a potem jej stan zdrowia znacznie się pogorszył. Nie chciała przyjmować lekarstw. Dopiero 21 marca 1603 r. dała się namówić na położenie się do łóżka. Przy łożu śmierci czuwał arcybiskup Canterbury. Zmarła w poniedziałek 24 marca 1603 r.[a]
Nie przeprowadzono sekcji zwłok ani nie zabalsamowano ciała. Szereg podań opisuje napęcznienie i rozerwanie trumny przez zwłoki Elżbiety przed pochowaniem jej szczątków. Elżbietę pochowano obok jej starszej siostry przyrodniej, Marii I, w opactwie westminsterskim.
Królowa Elżbieta była nie tylko opiekunką poetów i dramaturgów, ale również autorką wierszy[8]. Jej najbardziej znanym utworem jest wiersz zaczynający się od słów The doubt of future foes exiles my present joy[9].
W 1955 roku powstała Królowa dziewica (ang. The Virgin Queen) – amerykański film w reżyserii Henry’ego Kostera, opowiadający o życiu Elżbiety I
W ostatnich latach powstały dwa filmy historyczne przedstawiające dzieje królowej Elżbiety:
W obu filmach rolę królowej Elżbiety zagrała australijska aktorka Cate Blanchett.
W 1971 roku powstał serial telewizyjny Królowa Elżbieta, w którym główną rolę zagrała Glenda Jackson.
Jawaharlal Nehru (hindi: जवाहरलाल नेहरू, Javāharlāl Nehrū), (uproszczona wymowa pol. [dżewa’harlal nehru]), zwany także Pandit (Nauczyciel) Nehru (ur. 14 listopada 1889 w Allahabadzie, zm. 27 maja 1964 w Nowym Delhi) – indyjski polityk, przywódca socjalistycznego skrzydła Indyjskiego Kongresu Narodowego w okresie walki o niepodległość przeciwko dominacji brytyjskiej. 15 sierpnia 1947 został pierwszym premierem Indii, piastując tę funkcję aż do śmierci. Ojciec Indiry Gandhi (trzeciej premier Indii) i dziadek Rajiva Gandhiego (szóstego premiera Indii).
W 1929 roku po przeprowadzeniu przez Indyjski Kongres Narodowy kampanii na rzecz samostanowienia Indii (z udziałem Mahatmy Gandhiego) został przewodniczącym partii. Po wystąpieniu w 1942 roku z deklaracją niepodległości Indii wraz ze swoimi współpracownikami został aresztowany. Po wyjściu z więzienia brał udział w negocjacjach, których owocem było powstanie dwóch niezależnych państw – Indii i Pakistanu. Polityka Nehru po objęciu stanowiska premiera opierała się na interwencjonizmie gospodarczym i uczestnictwie w ruchu niezaangażowania w okresie zimnej wojny. Jako przywódca Indii wprowadzał w życie zasady niezaangażowania, świeckości, utworzył w kraju wielopartyjną demokrację i ustanowił system republikański. W okresie jego rządów Indie stały się regionalnym hegemonem w rejonie Azji Południowej. Obok Josipa Broz Tity, Sukarno i Gamala Abdel Nasera czołowa postać Ruchu Państw Niezaangażowanych. Pod przewodnictwem Nehru, Kongres wygrał wybory w 1951, 1957 i 1962 roku. Początkowo szukał możliwości sojuszu z Chinami, polityka ta była jednak stopniowo rewidowana na rzecz lepszych stosunków z Zachodem, a pod koniec jego rządów miejsce miała wojna chińsko-indyjska.
Po jego śmierci, jego politykę kontynuowała córka Indira Gandhi.
Urodził się 14 listopada 1889 roku w Allahabadzie w Indiach będących wówczas brytyjską kolonią. Jego ojciec Motilal Nehru (1861-1931) był potomkiem kaszmirskich braminów[1]. W 1886 roku przeniósł się z Agry do Allahabadu, został adwokatem. Motilal był dwukrotnym przewodniczącym Indyjskiego Kongresu Narodowego. Matka Jawaharlala, Swaruprani Thussu (1868-1938), pochodziła z bramińskiej rodziny w Lahore, w Kaszmirze. Swaruprani była drugą żoną Motilala (pierwsza umarła zaraz po urodzeniu dziecka). Jawaharlal był jedynym chłopcem i najstarszym z trójki rodzeństwa. Starsza siostra Jawaharlala, Vijaya Lakshmi, została później pierwszą kobietą-prezydentem Zgromadzenia Ogólnego Narodów Zjednoczonych[2] natomiast młodsza, Krishna Hutheesing, została pisarzem i napisała kilka książek na temat brata[3].
Kształceniem Jawaharlala zajmowały się guwernantki i nauczyciele domowi (głównie Anglicy). Pod wypływem nauczyciela, Ferdinanda T. Brooksa, Nehru zainteresował się teozofią[4]. Nehru w wieku trzynastu lat poprzez przyjaciółkę rodziny Annie Besant, dołączył do Towarzystwa Teozoficznego. Jego zainteresowanie się teozofią nie okazało się jednak trwałe i wkrótce po tym jak odszedł jego nauczyciel, Nehru opuścił Towarzystwo[5]. Chociaż Nehru był sceptyczny wobec religii, jego młodzieńcza fascynacja teozofią wpłynęła na jego późniejsze zainteresowania; Nehru poświęcał się studiowaniu pism buddyjskich i hinduistycznych[6]. Kiedy ukończył 15 lat, ojciec wysłał go do szkoły Harrow School[7]. W młodości był zagorzałym nacjonalistą. Jego nacjonalistyczne poglądy umocniły wojny burskie i wojna rosyjsko-japońska. W czasie wojny między Imperium Rosyjskimi a Japonią, sympatyzował z Japonią, tak jak wielu Azjatów sądził, że Japonia stoi na straży wolności narodów Azji[4]. Gdy Nehru rozpoczął w 1905 roku naukę w szkole w Harrow, w Wielkiej Brytanii, znalazł się pod wpływem myśli włoskiego rewolucjonisty Giuseppe Garibaldiego, którego książkę otrzymał od znanego brytyjskiego historyka Georga Macaulay Trevelyana jako nagrodę w osiągnięciach naukowych. Widział on w Garibladim rewolucyjnego bohatera którego idee mogłyby znaleźć odzwierciedlenie w Indiach[4].
W październiku 1907 roku, Nehru kontynuował edukację w Trinity College na Uniwersytecie Cambridge. Szkołę z wyróżnieniem w naukach przyrodniczych ukończył w 1910 roku[8]. W tym okresie, studiował politykę, ekonomię, historię i literaturę. Na jego poglądy polityczne i gospodarcze wpłynęły dzieła myślicieli i ekonomistów takich jak George Bernard Shaw, Herbert George Wells, John Maynard Keynes, Bertrand Russell, Goldsworthy Lowes Dickinson i Meredith Townsend[4]. Po zdobyciu w 1910 roku dyplomu uczelni wyjechał do Londynu. Został na dwa lata przyjęty na studia prawnicze w londyńskiej Inns of Court School of Law (Inner Temple)[9]. W okresie londyńskim poznał filozofię lewicowego Towarzystwa Fabiańskiego, szczególnie zainteresował się poglądami Beatrice Webb[4]. Egzamin kończący studia zdał w 1912 roku, a w sierpniu powrócił do Indii gdzie rozpoczął pracę jako adwokat[10].
Zainteresował się polityką jeszcze w czasie pobytu w Wielkiej Brytanii. W 1912 roku wziął udział w rocznej sesji Indyjskiego Kongresu Narodowego w Patnie[11]. W 1913 roku wstąpił do organizacji kongresowej w Zjednoczonych Prowincjach. Początkowo był zażenowany elitami partii a partia nie spełniała jego oczekiwań – Kongres był wówczas partią umiarkowanych elit. Mimo wątpliwości dołączył dla partii, a w ramach działalności politycznej organizował pomoc dla ruchu praw obywatelskich w Związku Południowej Afryki[12]. W 1913 roku uczestniczył w zbieraniu funduszy na kampanię praw obywatelskich prowadzoną przez Mahatmę Gandhiego. Później uczestniczył w kampanii przeciwko łamaniu praw pracowniczych i innych dyskryminacji spotykających Hindusów ze strony brytyjskich władz kolonialnych[13]. Wybuch I wojny światowej silnie spolaryzował społeczeństwo Indii. Większość wykształconych Hindusów uważała, że wojna może osłabić kolonialistów. Według biografii autorstwa Franka Moraesa, Nehru sympatyzował z Francją ze względu na podziw dla kultury tego kraju[14]. Uważał, że poparcie brytyjskiej polityki w czasie wojny nie służyło interesom indyjskim, a nawet wyśmiał Indyjską Służbę Cywilną (ICS) będącą reprezentantem polityki dyktowanej przez rząd Zjednoczonego Królestwa[15]. W czasie trwanie wojny, zgłosił się do pracy w chrześcijańskiej, organizacji humanitarnej Joannici Dzieło Pomocy. Został jednym ze stanowych sekretarzy Joannitów w Allahabadzie[16].
Wziął udział w protestach przeciwko ustawie prowadzącej w Indiach cenzurę. W latach wojny należał do przedstawicieli radykalno-niepodległościowego skrzydła partii które domagało się nadania Indiom autonomii[17]. Radykałowie pozostali w mniejszości a kierunek ruchu kongresowego wyznaczony został przez centrowego Gopala Krishne Gokhale[12] który pisał o tym że „myślenie o niepodległości to szaleństwo”[16]. Także ojciec Nehru, Motilal, był sympatykiem skrzydła umiarkowanego. Wpływ frakcji centrystycznej w Kongresie zaczął słabnąć po śmierci jej lidera, Gokhale w roku 1915[16]. Po śmierci Gokhale, przedstawiciele skrzydła radykalnego, tj. Annie Beasant i Bal Gangadhar Tilak zaczęli domagać się aby Kongres Narodowy przyjął postulat przemiany Indii z kolonii w dominium (kongres dotychczas walczył jedynie o poprawę sytuacji Hindusów). Postulat ten został w 1915 roku odrzucony przez umiarkowaną większość kierownictwa partii. Radykałowie w 1916 roku utworzyli Krajowy Ruch dla Autonomii. Organizacja przedstawiała postulat przekształcenia Indii w brytyjskie dominia na zasadzie według której działały m.in. Kanada, Australia czy Nowa Zelandia. Był członkiem zarówno Krajowego Ruchu dla Autonomii i Indyjskiego Kongresu Narodowego[18]. Dużą zmianą i kierunkiem w stronę odejścia od wczesnej polityki IKN było zawarcie paktu w Lucknow co zostało przez Nehru poparte. Do podpisania paktu doszło w czasie corocznego spotkania Kongresu w grudniu 1916 roku. W wyniku porozumienia muzułmanie i hindusi mieli wspólnie działać w ruchu antykolonialnym. Umowa podpisana została przez Kongres i Ligę Muzułańską. Na kongresie w Lucknow, Nehru po raz pierwszy spotkał Gandhiego[19].
W 1916 roku poślubił Kamalę Kaul. Ich jedyna córka, Indira urodziła się w 1917 roku. Kamala w listopadzie 1924 roku urodziła chłopca, jednak zmarł on po tygodniu po urodzeniu[20]. W 1917 roku został prywatnym sekretarzem Annie Besant, lidera Krajowo Ruchu Dla Autonomii[18][21]. Gdy w czerwcu 1917 roku Besant została aresztowana i internowana, Kongres i inne organizacje indyjskie zagroziły rozpoczęciem protestów jeśli nie zostanie ona uwolniona. Rząd pod presją opinii publicznej szybko uwolnił więzionego działacza[22]. Do pierwszego poważniejszego zaangażowania Nehru doszło w roku 1920. Został on wówczas liderem Kongresu w Zjednoczonej Prowincji (teraz Uttar Pradesh). Po incydentach w Chaura Chauri, kiedy doszło do zbrojnych starć między Hindusami a brytyjskimi siłami kolonialnymi, Indyjski Kongres Narodowy zawiesił w praktyce współpracę ruchu na poziomie krajowym. Nehru pozostał wierny poglądom Gandhiego i nie przystąpił do utworzonej przez jego ojca umiarkowanej Partii Swaraj. Wkrótce represje brytyjskie spadły nie tylko na Jawaharlar, lecz również i na jego ojca. Jawaharlar i Motilal zostali aresztowani 6 grudnia 1921 roku[23][24]. Jawaharlal został zwolniony po trzech miesiącach, tj. w marcu 1922 roku gdy Gandhi odwołał już wcześniejsze zawieszenie współpracy z rządem. Decyzja ta była dla Nehru dotkliwym ciosem. W 1922 roku wziął udział w kolejnych wystąpieniach Gandhiego. Tym razem doprowadziło to do dłuższego pobytu w więzieniu, otrzymał półtoraroczny wyrok więzienia. Odsiedział 9 miesięcy w areszcie w Lakhno. Na wolność wyszedł 31 stycznia 1923 roku[25].
W latach 20. został wybrany na sekretarza generalnego partii, urząd ten pełnił przez dwie kadencje. Jego pierwsza kadencja rozpoczęła się w czasie sesji w Kakinadzie w 1923 roku. Nehru w czasie kadencji współpracował z doktorem N. S. Hardikerem założycielem Hindustani Seva Dal (1923). Seva Dal była oddolną organizacją wewnątrz Kongresu Narodowego, grupującą zwolenników gandhizmu[26]. Po raz drugi sekretarzem generalnym wybrany został w czasie sesji w Madrasie w 1927 roku. W czasie jego kadencji jako sekretarza generalnego wraz z Czandrą Bose uważany był za przedstawiciela skrzydła radykalnego, działacze ci odrzucili żądanie utworzenia dominium na rzecz pełnej niepodległości Indii[27].
W 1926 roku, Jawaharlar wraz z rodziną wyjechał do Europy, było to powodowane stanem zdrowia Kamali, której zalecano wyjazd do Szwajcarii na leczenie sanatoryjne[28]. Odegrał wiodącą rolę w nagłośnieniu hinduskich żądań poza granicami kraju. Szukał zagranicznych sojuszników sprawy niepodległościowymi i nawiązywał kontakty z ruchami narodowowyzwoleńczymi i demokratycznymi na całym świecie. Jego wysiłki opłaciły się, w 1927 roku, Kongres został zaproszony do udziału w kongresie uciskanych narodów w Brukseli w Belgii. Spotkanie to miało koordynować walkę narodów walczących o niepodległość przeciwko imperializmowi. Nehru został na kongresie przedstawicielem Indii i został wybrany do Rady Wykonawczej Ligi przeciwko Imperializmowi – międzynarodowego organu założonego w czasie kongresu[29]. W dziesiątą rocznicę rewolucji październikowej odwiedził ZSRR[30]. Wizyta w Belgii i w ZSRR wpłynęły na dalszą ewolucję jego poglądów politycznych. Pobyt w Europie zakończył się na podróży do Związku Radzieckiego. Nehru wrócił do Indii wraz z rodziną, gdy stan zdrowia Kemali się poprawił.
W 1927 roku przedstawił rezolucję, w której domagał się „pełnej narodowej niepodległości”, tym samym stał się jednym z pierwszych liderów partii kongresowej którzy uważali że Indie powinny zerwać wszystkie więzi z Imperium Brytyjskim. Deklaracja ta została odrzucona przez Kongres, ze względu na sprzeciw ze strony Gandhiego[31]. W 1928 roku, Gandhi zaproponował rezolucję, w której Kongres wezwał rząd brytyjski do przyznania Indiom statusu dominium, status ten miał zostać Indiom nadany w przeciągu dwóch lat. Propozycja nie spodobała się Nehru i Czandrze Bose[32]. 29 grudnia 1929 roku na sesji w Lahore oficjalnie przejął kierownictwo w partii. Jako lider przeforsował rezolucję wzywającą Brytyjczyków do nadania Indiom pełnej niezależności. O północny w sylwestra 1929 roku, wywiesił trzy kolorową flagę indyjską na brzegu rzeki Ravi w Lahore[33]. Pod kierownictwem Kongresu w 1929 roku opracował politykę Kongresu i przyszłych, niepodległych Indii. Celami Kongresu miały być – wolność wyznania; prawo do stowarzyszenia; wolność słowa i myśli; równość wszystkich wobec prawa – bez względu na kastę, kolor skóry czy religię; ochrona regionalnych języków i kultur; zabezpieczenie interesów pracowniczych; zniesienie nietykalności; nacjonalizacja przemysłu; socjalizm i budowa świeckich Indii. Uchwała została opracowana przez Nehru na przełomie lat 1929/31 i została ratyfikowana przez Ogólnoindyjski Komitet Kongresu pod przewodnictwem Gandhiego[34]. Po sesji Kongresu w 1929 roku został uznany za najważniejszego lidera indyjskiego ruchu niepodległościowego, Gandhi natomiast cofnął się do roli przewodnika duchowego. W latach dwudziestych nawiązał kontakty z ruchem robotniczym w Indiach, głównie z ruchem związkowym, brał udział w zjeździe Ogólnoindyjskiego Kongresu Związków Zawodowych. W 1929 roku łączył stanowisko sekretarza generalnego Kongresu Narodowego ze stanowiskiem przewodniczącego Kongresu Związków Zawodowych[35].
Gandhi w latach 30. zrezygnował z członkostwa w Kongresie, jednak nie zerwał swoich kontaktów z Nehru i jego partią. Popularność Nehru zwiększyły wybory w 1935 roku w których Kongres zdobył władzę w samorządach większości stanów. W wyborach dobry wynik uzyskała również Liga Muzułmańska, której przewodniczył Muhammad Ali Jinnah. Ali Jinnah był rywalem Nehru, Jawaharlal chciał, aby liderem muzułmanów został sekularysta Abul Kalam Azad. Gandhi w sporze między Azadem a Jinnahem opowiedział się po stronie Jinnaha który w następnej dekadzie stał się twórcą Pakistanu. Poparcie dla przyszłego, opartego na socjalizmie ustroju Indii było trudne do osiągnięcia. Prawicowi kongresmeni tacy jak Sardar Patel, doktor Rajendra Prasad i Chakravarthi Rajagopalachari byli przeciwni lewicowemu programowi Nehru. Program poparli lewicowi kongresmeni Maulana Azad i Subhas Czandra Bose. Nehru, Azad i Bose w 1936 roku zainicjowali odwołanie dotychczasowego konserwatywnego prezesa Kongresu, doktora Prasady. Prasada na stanowisku prezesa zastąpiony został przez Nehru, który sprawował tę funkcję w latach 1936–1937. W 1938 roku zastąpił go Czandra Bose który pełnił funkcję do 1939 roku, a od 1940 do 1946 roku funkcję prezesa sprawował Azad[36][37]. Podczas drugiej kadencji sekretarza generalnego, zaproponował pewne uchwały dotyczące polityki zagranicznej Indii[38]. Od tego momentu, dano mu wolną rękę w kształtowaniu polityki zagranicznej Indii. Nehru nawiązał stosunki między Kongresem a rządami na całym świecie. W czasie narastania kryzysu w Europie związanego z ekspansjonistyczną polityką państw faszystowskich, Kongres opowiedział się przeciwko faszyzmowi i za demokracją[39]. Opracował przyszłe plany centralnego planowania gospodarki Indii. W 1938 roku, powołał Państwową Komisję Planowania, która miała stworzyć plany gospodarcze realizowane już w niepodległych Indiach[40]. Wiele planów Nehru i jego sojuszników zostało jednak odrzucone ze względu na podział Indii w 1947 roku. Jako jeden z pierwszych przywódców ruchu niepodległościowego opowiedział się przeciwko autorytaryzmowi państewek rządzonych przez książęta indyjskie. Przed objęciem przez niego władzy, ruch niepodległościowy ograniczał się jedynie do terytoriów znajdujących się pod bezpośrednim panowaniem brytyjskim. W czasie walk między Sikhami a skorumpowanym mahantami, został uwięziony w księstwie Nabha. W 1927 roku współtworzył Ogólnoindyjską Zjednoczoną Konferencje Ludową – organizacja ta wspierała ludność książęcych państewek. W 1935 roku został on wybrany przewodniczącym konferencji[41].
W proteście przeciwko zwiększeniu przez Brytyjczyków podatku na sól, Gandhi zaproponował przeprowadzenie akcji obywatelskiego nieposłuszeństwa. Nehru (jak i większość przywódców Kongresu) był początkowo ambiwalentnie nastawiony do planu. Gandhi zdecydował się na rozpoczęcie protestów które szybko zyskały szerokie poparcie, liderzy Kongresu zdali sobie sprawy z siły protestów, a do grona zwolenników protestów dołączył sam Nehru[42]. Nehru stanął na czele procesji która w symboliczny, protestacyjny sposób wydobyła trochę soli. 14 kwietnia, podczas podróży między Allahabadem a Raipurem, został on aresztowany i oskarżony o naruszenie prawa solnego. W rezultacie za „podburzanie do buntu” skazano go na sześć miesięcy więzienia. Wyrok odsiedział w więzieniu Naini w Allahabadzie. W czasie odbywania wyroku na prezydenta Kongresu mianował Gandhiego, odmówił on jednak przyjęcia tej funkcji a następcą Nehru został jego ojciec, Motilal. Ojciec Jawaharlala na początku protestu „kampanii solnej” podarował Kongresowi rezydencję rodziny Nehru, gdzie odtąd mieściły się centralne władze organizacji. Wkrótce Motilal został aresztowany i trafił do więzienia Naini gdzie przebywał już jego syn. W czasie pobytu w więzieniu, akcja obywatelskiego nieposłuszeństwa przyśpieszyła, rozpoczęły się kolejne aresztowanie a władze kolonialne zaczęły strzelać do protestujących. 11 października 1930 roku Jawaharlal został zwolniony z więzienia za względu na zły stan zdrowia jego ojca. Pobyt na wolności trwał niewiele dłużej niż tydzień. 19 października doszło do ponownego aresztowania. Władze brytyjskie uważały Nehru za zbyt niebezpiecznego działacza, wydały specjalne zalecenia aby Nehru przy najbliższej okazji ponownie aresztować i skazać na więzienie. W październiku 1930 roku Nehru otrzymał kolejny wyrok, tym razem dostał dwa lata więzienia. 1 stycznia do więzienia została wtrącona żona Nehru, Kamala[43]. Aby ułatwić sobie rozmowy z Kongresem na temat ewentualnego porozumienia i zakończenia protestu, w styczniu 1931 roku władze zwolniły z więzienia ok. 20 członków Kongresu. Jawaharlal i Kamala zostali zwolnienie kilka dni wcześniej z uwagi na dalsze pogorszenie się stanu zdrowia Motilala. Ojciec Jawaharlala zmarł we śnie w dniu 6 lutego 1931 roku. W imieniu Kongresu, do rokowań z Brytyjczykami przystąpił Gandhi. Rozmowy Gandhiego z wicekrólem trwały trzy tygodnie, 5 marca 1931 roku ogłoszono wypracowanie porozumienia. Warunki postawione przez Brytyjczyków rozczarowały Nehru. Szczególnie oprotestował on jeden z artykułów paktu, który mówił o problemach konstytucji i federacji Indii jednak nie wspomniał nic o niepodległości która była jednym z żądań stawianych w czasie kampanii solnej. Protesty solne zwróciły uwagę światowej opinii publicznej, świat coraz bardziej uznawał zasadność żądań niepodległości stawianych przez Kongres. Udział Nehru w protestach zwiększył jego rolę w Kongresie i polepszył jego relację z Gandhim[44]. Kolejne aresztowanie Nehru miało miejsce 26 grudnia 1931 roku po starciach Kongresu z rządem. Kongres został sprowokowany posunięciem rządu, który naruszał założenia paktu Irwin(wicekról)-Gandhi z marca tego roku. Nehru został aresztowany w czasie organizacji ruchu chłopskiego w Zjednoczonych Prowincjach. Kiedy przebywał w więzieniu, kampania nieposłuszeństwa osłabła. We wrześniu 1932 roku i w maju 1933 roku Gandhi przeprowadził głodówkę na znak protestu przeciw łamaniu przez Brytyjczyków warunków paktu. Nehru uważał, że głodówki Gandhiego są nielogiczne i nieracjonalne. 30 sierpnia 1933 roku Nehru został zwolniony z więzienia ze względu na pogarszający się stan zdrowia jego matki, Swarup Rani[45].
Na początku roku 1931 Nehru za radą lekarzy udał się z Kamalą i Indirą na urlop na Cejlonie[46]. Miał to być jego ostatni urlop z Kamalą przed długim pobytem w więzieniu i jej późniejszą chorobą, która spowodowała jej śmierć. Aresztowanie, a w konsekwencji wyrok pozbawienia wolności miało miejsce 12 lutego 1934 roku, tym razem został oskarżony o podżeganie do buntu. 11 sierpnia ze względu na zły stan zdrowia Kamali władze więzienne zwolniły Nehru na 10 dni. Gdy stan Kamali nadal się pogarszał, rodzina zdecydowała wysłać ją razem z Indirą do Europy na leczenie klimatyczne. Pod naciskiem opinii publicznej Jawaharlal został zwolniony z więzienia, dzięki czemu pojechał on do chorej żony, która znajdowała się w ciężkim stanie i przebywała w szpitalu, zmarła 28 lutego 1936[47]. Po śmierci żony powrócił do ojczyzny. Został przewodniczącym Kongresu Narodowego. W październiku 1940 roku po raz ósmy trafił do więzienia[48]. Przebywał w nim rok, wtedy bowiem nastąpiło przedterminowe zwolnienie działaczy i aktywistów Kongresu.
Gdy w połowie lat 30., świat zaczął dryfować w stronę kolejnej wojny światowej, w 1936 roku Nehru wyjechał do Szwajcarii, aby na krótko przed śmiercią ciężko chorej żony, zaopiekować się nią w sanatorium[49]. Mimo napiętej sytuacji międzynarodowej, Nehru podkreślał, że w razie wojny, miejsce Indii powinno znaleźć się przy państwach demokratycznych, choć, Indie powinny walczyć po stronie Wielkiej Brytanii i Francji tylko jeśli tylko zostanie im nadana niepodległość. Wizyta w Europie w 1936 roku, stała się przełomem w myśli politycznej i gospodarczej Nehru. Przyszły premier zainteresował się marksizmem i myślą socjalistyczne. Jego kolejny pobyt w więzieniu pozwolił mu bardziej szczegółowo studiować marksizm. Zainteresowały go pomysły i założenia marksizmu równocześnie odpychały go niektóre tezy zaproponowane przez Karola Marksa. Nehru w sferze ekonomii pozostawał marksistą, chciał wprowadzić ekonomiczne postulaty marksizmu na grunt warunków indyjskich. W Europie ściśle współpracował z Czandrą Bose, działacze ci rozstali się jednak pod koniec lat 30., gdy Bose zaczął szukać wsparcia u faszystów, a w tym samym czasie, Nehru poparł hiszpańskich republikanów walczących przeciwko wojskom generała Franco i był zdecydowanym antynazistą[50]. Gdy ludzie z wielu krajów zgłaszali się na ochotników, tworząc Brygady Międzynarodowe do walki z siłami faszystowskimi w Hiszpanii, Nehru wraz ze swoim pomocnikiem V.K. Krishną Menonem udał się do Hiszpanii aby udzielić wsparcia hinduskim ochotnikom[51]. Odmówił spotkania się z włoskim dyktatorem, Benito Mussolinim gdy ten wyraził chęć spotkania[52]. Już wtedy Nehru postrzegany był na świecie jako bojownik o demokrację i wolność[39][53]. Nehru przebywał w Genewie, gdy Liga Narodów omawiała sposób rozwiązania kryzysu czeskiego, a następnie udał się do Londynu[54].
Kiedy wybuchła II wojna światowa, wicekról markiz Linlithgow bez konsultacji z przedstawicielami Hindusów ogłosił udział Indii Brytyjskich w wojnie. Nehru pośpiesznie wrócił z wizyty w Chinach, zapowiadając że sympatie Indii w konflikcie między demokracją a faszyzmem powinny zwrócić się w stronę demokracji przeciwko faszyzmowi[55]. Po długich obradach Kongresu, Nehru poinformował rząd, że będzie współpracować z Brytyjczykami pod pewnymi warunkami – po pierwsze, Wielka Brytania musi dać gwarancję pełnej niepodległości Indii po wojnie i umożliwić wybory do zgromadzenia konstytucyjnego, po drugie mimo tego że indyjskie siły zbrojne pozostaną pod kontrolą brytyjską, Hindusi muszą uczestniczyć w administracji centralnej. Wicekról markiz Linlithgow nie brał tych żądań na poważnie i odrzucił postulaty Kongresu. 23 października 1939 roku, Kongres potępił postawę Brytyjczyków i wezwał ministrów Kongresu w poszczególnych stanach do dymisji na znak protestu. Nehru wezwał do oprotestowania Jinnaha i jego Ligę Muzułmańską, muzułmanie poparli jednak decyzje Brytyjczyków. W marcu 1940, Jinah przedstawił „uchwałę pakistańską”, według której muzułmanie są narodem i muszą mieć swoją ojczyznę. Ojczyzną muzułmanów miał stać się „Pakistan”. 8 października 1940 roku Linlitghow zatwierdził warunki Nehru, uznał on jednak że Indie staną się dominium, nie sprecyzował on jednak daty tego wydarzenia. W związku z brakiem przedstawienia przez Brytyjczyków konkretów, Gandhi i Nehru, porzucili pierwotne stanowisko popierania Wielkiej Brytanii i rozpoczęli ograniczoną kampanię obywatelskiego nieposłuszeństwa. Nehru został aresztowany i skazany na cztery lata więzienia. Z więzienia został zwolniony po roku (na trzy dni przed zbombardowaniem Pearl Harbor na Hawajach), wraz z innymi działaczami Kongresu. Wojska japońskie na wiosnę 1942 roku poprzez Birmę (obecnie Mjanma) dotarły do granic Indii. Rząd brytyjski zgodził się na pewne ustępstwa wobec Indii, w tym część postawionych na początku wojny przez Nehru. Premier Winston Churchill wysłał do Indii ministra Strafforda Crippsa, dzielącego z Nehru lewicowe poglądy polityczne[56]. Nehru był entuzjastycznie nastawiony do kompromisu indyjsko-brytyjskiego, Gandhi był do niego sceptyczny na skutek czego misja Cripsa zakończyła się porażką. Gandhi mimo różnić wyznaczył Nehru na swojego politycznego następcę i wezwał Brytyjczyków do opuszczenia Indii. Nehru choć nie chciał osłabić wysiłku wojennego aliantów, nie miał innego wyjścia i przyłączył się do żądań Gandhiego. Po uchwale na kongresie partii z 8 stycznia 1942 roku w Bombaju (obecnie Mumbai), na której przyjęto uchwałę w której zażądano opuszczenia Indii przez Brytyjczyków, cała komisja robocza Kongresu, w tym Gandhi i Nehru została aresztowana i osadzona w więzieniach[57]. Nehru opuścił więzienie w czerwcu 1945 roku[58].
Po zwolnieniu z więzienia, Nehru wraz z przedstawicielami IKN spotkali się z władzami kolonialnymi, Brytyjczycy powierzyli im zadanie utworzenia rządu Indii. 2 sierpnia 1946 roku powstał rząd tymczasowy a jego premierem został Nehru. Gdy Jawaharlal Nehru opowiedział się za pełną jednością Indii obrady rządu został zbojkotowane przez przedstawicieli muzułmanów. W maju 1947 roku zaczął się skłaniać się do przyjęcia w niepodległych Indiach statusu dominialnego w ramach Brytyjskiej Wspólnoty Narodów. Na przełomie wiosny i lata 1947 roku Nehru zrewidował swoje poglądy co do przyszłości Indii – wyraził zgodę najpierw na podział Pendżabu i Bengalu, a potem na podział całych Indii. 3 czerwca, po nieudanych próbach sformułowania rządu koalicyjnego, niechętnie poparł brytyjski plan podziału Indii, decyzja o podziale kraju została potwierdzona 15 czerwca 1947 roku przez Ogólnoindyjski Komitet Kongresu.
15 sierpnia objął urząd premiera niepodległych Indii. Jeszcze jako premier ustanowił ten dzień dniem niepodległości kraju. Kurs ustalony przez Nehru obejmował: utrzymanie pokoju, wyzwolenie ciemiężonych narodów, likwidację dyskryminacji, ubóstwa, chorób i ignorancji. Jako premier zmagał się z masową migracją ludności związaną z podziałem kraju, nie wszędzie wymiana ludności przebiegłą pokojowo – na terenie Prowincji Pogranicze i Pendżabu doszło do krwawych starć między hinduistami a muzułmanami. 30 stycznia 1948 roku zabity został Mahatma Gandhi, mordercą okazał się być radykalny hinduista Nathurama Godse[59]. Śmierć Gandhiego była dla Nehru dotkliwym ciosem, w kraju ogłoszono dwutygodniowy okres żałoby. Po morderstwie rząd podjął się zdecydowanej akcji przeciwko fanatykom religijnym i stłumił wystąpienia fundamentalistycznych organizacji – Rashtriya Swayamsevak Sangh, Muzułmańskiej Gwardii Narodowej i Khaksars[60]. Zabójstwo Gandhiego wpłynęło na świadomość obywateli hindusów którzy odwrócili się od partii religijnych[61].
Oprócz stanowiska premiera, pełnił urząd ministra spraw zagranicznych. Stosunkowo wcześnie określił główne kierunki i dążenia indyjskiej polityki zagranicznej. Chciał trzymać się z daleka od mocarstw które doprowadziły do wojen światowych. W czasie swoich rządów, Nehru często zwracał się o pomoc do córki Indiry, która pomagała mu w sprawach osobistych. Indira przeniosła się do siedziby Nehru i uczestniczyła w jego podróżach po Indiach i świecie. W okresie rządów ojca była uważana za kogoś w rodzaju szefa sztabu rządu[62].
Rząd przyjął model gospodarki mieszanej. Nehru wdrażał w życie reformy gospodarcze mające zbudować indyjską wersję planowania gospodarczego. Rząd zarządzał strategicznymi gałęziami przemysłu, tj. górnictwo, energia, przemysł ciężki. Prowadzono programy redystrybucji ziemi, budowy kanałów nawadniających i tam, rozwoju społecznego (popularyzacja chałupnictwa i aktywizacja zawodowa na obszarach wiejskich). W rolnictwie upowszechniono nawozy co przyczyniło się do zwiększenia produkcji rolnej. Zbudował wielkie zapory, nazywane przez niego „nowymi świątyniami Indii”, rozpoczął prace irygacyjne, wytwarzanie energii wodnej, uruchomiono program wykorzystania energii atomowej. Polityka przemysłowa rządu Nehru, której główne cele zostały zebrane w rezolucji „Polityka Przemysłowa 1956”, doprowadziła do wzrostu zróżnicowania produkcji i powstania w Indiach przemysłu ciężkiego[63]. Gospodarka cieszyła się stałą stopą wzrostu na poziomie 2,5% rocznie. W 1951 roku, w oparciu o Komisję Planowania Indii, sporządził pierwszy plan pięcioletni. Plan zakładał zwiększenie inwestycji rządowych w przemyśle i rolnictwie. W czasie jego rządów upowszechniono w Indiach nawozy, tym samym zwiększono produkcję rolną. Zapoczątkowano serię programów rozwoju społecznego mających na celu szerzenie różnych form chałupnictwa i zwiększaniu aktywizacji zawodowej w wiejskich obszarach Indii. Sposób prowadzenia polityki gospodarczej, wzbudził sprzeciw opozycji marksistowskiej – część opozycji uważała, że gospodarka Indii pod płaszczem demokratycznego socjalizmu zmierza w stronę kapitalizmu[64].
Indie mimo postępów w zwiększeniu produkcji rolnej, borykały się z niedoborami żywności. Wdrożono reformę rolne i programy szybkiego uprzemysłowienia. Reforma rolna została zrealizowana, w jej wyniku rozdano olbrzymie połacie ziemi, jednak część ziemi znajdującej się w rękach właścicieli ziemskich nie została rozparcelowana. Próby reform rolnictwa były udaremniane przez wiejskie elity klasowe które miały silne wpływy w prawicowych frakcjach Kongresu, przeciwnych polityce frakcji socjalistycznej. Produkcja rolna do początku lat 60. stale się zwiększała, było to spowodowane m.in. udanymi projektami irygacyjnymi i rozparcelowaniem ziemi która trafiła pod uprawę. Rząd wzorując się na istniejących w Stanach Zjednoczonych uczelniach rolniczych, utworzył krajowe uczelnie. Uczelnie te pracowały z wysoko wydajnymi odmianami pszenicy i ryżu, pierwotnie stosowanymi w Meksyku i na Filipinach. Dużą liczbę wysoko wydajnych zbóż importował do Indii minister Subramaniam który w kwestii innowacji okazał się jeszcze bardziej odważny od amerykańskich doradców rządu[65]. Wydajne odmiany zastosowano w latach 60., w czasie trwania tzw. zielonej rewolucji. Rewolucja polegała na zwiększeniu produkcji rolnej. Postępy w rolnictwie spowolniła seria monsunów[63].
Rząd do czasu uchwalenia nowej konstytucji posługiwał się konstytucją Indii Brytyjskich z 1935 roku. Indie Brytyjskie, które obejmowały dzisiejsze Indie, Pakistan i Bangladesz, zostały podzielona na dwa typy obszarów: prowincje Indii Brytyjskich, które były zarządzane bezpośrednio przez brytyjskich urzędników odpowiedzialnych przed gubernatorem generalnym Indii, oraz prowincje które były zarządzane przez książęta – lokalnych dziedzicznych władców, którzy uznali brytyjskie zwierzchnictwo w zamian za lokalną autonomię, w większości przypadków ustaloną w wyniku traktatu. W okresie od 1947 do 1950 roku, terytoria państw książęcych zostały polityczne zintegrowane z niepodległym państwem indyjskim. Przyłączone tereny zostały zorganizowane w nowych prowincjach takich jak Rajputana, Himachal Pradesh, Madhya Bharat i Vindhya Pradesh. Prowincje składały się z wielu książęcych państw, w tym Mysore, Hyderabad, Bhopal i Bilaspur. Nowa konstytucja Indii, weszła w życie w dniu 26 stycznia 1950 roku. Konstytucja utworzyła w Indiach, suwerenną, demokratyczną republikę. Konstytucja z 1950 rozróżniała trzy główne rodzaje prowincji: część prowincji, która dawniej była prowincjami podlegającymi gubernatorowi Indii, rządzone przez wybranego gubernatora i parlament. Część B, były to dawne państwa książęce lub grupy takich państw, prowincje te zarządzane były przez władcę państwa książeckiego, władze w tych prowincjach objęli rajpramukhowie mianowani przez prezydenta Indii. Część C, były to stany zarządzane zarówno przez komisarzy prowincji i książęta, według konstytucji miały one być zarządzane przez komisarza mianowanego przez prezydenta Indii. Część D, były to Andamany i Nikobary, zarządzane przez gubernatora mianowanego przez rząd centralny[66][67]. W grudniu 1953 roku, powołał Komisję Reorganizacyjną Stanów. Na czele komisji stał Fazal Ali stąd też Komisja Reorganizacyjna Stanów określana była często jako Komisja Fazala Ali. Komisja w 1955 roku stworzyła raport zalecający reorganizację stanów Indii[68]. W 1956 roku rząd opracował reformę administracyjną, w ramach reformy, część B, część C i część D zostały zniesione[69].
Stany Zjednoczone i Związek Radziecki konkurowały ze sobą aby pozyskać Indie jako sojusznika w zimnej wojnie. Utrzymywano dobre stosunki z Imperium Brytyjskim, w ramach Deklaracji Londyńskiej, Indie zgodziły się aby w styczniu 1950 roku, przyłączyć się do Wspólnoty Narodów i zaakceptować brytyjskiego monarchę jako symbol jednoczący państwa WN. Relacja większości obywateli na podpisanie deklaracji była pozytywna, decyzja została skrytykowana jedynie przez ugrupowania skrajnej lewicy i skrajnej prawicy. Na arenie międzynarodowej, był zdecydowanym zwolennikiem ONZ i propagatorem pacyfizmu. Był jednym z pionierów polityki niezaangażowania i współzałożycielem Ruchu Państw Niezaangażowanych, bloku państw pozostających neutralnymi i niezależnymi w okresie walki między blokami państw na czele z USA i ZSRR.
Indie przejawiały inicjatywę w ważnych problemach azjatyckich. Największym dokonaniem na arenie międzynarodowej lat 50. była zwołana przez Indie konferencja 29 państw Azji i Afryki w Bandungu. W konferencji uczestniczyli także przedstawiciele prozachodnich krajów tego regionu i w związku z tym istniało duże prawdopodobieństwo starcia między zwolennikami kursu proamerykańskiego a zwolennikami niezaangażowania[70], do czego nie dopuściły jednak mediacje Nehru. Konferencję zdominowały sprawy antykolonializmu, pokoju i zwiększenia roli państw Azji i Afryki. Delegaci krajów Afryki i Azji, postanowili zjednoczyć się „w walce z kolonializmem i dyskryminacją rasową”[71]. W lipcu 1956 roku współorganizował spotkanie na wyspie Vang w archipelagu Brioni. Na spotkaniu omówiono zasady współpracy krajów, które nie należały do bloków militarnych i politycznych. Przez kolejne dwa lata nawiązała się współpraca „Niezależnej Trójki”[71]. Do I Konferencji szefów rządów państw niezaangażowanych doszło we wrześniu 1961 roku w Jugosławii. W konferencji udział wzięło 25 państw oraz 3 w charakterze obserwatorów. W następnych latach grupę zasiliły kolejne kraje, a także grupy narodowowyzwoleńcze[71].
W 1956 roku skrytykował wspólną inwazję Brytyjczyków, Francuzów i Izraelczyków na Kanał Sueski. Jako premier Indii i lider Ruchu Państw Niezaangażowanych starał się doprowadzić do rozejmu i sprawiedliwej oceny obydwóch stron konfliktu. Zyskał w tym sojusznika w postaci prezydenta USA, Dwighta Eisenhowera, któremu udało się zahamować wpływy Francji i Wielkiej Brytanii. Kryzys sueski znacznie zwiększył prestiż Indii w krajach Trzeciego Świata. W czasie kryzysu, indyjski dyplomata, Menon przekonał egipskiego prezydenta Gamala Abdela Nasera na kompromis z Zachodem. Indie przyczyniły się do budowy w krajach zachodnich wizerunku Nassera jako człowieka skłonnego do kompromisu[72][73]. Nawet po kryzysie sueskim, utrzymywał dobre stosunki z Wielką Brytanią. Uczestniczył w arbitrażu Wielkiej Brytanii, Banku Światowego i Pakistanu. W 1960 roku Wielka Brytania, Indie i Bank światowy podpisały traktat z przywódcą Pakistanu, Ayub Khanem. Dzięki temu Pakistan dał Indiom dostęp do głównych rzek regionu Pendżab. Po raz kolejny wzrosła rola ministra, Krishny Menona – amerykański magazyn Time uznał ministra drugim najpotężniejszym człowiekiem w Indiach[74].
W 1954 roku po antykolonialnych zamieszkach w należących do Portugalii Dadrze o Nagarhaweli, Nehru zajął sporny teren[75], z kolei po latach nieudanych negocjacji, w 1961 roku upoważnił Armię Indyjską do wyzwolenia innej portugalskiej kolonii, Goa. Prowincja została oficjalne włączona do Indii a decyzja ta została w Indiach przyjęta entuzjastycznie. Część opozycji skrytykowała rząd za użycie sił zbrojnych wobec portugalskich, kolonialnych władz Goa. Z drugiej strony użycie potencjału militarnego przyniosło mu popularność wśród grup prawicowych i skrajnie prawicowych[76][77].
W okresie międzywojennym wiązał z Chinami duże nadzieje. W tych poglądach utwierdziła go wizyta w Chinach i sukces chińskiej rewolucji ludowej. Uważał, że władza KPCh pozytywnie wpłynie na relacje tego kraju z Indiami. Indie uznały oficjalnie ChRL 30 grudnia 1949 roku. Nehru poparł włączenie ChRL do ONZ i odmówił uznania Chin za agresora w konflikcie koreańskim[78] oraz udzielił wsparcia Chinom i KRLD toczącym walki z zachodnią koalicją[79]. Sprawą, która potencjalnie mogła zakłócić stosunki między Indiami a ChRL, było rozszerzenie ChRL na Tybet, Nehru zajął w tej kwestii stanowisko pojednawcze. Zwrócił się do Pekinu, prosząc o powściągliwość i poszukiwanie pokojowego rozwiązania sprawy Tybetu, tak aby nie zagroziło to przyjęciu Chin do ONZ i innych organizacji międzynarodowych[80]. W latach pięćdziesiątych ugruntowane zostały bliskie stosunki Indii z ChRL. Nehru uważał, że Chiny i Indie mogą zlikwidować przepaść między demokracjami ludowymi a państwami kapitalistycznymi.
W 1954 roku Nehru podpisał z Chinami pięć zasad pokojowego współistnienia znanego w Indiach jako Pańcza Sila (od słowa Pańcza: pięć, Sila: cnoty). Był to zestaw zasad, które regulowały stosunki indyjsko-chińskie, ich pierwsza oficjalna kodyfikacja w postaci traktatu została zawarta w umowie między Chinami i Indiami w 1954 roku. Zostały one zawarte w preambule do umowy między Tybetańskim Regionem Chin a Indiami, podpisanej w Pekinie w dniu 29 kwietnia 1954 roku[81]. Negocjacje paktu odbywały się w Delhi od grudnia 1953 roku do kwietnia 1954 roku. Traktat dotyczył spornych terytoriów Aksai i Południowego Tybetu, traktat obowiązywał do roku 1960, a od 1970 pięć zasad, było ponownie przestrzegane i przyczyniły się do ponownej normalizacji stosunków między państwami. Pięć zasad umocniło się w okresie rządów Indiry Gandhi i trzyletnich rządów Partii Ludowej (1977-1980).
Dobre stosunki psuły się na skutek m.in.: „wojny kartograficznej”. Wprawdzie w kwietniu 1960 roku doszło do spotkania Nehru z premierem Chin w sprawie rozbieżności granicznych, ale nie przyniosło ono porozumienia ani nie usunęło różnic. „Myślę, że (konflikt z ChRL) dotknął go głęboko i miał na niego ujemny wpływ. Podupadł na duchu. Zagrożone było wszystko, co zbudował: Indie miały przyjąć stanowisko militarystyczne, którego nie znosił” – twierdził K. Menon. W późniejszych latach, rząd uczestniczył w chińsko-indyjskich sporach granicznych i udzielił azylu politycznego Dalajlamie (który uciekł z ChRL)[82].
Premier Nehru pod auspicjami ONZ przeprowadził w 1948 roku plebiscyt w Kaszmirze. Kaszmir to sporne terytorium między Indiami a Pakistanem, o które oba państwa toczyły wojny. Nehru stał się bardziej ostrożny w stosunku do ONZ, a po tym jak Pakistan zgodnie z uchwałą ONZ, nie wycofał swojego wojska ze spornego terytorium, Nehru odmówił przeprowadzenia plebiscytu zaplanowanego na 1953 rok. Jego polityka w Kaszmirze i integracja stanu z Indiami często była broniona przed ONZ, przez dyplomatę Krishnę Menona który skutecznie bronił na arenie międzynarodowej politykę Indii względem regionu. W 1957 roku, Menon ogłosił w Radzie Bezpieczeństwa ONZ, ośmiogodzinne przemówienie broniące stanowiska Indii w sprawie Kaszmiru, do dzisiaj, mowa jest najdłuższą z do tej pory wygłoszonych przed tą instytucją. Umiejętna obrona interesu narodowego Indii w ONZ, wpłynęła na powiększenie poparcia dla indyjskiego Kaszmiru a indyjska prasa ogłosiła Menona „Bohaterem Kaszmiru”. Nehru znalazł się wówczas u szczytu popularności a jedynie niewielka krytyka, pochodziła ze strony militarystycznej skrajnej prawicy.
Nehru przewidywał rozwój broni jądrowej i w 1948 roku założył Indyjską Komisję Energii Atomowej[83]. Przewodniczącym organizacji został fizyk jądrowy, doktor Homi J. Bhabha. Kierunek rozwoju polityki nuklearnej Indii została osobiście ustalona przez Nehru i Bhabha. Nehru uważał, że rozwój programu umożliwi Indiom konkurencję z państwami wysoko uprzemysłowionymi i będzie podstawą budowania pozycji Indii w regionie, wówczas zagrożonej przez Pakistan.
Po wojnie koreańskiej (1950-1953) wzywał do rozładowania napięć i zaniechania gróźb użycia broni jądrowych[84]. Nehru obawiając się, że nuklearny wyścig zbrojeń doprowadzi do nadmiernej militaryzacji globu, co byłoby zbyt ryzykowne dla państw rozwijających się zlecił pierwsze w historii, badania skutków wybuchów jądrowych i promował denuklearyzacje[85].
W lipcu 1946 roku, dobitnie stwierdził, że w państwie indyjskim nie mogą militarnie przeważać książęta, lecz armia Indii[86]. W styczniu 1947 roku, Nehru odrzucił boskie prawo królów[87], a w maju tego samego roku oświadczył, że każde państwo rządzone przez książęta które odmówi przyłączenia się do Zgromadzenia Ustawodawczego będzie traktowane jako wróg. W czasie przygotowań indyjskiej konstytucji, wielu indyjskich liderów (z wyjątkiem Nehru) uważało, że możliwe będzie dalsze istnienie stanów książęcych tak jak przewidział to Akt Rządu Indii z 1935 roku. Dzięki staraniom Nehru, zdecydowano się aby wszystkie państwa książąt zostały włączone w skład republiki indyjskiej. Proces republikanizacji i jednoczenia Indii rozpoczęty przez Nehru został ukończony przez jego córkę w 1971 roku. Córka Nehru, Indira Gandhi wybrana na prezydenta w 1969 roku przygotowała ustawę która miała znieść oficjalne tytuły książąt[88]. Ustawa została jednak odrzucona przez Sąd Najwyższy Indii. Ostatecznie rząd przeforsował te zmiany poprzez 26. poprawkę do konstytucji[89].
Stał na czele frakcji Kongresu, która promowała język hindi jako język urzędowy. Po debatach z przedstawicielami narodowości posługującymi się innymi językami, w 1950 roku, językiem urzędowym stał się na równi język hindi i język angielski. Rozwiązanie to miało potrwać piętnaście lat, po upływie tego czasu hindi miał stać się jedynym językiem urzędowym. Aby zapewnić bezpieczeństwo stanom posługującym się innym językiem, Nehru w 1963 roku wprowadził reformę dzięki której już po 1965 roku stany mogły korzystać z języka angielskiego. Kwestia języku została rozwiązana przez następcę Nehru, Lal Bahadura Shastriego, który dzięki namową Indiry Gandhi, pozostawił angielski jako język urzędowy. Ustawa premier Gandhi z 1967 roku zagwarantowała nieograniczone korzystanie z hindi i angielskiego[90].
Był żarliwym zwolennikiem powszechnej edukacji dla dzieci i młodzieży w Indiach, wierzył, że wykształcenie ma zasadnicze znaczenie dla przyszłego rozwoju Indii. Jego rząd nadzorował tworzenie wielu wyższych uczelni, w tym All India Institute of Medical Sciences, Indian Institutes of Management i the National Institutes of Technology[91]. W planach pięcioletnich uwzględnił zagwarantowanie swobodnego i obowiązkowego nauczania wszystkich indyjskich dzieci. Rząd nadzorował tworzenie programów rozwoju edukacji na wsi i budowy tysięcy szkół. Rozpoczął inicjatywę walki z niedożywianiem, tj. bezpłatne rozdawanie dzieciom mleka i posiłków[92]. Zwłaszcza na obszarach wiejskich, otworzono ośrodki kształcenia dorosłych, szkoły zawodowe i technika.
W 1954 roku wprowadził ustawę małżeńską, ideą ustawy było to aby każdy obywatel miał prawo zawrzeć małżeństwo cywilne. Prawo to zostało wprowadzone we wszystkich prowincjach Indii, z wyjątkiem stanów Dżammu i Kaszmir. W 1955 roku wprowadzono ustawę regulującą małżeństwa hinduskie. Wprowadzono również ustawę dotyczącą małżeństw muzułmańskich, ustawa zapewniała ochronę muzułmańskim kobietom. Zdelegalizowano poligamię a dziedziczenie było regulowane przez ustawę spadkową, a nie jak dotychczas przez prawo islamskie. Także rozwód był regulowany poprzez prawo świeckie. Reformy eliminowały też nierówności społeczne wynikające z tożsamości kastowej i plemiennej. Nehru zwiększył reprezentację mniejszości w rządzie. Napisał artykuł 44 indyjskiej konstytucji, według którego wszystkich obywateli na terenie całych Indii dotyczy jednolity kodeks cywilny. Artykuł stał się podstawą świeckości Indii[93]. Po włączeniu w 1961 do Indii małego stanu Goa, będącego do tej pory portugalską kolonią, tamtejszy kodeks cywilny oparł się na starych przepisach portugalskich, przez rząd zakazane zostało jednak ortodoksyjne prawo muzułmańskie. Także w pozostałych krajach rząd zwolnił obywateli stanów muzułmańskich z przestrzegania zasad szariatu. Indyjski parlament uchwalił wiele zmian w prawie hinduskim które kryminalizowały dyskryminację kobiet czy też zwiększały prawa i wolności kobiet[94][95][96][97].
Od 1959 roku (z większym natężeniem w 1961), Indie prowadziły politykę tworzenia posterunków wojskowych na spornych obszarach granicy chińsko-indyjskiej, w tym czasie powstało 43 placówek na terenie wcześniej nie kontrolowanym przez Indie[98]. Po atakach armii chińskiej na część z tych obiektów rozpoczęły się regularne starcia między Indiami a Chinami. W wyniku działań zbrojnych, Chiny wycofały się do przedwojennych linii strefy wschodniej w Tawang, zachowały przy tym Aksai Chin należące dawniej do Indii Brytyjskich. Indie były w stanie wysłać do strefy działań zbrojnych jedynie 14 tysięcy żołnierzy a rząd został skrytykowany za brak skutecznej obrony granic. Nehru aby zapewnić Indiom bezpieczeństwo nawiązał bliższe stosunki z USA, uzyskując pomoc militarną ze strony tego kraju. Dobre stosunki Nehru i prezydenta USA Johna F. Kennedy’ego okazały się w czasie wojny przydatne. W 1962 roku związany z Amerykanami prezydent Pakistanu Ayub Khan, zagwarantował Indiom nieagresję[99]. Indie równocześnie utrzymywały dobre stosunki z ZSRR, co było krytykowane przez wolnorynkową część prawicy. Nehru był też krytykowany za zaangażowanie w Ruchu Państw Niezaangażowanych, część opozycji uważała, że Indie powinny wybrać jednego, stałego sojusznika.
Skutki wojny spowodowały gruntowne zmiany w indyjskiej armii – w ramach współpracy z USA, do wojska trafili amerykańcy doradcy którzy pomogli rządowi zreorganizować przestarzałą armię. Rząd uznał, że najlepszą drogą odparcia chińskiego ataku będzie użycie lotnictwa (jak później ujawniło CIA, Chińczycy nie posiadali wtedy w Tybecie wystarczającej liczby paliwa, ani lotnisk). Podczas trwania wojny, Nehru wysłał do prezydenta Kennedy’ego dwa listy w których poprosił rząd USA o dostarczenie Indiom dwunastu eskadr myśliwców oraz nowoczesnego systemu radarowego. Samoloty miały zostać użyte w obronie Indii a postulat zbombardowania terenu Chin odrzucono, ze względu na obawę działań odwetowych chińskiego lotnictwa. Nehru poprosił również o przeszkolenie indyjskich pilotów przez Amerykanów. Prośba ta została odrzucona ze względu na zaangażowanie USA w kryzys kubański.
Wojna położyła kresy wcześniejszej nadziei Nehru który liczył na to że Indie i Chiny utworzą silny blok państw Azji, który w czasie zimnej wojny, będzie w stanie przeciwstawić się wpływom bloku zachodniego i wschodniego[100]. Pod koniec wojny wojsko indyjskie wyszkoliło tybetańskie siły zbrojne, składające się z tybetańskich uchodźców w Indiach, oddziały uchodźców uczestniczyły następnie w wojnach z Pakistanem w latach 1965 i 1971, rząd Indii nawiązał też kontakty z tybetańskimi rewolucjonistami na terenie Tybetu[101].
Po 1962 roku lider Indii podupadł na zdrowiu i spędził miesiąc na rehabilitacji w Kaszmirze. Niektórzy historycy przypisują ten nagły spadek zdrowia Nehru, rozczarowaniem i zdziwieniem po wojnie chińsko-indyjskiej. Nehru uznał wojnę za zdradę ze strony Chińczyków[102], którzy do tej pory prowadzili względem Indii przyjazną politykę. Po powrocie z Kaszmiru w maju 1964 roku, doznał udaru mózgu, a miesiąc później dostał ataku serca. Zmarł 27 maja 1964 roku. Premier został poddany kremacji nad rzeką Jamuna, w jego pogrzebie uczestniczyły setki tysięcy żałobników[103].
Nehru oprócz polityki zajmował się też pisarstwem, napisał on w języku angielskim kilka książek poświęconych historii świata, Indii, a także swoją autobiografię.
Nehru napisał testament na długo przed śmiercią (21 czerwca 1954).
Gdy umrę, chciałbym, żeby ciało moje było spalone… Moim życzeniem jest, aby garść moich prochów została wrzucona do Gangesu w Allahabadzie, co nie ma żadnego znaczenia religijnego…(…) Większość zaś moich popiołów powinna być…uniesiona przez samolot i rozrzucona z góry ponad polami, gdzie trudzą się chłopi indyjscy, aby mogły się one zmieszać z kurzem i ziemią, stając się nieoddzielną częścią Indii.
Ostatnia wola nie dotyczyła zatem spraw ogólnych, lecz osobistych. Natomiast za rodzaj testamentu społeczno-politycznego można uznać zdania z odczytu „Indie wczoraj i dziś”.
Chcę naturalnie, by Indie postępowały naprzód w rozwoju materialnym, by wypełniały swoje plany 5-letnie i podniosły stopę życiową swojej licznej ludności. Chcę, by ustały ciasne spory dnia dzisiejszego(…)W szczególności żywię nadzieję, że skończy się przekleństwo kast(…) Troszczę się nie tylko o nasz postęp materialny, lecz także o jakość naszego narodu(…) Siła jest potrzebna, lecz ważniejsza jest rozwaga i mądrość. Tylko połączenie siły i mądrości daje dobre wyniki[104].
W 1955 roku został odznaczony najwyższym indyjskim odznaczeniem, Orderem Bharat Ratna[105][106]. W 1964 roku, przyszły prezydent Indii, Sarvepalli Radhakrishnan, założył fundację Jawaharlal Nehru Memorial Fund. Fundacja od 1968 przyznaje prestiżową nagrodę „Jawaharlal Nehru Memorial Fellowship”[107]. We współczesnych Indiach znajdują się liczne pomniki i instytucje publiczne nazwane imieniem Jawaharlala. Jedną z najbardziej prestiżowych indyjskich uczelni jest Uniwersytet Jawaharlala Nehru w Delhi. W pobliżu Bombaju znajduje się port morski imienia założyciela niepodległych Indii. O życiu Pandita Nehru opowiada kilka filmów dokumentalnych.
W Polsce, w Warszawie na terenie obecnej dzielnicy Mokotów, w latach od 1973[108] do 2019[109] znajdowała się ulica imienia Jawaharlala Nehru.
Tomasz z Akwinu, także Akwinata (łac. Thomas de Aquino; ur. w 1224 albo w 1225 w Roccasecca, zm. 7 marca 1274 w Fossanuova) – włoski filozof scholastyczny, teolog, członek Zakonu Kaznodziejskiego (dominikanów). Był jednym z najwybitniejszych myślicieli w dziejach chrześcijaństwa. Święty Kościoła katolickiego; jeden z doktorów Kościoła, który nauczając przekazywał owoce swej kontemplacji (łac. contemplata aliis tradere).
Urodził się w 1224 albo 1225 roku w zamku Roccasecca koło Neapolu[1]. Pochodził z arystokratycznej rodziny – jego rodzicami byli hrabia Akwinu Landulf i Teodora Caracciolo[1]. Naukę początkową odebrał w klasztorze benedyktyńskim na Monte Cassino, a w 1239 roku podjął studia w zakresie sztuk wyzwolonych w Neapolu[1]. Pięć lat później postanowił wstąpić do zakonu dominikanów i wyruszył do Paryża kontynuować naukę[2]. Jego rodzina nie zaakceptowała tego wyboru (chciała by został opatem na Monte Cassino, co było wówczas funkcją bardzo dochodową) i bracia Tomasza uwięzili go[2]. Po roku Akwinata nadal odmawiał zrzucenia habitu, więc bracia uwolnili go i zezwolili mu na wyjazd[2]. W Paryżu poznał swojego mistrza, Alberta z Lauingen, z którym w 1248 roku przeniósł się do Kolonii[3]. Tam uzyskał stopień bakałarza biblijnego i napisał pierwsze dzieła: „komentarz do Księgi Izajasza” (łac. Expositio super Isaiam ad litteram) i „postylle do Księgi Jeremiasza i Lamentacji Jeremiasza” (łac. Super Ieremian at Threnos)[4]. W 1252 roku wrócił do Paryża, uzyskał stopień bakałarza sentencjariusza i zaczął wykładać Sentencje Piotra Lombarda[4]. Napisał wówczas także „Komentarz do Sentencji” (łac. Scriptum super Sententias) oraz krótkie traktaty filozoficzne „O zasadach natury”(łac. De principiis naturae) i „O bycie i istocie” (łac. De ente et essentia)[5]. Cztery lata później został magistrem teologii i profesorem Uniwersytetu Paryskiego[5]. Popadł tam w konflikt z Wilhelmem z Saint-Amour, który usiłował zabronić zakonnikom wykładać na katedrze uniwersyteckiej[5]. Owocem tego sporu jest dzieło „Przeciw zwalczającym kult Boga i życie zakonne” (łac. Contra impugnantes Dei cultum et religionem)[5]. Papież potępił tezy Wilhelma i zezwolił mnichom wykładać[6]. W tym czasie Tomasz napisał „Komentarz do O Trójcy Boecjusza” i „O prawdzie”[7].
W 1259 roku udał się do Valenciennes, by wziąć udział w kapitule generalnej dominikanów, a następnie przeniósł się do Neapolu[7]. Tam dwa lata później, na prośbę byłego generała zakonu Rajmunda z Penyafortu, zaczął pisać „Summa contra gentiles”, którą ukończył około 1266 roku[7]. Jeszcze w 1261 roku przeniósł się do Orvieto, gdzie przez cztery lata uczył w szkole dominikańskiej[8]. W tym okresie stworzył kilka kolejnych dzieł, m.in.: „Komentarz do Księgi Hioba” (łac. Expositio super Iob ad litteram), „Jak uzasadnić wiarę” (łac. De rattionibus fidei) oraz na prośbę Urbana IV „Sprostowanie błędów greckich” (łac. Contra errores Graecorum) i „Złotą katenę do czterech Ewangelii” (łac. Catena aurea in quattuor Evangelias)[8]. Podczas pobytu w Orvieto sekretarzem Tomasza został Reginald z Piperno[8]. W 1265 roku wyruszyli oni do Rzymu, gdzie Akwinata miał nauczać w szkole dominikańskiej[8]. Wówczas napisał pierwszą część swojego magnum opus „Summa theologiae”[8]. Druga część powstała w Paryżu w okresie 1270–1272, a początek trzeciej części w Neapolu[8]. Nieukończoną ostatnią część uzupełnił po śmierci Tomasza, jego sekretarz Reginald[9].
Podczas pobytu w Rzymie napisał wiele traktatów i rozpraw, wizytował dwór papieski, a także poznał Wilhelma z Moerbeke[9]. W 1268 roku wrócił do Paryża, gdzie stworzył dzieła egzegetyczne: „Komentarz do Ewangelii Jana i Mateusza” (łac. Lectura super Ioannem, Lectura super Matthaeum), „Komentarz do Listu do Rzymian i do Koryntian” (łac. Lectura super epistolas Pauli) oraz wiele analiz arystotelesowskich[10]. Do jego dzieł zalicza się także pismo w obronie Piotra z Tarentaise – „Odpowiedź bratu Janowi z Vercelli” oraz rozprawy polemizujące z Gerardem z Abbeville („O doskonałości życia duchowego” i „Przeciw naukom tych, którzy odciągają od życia zakonnego”)[11]. W 1272 roku udał się do Neapolu, gdzie chciał założyć studium dominikańskie i został kaznodzieją w kościele San Domenico Maggiore[12]. 6 grudnia 1273 roku podupadł na zdrowiu, w wyniku czego przestał tworzyć[13]. Miesiąc później Grzegorz X poprosił Tomasza, by udał się na sobór w Lyonie[13]. Pomimo złego stanu zdrowia, dominikanin przychylił się do prośby[14]. Podczas podróży, stan jego zdrowia znacznie się pogorszył i postanowił zatrzymać się w klasztorze cystersów w Fossanuova[15]. Zmarł tamże 7 marca 1274 roku[15].
Miał przydomki „doktor anielski” (łac. doctor angelicus) oraz „doktor powszechny” (łac. doctor communis)[1].
System filozoficzny Tomasza można najkrócej scharakteryzować jako konsekwentne przystosowanie klasycznych poglądów Arystotelesa do treści zawartych w teologicznej doktrynie chrześcijańskiej. Tym sposobem zostały wprowadzone do katolickiej teologii pojęcia aktu i możności, formy i materii, zasada przyczynowego powiązania zdarzeń, rozumienie poznania jako procesu receptywnego, oraz pojęcie dowodu. Według św. Tomasza nie jest bezpośrednio znany umysłowi ludzkiemu ani Bóg, ani dusza, ani żadne prawdy ogólne. Człowiek rodzi się bez wiedzy i zdobywa ją dopiero w czasie życia. Jednocześnie Tomasz w sposób istotny wzbogaca myśl Arystotelesa; przede wszystkim wprowadza do swojego systemu kategorię istnienia, przez co istotnie modyfikuje całą koncepcję metafizyczną Stagiryty; rozwija także – nieznaną greckiej filozofii – filozoficzną i teologiczną koncepcję bytu osobowego.
Tomasz stał na stanowisku receptywności poznania oraz łączności poznania wyższego z niższym, umysłowego ze zmysłowym. Władze poznawcze człowieka są albo zmysłowe, albo umysłowe. Władze wyższe posługują się niższymi, więc umysłowe zmysłowymi. Od władz niższych, zmysłowych, musi się też rozpoczynać proces poznania. W doczesnym życiu (secundum praesentis vitae statum) umysł, złączony z ciałem, nie może się obejść bez pośrednictwa zmysłów; zmierza jednak do umysłowego ujęcia rzeczy.
Tomasz traktował rozum psychologicznie, jako władzę duszy, a nie metafizycznie, jako oddzielną substancję. Wraz z Arystotelesem dzielił rozum na czynny i bierny. Właściwego aktu poznawczego dokonuje dopiero rozum bierny. Ostatecznie wszelkie poznanie rozumowe, tak samo jak zmysłowe, jest receptywne.
Przedmioty materialne możemy poznawać rozumem; dzięki temu posiadamy o nich wiedzę ogólną i pewną. Poznanie rozumowe jest zawsze ogólne, więc rozumowo poznajemy tylko gatunki, a nie jednostki materialne. Własną duszę poznajemy tylko pośrednio. Tomasz uważał, że dane są nam rzeczy zewnętrzne, a nie przeżycia wewnętrzne. Twierdził, też, że poznajemy jedynie to, co rzeczywiste, a nie to, co potencjalne. Bezpośrednio poznajemy więc tylko czynności duszy, a władze duszy i samą duszę – wyłącznie na drodze refleksji.
Tomasz uważał, że wiara i wiedza stanowią dwie różne dziedziny poznania, które w poszukiwaniach prawdy o świecie uzupełniają się wzajemnie. Dziedzina wiedzy, w przekonaniu Tomasza, była rozległa: rozum poznaje nie tylko rzeczy materialne, ale również Boga, jego istnienie, jego własności, jego działanie. Istnieją jednak prawdy dla rozumu niedostępne, jak Trójca Święta, grzech pierworodny, wcielenie, stworzenie świata w czasie; są to prawdy wiary, które jedynie objawienie może ludziom udostępnić. Niektóre prawdy przekraczają rozum, ale żadna mu się nie sprzeciwia. Nie może być sprzeczności między objawieniem a rozumem; podwójnej prawdy o tej samej rzeczy, jednej objawionej, a drugiej wyprowadzonej przez rozum (jak uczyli awerroiści) być nie może, bo wszelka prawda, zarówno objawiona, jak i naturalna, pochodzi z jednego źródła: od Boga. Prawda, jaką Bóg zsyła na drodze łaski, uzupełnia, ale nie zmienia tej, którą udostępnia na drodze przyrodzonej.
Na tej podstawie powstała prosta koncepcja rozgraniczenia filozofii i teologii, na jaką się dawniejsza scholastyka nie zdobyła. Teologię buduje się na podstawie objawienia, filozofię wyłącznie na zasadach rozumu. Nawet gdy traktują o tych samych sprawach, każda traktuje inaczej. Filozofia, jeśli służy teologii, to jedynie w tym sensie, że przygotowuje do wiary (preambula fidei), i że jej broni.
Zagadnienie, czy teologia może być uznana za naukę Tomasz z Akwinu poruszał w Komentarzu do De Trinitate Boecjusza q. 2, podchodząc do nauki w znaczeniu arystotelesowskim: Czy w odniesieniu do spraw Bożych, które podpadają pod wiarę, może istnieć jakaś nauka – (łac.) Utrum de divinis quae fidei subsunt possit esse aliqua scientia? To samo zagadnienie zreferowane jest w Tomaszowym prologu Komentarza do Sentencji Piotra Lombarda, art. 1 oraz art. 3, rozw. 2. Wśród kwestii-argumentów przeciwko można wyróżnić trzy zasadnicze:
Przykładem trudności może być pytanie o Boga, czy może On być przesłanką rozumowania, będąc formą prostą (p. 3), której istoty człowiek nie może poznać w sposób adekwatny (p. 2)?
Odpowiedź Tomasza idzie w kierunku wykazania paralelizmu teologii do filozofii. W Komentarzu do De Trinitate Boecjusza q. 2 art. 2 przytacza on argumenty Arystotelesa z Organonu:
W myśl tych stwierdzeń można wskazać bieg refleksji teologicznej prowadzący od wiary do poznania rozumowego, od pewnej początkowej podstawy do rozwiniętego zbioru twierdzeń, od zasad do wniosków. Warunkiem istotnym jest przestrzeganie pewnej przejrzystości wewnętrznej prawd wiary. Należy nie dopuszczać do stawiania w opozycji rozumu, tego co jest dla niego oczywiste, i wiary, wraz z jej nieoczywistymi twierdzeniami[16].
Jak zwrócił uwagę Robert Barron, cechą refleksji teologicznej Tomasza było unikanie prób uchwycenia tego co boskie w definicje. Akwinata wykazywał, że Bóg, jako byt prosty wymyka się im, objawiając się w Jezusie Chrystusie człowiekowi jako moc ekstatyczna, od której żaden z bytów nie jest większy. Odwrotnie to Bóg jest tym, którym zdobywa nas i pociąga nas do siebie[17]. Dlatego według Tomasza teologia jest dziedziną spekulatywną, czyli kontemplacją, a nie praktyczną, gdyż zajmuje się bardziej sprawami boskimi niż ludzkimi działaniami[18]:
Chrześcijańska teologia zajmuje się Bogiem, który uczynił ludzi, nie jest zaś uczyniony przez nich. Jest zatem bardziej kontemplatywna niż praktyczna (STh I, q1 a4).
Arystotelesowską metafizykę, czyli filozofię bytu Tomasz wzbogacił o dodatkowy aspekt – istnienie. Dostrzeżone przez Arystotelesa dwie wewnętrzne przyczyny bytu: materia i forma, będące odpowiednio możnością i aktem, trzeba uzupełnić o istnienie, będące przyczyną sprawczą. Dopiero złożenie istoty (natura, łac. essentia, quidditas), która jest możnością, i istnienia (esse), które jest aktem, daje istniejący byt, substancję. Istotą każdej rzeczy jest to, co właściwe gatunkowi, do którego należy – co zawarte jest w definicji gatunku. Rzeczy stworzone istnieją nie dzięki swej istocie, lecz dzięki temu, co nazywamy istnieniem (łac. esse). W tym leży zasadnicza różnica między Bogiem a stworzeniem.
Bóg, jako Absolut, jest jedynym bytem, w którym istota jest tożsama z istnieniem. Wskazuje na to także Jego imię Jestem, który Jestem, objawione Mojżeszowi (por. Wj 3,14-15). W stworzeniach, które są bytami przygodnymi, niekoniecznymi – istnienie nie jest tożsame z istotą, lecz jest ich aktem, czyli urzeczywistnieniem. Nie jest ono konieczne, stąd istota może, ale nie musi istnieć. Natomiast Bóg jest bytem koniecznym, musi istnieć, bo to leży w Jego istocie. Jest bytem niezależnym – istnieje z własnej natury. Stworzenie jest bytem przygodnym i zależnym. Istnienie nie jest jego naturą – przychodzi z zewnątrz urzeczywistniając jego istotę. Stąd Bóg jest bytem prostym, a stworzenie – złożonym, bo składa się przynajmniej z formy, czyli istoty i istnienia.
Synonimem bytów jest substancja. Jest to złożenie istoty i istnienia. Są dwa rodzaje substancji: cielesne i duchowe. W bytach cielesnych, istota to forma i materia. Forma określa to, co w bycie jest z gatunku, a materia to co jest w nim jednostkowego – stąd gatunek może mieć wielu przedstawicieli, wiele jednostek należących do niego; forma jest źródłem tego co wspólne w gatunku, materia – tego co mnogie. Połączenie formy i materii z istnieniem daje substancję cielesną. Substancje duchowe nie mają materii, są złożeniem czystej istoty (formy) i istnienia. Na tym polega zasadnicza różnica między światem duchowym a cielesnym. Co cielesne, składa się z formy i materii, a co jest czysto duchowe, posiada tylko formę.
Uwzględnienie w strukturze bytu momentu istnienia otwiera możliwość dowartościowania tego, co jednostkowe i indywidualne. W ujęciu Arystotelesa o jednostkowości i indywidualności decydowała materia, która jest elementem potencjalnym, możnościowym, niedoskonałym; w ujęciu Akwinaty jednostkowość i indywidualność konstytuowana jest istnieniem, które jest najdoskonalszym elementem bytu – jest aktem wszystkich aktów, a przez to doskonałością wszystkich doskonałości (Esse est actualitas omnium actuum et propter hoc est perfectio omnium perfectionum [De potentia VII a2, ad 9]).
Transcendentalia (łac. transcendere – „przekraczać”, przekraczające kategorie) to właściwości przysługujące każdemu bytowi (nie są to akcydensy); „coś, co mogę orzec o każdym bycie” [De veritate q1 a1]
Transcendentalia nie każdemu bytowi przysługują w ten sam sposób, ponieważ im bardziej coś istnieje, tym bardziej przysługują mu poszczególne transcendentalia; np. poszczególne transcendentalia bardziej przysługują osobom niż bytom, które nie są osobami.
Według Tomasza istnienie Boga nie jest prawdą oczywistą (propositio per se nota), nie wymagającą dowodzenia. Potrzebny jest dla niej dowód. Twierdząc tak, Tomasz stawał w opozycji do panującego w scholastyce przekonania, zrywał z tezą obozu augustyńskiego, dla którego Bóg był „jak najbardziej obecny w duszy ludzkiej”.
Istnienie Boga nie jest dogmatem opartym na apriorycznym rozumowaniu; nie wynika ani z pojęcia prawdy, jak chciał Augustyn, ani z pojęcia doskonałego bytu, jak chciał Anzelm. Zarówno w dowodzie Augustyna, jak i Anzelma, Tomasz widział błędy. Przyznawał, że gdybyśmy znali istotę Boga, to pojęlibyśmy od razu, że istnieje. Nie pozostaje więc nic innego, jak oprzeć dowód istnienia Boga na doświadczeniu – mimo że w nim jest dany byt skończony i niedoskonały, bardzo daleki od Boga; niemniej jest to jedyna droga do „upewnienia się” o Jego istnieniu. Dowód istnienia Boga musi być aposterioryczny, musi wyprowadzać istnienie Boga nie z nieznanej nam Jego istoty, lecz ze znanych nam Jego dzieł.
Według Tomasza za istnieniem Boga przemawia pięć argumentów, nazywanych drogami (łac. Quinque viae)
Tomiści podkreślają zasługi Tomasza w kwestii poglądów na temat Boga. Arystoteles Boga określał jako byt najwyższy, Tomasz zaś nie stawiał go w hierarchii bytów, lecz definiował jako czyste istnienie, czyli Bóg jako podstawa każdego bytu, a nie jako jeden z bytów o najwyższych przymiotach.
Człowiek jest osobą. Status bytu osobowego uzyskuje się dzięki godności, która jest doskonałością sposobu istnienia. Doskonałość ta charakteryzowana jest jako szczególna indywidualność i jednostkowość. Podstawą godności jest wolny wybór możliwy dzięki rozumności[19].
Dzięki przyjęciu koncepcji Arystotelesa, Tomasz uzyskał filozoficzne fundamenty, aby porzucić pogląd Platona, że tylko dusza jest człowiekiem, a ciało jest nie częścią, lecz narzędziem człowieka. Zdaniem Tomasza cielesność również należy do natury człowieka. Dusza, będąca rzeczywistym bytem jest integralnie powiązana z ciałem, tak dalece, że w sensie właściwym o ciele człowieka można mówić jedynie wtedy, gdy człowiek żyje, gdy dusza organizuje materię; to dusza sprawia, że powstaje ciało człowieka, i stąd mylące jest mówienie o połączeniu duszy i ciała, a należy raczej mówić o połączeniu duszy i materii, którego „efektem” jest człowiek, w tym jego ciało. Stąd człowiek jest osobą jako posiadający ciało, a sama dusza nie jest bytem osobowym – jest ona, o ile istnieje bez ciała, tzw. „bytem niekompletnym”.
W Kwestii dyskutowanej «O duszy» Tomasz twierdził, że dusza jest jedną substancją, łączącą się z ciałem nie tylko jako element je poruszający (ut motor), jak uważał Platon, lecz, jak też uznawał Arystoteles, jest ona przede wszystkim formą ciała. Mimo iż ma w sobie trzy elementy: wegetatywny, zmysłowy i racjonalny jest jedną substancją. Gdyby trzy rodzaje dusz: rozumna – charakterystyczna dla ludzi, zmysłowa – charakterystyczna dla pozostałych zwierząt i wegetatywna – charakterystyczna dla roślin, dająca wzrost, nie tworzyły w duszy jednej substancji, ciało nie stanowiłoby jedności, lecz złożenie (aggregatio), gdyż trzy dusze byłyby formami trzech osobnych elementów ludzkiego ciała: roślinnego, zwierzęcego i ludzkiego. Człowiek ma zatem tylko jedną substancję duszy – una anima secundum substantiam, która jest rozumna, zmysłowa i wegetatywna[20]:
Dusza rozumna daje ciału ludzkiemu wszystko to, co dusza zmysłowa daje zwierzętom nierozumnym (bruta), wegetatywna roślinom i jeszcze dodaje coś więcej. Dlatego jest ona w człowieku zarazem wegetatywną i zmysłową i rozumną[21].
Tomasz uważał też, że dusza kształtuje zrodzenie i wzrost płodu. Do czasu, gdy poczęcie osiągnie formę doskonałą, w nasieniu ojca jest najpierw sama dusza wegetatywna, potem, w procesie, który prowadzi do poczęcia, ustępuje ona duszy, która jest i wegetatywna i zmysłowa. A następnie, gdy dojdzie do poczęcia człowieka, gdy osiągnie ono doskonałą, skończoną postać (forma perfecta), zastępuje ją dusza, która jest jednocześnie wegetatywna i zmysłowa i rozumna[22].
Dusza ludzka, jako byt przygodny, ma w sobie wewnętrzne naturalne pragnienie (desiderium naturale) wiecznego istnienia[23]. Wynika ono z samej jej natury jako bytu[24] i przejawia się w postaci podwójnego pragnienia: szczęścia oraz wizji Pierwszej Prawdy[25]. Nie jest ono związane z konkretną władzą duszy. Jest wychyleniem, ruchem ku przyszłości, które może przyjąć formę rezygnacji, rozpaczy lub nadziei[26][27].Władzą duszy, której dobro jest naturalnym przedmiotem, jest wola. Dążenie do dobra jest jej naturą. W przypadku innych władz człowieka, kierowanie ich ku dobru jest zadaniem cnót. Wola ma to dążenie sama z siebie i nie potrzebuje pomocy ze strony osobnej sprawności, cnoty. Jednak w odniesieniu do dobra, które przekracza zakres mocy woli (proportio potentiae) – cnota jest jej niezbędna. Takim dobrem przewyższającym jej zdolności jest Dobro, którym jest Bóg. Cnotami wlanymi, które kierują miłość woli ku temu Dobru i umożliwiają ją, są cnoty teologalne miłości i nadziei[28].
Tomasz z Akwinu wyróżniał jedenaście podstawowych ludzkich uczuć. Klasyfikował je w ramach dwóch zmysłowych władz pożądawczych – podstawowej prostej władzy popędu zasadniczego (miłość, pożądanie, przyjemność, nienawiść, odraza, wstręt) oraz władzy popędu zdobywczego (nadzieja, rozpacz, odwaga, lęk, gniew)[29].
Kobieta, w sensie płci, powołana została do bytu jako „pomoc do rodzenia”[30]. Jest ona częścią wspólnoty z mężczyzną, z którym łączy się w „jedno ciało” dla dzieła rodzenia:
„Utworzenie kobiety z żebra mężczyzny było stosowne dla uwydatnienia, że między mężczyzną a kobietą winna istnieć więź wspólnoty ... Ani kobieta nie powinna panować nad mężczyzną i dlatego nie została utworzona z jego głowy, ani też mężczyzna nie powinien jej lekceważyć jakby jakąś podległą mu niewolnicę i dlatego nie została utworzona z jego stóp” [Summa theologiae I, q.92]
Kobieta łączy się z mężczyzną nie tylko z uwagi na rodzenie, ale i na inne zadania w życiu domowym. Święty Tomasz zwracał uwagę na to, że mężczyzna nie powinien lekceważyć kobiety „jakby jakąś podległą niewolnicę”, a między nimi powinna istnieć „więź wspólnotowa”[31]. Związek kobiety i mężczyzny powinien opierać się na pewnej równości, ponieważ na niej budowana jest przyjaźń. A że „gorącej przyjaźni nie można mieć z wieloma osobami”, konieczne jest pozostanie w monogamii, gdyż inaczej „nie będzie z obu stron równości w przyjaźni”[32].
„Było stosowne, by pierwsza kobieta została utworzona z mężczyzny ... by przez to podkreślić godność pierwszego człowieka..., a także, by mężczyzna bardziej miłował kobietę i wiązał się z nią nierozerwalnie, będąc świadomym jej pochodzenia z niego”. [Summa theologia I,q.92]
Równość nie jest jednak pełna, a mężczyzna ma władzę nad kobietą wynikającą z większej rozumności.
Dwojakie jest podleganie: jedno niewolnicze; według niego przełożony posługuje się poddanym dla swojego pożytku: i takie podleganie nastało po grzechu (por. Rdz 3,16). Jest jeszcze inne podleganie: domowe i obywatelskie; według niego przełożony posługuje się podległymi dla ich pożytku i dobra: i to podleganie istniałoby także przed grzechem. Bowiem w mnogości ludzi brak by było dobra porządku, gdyby jedni nie byli rządzeni przez innych mądrzejszych. I według tak pojmowanego poddaństwa kobieta z natury jest podległa mężowi, gdyż z natury mężczyzna ma większe rozeznanie rozumu – a, jak niżej powiemy, stan niewinności pierworodnej nie wyklucza nierówności wśród ludzi[33].
W ślad za Arystotelesem analizował Tomasz metafizyczne aspekty poczęcia kobiety z nasienia męskiego, przyjmując i rozwijając twierdzenia filozofa o słabości ciała kobiecego[34]. W odniesieniu do natury partykularnej, czyli zrodzenia tej konkretnej kobiety – utworzenie jej z nasienia jest czymś niedoszłym i niewydarzonym ((łac.) aliquid deficiens et occasionatum[35] dosł. naznaczone brakiem i wydarzające się przypadkowo) – gdyż nasienie męskie powinno wydać naturę męską. Gdy tak się nie dzieje, jest to kwestia przypadkowo wydarzającej się niedoskonałości aktu – niedoskonałości w rozumieniu metafizycznym, gdyż skutek ma być pełnym odbiciem przyczyny. Powodem tej przypadkowej niedoskonałości metafizycznej może być „słabość siły czynnej” w nasieniu bądź kwestia niedyspozycji materii, albo też spowodowało to coś z zewnątrz, np. wiatry południowe, o których [Filozof] pisze, że są wilgotne. Natomiast co do natury powszechnej, czyli kobiety jako kobiety, jej istnienie w świecie nie jest czymś przypadkowym, gdyż taki był zamysł natury ((łac.) intentio naturae), która zależna jest od Boga jako jej twórcy[36]. Siła czynna i bierna zostały rozdzielone pomiędzy mężczyznę i kobietę, ponieważ człowiek został (w odróżnieniu od roślin i zwierząt) „przyporządkowany jeszcze bardziej znamienitemu dziełu życia, którym jest myślenie”[37].
„Ze względu na to, w czym głównie ziszcza się treść obrazu, mianowicie ze względu na naturę myślącą zarówno w mężczyźnie, jak i w kobiecie widnieje obraz Boga. Pod pewnym ubocznym względem prawdą jest, że obraz Boga znajduje się w mężczyźnie w inny sposób niż w kobiecie, gdyż mężczyzna jest początkiem i celem kobiety, tak jak Bóg jest początkiem i celem całego stworzenia. Stąd też Apostoł po słowach: “Mężczyzna... jest obrazem i chwałą Boga, a kobieta jest chwałą mężczyzny” uzasadnia to twierdzenie dodając (1 Kor. 11,8): To nie mężczyzna powstał z kobiety, lecz kobieta z mężczyzny”. [Summa theologiae I,q.93]
Według niektórych uczonych redukowanie Tomaszowej koncepcji kobiety do definicji przejętej od Arystotelesa jest czymś krzywdzącym[38][39].
Czerpiąc z wiedzy starożytnych, średniowieczna wiedza przyrodnicza próbowała wyjaśnić brak równości ciał kobiety i mężczyzny, a w konsekwencji także ich płci. W tym kontekście refleksję Akwinaty uznać trzeba za zmierzającą do ukazania ich równej godności jako ludzi i jako dzieci Bożych, noszących w sobie obraz Boży – mimo braku równości między nimi na poziomie cielesnym, płciowym. Kobieta i mężczyzna jednakowo są obrazem Boga, o czym Tomasz nauczał zaraz po omówieniu zagadnienia różnic między płciami, w następnej kwestii Sumy Teologii:
Po słowach: ‘Na obraz Boży go stworzył’, Pismo św. dodało: ‘Stworzył mężczyznę i niewiastę’: nie żeby obraz Boga ujmować według odrębności płci, ale że obraz Boga jest wspólny obu płciom, gdyż jest w umyśle, a co do niego nie istnieje odrębność płci. Stąd też po słowach: ‘Według obrazu Tego, który go stworzył’ [Kol 3, 10]. Apostoł dodaje: ‘Nie ma już mężczyzny i kobiety’[Ga 3, 28]”. [Summa theologiae I q.93, a.6, ad 2, przeł. P. Bełch.]
Akwinata podkreślił, że nie ma różnicy między kobietą a mężczyzną w tym, co dotyczy duszy: „kobieta nie różni się od mężczyzny w tym, co jest rzeczą duszy” (secundum rem, in his quae sunt animae mulier non differt a viro [In 4 Sent d. 25 q.2, a.1, sol. 1, ad 2.]).
Tomasz zauważył jednocześnie, że znane są przypadki (wskazując na przykłady męczeństwa za wiarę), w których kobiety wykazywały się równie dobrym lub lepszym osądem i równą lub większą siłą ducha niż mężczyźni [Summa theologiae II-II, q.156, a.1, ad 1; III, q. 72, a. 8, ad 3]. W komentarzu do Ewangelii Mateusza zauważył także, że wśród kobiet spotkać się można ze skrajnościami tak co do pobożności, jak i okrucieństwa [Mt 14, 5-8][40].
W dziedzinie filozofii państwa głosił, że na czele państwa jest władza świecka i duchowna, twierdził też, iż istnieje ustrój republiki i monarchii. Najlepsza jest monarchia, bo to ona troszczy się o lud, zaś król musi przestrzegać prawa bożego i natury. Za tyrana uznawał tego, co nielegalnie zdobył władzę (uzurpacja, symonia), a także tego, kto (nawet zdobywszy ją legalnie) łamie Prawo Boże.
Problematyka prawa obejmuje dwa obszary: prawo-lex, oraz prawo-ius. Tomasz wyróżnia cztery zasadnicze typy prawa-lex: prawo wieczne (lex aeterna), prawo naturalne (lex naturalis), prawo pozytywne zwane też prawem ludzkim (lex positiva seu humana) oraz prawo Boże (lex divina). Prawo pozytywne obejmuje prawo narodów (ius gentium) i prawo poszczególnych państw (ius civile). Prawo Boże jest prawem objawionym i jako takie jest domeną teologii. Obejmuje prawo Starego Testamentu (lex vetus) oraz prawo Nowego Testamentu (lex nova).
Prawo-lex:
Prawo sprawiedliwe musi spełniać przynajmniej te cztery warunki.
Prawo-ius to przede wszystkim „sama rzecz sprawiedliwa” (ipsa res iusta) – to coś, co jest dla kogoś odpowiednie, współmierne. Tomasz w nawiązaniu do Arystotelesa pisze o odpowiedniości opartej na pewnego typu równości.
Tomaszowa filozoficzna koncepcja prawa naturalnego jest koncepcją bardzo złożoną i powiązaną z jego całym systemem filozoficznym. Swój pełen kształt uzyskała dopiero w Summa theologiae, gdzie znalazła się definicja głosząca, że prawo naturalne (lex naturalis) „nie jest niczym innym, jak uczestniczeniem prawa wiecznego w stworzeniu rozumnym” (STh I-II q91 a2 co). Pierwsza reguła prawa naturalnego stwierdza, że dobro należy czynić i podążać za nim, a zła należy unikać („bonum est faciendum et prosequendum, et malum vitandum”, STh I-II q94 a2 co). Jednak reguły prawa naturalnego są wtórne wobec prawa naturalnego pojętego jako zespół dóbr będących odpowiednimi celami człowieka, powodującymi jego konkretne działania i nadającymi kształt tym działaniom („Prawo nie jest niczym innym jak racją działania, wszelka zaś racja działania zawarta jest w celu” CG, III, cap. 114). Są to cele, których realizacja prowadzi do możliwie pełnego rozwoju człowieka, a tym samym do szczęścia. W dookreśleniu tych celów bierze udział nie tylko poznanie, ale także wolne wybory dokonywane przez człowieka[41].
Tomasz ukazał swoją wizję teologiczną moralności człowieka w Sumie teologicznej w tomach I-IIae (Prima secundae) i II-IIae (Secunda secundae). Życie człowieka jest drogą do szczęścia królestwa niebieskiego. Po upadku grzechu pierwszych rodziców, ludzkość straciła zdolność dążenia do szczęścia wiecznego. Dzięki odkupieniu Chrystusa, człowiek otrzymuje cnoty teologiczne, wiary, nadziei i miłości, które czynią tę drogę na nowo możliwą. Jest ona wyznaczana przez Nowe Prawo Ewangelii[42]. Natura ludzka wspomagana jest Darami Ducha Świętego, które wzmacniają i podnoszą na poziom nadprzyrodzony, poziom łaski, cnoty naturalne. Systematykę naturalnych cnót moralnych, Tomasz w dużej mierze przejął od Arystotelesa. Wady i grzechy na nowo powstrzymują lub całkowicie uniemożliwiają, zależnie od tego czy są ciężkie czy lekkie, drogę do królestwa.
Grzechy, według rozumienia Tomasza, zawsze mają w sobie coś ze śmierci ze względu na swój przedmiot. Polegają bowiem na zwróceniu się wolnej woli przeciw miłości, prowadzącej ku celowi ostatecznemu, którym jest życie człowieka w szczęściu wiecznym królestwa niebieskiego. Stąd zwrócenie się przeciw miłości Bożej, jak bluźnierstwo czy krzywoprzysięstwo, albo przeciw miłości bliźniego, jak zabójstwo lub cudzołóstwo i in., jest grzechem śmiertelnym – czyli przynoszącym śmierć duchową – samo w sobie. Podczas gdy grzechem powszednim można określić postawę woli, która pozwala na pewne nieuporządkowanie w dążeniu do realizacji miłości Boga i bliźniego. Będą to np. pustosłowie, śmiech ponad miarę. Ponieważ w grzechu znaczenie ma nie tylko przedmiot, lecz także podmiot, czyli predyspozycje i intencje osoby grzeszącej – to, co obiektywnie jest grzechem lekkim, powszednim, może stać się grzechem ciężkim. Po pierwsze wtedy, gdy człowiek stawia przedmiot powszedniego grzechu wyżej niż cel ostateczny swojego życia. Po drugie, gdy grzech obiektywnie lekki ukierunkowany jest ku popełnieniu grzechu ciężkiego, np. gdy ktoś używa pustosłowia dla skłonienia siebie i kogoś do popełnienia cudzołóstwa[43].
Grzech śmiertelny jest więc, według innego sformułowania Tomasza, odwróceniem się od dobra niezmiennego, którym jest Bóg, a zwróceniem się ku dobru zmiennemu, czyli stworzeniom. Owo nieuporządkowane zwrócenie się ku stworzeniom jest elementem materialnym grzechu, nazywa się je inaczej pożądliwością /(łac.) concupiscentia/, podczas gdy elementem formalnym jest utrata przymierza z Bogiem, czyli pierwotnej sprawiedliwości[44]. Jest ono nazywane winą śmiertelną – culpa mortalis (Summa theologiae III q86 a4; por. I-IIae q87 a4). Wina śmiertelna jest gładzona, kiedy przez łaskę usunięty zostaje stan odwrócenia umysłu od Boga. Wolna wola zwraca się wtedy ku Bogu w akcie wiary – actus fidei formatae. Zaś w odniesieniu do grzechu wolna wola dokonuje aktu pokuty – actus poenitentiae (STh III q86 a6 rad1). To nieuporządkowane zwrócenie się ku dobru zmiennemu pociąga za sobą karę doczesną – reatus poenae, która jedynie może przywrócić nieuporządkowanie grzechu do porządku sprawiedliwości (STh III q86 a5)[45].
W ujęciu Tomasza pierwszym i najważniejszym przedmiotem wiary jest poznanie pierwszej prawdy, czyli Boga – tak jak On się objawia w swoim słowie /por. 1 Tes 2,13/ (STh II-II q1 a1). Dlatego można ją umieścić pośrodku, między wiedzą a opinią (STh II-II q1 a2). Ale ze względu na autorytet słowa Bożego, jest ona pewniejsza od ludzkiej wiedzy (STh II-II q4 a8). Wiara odnosząc człowieka do Boga, sprawia, że zdolny jest on posiąść wieczne szczęście /(łac.) beatitudo/ (STh II-II q2 a5 oraz 6 ad1), gdyż dzięki niej przyjmuje łaskę zbawienia z grzechu i śmierci, które wysłużył Chrystus przez swoje wcielenie oraz mękę i zmartwychwstanie (STh II-II q2 a7)[46][47].
Na soborze trydenckim tylko dwie księgi umieszczono na ołtarzu – Biblię i „Sumę teologiczną” Tomasza z Akwinu.
W 1319 roku Kościół rzymskokatolicki rozpoczął wstępne postępowanie w sprawie kanonizacji Tomasza z Akwinu. Akt kanonizacji ogłosił 18 lipca 1323 roku papież Jan XXII[15]. W 1567 roku Pius V włączył go do grona doktorów Kościoła[15].
W ikonografii św. Tomasz przedstawiany jest w białym habicie dominikańskim, w czarnej kapie i białym szkaplerzu. Jego atrybutami są m.in. anioł, gołąb, infuła u nóg, której nie przyjął, kielich z Hostią, księga, laska, monogram IHS, monstrancja, pióro pisarskie, różaniec, słońce na piersiach, które symbolizuje jego Boską inspirację. Jego znakiem jest także Chrystus w aureoli.
W nadzwyczajnej formie rytu rzymskiego święto św. Tomasza przypada w dies natalis – 7 marca. Obchód ten niemal zawsze wypada w okresie wielkopostnym, stąd podczas reformy liturgicznej Pawła VI został on przeniesiony na 28 stycznia. Data ta funkcjonuje w zwyczajnej formie rytu rzymskiego, jako obowiązkowe wspomnienie liturgiczne.
 Artykuły na Stanford Encyclopedia of Philosophy (ang.) [dostęp 2018-01-28]:
 Artykuły na Internet Encyclopedia of Philosophy (ang.) [dostęp 2018-06-28]:
Epistemologia (od stgr. ἐπιστήμη, episteme – „wiedza; umiejętność, zrozumienie”; λόγος, logos – „nauka; myśl”), teoria poznania lub gnoseologia[1] – dział filozofii, zajmujący się relacjami między poznawaniem, poznaniem a rzeczywistością. Epistemologia rozważa naturę takich pojęć jak: prawda, przekonanie, sąd, spostrzeganie, wiedza czy uzasadnienie. „Co to jest poznanie i czym jest poznawanie? Wyjaśnianie praw dotyczących zarówno poznawania i tego, co jest poznane”[2].
Problematyką poznania filozofowie zajmowali się od czasów starożytnych. Dopiero jednak w epoce nowożytnej pojawiła się świadomość, że jest to odrębna dyscyplina filozoficzna. Kantyści nazywali ją krytyką poznania. W Metafizyce Alexandra Baumgartena użył on terminu „gnoseologia” (1799). W 1827 roku Thomas Krug w słowniku filozoficznym użył słowa Erkenntnislehre. Określenie Erkenntnistheorie (teoria poznania) pojawiło się w wydanej w 1862 roku książce Eduarda Zellera Über Bedeutung und Aufgabe der Erkenntniss-Theorie. Ein akademischer Vortrag i zyskało znaczną popularność w obszarze języka niemieckiego[3].
Termin epistemology pojawił się po raz pierwszy w książce Jamesa Perriera Theory of knowing (1856) i został utworzony od starogreckiego pojęcia episteme (stgr. ἐπιστήμη), oznaczającego wiedzę pewną, naukową, dotyczącą istoty, i przeciwstawianą opiniom, wiedzy praktycznej czy doświadczeniu[4].
Teoria poznania wiąże się z innymi dyscyplinami filozoficznymi lub im bliskimi, np. ontologią, logiką i psychologią. Ontologia docieka o tym co i jak istnieje. Czy analiza istnienia ma poprzedzać analizę poznania, tj. czy bez istnienia nie byłoby poznania? Logika jest systemem tautologii jako zdań prawdziwych we wszystkich modelach a epistemologia na pewno z tautologii się nie składa. Przedmiot badań psychologii jest częściowo wspólny, jednak każda z tych nauk bada go z innego punktu widzenia. Psychologia zajmuje się przebiegiem procesów poznawczych, a epistemologia interesuje się, wedle czego poznanie ma być oceniane jako prawda i fałsz...[5].
Co to jest wiedza (poznanie), to problem definicji wiedzy, a co jest wiedzą, to problem kryterium wiedzy. Jak dotrzeć do wiedzy, czyli jaka jest właściwa droga poznawania, to problem sposobu poznania. Rozdzielając dwa główne nurty w tej kwestii rozróżnia się: empiryzm metodologiczny (aposterioryzm) mówiący, że źródłem poznania jest doświadczenie i racjonalizm metodologiczny (aprioryzm), który twierdzi, że jest nim rozum. Spór toczy się też o to, czy istnieje wiedza wrodzona (natywizm głosi iż wiedza jest wrodzona), czy nabywamy ją wraz z doświadczeniem, czyli o pochodzenie poznania. I tu mamy też dwa główne stanowiska: empiryzm genetyczny i racjonalizm genetyczny. Naturą poznania naukowego zajmuje się filozofia nauki. Czy wiedza jest w ogóle możliwa i jak bronić się przed zarzutami sceptycznymi to problem sceptycyzmu.
Poszukiwanie definicji prawdy jest problemem podstawowym dla epistemologii. Tak jak w przypadku teorii wiedzy, teoria prawdy poszukuje jej definicji i kryterium bycia prawdą. Czy prawda istnieje i czy jest poznawalna, to zagadnienia blisko spokrewnione z problemem sceptycyzmu. Wiedza wspiera się na pojęciu prawdy, stąd pojęcie to jest podstawowe dla całej filozofii.
Jaka jest relacja między materią a duchem, rzeczą myślącą a rozciągłą czy, używając współczesnej terminologii, umysłem a ciałem czy zjawiskami fizycznymi a psychicznymi, to tak zwany problem psychofizyczny. Wyróżnia się tu przede wszystkim monizm (istnieje jedna substancja) i dualizm (istnieją odrębne substancje: ciało i duch).
Problemy pojęciowej analizy percepcji, jej przedmiotu, wiarygodności oraz relacji percepcji i wiedzy to podstawowe problemy filozofii percepcji. Podstawowymi współczesnymi poglądami w filozofii percepcji są: realizm bezpośredni (spostrzeganie jest bezpośrednie, wymaga oddziaływania przyczynowego ze strony przedmiotu, przedmiotami percepcji są przedmioty w przestrzeni fizycznej, percepcja jest zasadniczo wiarygodna), realizm pośredni (przedmiotem percepcji są reprezentacje rzeczy istniejących w przestrzeni fizycznej) oraz fenomenalizm (mamy dostęp jedynie do doznań i nie istnieje możliwość dotarcia do przedmiotów świata zewnętrznego).
Epistemologia jako odrębny dział filozofii została zapoczątkowana przez Kartezjusza, ale zagadnienia epistemologiczne były obecne właściwie już od początku refleksji filozoficznej.
W filozofii starożytnej problematyka epistemologiczna nie była wyróżniona, lecz rozważano ją przy okazji problematyki logicznej czy metafizycznej.
W użyciu funkcjonowało kilka pojęć, których znaczenie i wzajemne relacje były różnie określane przez poszczególnych filozofów[6] :
Już w pierwszym okresie filozofii starożytnej filozofowie przyrody i sofiści głosili pewne tezy na temat poznania, z których wiele antycypowało podstawowe zagadnienia epistemologii. Solon zalecał, żeby na podstawie rzeczy jasnych wnioskować o niejasnych, a Chilon był autorem zwrotu „poznaj samego siebie”. Tezę Heraklita (zm. ok. 480 p.n.e.), że wszystko płynie można rozumieć jako zwrócenie uwagi na problem identyczności przedmiotów w czasie. Dostrzegł, że zmienność podlega konieczności, który określał mianem logosu. Krytykował również bezrefleksyjne opieranie się na poznaniu zmysłowym, o ile nie jest poparte rozumem. Człowiek posiada możliwość poznania samego siebie poprzez myślenie, które umożliwia właśnie rozum. Pitagorejczycy znali dedukcję (dowód istnienia liczb niewymiernych) i przekonani byli o tym, że świat można opisać za pomocą matematyki (Filolaos był zwolennikiem koncepcji poznania jako uchwycenia stosunków ilościowych). Stosując metodę dedukcyjną stali się tym samym, prekursorami racjonalizmu i aproioryzmu w filozofii, obecnego już w poglądach szkoły eleackiej. Parmenides, który ją zapoczątkował, stawiał równość między bytem a poznaniem i twierdził, że wadliwość mniemań jest nieusuwalna. Episteme, w przeciwieństwie do doksy, odnosi się do bytu i jest bezbłędna. Melissos krytykował poznanie zmysłowe, a Anaksagoras jako pierwszy pojmował rozum jako autonomiczną zdolność poznawczą. Demokryt był prekursorem Locke'a w twierdzeniu, że jakości są subiektywne i uważał, że rzeczy podobne poznajemy przez podobne (atomy emitują swoje podobieństwa, które docierają do naszych zmysłów). Twierdził, że ilość jest tym aspektem rzeczy, który można poznać w sposób obiektywny. Słynną tezę Protagorasa, że człowiek jest miarą wszechrzeczy można rozumieć jako spostrzeżenie, że każdemu argumentowi da się przedstawić kontrargument, a rzeczywistość widzi się różnie w zależności od punktu obserwacji. Protagoras twierdził też, że nie można poznać bogów. Gorgiasz swojej tezy, że nic nie istnieje, a nawet jeśli istnieje, to nie jest poznawalne, próbował dowodzić nie wprost. Sokrates (zm. 399 p.n.e.) rozumiał cnotę (etykę) jako rodzaj wiedzy i znał metody definiowania i analizy pojęć.
Platon (ur. 427, zm. 347 p.n.e.) był twórcą klasycznej teorii wiedzy jako prawdziwego, dobrze uzasadnionego sądu (Teajtet). Jego koncepcja poznania była jednak ściśle związana z jego metafizyką. Platon rozróżnił[6] :
Pochodzenie wiedzy tłumaczył koncepcją preegzystencji dusz i metempsychozą, a więc był twórcą natywistycznej koncepcji wiedzy wrodzonej, która jest nabywana poprzez przypominanie – anamnezę. Platon nie utożsamiał wiedzy z prawdziwym sądem, zwrócił bowiem uwagę na fakt, iż można głosić coś prawdziwie wcale o tym nie wiedząc.
Swoją teorię poznania zobrazował w metaforze jaskini. Kolejne stopnie poznania rzeczywistości obrazowane są jako kolejne etapy wyzwalania się duszy. Cienie postrzegane na ścianie jaskini przez uwięzione w niej dusze są pozorem rzeczywistości, a postrzeganie ich związane jest z wyobraźnią (εἰϰασία). Dusza, która odwraca się od pozorów, by postrzec przedmioty, które rzucają cienie, opiera się na wierze (πίστιϛ). Przejście na wyższy poziom poznania, od oglądu przedmiotów zmysłowych ku przedmiotom prawdziwym dokonuje się za pomocą odbić obrazów: nauk matematycznych (poznanie pośredniczące, διάνοια)[10]. Poznanie rzeczywistości poza jaskinią następuje dzięki noezie (stgr. νόησις, noesis), intuicyjnemu wglądowi w istotę rzeczywistości.
Jego uczeń, Arystoteles (ur. 384, zm. 322 p.n.e.) był autorem koncepcji wiedzy jako systemu dedukcyjnego, w którym wychodzimy od przesłanek (aksjomatów) i wyprowadzamy kolejne twierdzenia za pomocą dedukcji. Wiedza (episteme) według jego modelu miała być ogólna i pewna.Stagiryta łączył z przekonaniem, że poznanie pochodzi z doświadczenia, choć rozum jest czymś w rodzaju jego moderatora, łączącego w sobie czynności zmysłów i myślenie. Przedmiotem poznania dla Arystotelesa jest substancja, połączenie materii i formy. Wprowadza również pojęcie przyczyny teleologicznej – celowej, jako racji powstawania rzeczy.
Jako pierwszy (przed Johnem Lockiem) użył pojęcia tabula rasa (de Anima 430A) na określenie umysłu, który wszelką wiedzę może zdobyć jedynie poprzez doświadczenie. Stanął w ten sposób w opozycji do platońskiej koncepcji wiedzy wrodzonej (natywizm). Podzielił nauki na teoretyczne (theoria), praktyczne (phronesis) i poetyczne (poetike). Stworzył podstawowe narzędzia logiczne dla nauki, teorię retoryki i ugruntował pojęcia takie jak spostrzeżenie, pamięć, doświadczenie i poznanie naukowe.
Choć filozofia sceptyków powstała na gruncie etyki, jako że zalecali oni powstrzymanie się od wszelkich sądów ze względu na dążenie do ideału moralno-życiowego, to ich osiągnięcia są doniosłe z punktu widzenia epistemologii. Skupili się na argumentowaniu za tym, że nic nie można poznać z pewnością, czyli że ideał episteme jest nierealizowalny. Pirron (zm. ok. 286 p.n.e.) twierdził, że dla każdych dwóch zdań A i nie-A trzeba przyjąć ich równe prawdopodobieństwo i mówić tylko „nie wiem, że A i nie wiem, że nie-A”. Ainezydem stworzył listę argumentów przeciw poznaniu bezpośredniemu:
Niepewność była dla sceptyków nieusuwalnym atrybutem wszystkich sądów, choć nie podważali racji mówienia o tym, co się wydaje (dostępu do doświadczenia wewnętrznego). Sekstus Empiryk rozróżnił: dogmatyzm, którego zwolennicy głoszą, że prawda jest poznawalna i że oni są jej właścicielami, akademizm mówiący o prawdzie, że jest niepoznawalna, bo jej nie ma i wreszcie sceptycyzm. Ideałem dla starożytnych sceptyków był mędrzec, który uznaje, że nie wie, czy jest prawda, czy jej nie ma, ale nie przeszkadza mu to w jej poszukiwaniu.
Augustyn z Hippony (ur. 354, zm. 430) twierdził, że człowiek może poznać Boga, ale potrzebny jest do tego akt oświecenia (illuminatio) i uważał, że choć Objawienie ma status wiedzy pewnej, to warto jest dążyć do zrozumienia prawd wiary. W średniowieczu kluczowym problemem stał się stosunek wiary do rozumu.
Greckie pistis (zaufanie), Rzymianie tłumaczyli jako fides a doksa jako opinio. Nie było słowa na oznaczenie wiary. Łacińskim fides zaczęli nazywać ją dopiero pierwsi chrześcijanie. Orygenes wierzył w zgodność Objawienia z filozofią grecką. Tertulian uważał, że rozumowe poznanie jest bezwartościowe, bo prawdy wiary są pewne, lecz dla rozumu niepojęte. Był autorem formuły credo quia absurdum („wierzę, choćby to było absurdalne”). Pseudo-Dionizy twierdził, że możliwości poznawcze człowieka są ograniczone a o Bogu możemy tylko mówić, czym nie jest. Anzelm z Canterbury, autor formuły fides quarens intellectum („wiara domaga się rozumu”), wierzył w spójność prawd wiary i rozumu. Jednak Piotr Abelard, twórca tekstu zawierającego zdania sprzeczne w Biblii, Sic et non, kwestionował jego optymizm. Awerroes głosił wyższość filozofii nad teologią. Siger z Brabancji zasłynął teorią dwóch prawd: rozumowej i religijnej. Tomasz z Akwinu (ur. 1225, zm. 1274) zajmując kompromisowe stanowisko, wyróżniał prawdy wiary niedostępne dla rozumu, prawdy rozumu obojętne dla wiary i prawdy wiary udowadnialne rozumowo. Jego formuła nihil est in intellectu quod non prius fuerit in sensu (zapożyczona od Arystotelesa) to zasada empiryzmu genetycznego.
William Ockham redukował poznanie do wiedzy o indywiduach, więc nie dopuszczał do niej konieczności ani pewności i stworzył metodologiczną zasadę niemnożenia bytów ponad potrzebę (brzytwa Ockhama).
Głównymi twórcami buddyjskiej epistemologii (język angielski: valid cognition; transliteracja Wyliego: tshad ma) byli komentatorzy dzieł doktryn jogaczary i madhjamaki: Dignaga (ok. 480-540)[11] i Dharmakirti (ok. 530-600) z Indii[12][13]. Dignaga napisał m.in. "Kompendium Prawomocnego Poznawania" (Sanskryt: Pramāṇasamuccaya; transliteracja Wyliego: tshad ma kun las btus pa) a najważniejszym dziełem Dharmakirtiego jest Pramanavartika (s. Pramāṇavārtika, tyb. tshad ma rnam 'grel)  stanowiąca komentarz do tekstu Dignagi i będąca jednym z siedmiu traktatów Dharmakirtiego –  "Siedem Traktatów Prawomocnego Poznawania" (transliteracja Wyliego: tshad ma sde bdun). Dzieła te były kontynuacją wielowiekowego rozwoju myśli buddyjskiej począwszy od starożytnych wczesnych szkół buddyjskich. Dla Dignagi istnieją dwa ważne sposoby poznania (pramāna), postrzeżenie (pratyakṣa) i wnioskowanie (anumāna). Pomimo to, nie odrzuca innych źródeł poznania, takich jak porównanie czy świadectwo autorytetu, uznając je jednak za formy postrzeżenia i wnioskowania. W ten sposób, jego myśl niejako zawiera w sobie wcześniejsze ustalenia buddyjskiej epistemologii.
Francis Bacon wystąpił przeciwko logicznym naukom Arystotelesa i stworzył metodę naukową, Galileusz rozwinął ją jako metodę hipotetyczno-dedukcyjną.
Kartezjusz (ur. 1596, zm. 1650) dokonał epistemologicznej reorientacji filozofii. Stworzył geometrię analityczną, co skłoniło go do stwierdzenia, że filozofia powinna stosować podobną metodę. Wiedza powinna więc być systemem prawd pewnych, koniecznych, powiązanych logicznie i niezależnych od doświadczenia. Stwierdzenie cogito ergo sum („myślę, więc jestem”) oznacza spostrzeżenie, że nie może istnieć akt myślenia bez podmiotu myślenia. Uznał to za przezwyciężenie sceptycyzmu, który wątpi we wszystko i stworzenie aksjomatycznej podstawy dla absolutnego systemu. Wychodząc od tej podstawy, w umyśle odnalazł Kartezjusz idee wrodzone Boga, duszy (res cogitans) i ciała (res extentia) i rozłożył akcenty tak, że położył nacisk na problem psychofizyczny.
W Rozprawie o metodzie zawarł podstawowe zasady metodologiczne:
Kartezjusza krytykował Thomas Hobbes, który dowodził, że z cogito nie wynika w żaden sposób twierdzenie o niezależności umysłu.
John Locke (ur. 1632, zm. 1703) przeciwstawił Kartezjuszowi i jego koncepcji wiedzy wrodzonej wizję umysłu jako niezapisanej tablicy (tabula rasa), która zapisuje się dopiero w wyniku doświadczenia. Tak jak Kartezjusz nie kwestionował doświadczenia wewnętrznego i twierdził, że w bezpośrednim doświadczeniu mamy dostęp do wiedzy pewnej. To on właśnie rozróżnił doświadczenie na zewnętrzne (wrażenia zmysłowe) i wewnętrzne (refleksja). Intuicja działająca w obrębie pamięci daje według niego pewność poznawczą i jest podstawą logiki oraz matematyki. Idee dostarczane przez nią są proste. Idee złożone powstają w wyniku abstrakcji z prostych. Niepewność tkwi w poznaniu zewnętrznym.
Głosił przyczynową teorię percepcji mówiąc, że idee mają przyczyny w poznaniu zewnętrznym. Argumentował za tą teorią, że zmysły nie wytworzyłyby wrażeń bez przyczyny i dowodził jej słuszności z różnicy między ideami powstającymi w umyśle i ideami-wrażeniami, na które nie mamy wpływu.
Zastanawiał się też nad tym, czy wszystkie nasze idee mają odpowiedniki w rzeczywistości. To doprowadziło go do rozróżnienia między jakościami pierwotnymi (kształt, objętość, ruch), które są obiektywne i odpowiadają własnościom ilościowym a wtórnymi (na przykład barwy, zapachy, dźwięki), które są subiektywne i są tożsame z własnościami jakościowymi.
Naiwny realizm twierdzi, że wszystkie jakości są pierwotne. George Berkeley stwierdził, że wszystkie są wtórne. Odrzucił pojęcie substancji jako zbędną metafizykę, ograniczył poznanie wyłącznie do poznania zmysłowego (nawet matematyka była nauką o zmysłowo ujmowalnych przedmiotach) i podważył istnienie rzeczywistości fizycznej tworząc formułę esse est percipi („być znaczy być postrzeganym”). Na zarzuty formułowane przeciwko jego teorii, iż rzeczy istnieją, pomimo tego że nie są postrzegane, odpowiadał, że dzieje się tak, ponieważ są one postrzegane przez Boga.
Uważany za najwybitniejszego przedstawiciela nowożytnego sceptycyzmu, David Hume (ur. 1711, zm. 1776) uznawał, że problem obiektywności poznania jest bezprzedmiotowy i stwierdził, że dowiedzenie istnienia świata zewnętrznego jest niemożliwe. Jednak dodawał, że wiara w istnienie rzeczywistości jest przydatna ze względu na jej walory praktyczne.
Rozróżnił impresje (lockowskie wrażenia), których doznajemy, gdy coś widzimy lub słyszymy i idee, które tworzymy, gdy coś wyobrażamy sobie lub myślimy. Proces poznania przebiega według niego od impresji do złożonych idei, które powstają za sprawą operacji umysłowych, polegających na kojarzeniu idei przez relację podobieństwa, styczność w czasie i przestrzeni oraz ideę przyczynowości.
Wyróżnił fakty i relacje między ideami, którymi zajmuje się matematyka – wiedza pewna, lecz nie mówiąca nic o faktach. Wiedza o faktach natomiast byłaby pewna, gdyby opierała się wyłącznie na impresjach, ale opiera się też na ideach leżących u podstaw doświadczenia: idei substancji i przyczynowości.
Te dwa pojęcia Hume krytykował. Tak naprawdę doświadczamy tylko ciągów zdarzeń, mówił, a wiązanie ich w związki przyczynowe, to już działanie umysłu wynikające z wyrobionego nawyku. Tymczasem związek przyczynowy nie jest konieczny, bo przyszłość może przynieść inne dane.
Gottfried Wilhelm Leibniz przeformułował Tomaszową zasadę empiryzmu genetycznego dodając do niej nisi intellectus ipsae  (z wyjątkiem samego umysłu). Planował stworzenie specjalnego języka (lingua universalis), w którym można by jednoznacznie przedstawić a następnie rozwiązać każdy problem.
Wydzielił dwa rodzaje prawd odpowiadających dwóm zasadom myślenia: prawdy dotyczące faktów (zasada racji dostatecznej) i prawdy rozumowe (zasada sprzeczności). Każda prawda jest a priori (predykat zawarty w podmiocie) i konieczna (prawdziwa we wszystkich możliwych światach), ale tylko z punktu widzenia Boga. My musimy uznać prawdy dotyczące faktów za przypadkowe, a pełne ich dowody są nieskończone (właściwie to samo mówił Hume w krytyce przyczynowości).
Thomas Reid bronił zaufania do własnego zdrowego rozsądku, czym stał się prekursorem Moore'a.
Immanuel Kant (ur. 1724, zm. 1804) dokonał rozróżnienia na sądy analityczne, czyli takie, które wynikają z definicji (por. klasyczny rachunek zdań), zaś wiedza, którą zdobywamy na ich mocy nie może wykraczać poza granicę już znanych pojęć i syntetyczne (por. indukcja niezupełna), które pozwalają na zdobywanie wiedzy istotnie nowej. Z drugiej strony podzielił sądy na aprioryczne (poza doświadczeniem, wynikające z wrodzonych własności umysłu i zmysłów) i aposterioryczne (na podstawie zdobywanego doświadczenia).
Stosując te kategorie do wcześniejszych dziejów filozofii można powiedzieć, że platońska episteme miała zawierać tylko sądy a priori. Leibniza prawdy rozumu są a priori, prawdy faktów syntetyczne a posteriori, choć dla Boga wszystkie są analityczne a priori. Hume akceptował tylko istnienie analitycznych a priori (relacje między ideami) i syntetycznych a posteriori (fakty).
Natomiast Kant argumentował za możliwością istnienia sądów syntetycznych a priori, które jego zdaniem są obecne w matematyce i przyrodoznawstwie i byłyby składnikami przyszłej metafizyki. Argumentował za pomocą dedukcji transcendentalnej, czyli poszukiwania argumentów na konieczność warunków możliwości tego, co jawi się nam jako wymagające uzasadnienia.
Początkiem poznania jest doświadczenie, którego składnikami apriorycznymi są czas i przestrzeń, jakie określa transcendentalnymi warunkami poznania, oraz kategorie intelektu, z których za najważniejsze uznawał kategorię przyczynowości i substancji. Doświadczamy więc z pomocą wrodzonych narzędzi.
Możemy, zdaniem Kanta, rozróżnić zjawiska naszej świadomości (fenomeny) i rzeczy same w sobie (noumeny). Te ostatnie nie są poznawalne, ale musimy przyjąć ich istnienie, by wytłumaczyć istnienie zjawisk.
Kant rozróżnił też pytania de iure (dotyczące kwestii dopominających się wyjaśnienia i wymagających transcendentalnej dedukcji) i quid facti dotyczące tego, co jest. Teoria poznania zajmuje się według niego pytaniami de iure.
Hegel odrzucał epistemologię (teorię poznania), gdyż nie miał zaufania do bezkrytycznie stosowanego pojęcia poznania w epistemologii[14]. Poznanie w ujęciu epistemologicznym nie jest absolutne a poza Absolutem nie można znaleźć ani prawdy, ani wiedzy i dlatego należy odrzucić epistemologię. Dlatego w przypadku filozofii Hegla nie można mówić o epistemologii, ponieważ utożsamiał wiedzę z bytem – racjonalnym, logicznym i koniecznym.
Bradley, heglista, głosił holistyczną koncepcję, wedle której wszelkie wyodrębnianie faktów z całości wiedzy prowadzi do uproszczeń.
Schleiermacher stworzył hermeneutykę, czyli teorię rozumienia tekstu. Stwierdził, że w przypadku każdego tekstu mamy do czynienia z sytuacją koła hermeneutycznego: żeby zrozumieć tekst, trzeba zrozumieć inny, być może całą kulturę i nie ma wyjścia poza ten krąg.
Szkoła marburska rozwija epistemologię jako krytykę poznania, której założenia można ująć następująco:
Szkoła badeńska rozwija epistemologię opartą na filozofii wartości[16]. Heinrich Rickert uznaje, że przedmiotem poznania są wartości poznawcze.
Wilhelm Windelband rozróżnił nauki idiograficzne skupione na opisie (np. historia) i nomotetyczne formułujące prawa przyrody (np. fizyka).
Nowa Szkoła Friesa rozwija krytyczną filozofię Immanuela Kanta w duchu Jakoba Friedricha Friesa. Jej reprezentant Leonard Nelson w rozprawie O niemożliwości teorii poznania (1911) dowodzi, że naukowa teoria poznania nie jest możliwa, ponieważ nie da się uzasadnić naukowo obiektywnej ważności poznania bez założenia tego rodzaju ważności[17].
August Comte twierdził, że wiedza winna służyć celom praktycznym, by można było realizować cel ulepszania życia społecznego. Rozróżnił nauki na abstrakcyjne (matematyka, astronomia, fizyka, chemia, biologia i socjologia, przy czym każda z nich jest coraz mniej ogólna i podporządkowana poprzedniej), a filozofię sprowadzał do teorii wiedzy. John Stuart Mill twierdził, że twierdzenia matematyki są uogólnieniami indukcyjnymi i opracował kanony indukcji eliminacyjnej chcąc odeprzeć zarzuty Hume'a.
Empiriokrytycyzm (Ernst Mach, Richard Avenarius) stworzył zasadę ekonomii myśli mówiącą, że aparat pojęciowy musi być jak najprostszy. William James opracował zasadę pragmatyczną, która mówi, że należy rozpatrywać praktyczne konsekwencje działań oparte na określonych przekonaniach.
W nurtach okresu pozytywistycznego leżących na peryferiach tej myśli należy wymienić Fryderyka Nietzsche, który był relatywistą i uważał poznanie za akt biologiczny, warunkowany celami praktycznymi i twierdził, że rzeczywistość ujmujemy zawsze fałszywie z uwagi na różne ograniczenia. Argumentował też za tym, że każde uogólnienie jest nieadekwatne, a poznając rzeczywistość odwołujemy się do własnych konstrukcji poznawczych. Hans Vaihinger twierdził, że nasze poznanie operuje użytecznymi fikcjami. Henri Bergson był zwolennikiem intuicji, która w jego koncepcji dostarcza poznania bezpośredniego i jest jak uchwycenie budowli jednym rzutem oka. Theodor Ziehen, przedstawiciel psychologizmu chciał redukować epistemologię do psychologii.
Edmund Husserl (ur. 1859, zm. 1938) uważał filozofię za pierwszą i fundamentalną naukę w gmachu wiedzy, która w odróżnieniu od nauk szczegółowych ma dostarczyć episteme. Zjawiska należy według niego opisywać takimi, jakimi są, w myśl hasła powrotu do rzeczy samych. Fenomenologia (epistemologia) jako podstawa ma być bezzałożeniowa. Jego zasada wszystkich zasad zalecała opisywanie wszystkiego tak, jak jawi się w świadomości. Wymaga to, zdaniem Husserla, redukcji ejdetycznej, czyli oczyszczenia świadomości z balansu poznawczego, wzięcia w nawias wiedzy naukowej i prowadzi do uchwycenia istoty fenomenów. Uczeń Husserla, Roman Ingarden, stworzył projekt czystej epistemologii, niezależnej od nauki i innych dyscyplin filozoficznych.
Bernard Bolzano wyróżnił kategorię sądów i przedstawień samych w sobie (niezależne od umysłu i konstytuujące realność badaną przez logikę) oraz sądów i przedstawień w sensie subiektywnym. Zdefiniował pojęcie analityczności, prawdy logicznej, wynikania logicznego i prawdopodobieństwa.
Gottlob Frege sformułował sławne rozróżnienie sensu i znaczenia. Zdefiniował zdania aprioryczne jako te, które są sprowadzalne do logiki.
Konwencjonalistów łączył pogląd, że uznanie stwierdzenia zależy od przyjęcia umów terminologicznych (konwencji). Antycypacji tego poglądu można dopatrywać się u sofistów. Do konwencjonalistów należeli: Henri Poincaré, Pierre Duhem i Kazimierz Ajdukiewicz. Konwencje nie oznaczają tu jednak całkowitej arbitralności. Chodziło im o to, że nie są jednoznacznie wyznaczane przez fakty. Stanisław Leśniewski krytykował konwencjonalizm mówiąc, że możemy umówić się do wszystkiego, ale to nie wpłynie na to, jakim jest świat.
Bertrand Russell stworzył projekt naukowej epistemologii: kombinacji logiki i metody przyrodniczej. Sądził, że struktura logiki jest izomorficzna ze strukturą świata. Stałym logicznym odpowiadają indywidua, zdaniom atomicznym – fakty, itd. Indywidua, proste istności stanowiące fundament dla własności są według niego poznawalne bezpośrednio a nie przez opis.
George Edward Moore bronił zdrowego rozsądku jako źródła trafnych przekonań. Wprowadził termin „dana zmysłowa” i badał relacje między danymi a własnościami rzeczy. W filozofii percepcji wyróżnił:
Moore sformułował też paradoks analizy. Mówi on, że jeśli analizowane pojęcie jest tożsame z objaśniającym je kontekstem, to analiza jest trywialna. Jeśli nie, to jest niepoprawna, gdyż zmienia sens analizowanego pojęcia.
Ludwig Wittgenstein w słynnej tezie stwierdził, że granice języka są granicami naszego świata, a więc poznanie jest ściśle związane z językiem. Choć z początku postulował konieczność języka doskonałego w rodzaju leibnizowskiej lingua universalis, później przekonał się do pracy nad językiem potocznym. Filozofia miałaby ograniczyć się do istotnych dla niej kontekstów takich jak „wiedzieć”, „wierzyć”, „widzieć”. Kontynuował tę myśl John Austin, który sformułował fenomenologię lingwistyczną opartą na zaufaniu właśnie do języka potocznego.
Gilbert Ryle zdefiniował pojęcia „wiedza jak” – praktyczna i „wiedza, że” – teoretyczna.
Filozofowie z kręgu Koła Wiedeńskiego (Moritz Schlick, Otto Neurath czy Rudolf Carnap) chcieli stworzyć Encyklopedię jedności nauki, gdzie całość wiedzy miała być ujęta w jednolitym języku fizykalistycznym. Filozofię sprowadzali do logiki.
Bezsensowność metafizyki próbowali wykazać w drodze analizy logicznej. Za kryterium sensowności zdania przyjęli jego weryfikowalność i uznali równozakresowość nazw „sprawdzalny”, „sensowny”, „naukowy” z jednej strony i „niesprawdzalny”, „bezsensowny” i „metafizyczny” z drugiej. Jak zauważył Roman Ingarden sama zasada sensowności jest nieweryfikowalna, czyli bezsensowna według ich własnego kryterium.
Krytykował zasadę sensowności a zamiast weryfikacji zaproponował falsyfikowalność jako kryterium wiedzy naukowej. Uważał, że celem nauki jest zbliżanie się do prawdy poprzez zwiększanie prawdopodobieństwa i wyznawał ewolucjonizm traktując wiedzę jako narzędzie w procesie ewolucji.
Noam Chomsky stworzył gramatykę generatywną, system reguł wywodzących strukturę zdania z prostych elementów o charakterze syntaktycznym. W jego koncepcji uczenia się języka przez dzieci odrodziła się koncepcja wiedzy wrodzonej.
Myślenie – ciągły proces poznawczy polegający na skojarzeniach i wnioskowaniu, operujący elementami pamięci takimi jak symbole, pojęcia, frazy, obrazy i dźwięki. 
W świetle neurobiologii (m.in. António Damásio) głównym składnikiem myśli są obrazy percepcyjne o różnych modalnościach zmysłowych, np. słuchowe, wzrokowe, węchowe, smakowe[1][2], które odpowiadają przedmiotom, procesom zachodzącym z udziałem przedmiotów lub odpowiadającym im słowom[a]. Tworzone reprezentacje percepcyjne są topograficznie zorganizowane w mózgu. Są one aktywowane z udziałem „reprezentacji dyspozycyjnych”, tworzonych w innym miejscu mózgu[b], wykorzystywanych w toku myślenia[3].
Myślenie może być pojmowane również jako ruch świadomości oraz skupienia i koncentracji. W praktyce jest jednak wywoływane nieświadomie bez udziału woli i wiąże się z naszymi poprzednimi myślami i działaniami.
Ludzkie myślenie jest realizowane przez
procesy psychiczne/kognitywne opierające się na systemie pojęć o różnym stopniu konkretności łączone w mózgu w mniej lub bardziej świadomy sposób.
Według interpretacji A. M. Gadomskiego proces myślenia jest badany na poziomie dynamiki sieci neuronowych, zaś funkcje myślenia są interpretowane na poziomie symboli jako własności abstrakcyjnego umysłu. W tym ujęciu, procesy mózgowe są nośnikami operacji symbolicznych, takich jak skojarzenia i wnioskowanie, oraz wspierane są nieświadomymi/podświadomymi operacjami poszukiwania w pamięci.
W komputerowej symulacji myślenia w celu rozwiązania danego zadania tworzy się program operacji oraz w trakcie wykonywania konfrontuje się go z oczekiwanym wynikiem.
Istnieje wiele bardziej i mniej ogólnych modeli myślenia, racjonalnego, irracjonalnego, emocjonalnego, oraz modeli mniej lub bardziej zależnych od kontekstu.
Wyróżnia się aspekt operacyjny, dynamiczny i motywacyjny myślenia[4].
W tym aspekcie myślenie to ciąg pojęć. Pojęcia te wchodzą ze sobą w złożone relacje (powiązania). Myślenie umożliwia wyodrębnianie różnych cech danego pojęcia, zauważanie podobieństw pomiędzy różnymi pojęciami, abstrahowanie od cech różniących pojęcia, uogólnianie i uszczegóławianie, czego efektem jest też tworzenie nowych pojęć.
Operacyjne własności myślenia to np.: abstrahowanie, uogólnianie, uściślanie, kojarzenie, zapamiętywanie.
Myślenie o myśleniu to meta-myślenie, jest ono podstawą ludzkiej samoświadomości.
Przykładami objawów zaburzeń myślowych są:
W tym aspekcie myślenie to proces, który wymaga odpowiedniego tempa i selektywności. Ta strona myślenia ma wiele wspólnego z uwagą.
Objawami zaburzeń są:
Ta strona myślenia zajmuje się ukierunkowaniem tego procesu na konkretny cel.
Zaburzenia myślenia możemy podzielić na:
Słowa czy język, pisany czy mówiony, zdają się nie odgrywać żadnej roli w mechanizmie myślenia. Wydaje się, że jednostkami psychicznymi, którymi posługuje się myśl, są pewne znaki oraz bardziej lub mniej jasne obrazy, które mogą być na zawołanie „odtwarzane” i łączone. […] Elementy, o których mowa, są w moim przypadku wizualne i… mięśniowe. Konwencjonalne słowa i inne znaki wymagają żmudnych poszukiwań jedynie w drugim stadium, gdy wspomniana gra asocjacji jest już w pełni wykształcona i może być przywołana na życzenie.
Konfucjanizm (儒學, rúxué, lub 儒家, rújiā) – system filozoficzno-religijny zapoczątkowany w Chinach przez Konfucjusza (Kong Fuzi, Kongzi) w V wieku p.n.e., a następnie rozwinięty m.in. przez Mencjusza (konfucjanizm idealistyczny) i Xunzi (konfucjanizm realistyczny) w III wieku p.n.e. Konfucjanizm głosi, że zbudowanie idealnego społeczeństwa i osiągnięcie pokoju na świecie jest możliwe pod warunkiem przestrzegania obowiązków wynikających z hierarchii społecznej oraz zachowywania tradycji, czystości, ładu i porządku[1].
Dokładne określenie, czy konfucjanizm jest religią, czy filozofią jest trudne. Według europejskich kryteriów jest to mieszanka pojęć religijnych, społecznych, ekonomicznych, etycznych i obyczajowych tworzących spójny, choć nie do końca zdefiniowany światopogląd.
Konfucjanizm rozpowszechnił się w Chinach i Korei (początek I w. n.e.), a także w Wietnamie i Japonii, stając się dominującą doktryną państwowo-religijną tych krajów, kształtującą ich politykę i obyczaje do czasów współczesnych. Jego złożoność powodowała, że w pełnej formie był on wyznawany tylko przez ludzi wykształconych, w tym urzędników państwowych zwanych mandarynami oraz dwór cesarski. Osoby znające dobrze tradycję, rytuał i podstawy filozoficzne konfucjanizmu nazywały siebie junzi (chiń. syn władcy), co oznacza „człowiek wyższego stanu”, „człowiek szlachetny”. Dla pozostałych ludzi konfucjanizm sprowadzał się do stosunkowo prostych reguł postępowania, odmiennych dla różnych ludzi zależnie od ich miejsca w hierarchii społecznej. Niemniej istnieje też rodzaj „dekalogu”, który jest wspólny wszystkim wyznawcom konfucjanizmu znany jako pięć powinności.
Duch konfucjanizmu jest w wielu krajach Dalekiego Wschodu wciąż żywy i głęboko zakodowany w świadomości wielu ludzi, spełniając podobną rolę kulturową jak „podstawowe wartości chrześcijańskie” w krajach Zachodu.
Konfucjanizm wywodzi się w niemal prostej linii ze starych wierzeń chińskich, w których kluczową rolę odgrywał kult przodków. W wierzeniach tych przyjmowano zasadę, że mężczyźni, którzy przeszli przez życie w godny sposób (czyli założyli i utrzymali liczną i dobrze prosperującą rodzinę) tworzą po śmierci rodzaj grupowego ducha, który kieruje losem późniejszych pokoleń.
Koncepcja wspólnego ducha przodków została w konfucjanizmie przeniesiona na wyższy stopień abstrakcji. W konfucjanizmie przyjęto, że wszyscy „godni” przodkowie, ze wszystkich wcześniejszych pokoleń łączą się w jeden, bezosobowy byt absolutny sterujący losami świata, który oznaczano w tekstach tym samym znakiem co słowo „niebo”. Niebo (天, Tian) jest kluczowym pojęciem w konfucjanizmie, choć nie jest ono dokładnie zdefiniowane.
Sterowanie losami świata przez niebo nie odbywa się jednak bezpośrednio, tak jak to ma miejsce w przypadku koncepcji chrześcijańskiego Boga, lecz poprzez rodzaj życzliwej propozycji – rodzaju idealnego planu dla świata, którego pełne wykonanie zapewniłoby stworzenie doskonałych relacji między wszystkimi stworzeniami. Stan taki określa się jako tzw. niebiański spokój. Osiągnięcie tego stanu zależy jednak od dobrej woli i działań ludzi aktualnie żyjących na świecie i niebo nie ma żadnej bezpośredniej mocy, którą by mogło narzucić ludziom ów stan. Postępowanie przybliżające do tego stanu jest w systemie etycznym konfucjanizmu równoznaczne z postępowaniem dobrym, zaś oddalające od tego stanu postępowaniem złym.
W tej sytuacji zadaniem mądrych i godnych ludzi jest więc w pierwszej kolejności rozpoznanie planu nieba – czyli nauczenie się co jest zgodne, a co nie jest z niebiańskim porządkiem. Jest to podstawowa wiedza, bez której nie można w żaden sposób osiągnąć stanu niebiańskiego spokoju. Wiedza ta nie jest nikomu dana automatycznie i można ją posiąść wyłącznie poprzez obserwację skutków działań swoich i innych ludzi, przy czym przyjmuje się tutaj, że dowodem na działanie zgodne z niebiańskim porządkiem, jest fakt, że w długiej perspektywie czasowej owo działanie przyniosło pozytywny skutek. Szczególne znaczenie ma tu zwłaszcza studiowanie efektów działań wcześniejszych pokoleń, aby uniknąć powielania ich błędów, a za to powtarzać jak najczęściej działania, które przyniosły pozytywne skutki.
Im więcej tego rodzaju wiedzy się zgromadzi i im bardziej będzie ona szczegółowa, tym większa szansa osiągnięcia niebiańskiego spokoju przez aktualnie żyjące pokolenie ludzi. Raz ustalona zgodność danego działania z niebiańskim porządkiem staje się swojego rodzaju świętością, którą należy w przyszłości jak najdokładniej powielać. Stąd, tradycja w konfucjanizmie ma tendencje do przeradzania się w sztywny rytuał.
Podejście konfucjanizmu do tak rozumianej tradycji bywa szokująco szczegółowe dla ludzi Zachodu. Obejmuje ono nie tylko podstawowe zasady etyczne czy wiedzę socjotechniczną, ale także określa możliwie jak najbardziej precyzyjnie jak należy się zachowywać w danej sytuacji – łącznie z określonymi gestami, sposobem ubioru, a nawet mimiką. Przypomina to trochę rodzaj sztywnej dworskiej etykiety, tylko obejmującej całe społeczeństwo. Im bardziej precyzyjna i im bardziej wszechobejmująca etykieta, tym bardziej zbliżamy się do stanu niebiańskiego spokoju.
Te zbiory precyzyjnych norm zachowań stanowią w konfucjanizmie najbardziej istotną część wiedzy praktycznej, wobec której zwykła wiedza techniczna czy ekonomiczna ma znaczenie drugorzędne.
Do postępowania zgodnego z niebiańskim porządkiem jest jednak potrzebna nie tylko jak najbardziej dogłębna jego znajomość, lecz także silna wola, aby postępować zgodnie ze swoją wiedzą niezależnie od okoliczności. Taka niezłomna wola jest podstawową cnotą konfucjańską i przyjmuje się, że podstawowym celem wychowania w rodzinie jest właśnie zaszczepienie tej woli ludziom. Konfucjusz przyjmował, że nawet jeśli rodzice posiadają tylko szczątkową wiedzę na temat niebiańskiego planu – zawsze mogą zaszczepić przynajmniej ogólną cnotę swoim dzieciom, poprzez nauczenie dobrowolnego podporządkowania się ich nakazom i to, paradoksalnie, nawet wtedy, gdy te nakazy nie są do końca zgodne z niebiańskim porządkiem. Ten rodzaj myślenia – podporządkowywanie się autorytetowi w celu kultywowania cnoty – nawet wtedy, gdy być może nie ma on racji – jest bardzo charakterystyczne dla całego systemu etycznego konfucjanizmu. Z drugiej strony cnota konfucjańska w przypadku osób mających władzę i autorytet wyraża się w pojęciu, które tłumaczy się często jako „humanizm”. Humanizm władcy to umiejętność ograniczania swojej władzy, tak aby jego decyzje nie krzywdziły poddanych jego władzy osób.
Z tego rodzaju koncepcji wywodzi się zaproponowana przez Konfucjusza struktura idealnego państwa, w pełni zgodnego w jego mniemaniu z niebiańskim porządkiem. Jakkolwiek na przestrzeni dziejów koncepcja ta ewoluowała, jej podstawowe zręby są do dzisiaj obecne w sposobie myślenia np. współczesnych chińskich komunistów.
Wyraźnym przejawem takiego rozumienia funkcji państwa są relacje pomiędzy władzą a podmiotami gospodarczymi w wielu krajach Azji Wschodniej, także tych o kapitalistycznym ustroju. Administracja kieruje do takich podmiotów prośby, których niewykonanie nie grozi żadnymi sankcjami. Jednak przedsiębiorcy dostosowują się do wytycznych władzy nie chcąc utracić twarzy i zdając sobie sprawę z tego, że odmową naraziliby się na brak życzliwości i utrudnienia ze strony administracji[2].
Za podstawę struktury idealnego państwa Konfucjusz uważał tradycyjną chińską rodzinę – składającą się z głowy rodu i następnie hierarchicznie podporządkowanych mu potomków. W rodzinie takiej głowa rodu ma absolutną władzę, ale jednocześnie powinna to też być osoba kierująca się zawsze cnotą i posiadająca jak największą wiedzę na temat tej części niebiańskiego porządku, jaka dotyczy jego rodziny. Członkowie rodziny powinni się podporządkowywać woli jej głowy, nawet jeśli sądzą, że ona błądzi i niewłaściwie interpretuje zasady niebiańskiego porządku. Nawet, gdy rzeczywiście tak jest – brak podporządkowania się i tak będzie sprzeczny z niebiańskim porządkiem, gdyż porządek ten wymaga zawsze podporządkowywania się autorytetowi. Oczywiste jest jednak, że głowa rodziny posiadająca cnotę i wiedzę będzie zawsze tak postępowała, aby stosunki wewnątrz rodziny rozwijały się możliwie jak najbardziej harmonijnie ku ogólnemu dobru wszystkich jej członków, czyli kierowała się zasadą humanizmu. W przypadku gdy tak nie jest – rodzina się wcześniej czy później rozpadnie, a po jakimś czasie powstanie nowa – na czele której będzie stał ktoś cnotliwy.
Struktury państwa powinny być skonstruowane na takich samych zasadach jak rodzina – a zatem na czele takiego państwa powinna stać światła i cnotliwa osoba. Taka osoba powinna mieć absolutną władzę, ale jej wiedza o niebiańskim porządku i cnota powinny gwarantować, że władza ta nie zostanie nadużyta. Wszystkie rodziny mają obowiązek podporządkowywać się tej władzy – ponownie nawet wtedy, gdy się z nią nie zgadzają, gdyż ew. bunt – ewidentnie sprzeczny z zasadami niebiańskiego porządku i tak nie przybliży ich do stanu niebiańskiego spokoju, pogłębiając tylko chaos powodowany złymi rządami. Z drugiej jednak strony – władca, który nie ma cnoty i wiedzy – jeśli stworzy takie struktury państwa, które gwałcą i ignorują potrzeby większości rodzin, doprowadzi wcześniej czy później swoje państwo do upadku. Stąd dobrze zorganizowane państwo, spełniające zasady niebiańskiego porządku można poznać po tym, że nie zmusza ono głów rodzin do podejmowania działań stojących w ostrym konflikcie z interesem tych rodzin. Tak więc – obowiązkiem rodzin jest pełne posłuszeństwo, ale obowiązkiem dobrego władcy jest takie urządzenie państwa, w którym rodziny nie są zmuszane do działań stojących w sprzeczności z ich interesem.
Rytuał, który w całym systemie konfucjańskim odgrywa tak istotną rolę, zyskuje szczególnie duże znaczenie w rządzeniu państwem. Nauczanie i jak najwierniejsze kultywowanie rytuału jest według tego systemu najbardziej efektywnym sposobem rządzenia. Rytuał ten powinien być szczególnie kultywowany na dworze władcy, który stanowi wzorzec właściwej organizacji dla wszystkich poddanych. Z drugiej strony wciągnięcie w rytuał jak największej rzeszy ludzi daje im poczucie przynależności do państwa i satysfakcję z zajmowania w nim ściśle określonego miejsca.
Dla państw konfucjańskich charakterystyczne było budowanie specjalnych pałaców paradnych, które nie posiadały ścian i stały na podwyższeniu w centralnym punkcie ogromnych placów, na którym jak najczęściej odbywano specjalne ceremonie, w których każdy szczegół – od stroju władcy po miejsce gdzie powinna stać sprzątaczka były możliwie jak najściślej ustalone i niezmienne, tworząc rodzaj skomplikowanej liturgii, tworzącej choć na chwilę niebiański spokój na ziemi. Największym tego rodzaju miejscem na świecie – mogącym pomieścić naraz ok. miliona osób jest słynny Plac Niebiańskiego Spokoju (Tian’anmen) w Pekinie.
Ideałem konfucjańskim było stworzenie takiego państwa, w którym rządziłby sam, czysty rytuał, działający zupełnie samorzutnie i nie wymagający już zupełnie „ręcznych” ingerencji władcy. W praktyce jednak takie podejście do rządzenia państwem powodowało, że władca był wcześniej czy później pozbawiany realnej władzy, którą faktycznie przejmowała grupa konserwatywnie nastawionych urzędników państwowych stojących formalnie na straży przestrzegania rytuału, a w rzeczywistości wykorzystujących ją do osiągania własnych, osobistych celów.
Państwowy rytuał konfucjański, mimo że bywał sprawnym narzędziem władzy, stanowił też poważną przeszkodę w dokonywaniu jakichkolwiek zmian struktury społecznej, co w rezultacie doprowadzało, na skutek niemożności adaptowania się do zmian zachodzących w zewnętrznym świecie, do stopniowego upadku kolejnych „edycji” tych państw.
W niemal wszystkich państwach zorganizowanych na ideach konfucjańskich stale występowała niechęć do działań zbrojnych. Działania te, z natury rzeczy są bowiem zalążkiem chaosu i jako takie zawsze burzą niebiański spokój. We wszystkich państwach konfucjańskich uważano więc, że sprawy związane z wojskowością stanowią zło konieczne. Żołnierze w tych krajach stanowili zawsze najniższą grupę społeczną, zaś kierowanie do służby wojskowej było traktowane jak wielkie nieszczęście. Większość elit urzędniczych trzymała się od spraw wojskowości z dala, zaś wyznaczenie wyższego urzędnika na dowódcę wojsk było traktowane jak rodzaj kary.
Państwa konfucjańskie starały się zastępować jak najczęściej działania zbrojne działaniami dyplomatycznymi – starając się albo zjednać sobie wrogów, albo ich ze sobą skłócić. Z działań zbrojnych z kolei preferowano budowanie i bronienie fortyfikacji oraz wystawianie olbrzymich, ale słabo uzbrojonych i wyszkolonych armii obronnych, które same nie były zdolne do kontrataku i zwykle stawały się bezbronne w obliczu najeźdźców, zwłaszcza gdy ci stosowali nowe, nieznane wcześniej uzbrojenie i techniki walki.
Konfucjusz (551–479 p.n.e.)
Pierwsi uczniowie
Mencjusz (ok. 372 p.n.e. – ok. 289 p.n.e.) – odłam idealistyczny
Gaozi – odłam neutralistyczny – znany z debaty z Mencjuszem
Xunzi (ok. 298 p.n.e. – ok. 238 p.n.e.) – odłam realistyczny
„Sześcioksiąg” urzędową nauką państwową.
Szkoła nowych tekstów
Szkoła starych tekstów
Krytycyzm – w zasadzie prąd mało konfucjański
Neokonfucjaniści uważali za doskonałe teksty „Czteroksięgu” (Sishu):
Proto-neokonfucjaniści – teoretycy Drogi:
Pięciu mędrców neokonfucjańskich z XI wieku – Kosmologowie:
Rewizjoniści
Szkoła badania zasad (lixue) (nazwana przez tłumacza dzieł Feng Youlana, Derka Bodde’a: szkołą idei platońskich) – nawiązująca do interpretacji Chenga Yi:
Szkoła badania umysłu (lub: ... serca) (xinxue) – nawiązująca do interpretacji Chenga Hao:
Szkoła „jednego neokonfucjanizmu”
Szkoła badania [tekstów z czasów] Hanów (hanxue)
Konfucjaniści przełomu XIX i XX wieku (szkoła nowych tekstów)
Neo-neokonfucjaniści
Post-neokonfucjaniści (XX – XXI wiek)
Neokonfucjaniści wietnamscy – zob. też Filozofia wietnamska
Konfucjaniści koreańscy – zob. też Filozofia koreańska
Konfucjaniści japońscy – zob. też Filozofia japońska
Konfucjaniści z Bostonu (Amerykanie)
 Artykuły na Routledge Encyclopedia of Philosophy (ang.), rep.routledge.com [dostęp 2023-05-12]:
Społeczeństwo – podstawowe pojęcie socjologiczne, jednakże niejednoznacznie definiowane. Terminem tym tradycyjnie ujmuje się dużą zbiorowość społeczną, zamieszkującą dane terytorium, posiadające wspólną kulturę, wspólną tożsamość oraz sieć wzajemnych stosunków społecznych. Społeczeństwo ponadto posiada własne instytucje pozwalające mu na funkcjonowanie oraz formę organizacyjną w postaci państwa, narodu lub plemienia.
Termin ten w mowie potocznej często stosuje się dość swobodnie określając różne kategorie czy warstwy społeczne np. „społeczeństwo górników”, „społeczeństwo nauczycieli”. Używa się go też na określenie całej żyjącej ludzkości, wówczas używa się określenia społeczeństwo globalne.
Ze względu na duże różnice pomiędzy współczesnymi i historycznymi społeczeństwami wyróżnia się ze względu na stopień rozwoju społeczeństwa:
W przypadku społeczeństw tradycyjnych, terminem tym określa się formy społeczeństw przedindustrialnych i wyróżnia się różne formy ich rozwoju:
Karol Marks wyróżniał dla tego okresu trzy następujące po sobie formacje społeczne: formację wspólnoty pierwotnej, formację niewolniczą (formacja antyczna i azjatycka) oraz formację feudalną.
W okresie formowania się społeczeństw przemysłowych powstawały takie jego określenia jak:
W przypadku społeczeństwa poprzemysłowego, które powstawać zaczęło w drugiej połowie XX wieku, używa się też alternatywnych pojęć: 
W literaturze pojawia się również określenie społeczeństwa zwierzęcego na odróżnienie uspołecznionych zbiorowości ludzkich od nieuspołecznionych zbiorowości tworzonych przez inne gatunki w ujęciu biologicznym.
Istnieją trzy sposoby pojmowania stosunków techniki ze społeczeństwem. Pierwszy sposób polega na ukazaniu życia społecznego jako odbicia techniki lub nawet mieszania mechanizmów instrumentalnych i ludzkości. Drugi sposób jest rozwiązaniem idealizmu lub spirytualizmu, czyli totalnego oderwania ducha od materii. Technika jawi się jako byt autonomiczny i złowróżbny lub jako zwykła siła w służbie człowieka. Trzeci sposób polega na uznaniu stopnia zależności lub obojętności świata ludzkiego względem techniki. Rozwiązanie to otwiera drogę do nauk społecznych. Technika jawi się tutaj jako sposób bycia, tryb istnienia[1].
Demokracja (gr. dḗmos „lud”, krátos „władza” – dosł. „rządy ludu, ludowładztwo”[1]) – jeden z typów ustroju państwa, zakładający udział obywateli w sprawowaniu władzy.
Demokracja ma swój początek w starożytnej Grecji[2]. Znaczący wkład w rozwój demokracji miał także starożytny Rzym[3] oraz kultura Zachodu (Europa[3], Ameryka Północna i Południowa[4]).
Istnieje wiele wariantów demokracji. Podstawowe rozróżnienie dotyczy formy, w jakiej obywatele wykonują swoją wolę. W demokracji bezpośredniej obywatele bezpośrednio i aktywnie uczestniczą w podejmowaniu decyzji politycznych, natomiast w demokracji pośredniej (przedstawicielskiej) władza polityczna sprawowana jest pośrednio poprzez wybieranych przedstawicieli. Demokracja przedstawicielska wywodzi się w znacznym stopniu z idei i instytucji, które rozwijały się w Europie w średniowieczu, oświeceniu oraz podczas rewolucji amerykańskiej i francuskiej[5].
Obecnie powszechną formą ustroju demokratycznego jest demokracja parlamentarna. Gwarantem istnienia demokracji parlamentarnej jest konstytucja, najczęściej w formie pisanej.
Według minimalistycznej teorii demokracji (zakładającej wyłącznie typ przedstawicielski, bez elementów udziału bezpośredniego obywateli) demokracja to system rządów (reżim polityczny, ustrój polityczny) i forma sprawowania władzy, w których źródło władzy stanowi wola większości obywateli[6]. Według tej definicji jedyną rolą obywateli jest ustanowienie władzy w drodze wyborów.
Nie ma ogólnie przyjętej definicji słowa demokracja[7]. Znaczenie tego pojęcia uległo zmianie od starożytności. Również współcześnie istnieje wiele form ustrojów uznawanych za demokratyczne. Słowo demokracja używane jest obecnie nie tylko do określenia formy państwa, lecz również do metod sprawowania władzy czy sposobów podejmowania decyzji.
Pojęcie demokracji pochodzi z V w. p.n.e.[8] Starogreckie słowo δημοκρατία (demokratia) powstało ze słów δῆμος (demos), oznaczającego lud, dzielący coś wspólnie[9], rozumianą ówcześnie jako ogół obywateli jednostki osiedleńczej oraz κρατέω (krateo), czyli „rządzę”. Ograniczone było do osób posiadających pełnię praw obywatelskich, stałych mieszkańców gmin danego miasta-państwa (demos to także gmina)[10]. Wyłanianie władz, bez uprzedniego precyzyjnego wskazania i tym samym ograniczenia bazy wyborczej, sytuacja, gdy sama tylko liczebność głosujących, przy braku także innych kryteriów, kierowała decyzjami organizacji lokalnej czy państwowej, językiem starożytnych, stanowiłyby wynaturzenie demokracji, ochlokrację[11].
W starożytności jako demokrację oznaczano ustrój polityczny istniejący w niektórych greckich polis, w szczególności w Atenach, przeciwstawiany monarchii (rządom jednej osoby) czy oligarchii (rządom nielicznych). Ta opozycja nie jest już obecnie tak wyraźna. Współczesne rządy demokratyczne zawierają zarówno elementy oligarchii, jak i monarchii. Karl Popper definiował demokrację w kontraście do dyktatury czy tyranii. Głównym kryterium rozróżniającym było to, czy lud może kontrolować władzę bez konieczności przeprowadzania rewolucji[12].
Często pod pojęciem demokracji rozumie się demokrację liberalną, która jest jedną z odmian demokracji przedstawicielskiej. Zawiera ona takie elementy jak: pluralizm polityczny, równość wobec prawa, prawa obywatelskie, prawa człowieka oraz społeczeństwo obywatelskie.
Demokracja jest też często utożsamiana z republikańską formą rządu, chociaż w niektórych językach termin republika obejmuje lub obejmował zarówno demokrację, jak i arystokrację[13][14].
Czasami demokracja i republika są sobie przeciwstawiane. W pismach Jamesa Madisona republiką nazywa on demokrację pośrednią, nazwę demokracji rezerwując dla demokracji bezpośredniej. W „Federaliście nr 10” wskazywał on na wyższość republik nad demokracjami[15].
Współcześnie termin republika ma wiele znaczeń. Najczęściej odnosi się do demokracji przedstawicielskiej z możliwością wyboru głowy państwa takiej jak prezydent, w odróżnieniu do państw z dziedzicznym monarchą, gdzie szef rządu również wybierany jest na czas określony[16].
Ustrój demokratyczny podlega również krytyce ze strony republikanizmu. Już od starożytności zwolennicy tego nurtu filozoficznego wskazują, że demokracja może być także formą tyranii, jeśli jest traktowana jedynie jako rządy większości (tyrania większości). Większość może wtedy zadecydować o naruszeniu praw mniejszości. Republikanie wskazują więc, że demokratyczna władza musi funkcjonować w ramach prawnych. To rządy prawa, a nie rządy większości powinny być naczelną zasadą ustrojową[17].
Nie ma ogólnie przyjętych kryteriów uznawania dane państwo za demokrację. Istnieją też duże różnice pomiędzy poszczególnymi formami demokracji. Np. kluczowym warunkiem dla idealnej formy demokracji przedstawicielskiej (pośredniej) są wolne i uczciwe wybory, to znaczy dostępne w równym stopniu[18] dla każdego obywatela i przeprowadzane według zrozumiałych reguł[19]. Co więcej, wolność poglądów, wolność słowa i wolność prasy są postrzegane jako podstawowe prawa umożliwiające obywatelom głosowanie zgodnie z własnymi przekonaniami[20][21].
Szczególnie duże różnice zachodzą między starożytnymi a współczesnymi formami demokracji.
Demokracja może być stopniowana – tzn. w danym ustroju politycznym faktyczny udział we władzy ogółu obywateli może być większy albo mniejszy.
Według współczesnych kryteriów, demokracja opiera się na[22]:
Niektórzy autorzy uważają, że podstawową cechą demokracji jest zdolność jednostki do pełnego i nieskrępowanego uczestnictwa w życiu swojej społeczności[23]. Biorąc pod uwagę znaczenie umowy społecznej i woli powszechnej dla demokracji, może być ona charakteryzowana jako forma politycznego kolektywizmu, w której wszyscy uprawnieni obywatele mają równy głos w podejmowaniu decyzji, które wpływają na ich życie[24].
Określenie niektórych cech demokratycznego państwa zawiera Międzynarodowy pakt praw obywatelskich i politycznych[25] – traktat uchwalony w wyniku konferencji ONZ w Nowym Jorku, na mocy rezolucji Zgromadzenia Ogólnego nr 2200A (XXI) z 16 grudnia 1966 roku; ratyfikowany przez Polskę w 1977 roku (publikacja w Dz.U. z 1977 r. Nr 38, poz. 167[26]). Jego artykuł 25. stanowi, że:
Każdy obywatel ma prawo i możliwości, bez żadnej dyskryminacji, o której mowa w artykule 2, i bez nieuzasadnionych ograniczeń: a) uczestniczenia w kierowaniu sprawami publicznymi bezpośrednio lub za pośrednictwem swobodnie wybranych przedstawicieli; b) korzystania z czynnego i biernego prawa wyborczego w rzetelnych wyborach, przeprowadzanych okresowo, opartych na głosowaniu powszechnym, równym i tajnym, gwarantujących wyborcom swobodne wyrażenie woli; c) dostępu do służby publicznej w swoim kraju na ogólnych zasadach równości.
Wskaźnik demokracji to opracowany w 2006 r. przez Economist Intelligence Unit, indeks mający opisywać stan demokracji w 167 krajach świata[27].
Wskaźnik demokracji opiera się na 60 wskaźnikach pogrupowanych w pięciu różnych kategoriach: proces wyborczy i pluralizm, swobody obywatelskie, funkcjonowanie administracji publicznej, partycypacja polityczna oraz kultura polityczna.
Na podstawie indeksu państwa zostały podzielone na „demokracje pełne”, „demokracje wadliwe”, „systemy hybrydowe” oraz na „systemy autorytarne”.
Przy zastosowaniu wskaźnika demokracji, następujące kraje zostały sklasyfikowane w 2011 jako demokracje pełne[28]:
Jako wadliwe demokracje określa się następujące kraje: Argentyna, Benin, Botswana, Brazylia, Bułgaria, Chile, Chorwacja, Cypr, Czarnogóra, Dominikana, Estonia, Filipiny, Francja, Ghana, Grecja, Gujana, Indie, Indonezja, Izrael, Jamajka, Kolumbia, Lesotho, Litwa, Łotwa, Macedonia Północna, Malezja, Mali, Meksyk, Mołdawia, Mongolia, Namibia, Panama, Papua-Nowa Gwinea, Paragwaj, Peru, Polska, Portugalia, Południowa Afryka, Republika Zielonego Przylądka, Rumunia, Salwador, Serbia, Słowacja, Słowenia, Sri Lanka, Surinam, Tajlandia, Tajwan, Timor Wschodni, Trynidad i Tobago, Węgry, Włochy, Zambia[28].
Termin „demokracja” wywodzi się ze starożytnej myśli politycznej i filozoficznej i pojawił się po raz pierwszy w Atenach[2][3]. Pierwsza demokracja powstała właśnie w Atenach pod przywództwem Klejstenesa w latach 508–507 roku p.n.e. Stąd też Klejstenesa nazywa się niekiedy „ojcem demokracji ateńskiej”[29].
Demokracja ateńska miała formę demokracji bezpośredniej, o dwóch cechach charakterystycznych: losowym wyborze obywateli na niektóre urzędy administracyjne i sądowe, oraz Zgromadzenie Ludowe (Eklezję) o uprawnieniach prawodawczych, składające się z wszystkich obywateli[30]. Z obywatelstwa wykluczone były kobiety, niewolnicy, cudzoziemcy (μέτοικοι, metojkowie) i chłopcy poniżej 18 lat[31].
Szacuje się, że na ok. 200 000 do 400 000 mieszkańców Aten, obywatelami było ok. 30 000-60 000 (według danych z Polityki Arystotelesa). Wykluczenie dużej części populacji z obywatelstwa było ściśle związane ze starożytnym rozumieniem tego pojęcia. W starożytności przywileje związane z obywatelstwem związane były z obowiązkiem uczestnictwa w kampaniach wojennych (obywatel odpowiadał na decyzję władcy o wojnie i wstępował do armii, dzięki czemu mógł się stać obywatelem). Nie w każdym polis jednak wojownik po wygranej bitwie stawał się obywatelem. Na przykład w Sparcie pełnoprawnymi obywatelami byli spartiaci, którzy oprócz walki musieli pochodzić z wpływowych rodzin i znać prawo polis.
Demokracja ateńska była bezpośrednia, ponieważ decyzje były tam podejmowane przez wszystkich obywateli na Zgromadzeniu Ludowym, proces polityczny był pod ciągłym ich nadzorem, i wielu obywateli było stale zaangażowanych w publiczną działalność polis[32].
Obok wyborów losami panowała też zasada ostracyzmu (Po reformie Klejstenesa), pozwalająca obywatelom na czasowe odwołanie urzędnika, jednak decydowało o tym zgromadzenie. Decyzja zapadała, gdy większość zagłosowała przeciwko urzędnikowi. Wówczas sądzono go oraz wyznaczano karę (najczęściej opłatę).
Do rozwoju demokracji przyczyniła się także Republika rzymska, choć tylko mniejszość jej mieszkańców była obywatelami z prawem do głosowania i wybierania reprezentantów. Dzięki możliwościom manipulowania okręgami wyborczymi, większość wysokich urzędników, w tym członkowie Senatu, pochodziła z kilku potężnych i zamożnych rodów[33].
W średniowieczu i nowożytności istniało wiele instytucji w których przeprowadzano wybory czy odbywały się zgromadzenia. Często jednak tylko niewielka liczba ludności mogła brać w nich udział. Ustroje z elementami demokracji miały: Rzeczpospolita Obojga Narodów (10% ludności), Islandia (Althing), Wyspy Owcze (Løgting), niektóre średniowieczne włoskie miasta-państwa, takie jak Wenecja, wczesnośredniowieczna Irlandia (túath – małe królestwa), Republika Nowogrodzka i Pskowska w średniowiecznej Rosji (wiece). Demokratyczny charakter miały także skandynawskie Tingi, czy stany w Tyrolu i Szwajcarii. Udział w tych instytucjach był często ograniczany do mniejszości, więc można je również sklasyfikować jako oligarchie. Większość regionów w średniowiecznej Europie była rządzona przez duchowieństwo albo panów feudalnych.
Poza Europą demokratyczny charakter miał wybór Gopala w Bengalu, regionie subkontynentu indyjskiego (w obrębie systemu kastowego) czy rządy autonomicznego miasta kupieckiego Sakai w XVI-wiecznej Japonii.
Karta z Kurukan Fuga podzieliła Imperium Mali na władające klany reprezentowane na wielkim zgromadzeniu, zwanym Gbara. Ustrój Mali był jednak bliższy monarchii konstytucyjnej niż republiki demokratycznej. Nieco bliższa współczesnej demokracji była kozacka republika na Ukrainie w XVII i XVIII w.: Hetmanat. Jej przywódca, Hetman, był obieralny przez reprezentantów poszczególnych regionów.
Powstanie parlamentu angielskiego wiązało się z ograniczeniem władzy królewskiej zapisanym w Wielkiej Karcie Swobód. Karta potwierdzała ochronę pewnych praw królewskich podwładnych, niezależnie od ich statusu. Zakładała również uprawnienia, wyrażone dosłownie w Habeas Corpus Act chroniącym jednostki przed bezprawnym uwięzieniem i dającym im prawo do apelacji. Pierwszym pochodzącym z wyboru parlamentem w Anglii był Parlament de Montforta (1265 r.).
Tylko nieznaczna część społeczeństwa miała faktyczny głos. Parlament był wybierany przez kilka procent ludności (mniej niż 3% aż do 1780 roku[34]). Monarcha miał prawo do zwoływania parlamentu na swoje życzenie, z reguły, gdy potrzebował środków finansowych.
Siła parlamentu wzrastała stopniowo przez wieki. W 1689 ustanowiono Deklaracje praw, która skodyfikowała najważniejsze prawa i doprowadziła do wzrostu znaczenia parlamentu. Prawo wyborcze powoli zyskiwało na sile w związku z czym parlament uzyskiwał coraz większą władzę, aż w końcu monarcha zaczął pełnić tylko rolę reprezentacyjną[35]. Prawo wyborcze uczyniono bardziej jednolitym. Ustawą Reform Act 1832 zlikwidowano tzw. zgniłe miasteczka, które dawały nielicznym mieszkańcom duży wpływ na wybór członków parlamentu.
Angielscy purytanie, którzy począwszy od 1620 roku emigrowali do Nowego Świata, zakładali kolonie w Nowej Anglii. Wprowadzali tam rządy demokratyczne, przyczyniając się do rozwoju demokracji w Stanach Zjednoczonych[36].
Po raz pierwszy w historii nowożytnej demokratyczną konstytucję przyjął krótko istniejący rząd Republiki Korsykańskiej w 1755. Konstytucja Korsyki jako pierwsza opierała się na zasadach oświeceniowych. Wprowadziła ona również prawo wyborcze dla kobiet, które w innych demokracjach ustanowiono dopiero w XX wieku.
W 1789 roku, rewolucyjna Francja przyjęła Deklarację Praw Człowieka i Obywatela. Powstał również Konwent Narodowy, wybrany w wyborach w 1792 roku. Uprawnionymi do głosowania byli wszyscy mężczyźni[37].
Powszechne prawo wyborcze dla mężczyzn zostało ostatecznie ustanowione we Francji w marcu 1848 roku w wyniku rewolucji lutowej 1848 roku[38]. W 1848 roku miała też miejsce Wiosna Ludów. W licznych krajach lud zaczął się domagać m.in. ustanowienia liberalnych konstytucji i bardziej demokratycznych rządów[39].
Mimo że Stany Zjednoczone nie zostały określone przez ojców założycieli jako demokracja, to nowe państwo opierało się na demokratycznych zasadach, takich jak naturalna wolność i równość (obejmująca białych mężczyzn)[40].
Konstytucja Stanów Zjednoczonych została przyjęta w 1788 roku, zapewniając obieralny rząd i chroniąc prawa i wolności obywatelskie.
W epoce kolonialnej, przed rokiem 1776 oraz jakiś czas później, prawo do głosowania mieli tylko dorośli biali mężczyźni, którzy byli właścicielami nieruchomości. Czarnoskórzy, w większości niewolnicy i większość kobiet nie byli objęci prawem wyborczym. Demokracja była szersza na Dzikim Zachodzie, gdzie struktury społeczne, ekonomiczne i polityczne były luźniejsze[41]. W stanach południowych niewolnictwo było podstawową instytucją ekonomiczną i gospodarczą podtrzymującą porządek społeczny, co utrudniało demokratyzację. Wiele organizacji popierało migracje czarnych osób do innych stanów, gdzie mogli cieszyć się większą wolnością oraz równością.
Spis ludności Stanów Zjednoczonych z 1860 roku wykazał, że populacja niewolników wzrosła do czterech milionów[42]. Odbudowa Stanów Zjednoczonych po wojnie secesyjnej (1861–1865) spowodowała przyznanie nowo uwolnionym niewolnikom (tylko mężczyznom) obywatelstwa wraz z ograniczonym prawem do głosowania.
Pełne nadanie praw wyborczych obywatelom nastąpiło dopiero na skutek działań ruchu na rzecz praw obywatelskich (1955-1968). Działania te przyczyniły się do uchwalenia przez Kongres Stanów Zjednoczonych Voting Rights Act of 1965[44][45]. W XX wieku wystąpiło zjawisko „fal demokracji”, czyli nasilających się w pewnych okresach procesach demokratyzacji. Na ich występowanie miało wpływ wiele różnych czynników takich jak: wojny, rewolucje, dekolonizacja czy też zmiany religijne lub ekonomiczne. Wskutek I wojny światowej oraz rozpadu Imperium Osmańskiego i Austro-Węgier w Europie powstały nowe państwa narodowe. Większość z nich, przynajmniej z nazwy, miała być demokratyczna.
Po okresie rozwoju demokracji w latach dwudziestych XX wieku, nadeszły czasy wielkiego kryzysu, w których większość krajów Europy, Ameryki Łacińskiej i Azji, zwróciła się ku dyktaturze. Szczególnym przypadkiem były ustroje totalitarne, które powstały w nazistowskich Niemczech i Związku Radzieckim[46].
II wojna światowa przyniosła ostatecznie odwrócenie tej tendencji w Europie Zachodniej. Demokratyzacja amerykańskich, brytyjskich, i francuskich sektorów okupowanych Niemiec, Austrii, Włoch i Japonii służyła później jako wzór dla późniejszych zmian systemowych.
Jednak większość Europy Wschodniej, łącznie z radziecką strefą okupacyjną w Niemczech stała się częścią bloku wschodniego. Państwa te formalnie były demokratyczne (najczęściej określały się jako demokracje ludowe), jednak według standardów demokracji liberalnych, były to jedynie demokracje pozorne.
Po wojnie nastąpiła też dekolonizacja i większość nowo powstałych niepodległych państw posiadała demokratyczne konstytucje. Największa istniejąca po dziś dzień demokracja wyłoniła się w Indiach[47].
W 1960 roku zdecydowana większość państw na świecie była formalnie demokratyczna, choć w wielu przypadkach była to demokracja pozorna (zwłaszcza w państwach socjalistycznych i byłych koloniach).
W kolejnej fali demokratyzacji, zmiana ustroju zaszła w licznych dyktaturach. Portugalia (1974), Hiszpania (1975) i kilka z dyktatur wojskowych w Ameryce Południowej w późnych latach siedemdziesiątych i wczesnych latach osiemdziesiątych powróciły do rządów cywilnych. Argentyna powróciła do nich w 1983 roku, Boliwia, Urugwaj w 1984 roku, Brazylia w 1985 roku, a Chile na początku lat dziewięćdziesiątych. Kraje Południowo-Wschodniej Azji podążały tą ścieżką w drugiej połowie lat osiemdziesiątych.
Gospodarcza recesja lat osiemdziesiątych XX w. wraz z osłabieniem ucisku ze strony Związku Radzieckiego, przyczyniły się do tzw. jesieni ludów i rozpadu ZSRR. Wiązały się z tym koniec zimnej wojny, demokratyzacja i liberalizacja dawnych krajów bloku wschodniego. Największy sukces spośród młodych demokracji odniosły te znajdujące się kulturowo i geograficznie najbliżej Zachodniej Europy. Obecnie to one są członkami lub kandydatami Unii Europejskiej. Niektórzy badacze uważają, że współczesna Rosja nie jest prawdziwą demokracją, lecz przybiera formę dyktatury[48].
W latach 90. XX w. fala ruchów wolnościowych rozlała się wśród narodów Południowej Afryki. Przykładem ostatnich prób demokratyzacji na innych kontynentach są upadek rządów Suharto w Indonezji (1998), rewolucja buldożerów w Federalnej Republice Jugosławii (2000), rewolucja róż w Gruzji (2003), pomarańczowa rewolucja na Ukrainie (2004-2005), cedrowa rewolucja w Libanie (2005), tulipanowa rewolucja w Kirgistanie (2005), oraz jaśminowa rewolucja w Tunezji (2010-2011). Ta ostatnia, jako arabska wiosna, rozprzestrzeniła się na inne kraje Bliskiego Wschodu i Afryki Północnej, takie jak Egipt czy Syria.
Według Freedom House w 2007 roku istniały 123 demokracje elektoralne (podczas gdy w 1972 było ich 40)[49]. Według World Forum on Democracy, demokracje elektoralne to obecnie 120 na 192 istniejących państw. Stanowią one tym samym 58,2% populacji świata. Jednocześnie podaje się, że liczba liberalnych demokracji to 85, czyli 38% populacji globalnej. Za liberalne demokracje Freedom House uznaje państwa, które są wolne oraz w których przestrzega się podstawowych praw człowieka oraz rządów prawa[50].
W 2010 roku ONZ ustanowiła 15 września Międzynarodowym Dniem Demokracji[51].
Demokracja może przybierać wiele form. Niektóre z form demokracji zapewniają lepszą reprezentację i więcej wolności dla obywateli niż inne[52][53]. Jeżeli demokracja nie posiada mechanizmów zabezpieczających, to jedna z gałęzi władzy może zdominować pozostałe, niszcząc w konsekwencji demokrację[54][55][56].
Opisane dalej formy demokracji politycznej nie wykluczają się nawzajem. Wiele cech lub aspektów demokracji nie istnieje zależnie od siebie nawzajem i mogą one występować razem w danych systemach politycznym.
Demokracja bezpośrednia jest systemem politycznym, w którym obywatele uczestniczą bezpośrednio (a nie przez pośredników) w podejmowaniu decyzji. Zwolennicy demokracji bezpośredniej twierdzą, że demokracji nie można sprowadzić do kwestii proceduralnych. Demokracja bezpośrednia daje obywatelom głosującym możliwość:
Te trzy rodzaje instytucji w różnym stopniu istnieją we współczesnych rozwiniętych demokracjach. Można to postrzegać jako przejaw zwracania się ku bezpośrednim demokracjom.
Elementy demokracji bezpośredniej funkcjonują w wielu krajach np. na poziomie lokalnym, często współistniejąc ze zgromadzeniami przedstawicielskimi[24].
Demokracja pośrednia (przedstawicielska) polega na wyborze przedstawicieli przez osoby, które mają być reprezentowane. Jeśli głowa państwa jest wybrana w sposób demokratyczny, wtedy taki ustrój nazywamy republiką demokratyczną[57]. Polega to najczęściej na wyborze kandydata większością względną lub bezwzględną.
Przedstawiciele mogą reprezentować określony okręg wyborczy lub reprezentować cały elektorat dzięki systemom proporcjonalnym. Czasami oba systemy są łączone. Niektóre demokracje przedstawicielskie wprowadzają również elementy demokracji bezpośredniej, takie jak referenda. Cechą charakterystyczną demokracji przedstawicielskiej jest to, że wybrani przez obywateli reprezentanci, zachowują swoją wolność w określaniu tego, jak najlepiej realizować dobro wspólne (tzw. mandat wolny).
Demokracja parlamentarna jest demokracją przedstawicielską, gdzie rząd powołany jest przez przedstawicieli. Jest ona przeciwstawiana systemowi prezydenckiemu. W ramach demokracji parlamentarnej, władzę wykonawczą sprawuje rząd, który podlega ciągłej kontroli ze strony parlamentu wybranego przez wyborców[58][59][60][61][62].
W systemach parlamentarnych premier może zostać odwołany w dowolnym momencie, kiedy parlament uważa, że nie wykonuje on swoich zadań zgodnie z oczekiwaniami władzy ustawodawczej. Odbywa się to poprzez wotum nieufności[63].
Demokracja prezydencka jest systemem, gdzie społeczeństwo wybiera prezydenta, który pełni obowiązki jako głowa państwa i szef rządu i gdzie kontroluje większość władzy wykonawczej. Prezydent pełni swoją funkcję przez określony czas (kadencję). Wybory mają zazwyczaj z góry określoną datę i trudno ją zmienić. Prezydent ma bezpośrednią kontrolę nad gabinetem (rządem), którego członków powołuje[63].
Władza prezydencka pozostaje oddzielona od władzy ustawodawczej i obie władze nie mogą skracać swojej kadencji. Wskutek tego urząd prezydenta oraz władza ustawodawcza mogą się niekiedy znajdować pod kontrolą dwóch różnych partii, blokując wzajemnie swoje działania. Może to być jednym z powodów dla którego system prezydencki nie jest zbyt popularny poza Stanami Zjednoczonymi[63].
Obok systemu prezydenckiego istnieje również system semiprezydencki, w którym władza wykonawcza sprawowana jest przez prezydenta i premiera. Podział kompetencji między premierem i prezydentem jest różny w poszczególnych krajach[63].
Niektóre demokracje łączą w sobie elementy przedstawicielskie z demokracją bezpośrednią. Określane są one jako „demokracje hybrydowe” lub demokracje mieszane[64]. Przykładem takiego systemu jest Szwajcaria i niektóre stany w Stanach Zjednoczonych.
W Szwajcarii centralną władzę ustawodawczą sprawuje Zgromadzenie Federalne. Jednocześnie dozwolone są inicjatywa ustawodawcza i referenda zarówno na szczeblu lokalnym, jak i federalnym.
W Stanach Zjednoczonych nie ma mechanizmów demokracji bezpośredniej na szczeblu federalnym, ale w ponad połowie stanów i w wielu miejscowościach istnieje instytucja inicjatyw obywatelskich (zwane po angielsku ballot initiatives, ballot measures, ballot questions czy propositions). W znaczącej większości stanów istnieją też referenda[65]. Na poziomie lokalnym (szczególnie w Nowej Anglii) zgromadzenia gminne są często stosowane, szczególnie na obszarach wiejskich.
Współcześnie demokratyczny charakter mają również monarchie parlamentarne, np. w Wielkiej Brytanii, Holandii czy krajach skandynawskich. Władza monarchy jest tam znacznie ograniczona, lub wręcz symboliczna, a polityka prowadzona jest przez instytucje o charakterze demokratycznym.
Poszczególne stanowiska polityczne są bardzo zróżnicowane w swoim stosunku do demokracji. Najczęściej są zwolennikami pewnej wersji demokracji, odrzucając inne. Stosunek do demokracji zmieniał się też w czasie, szczególnie pomiędzy XIX a XX wiekiem.
Współczesny liberalizm jest ściśle związany z demokracją. Wiele z idei kluczowych dla liberalizmu jest również ważnym elementem systemu demokratycznego (np. odpowiedzialny rząd, równość szans, równość wobec prawa, państwo prawa)[66]. Demokracja liberalna jest obecnie dominującą formą ustroju demokratycznego.
W początkowym okresie istnienia liberalizmu (pierwsza połowa XIX w.) liberalizm miał jednak charakter antydemokratyczny. Liberałowie byli wtedy rzecznikami interesów burżuazji, przeciwstawiając się demokracji, którą uważali za rządy motłochu, niebezpieczne dla praw jednostek i własności. Zamiast powszechnego prawa wyborczego byli zwolennikami cenzusu majątkowego.
Także niektóre współczesne nurty liberalizmu, takie jak neoliberalizm[potrzebny przypis] czy libertarianizm[67] są bardzo krytyczne wobec demokracji, krytykując ją za nieefektywność gospodarczą i możliwość naruszania praw jednostek (tyrania większości).
XIX wieczny konserwatyzm był przeciwnikiem demokracji, traktując ją jako jedną z oświeceniowych idei związanych z rewolucją francuską. Konserwatyści byli zwolennikami hierarchii i rządów elit.
W XX wieku ten stosunek uległ zmianie. Konserwatyści w większości zaakceptowali instytucje demokratyczne i powszechne prawo wyborcze[68]. Pozostali jednak nieufni wobec demokracji, która może naruszać utrwalony ład społeczny i tradycyjne wartości.
Stanowisko lewicy wobec demokracji jest zróżnicowane. Lewica jest gorącym zwolennikiem demokracji, jednak rozumie ją często w odmienny sposób niż liberałowie.
Socjaldemokraci są zwolennikami demokracji parlamentarnej i w jej ramach starają się przeprowadzić reformy społeczne[69]. Z kolei ortodoksyjnych marksistów cechuje wrogość do demokracji liberalnej. Najczęściej rozumieją oni pod tym pojęciem scentralizowaną demokrację parlamentarną. Uznają oni tego typu ustrój za demokrację fasadową. Marksiści, leniniści i trockiści preferowali demokrację bezpośrednią sprawowaną za pomocą komun zwanych radami albo sowietami. W kapitalizmie widzieli źródło politycznego elitaryzmu. System radziecki miał funkcjonować poprzez demokrację rad, której najniższym szczeblem była demokracja w miejscu pracy. Nie licząc początkowego okresu rewolucji październikowej, instytucje demokratyczne w Związku Radzieckim miały charakter fasadowy.
Anarchiści są podzieleni w kwestii stosunku do demokracji. Główną linią sporu pozostaje to, czy zasada większości jest tyrańska czy też nie. Dla wielu anarchistów jedyną akceptowalną formą demokracji jest demokracja bezpośrednia. Pierre-Joseph Proudhon twierdził, że jedyną akceptowalną formą demokracji bezpośredniej jest ta, w której uznaje się, że decyzje większości nie są wiążące dla mniejszości, nawet wtedy, gdy są jednomyślne[70]. Niektórzy anarchiści tacy jak Murray Bookchin krytykują indywidualistycznych anarchistów za przeciwstawianie się demokracji i uważają, że da się ją jednak pogodzić z ideami anarchistycznymi[71].
Niektórzy anarchokomuniści sprzeciwiają się większościowemu charakterowi demokracji bezpośredniej, czując, że może ona naruszać indywidualną wolność. Byli oni zwolennikami demokracji konsensualnej, zbliżając się do poglądów Proudhona[72]. Anarchiści często przywołują też poglądy Henry’ego Davida Thoreau, który jednak nie identyfikował się z anarchistami, będąc zwolennikiem „lepszego rządu”[73]. Zdaniem Thoreau wskazywał, że nikt nie powinien rządzić, bez zgody rządzonych.
Demokracja od zawsze spotykała się z krytyką i opozycją ze strony zwolenników innych form ustrojowych czy elit. Wprowadzenie rządów demokratycznych w dotąd niedemokratycznych państwach następowało często poprzez rewolucję. Monarchia tradycyjnie była przeciwstawiana demokracji, będąc za przywilejami stanowymi. W niektórych przypadkach osiągano kompromis w postaci rządu mieszanego (np. monarchii parlamentarnej).
Postoświeceniowe ideologie takie jak faszyzm, nazizm czy fundamentalizm sprzeciwiają się demokracji na innych podstawach, uznając demokrację ogólnie za wadliwą i szkodliwą dla pożądanego kursu rozwoju społeczeństwa.
Ekonomiści od czasów Miltona Friedmana poddawali ostrej krytyce efektywność demokracji. Swoje stanowisko opierali na hipotezie irracjonalnego wyborcy. Ekonomiści argumentowali, że osoby uprawnione do głosowania są w wysokim stopniu niedoinformowane w wielu dziedzinach polityki, szczególnie zaś w kwestiach ekonomicznych. Mają też silną skłonność do skupiania się na kilku kwestiach, o których mają większą wiedzę.
XX-wieczni myśliciele włoscy, Vilfredo Pareto oraz Gaetano Mosca, niezależnie od siebie doszli do wniosku, że to, co zwiemy „demokracją”, jest jedynie fasadą, skrywającą rzeczywiste rządy elit. Uznawali oni rządy oligarchii za niezmienną cechę społeczności ludzkich. Masy są apatyczne i podzielone, natomiast elity przedsiębiorcze, aktywne oraz spójne. Demokracja zmienia jedynie jawną opresję w manipulację[74]. W koncepcjach socjalistycznych głoszono iż prawdziwa demokracja w sferze politycznej może się zrealizować tylko w warunkach demokracji społeczno-ekonomicznej.
Platon w swojej Politei ustami Sokratesa wyraził krytykę ustroju demokratycznego: „Demokracja jest niewolącą formą rządu. Wprowadza chaos i nieporządek, a nadto niesprawiedliwą równość”[75]. Platon przedstawił listę ustrojów w kolejności od najlepszych do najgorszych. Demokracja jest ustrojem zdegenerowanym, który w końcu przemieni się w tyranię.
Pojęcie tyranii większości zostało wprowadzone przez Alexisa de Tocqueville’a[76]. Zauważył on, że w demokracji istnieje niebezpieczeństwo opresji wobec mniejszości czy jednostek. Większość może bowiem przegłosować naruszanie wolności czy praw podstawowych.
James Madison w swoich pismach przeciwstawiał demokrację pośrednią (nazywaną przez niego „republiką”), demokracji bezpośredniej (nazywanej „demokracją”). W „Federaliście nr 10”. uzasadniał on, iż republika jest najbardziej pożądaną formą rządu, gdyż „demokracje przedstawiały obraz walk i zamieszek i zawsze stwarzały zagrożenie dla bezpieczeństwa osobistego i praw własności. Zazwyczaj też krótko trwały i koniec ich bywał gwałtowny.”[77]. Przed „tyranią większości” chronić mógł pośredni charakter demokracji.
Demokracja jest też krytykowana za nieoferowanie wystarczającej stabilności politycznej. Wraz z częstymi zmianami rządów, dochodzi do zmian w polityce wewnętrznej i zagranicznej. Nawet jeśli partia polityczna utrzymuje się przy władzy, głośne protesty, kampanie medialne czy ostra krytyka rządzących, mogą doprowadzić do nieoczekiwanej zmiany na politycznej scenie. Częste zmiany polityki odnośnie do kwestii gospodarczych czy imigracyjnych mogą prowadzić do ograniczenia inwestycji i opóźniać wzrost gospodarczy. Z tych względów pojawiają się poglądy, że demokracja jest niepożądana dla krajów rozwijających się, dla których wzrost ekonomiczny i obniżenie stopy ubóstwa jest głównym priorytetem[78].
W demokracjach pośrednich może być nieopłacalne dla rządzących przeprowadzenie uczciwych wyborów. Badania wskazują, że urzędujący, którzy fałszowali lub manipulowali wyborami utrzymują się na stanowisku statystycznie 2,5 raza dłużej, niż ci wybrani w sposób uczciwy[79]. W demokracjach o dochodzie na głowę przekraczającym 2,700$, przemoc polityczna okazała się mniej częsta niż w tych poniżej tej granicy[79]. Te same badania wykazały, że poważne naruszenia wyborów zdarzają się częściej w krajach z małym dochodem na mieszkańca, małą populacją, bogatym w zasoby naturalne i w takich, w których brakuje instytucjonalnych mechanizmów gwarantujących zachowanie równowagi politycznej. Takimi krajami są np. kraje Afryki subsaharyjskiej czy Afganistan[79].
Rządy gdzie często przeprowadzane są wybory mają tendencje do bardziej stabilnej polityki gospodarczej aniżeli rządy gdzie wybory przeprowadzane są rzadziej. Jednakże ten trend nie odnosi się do rządów, gdzie wybory są fałszowane[79].
Niektórzy filozofowie i naukowcy wyszczególnili historyczne i społeczne czynniki, które przysłużyły się ewolucji demokracji. Czynniki społeczne, takie jak protestantyzm, wpłynęły na rozwój demokracji, rządów prawa, praw człowieka i wolności politycznej (praktykowano obieralność pastorów, swobodę i tolerancję wyznaniową).
Inni wskazywali na wpływ dobrobytu (np. S. M. Lipset, 1959). W teorii Ronalda Ingleharta poprawa standardów życia sprawiła, że ludzie zaczęli postrzegać przetrwanie za coś oczywistego i niepodważalnego. Przez to większą uwagę zaczęto poświęcać wartościom zorientowanym na indywidualną ekspresję, które w jego ocenie są wysoce skorelowane z demokracją[80].
Niedawno powstałe teorie podkreślają role edukacji i kapitału ludzkiego, a w ramach nich – zdolności poznawczych zwiększających stopień tolerancji, racjonalności, politycznej świadomości i partycypacji. Wyszczególnia się dwa efekty edukacji i zdolności poznawczych: efekt poznawczy (pozwala dokonywać bardziej racjonalnych wyborów i lepiej przetwarzać dostępne informacje) oraz efekt etyczny (poparcie dla demokratycznych wartości, wolności, praw człowieka itp.), które są zależne od inteligencji jednostki[81][82][83].
Współczesne teorie demokracji, w przeciwieństwie do tradycyjnych, łączy przekonanie o braku możliwości wytłumaczenia dlaczego demokracja rozpowszechnia się i utrzymuje. Sceptycznie ocenia się teorię modernizacji, wykazując, że nie ma przekonującego dowodu na prawdziwość twierdzenia jakoby istniało większe prawdopodobieństwo wyłonienia się systemu demokratycznego w sytuacji poprawy dobrobytu, zwiększenia poziomu wykształcenia lub zmniejszenia się nierówności w danym kraju[84]. Brak także przekonującego argumentu za twierdzeniem o korelacji zachodzącej pomiędzy zasobnością danego państwa w złoża ropy a oporem dla przeprowadzenia procesów demokratyzacji. Dzieje się tak pomimo licznych rozważań teoretycznych, zbiorczo określanych mianem „Klątwy surowcowej”, które starają się wykazać, że zasoby ropy naftowej są czynnikiem odpowiedzialnym za zerwanie kluczowego dla demokracji przedstawicielskiej związku pomiędzy opodatkowaniem obywateli a odpowiedzialnością rządu[85]. Brak dowodów na prawdziwość tradycyjnych teorii demokratyzacji zmusił badaczy do szukania pierwotnych determinantów współczesnych instytucji politycznych, czy to geograficznych, czy demograficznych[86][87].
W XXI wieku, demokracja stała się tak popularną metodą podejmowania decyzji, że jej zastosowanie wykracza daleko poza politykę i obserwowalne jest w takich dziedzinach jak: rozrywka, jedzenie i moda, konsumpcja, urbanistyka, edukacja, sztuka, literatura, nauka czy teologia. To wszystko sprawia, że zaczęto ją określać dość krytycznym mianem „głównego dogmatu naszych czasów”[88]. Wykazuje się, że nastawienie na zdobywanie popularności np. w sztuce czy literaturze, przyczynia się do niepublikowania innowacyjnej pracy twórczej. Podobnie, rynkowe podejście do edukacji sprawia, iż ambitne badania nie są podejmowane. Nauka, która ze swej natury jest dyscypliną opartą na prawdzie, nie może opierać się na demokratycznych ideach prowadzących do przekonania, że poprawny wniosek może zostać osiągnięty na drodze powszechnego głosowania.
W 2010 roku think tank powiązany z niemiecką armią analizował jak teoria Hubberta (dotycząca szczytu wydobycia ropy naftowej) może wpłynąć na zmianę światowej gospodarki. Wnioski z badania budzą obawy o szanse przetrwania demokracji. Sugerują, że część populacji może postrzegać wstrząs wywołany osiągnięciem szczytu wydobycia ropy jako kryzys ogólnosystemowy. To zaś stworzy „przestrzeń dla ideologicznych i radykalnych alternatyw względem istniejących form rządów”[89].
W swojej Polityce, Arystoteles wprowadził typologię ustrojów politycznych, według tego, ile osób sprawuje władzę[90]. Każda z form ustrojowych miała wersję właściwą i zdegenerowaną. Ustrojami tymi były: monarchia i tyrania (rządy jednostki), arystokracja i oligarchia (rządy niewielu) oraz politeja i demokracja (rządy wielu). Demokracja uznawana była przez niego za zdegenerowaną politeję, która z kolei była najlepszym ustrojem, łączącym rządy wielu i władzę elit. Dla Arystotelesa u podstaw demokracji i politei leży wolność, skoro tylko w demokracji obywatele mogą mieć udział w wolności. Wolność z kolei jest tu rozumiana jako możliwość udziału w rządach.
Teorie demokracji mają określić istotę demokracji i jej idealną postać, do której powinny dążyć instytucje demokratyczne. Trzema najbardziej znanymi teoriami są teorie agregatywna, deliberatywna i radykalna.
Wśród współczesnych teorii politycznych wyróżniamy trzy spierające się ze sobą koncepcje dotyczące fundamentalnych przesłanek dla demokracji: demokrację agregatywną, demokrację deliberatywną, oraz demokrację radykalną[91].
Agregatywna (agregacyjna) teoria demokracji zakłada, że celem procesów demokratycznych jest poznanie preferencji obywateli oraz zebranie ich razem w celu ustalenia jakie polityki społeczeństwo powinno wprowadzać w życie. Główną kwestią są więc procedury głosowania, a opcje polityczne, które zdobyły najwięcej głosów zostają wprowadzone w życie. W ramach teorii agregacyjnej, toczy się spór pomiędzy zwolennikami demokracji pośredniej (minimaliści) i bezpośredniej.
Minimalistyczna teoria demokracji została omówiona w pracy Josepha Schumpetera Kapitalizm, socjalizm i demokracja[92]. Jest to teoria uzasadniająca demokrację pośrednią. Demokracja jest tu systemem rządów, w którym obywatele przekazują zespołom liderów politycznych w okresowych wyborach prawo do sprawowania władzy. Obywatele nie mogą i nie powinni rządzić sami, ponieważ w większości spraw przez większość czasu nie mają jasnego punktu widzenia lub ich punkt widzenia nie jest dobrze ugruntowany. Zwolennikami minimalistycznej teorii są William H. Riker, Adam Przeworski oraz Richard Posner. Istotną pracą z tego nurtu jest również książka Anthony’ego Downsa Ekonomiczna teoria demokracji (1957)[93].
Zwolennicy demokracji bezpośredniej wskazują, że obywatele powinni głosować bezpośrednio na propozycje ustawodawcze, a nie poprzez reprezentantów. Argumentują oni, że aktywność polityczna może stanowić wartość samą w sobie, ponieważ łączy oraz uczy obywateli, a powszechny udział obywateli w działaniach politycznych może powstrzymać elity przed zawłaszczaniem władzy.
Według teorii agregacyjnych, kluczową instytucją demokratyczną są wybory, wokół nich też koncentruje się polityka. Rządy będą się skłaniały do tworzenia prawa i prowadzenia polityk, które są zbliżone do poglądów medianowego wyborcy, czyli ani skrajnie lewicowe, ani skrajnie prawicowe. Taki rezultat jest wynikiem oportunizmu politycznych elit konkurujących o głosy.
Z kolei zdaniem Roberta A. Dahla podstawową zasadą demokracji jest to, że w decyzjach zbiorowych, interesy każdego członka wspólnoty politycznej są brane pod uwagę w równym stopniu, co nie oznacza, że interesy te są w równym stopniu zaspokajane. Używa on terminu poliarchii w odniesieniu do społeczeństw, w których istnieje pewien zbiór instytucji i procedur (związane głównie z wyborami), które są postrzegane jako dążące do takiej demokracji[94].
Teoria demokracji deliberatywnej opiera się na poglądzie, że istotą demokracji jest deliberacja, czyli rozważanie różnych poglądów i stanowisk. W przeciwieństwie do agregatywnej teorii demokracji, teoria deliberatywna utrzymuje, że demokracja nie polega na wyborach. Proces deliberacji pozwala jednostkom określać ich preferencje, ustalać konsens czy rozwiązywać konflikty. Autentyczna deliberacja powinna być wolna od zniekształceń wywieranych przez polityczną władzę czy interesy ekonomiczne[95].
Demokracja radykalna wskazuje na hierarchiczne i opresyjne relacje władzy, które występują w społeczeństwie. Demokracja powinna uczynić te relacje widocznymi i doprowadzić do ich zmiany.
Pod koniec XX wieku, przemiany związane z globalizacją doprowadziły do wypracowania projektów zaprowadzenia demokracji w skali globalnej. Uzasadnieniem dla takiej demokracji globalnej (kosmopolitycznej) jest to, że decyzje podejmowane w ustrojach państwowych dotykają ludzi spoza danego obszaru. W demokracji kosmopolitycznej ludzie których dane decyzje dotyczą, mają również możliwość wypowiedzenia się na dany temat i mogą głosować[96].
Zdaniem zwolenników tego systemu, każda próba rozwiązania globalnych problemów jest niedemokratyczna bez wdrożenia choćby części założeń demokracji kosmopolitycznej. Takimi zasadami są zasada praworządności, rozwiązywanie konfliktów bez przemocy; i równości obywateli poza granicami państwa. Wprowadzenie tych zasad w życie, wymagałoby reformy istniejących organizacji międzynarodowych, na przykład Organizacji Narodów Zjednoczonych, a także stworzenia nowych instytucji takich jak światowy parlament, który odpowiednio wzmocni kontrolę publiczną i odpowiedzialność w polityce międzynarodowej.
Idee leżące u podstaw demokracji kosmopolitycznej można odnaleźć w pismach Alberta Einsteina[97], Kurta Vonneguta, felietonistę George’a Monbiota oraz profesorów Davida Helda i Daniele’a Archibugiego[98]. Stworzenie Międzynarodowego Trybunału Karnego w 2003 roku było postrzegane jako ważny krok naprzód przez wielu zwolenników tego rodzaju demokracji kosmopolitycznej.
 Artykuły na Stanford Encyclopedia of Philosophy (ang.) [dostęp  2018-01-26]:
Socjalizm (łac. socialis – społeczny) – wieloznaczne pojęcie, odnoszące się do ideologii politycznej wywodzącej się z filozofii politycznej rozwijanej w latach 30. i 40. XIX wieku we Francji lub doktryna gospodarcza charakteryzująca się społeczną własnością środków produkcji oraz samorządnością pracowniczą. Społeczna własność może być publiczna, kolektywna, kooperacyjna lub udziałowa. Choć trudno objąć jedną definicją wszystkie rodzaje socjalizmu, społeczna własność środków produkcji jest jednym wspólnym elementem. 
Socjalizm wywodzi się od filozofii rozwijanej na przełomie XVIII i XIX wieku przez działaczy, tj. Henriego de Saint-Simon, Pierre’a Leroux, Charlesa Fouriera i Roberta Owena. Celem ówczesnych socjalistów było zbudowanie społeczeństwa, w którym wszyscy jego członkowie mieliby równy stan posiadania, gdzie siły rynkowe nie są głównym mechanizmem podziału bogactwa oraz państwa, w którym funkcjonowanie społeczeństwa opiera się nie na własności prywatnej, lecz na wspólnej własności, wzajemnej współpracy i altruizmie. Teoretykami socjalizmu połowy XIX w. byli: Karl Marx, Friedrich Engels, Pierre-Joseph Proudhon i Michaił Bakunin – twórcy kolejno marksizmu, anarchizmu i anarchokolektywizmu. Wkrótce, na gruncie marksistowskiej wersji socjalizmu, powstały ideologie: komunistyczna oraz socjaldemokratyczna. Elementy ideologii socjalistycznej są łączone m.in. z: chrześcijaństwem, islamem, nacjonalizmem etc.
Socjaliści są zwolennikami różnorodnych poglądów filozoficznych.
Socjaliści marksistowscy są zwolennikami materializmu historycznego. Celem marksistów jest przede wszystkim emancypacja pracy. Wiele form teorii socjalistycznej utrzymuje, że zachowania człowieka są w dużej mierze kreowane przez środowisko społeczne; zwłaszcza socjalizm marksistowski głosi, iż społeczne obyczaje, wartości, cechy kulturowe i gospodarcze praktyki są wykreowane przez społeczeństwo, nie są one zaś wynikiem naturalnego, niezmiennego prawa[1].
Socjaliści twierdzą na ogół, że kapitalizm koncentruje władzę i bogactwo w małej grupie społecznej, która kontroluje środki produkcji i czerpie bogactwo poprzez system wyzysku. Tworzy to - ich zdaniem - warstwowy system społeczny oparty na nierównych stosunkach społecznych, niezapewniających równych szans dla każdego człowieka; ów system chce zmaksymalizować swój potencjał i nie wykorzystuje dostępnych technologii oraz środków w interesie społeczeństwa. Socjalizm deklaruje, że kapitalizm jest złym systemem ekonomicznym, gdyż w dużej mierze służy interesom właściciela kapitału i wiąże się z wykorzystaniem pracowników. Aby to zmienić, chce zastąpić go innym systemem lub znacznie zmodyfikować system kapitalistyczny drogą reform (reformizm), w celu stworzenia bardziej sprawiedliwego społeczeństwa, gwarantującego lepsze standardy życiowe.
Współcześnie ugrupowanie reformistyczne (socjaldemokracja) głoszą chęć stopniowej reformy kapitalizmu. Przeciwni temu są marksiści będący zwolennikami kolektywnej kontroli produkcji, będącej kompletnym zanegowaniem kapitalizmu.
Socjaliści źródeł swojej ideologii szukali już w filozofiach starożytnych m.in. w filozofii Mazdaka i Platona[2] i Arystotelesa[3]. W okresie średniowiecza i reformacji narodziło się wiele grup religijnych mających charakter protosocjalistyczne. Grupy te czerpały na ogół inspirację z Biblii[4]. Niektóre odłamy purytanizmu w okresie wojny domowej w Anglii (szczególnie diggerzy) również nawoływały do zniesienia prywatnej własności i zaprowadzenia radykalnej równości[5].
Krytyka własności prywatnej zyskała na popularności w oświeceniu. Duże wpływy zyskały prace Jeana-Jacques’a Rousseau. W Rozprawie o pochodzeniu i podstawach nierówności między ludźmi (1755 r.) dowodził on, że powstanie własności prywatnej było źródłem upadku moralnego ludzkości[6]. Do radykalnej równości i wspólnoty własności nawoływały niektóre grupy okresu rewolucji francuskiej. Najbardziej znanym aktywistą był François Noël Babeuf, postulujący wspólną własność ziem i ogółu gospodarki oraz polityczną równość obywateli[7]. Do poglądów Babeufa nawiązali nie tylko utopijni socjaliści, czartyści, ale także Marks i Engels, a jego doktryna – mimo prześladowań ze strony władz – posiadała wielu zwolenników. Zasady Babeufa noszą nazwę babuwizmu[8]. Babeuf utworzył organizację zwaną jako Sprzysiężenie Równych.
Termin socjalizm przypisuje się najczęściej francuskim filozofom i ekonomistom Pierre’owi Leroux i Marie Roch Louis Reybaud[9]. W Wielkiej Brytanii słowo to zostało użyte po raz pierwszy przez twórcę ruchu spółdzielczego Roberta Owena w 1827 roku[10].
Słowo to miało oznaczać model wspólnoty lub własności publicznej mający swój zalążek już w starożytności. Utopijni socjaliści odwoływali się między innymi do perskiego filozofa Mazdaka, który zalecał ustanowienie posiadłości komunalnej i pracę na rzecz dobra publicznego[11]. Za zalążek myśli socjalistycznej (choć z wieloma kontrowersjami) uznaje się też poglądy Platona[12] i Arystotelesa[13].
Według niektórych relacji, użycie słów socjalizm lub komunizm było związane z postrzeganiem religii w danej kulturze. W Europie komunizm został uznany za bardziej ateistyczny, w Anglii jednak słowo komunizm kojarzyło się z komunią, a więc miało podtekst katolicki, więc ateiści w ruchu robotniczym nazywali siebie socjalistami[14].
Poprzednikiem doktryny socjalistycznej był protosocjalizm, do rozwoju tej idei przyczyniła się m.in. reformacja. W okresie reformacji w Europie działali tacy myśliciele jak Tomasz Morus i Tommaso Campanella propagujący reformę społeczeństwa i religii[15]. Ruch osłabł po pojawieniu się purytanów. Następnie powstały takie grupy jak lewelerzy, którzy popierali reformy wyborcze, progresywne opodatkowanie i ustrój republikański[16].
Najbardziej znanym aktywistą z przełomu XVIII i XIX wieku był François Noël Babeuf[7][8]. To z poglądów Babeufa czerpał ruch czartystowski, działający w latach 1836–1849. Nazwa czartystów wzięła się z jednej z petycji wydanej do parlamentu, „Karta praw ludu” z 1842 roku (The People’s Charter)[16].
Za pierwszego socjalistę współczesnego niekiedy uznawany jest Henri de Saint-Simon, postulował on budowę nowego, socjalistycznego społeczeństwa które eliminowałoby będące poza kontrolą społeczeństwa aspekty kapitalizmu i opierało się na równości szans[17]. Zdaniem de Saint-Simona praca w nowym społeczeństwie nagradzana była poprzez wkład i zdolności pracownika. Słowo socjalizm według Saint-Simona miało być przeciwieństwem liberalnej doktryny indywidualizmu. Socjaliści utopijni odrzucali indywidualizm w znaczeniu postulowanym przez liberalizm, uznawali że nie rozwiąże on problemów społecznych; ubóstwa, ucisku i nierówności. Ich zdaniem indywidualizm wspierał egoizm i społeczeństwo oparte na konkurencji. Socjalizm, w przeciwieństwie do indywidualizmu, miał być społeczeństwem opartym na współpracy międzyludzkiej[18].
Pierwszymi zachodnioeuropejskimi krytykami struktur społecznych byli: Robert Owen, Charles Fourier, Pierre-Joseph Proudhon, Louis Blanc, Charles Hall i Saint Simon, działacze ci byli pierwszymi socjalistami współczesnymi, krytykującymi ubóstwo i nierówności z czasów rewolucji przemysłowej. Według Owena, dotychczasowy ład społeczny miały zastąpić małe spółdzielcze komuny opierające się na własności publicznej[19]. Charles Fourier stawiał za punkt odniesienia indywidualne pragnienia jednostki, i zakładał, że praca musi być dla ludzi przyjemna.
Owen zaangażował się w ruch spółdzielczy działacz ten znany jest z założenia firm działającej na zasadach socjalizmu w New Lanark. Pracownicy osady dysponowali ubezpieczeniami społecznymi oraz dziesięcioipółgodzinnym dniem pracy[20]. Owenowskie osady komunalne powstały też w Stanach Zjednoczonych. Podobne projekty zaprojektowali Fourier i Étienne Cabet, którzy utworzyli na terenie USA spółdzielcze osady[20].
Na ruch socjalistyczny duży wpływ miały tradycje związkowe. Istotne znaczenie dla rozwoju ruchu miały wystąpienia robotnicze, począwszy od męczenników z Tolpuddle (związek farmerów w Wielkiej Brytanii)[21].
18 marca 1871 roku wybuchł zryw rewolucyjny, zwany jako Komuna Paryska. Bezpośrednią przyczyną rebelii była wieść o poddaniu się wojsk francuskich w wojnie z Prusami[22]. Oburzeni mieszkańcy zajęli stolicę i powołali własne oddziały. Wojska wierne rządowi zaatakowały Paryż, zostały jednak odparte przez komunardów a część wojskowych przeszła na stronę rewolucji[23]. Komunardzi podzielili się na przedstawicieli większości i mniejszości. Większość stanowili blankiści. Grupa domagała się uspołecznienia i nowych wyborów[24]. Mniejszość domagała się reform społecznych, nie godziła się jednak na walkę z opozycją. Zdaniem działaczy mniejszości państwo powinno zostać zlikwidowane, a na jego miejsce powinny powstawać lokalne komuny zrzeszone w prowincje, Stany Zjednoczone Europy, a następnie republikę światową. Na czele mniejszości stali proudhoniści[25].
W Komunie udział wzięli obcokrajowcy którzy następnie włożyli duży wkład w tworzenie rewolucyjnych organizacji politycznych, byli to m.in. Jarosław Dąbrowski i Walery Wróblewski. Kulminacyjnym momentem stłumienia Komuny był krwawy tydzień, podczas którego doszło do porachunków między rządem a komunardami[26].
Międzynarodowe Stowarzyszenie Robotników (IWA), znane potem jako Pierwsza Międzynarodówka, została założona w Londynie w 1864 roku. Genezą powstania międzynarodowej organizacji łączącej ruch socjalistyczny było brutalne stłumienie przez rosyjskiego cara powstania styczniowego, aktywnie wspieranego przez światowy ruch robotniczy. IWA przeprowadziła pierwszą konferencję w 1865, a jej pierwszy kongres odbył się w Genewie w 1866. Ze względu na różnorodność filozofii w Międzynarodówce, od samego początku istniał spór między marksistami a mutualistami pod przywództwem kolektywisty Michaiła Bakunina[27]. Międzynarodówka początkowo stawiała sobie za cel walkę o prawa pracownicze i pomoc strajkującym robotnikom. Na skutek sporu między Bakuninem a Marksem, IWA rozpadła się w 1877 roku.
Idee Marksa i Engelsa z biegiem czasu zdobywały coraz większą popularność, w szczególności w Europie Środkowej. Socjaliści postanowili po raz drugi zjednoczyć się w organizacji międzynarodowej. W 1889, na stulecie rewolucji francuskiej z 1789 roku, powstała Druga Międzynarodówka. W kongresie założycielskim udział brało 384 delegatów z 20 krajów, reprezentujących około 300 organizacji pracowniczych i socjalistycznych[28]. II Międzynarodówka określana była jako Międzynarodówka Socjalistyczna. Engels został wybrany honorowym prezesem II Międzynarodówki w czasie trzeciego kongresu w 1893 roku. W pracach Międzynarodówki Socjalistycznej nie brali udziału anarchiści, którzy utworzyli kilka lat wcześniej Międzynarodówkę Antyautorytarną[29].
Sukcesy wyborcze partii robotniczych we Francji i Niemczech doprowadziły do powstania w ruchu socjalistycznym nurtu reformistycznego[30]. Po śmierci Marksa, na łamach partii socjaldemokratycznych, doszło do serii dyskusji nad interpretacjami jego filozofii. Kluczowe różnice, które doprowadziły do wykształcenia szeregu zwalczających się frakcji, dotyczyły metody dojścia do komunizmu – socjalizm rewolucyjny opowiadał się za przejściem gwałtownym, reformizm zakładał stopniowe, pokojowe przekształcenia.
Reformiści (Georg von Vollmar) uważali, że partie socjaldemokratyczne powinny zawiązywać sojusze z liberałami. W 1899 do rządu Francji wszedł działacz robotniczy Alexander Millerand, tym samym padł on ofiarą krytyki lewego skrzydła ruchu socjalistycznego. Obok reformizmu rozwijał się rewizjonizm, którego czołowym reprezentantem był Eduard Bernstein[30]. Bernstein, mimo przywiązania do tez Marksa, widział – jego zdaniem – błędne tezy, szczególnie ostro krytykując m.in. materializm historyczny[31].
Fryderyk Engels skrytykował propozycje sojuszu z liberałami, uważając że w ten sposób niemiecka socjaldemokracja nie wprowadzi postulowanych przez siebie reform.
I wojna światowa doprowadziła do podziałów w II Międzynarodówce, spór dotyczył głównie poparcia partii socjalistycznych dla wojny światowej i coraz większego rozłamu między rewolucjonistami a reformistami. W lutym 1917 roku wybuchła rewolucja w Rosji, która obaliła carat i wprowadziła swobody obywatelskie. Jednocześnie robotnicy, żołnierze i chłopi zaczęli wybierać wielopartyjne rady delegatów, które stały się organem dwuwładzy. We wrześniu 1917 Rosja stała się państwem republikańskim, a Rząd Tymczasowy zwołał wybory do Zgromadzenia Konstytucyjnego. W kwietniu tego roku ze Szwajcarii do Rosji przybył Włodzimierz Lenin, wzywając do obalenia Rządu Tymczasowego (składającego się wówczas z koalicji liberałów i socjalistów-rewolucjonistów (eserowców) z hasłem przekazania całej władzy w ręce rad. W październiku liderzy bolszewików, Włodzimierz Lenin i Lew Trocki, podjęli decyzję o zamachu stanu przeciw Rządowi Tymczasowemu, a Komitet Centralny partii bolszewickiej (przy sprzeciwie Lwa Kamieniewa i Grigorija Zinowiewa opowiadających się za powołaniem rządu koalicyjnego wszystkich partii socjalistycznych Rosji (tj. eserowców, bolszewików i mienszewików)) uchwalił powstanie zbrojne. W nocy z 6 na 7 listopada 1917 (24/25 października s.s) bolszewicy obalili rząd Aleksandra Kiereńskiego (z Partii Socjalistów-Rewolucjonistów (eserowców)). Wydarzenie to przeszło do historii jako rewolucja październikowa. Nowy rząd zaproponował natychmiastowe zawieszenie broni na wszystkich frontach i oddanie gruntów rolnych w ręce chłopów[32]. Na gruncie pragmatyzmu politycznego Lenin porzucił tezę Marksa dotyczącą priorytetu ekonomii nad polityką[33].
Rząd Tymczasowy rozpisał we wrześniu 1917 wybory do Zgromadzenia Ustawodawczego Rosji (Konstytuanty). Odbyły się one już po przewrocie bolszewickim, na przełomie listopada i grudnia 1917. Wybory do Konstytuanty wygrała Partia Socjalistów-Rewolucjonistów zdobywając 17 943 000 głosów (40,4%) wobec 10 661 000 (24%) oddanych na bolszewików. Konstytuanta została rozpędzona przez bolszewików przy użyciu siły po pierwszym posiedzeniu, w czasie którego odmówiła samorozwiązania i przekazania swoich kompetencji Radzie Komisarzy Ludowych. Demonstracje w obronie Konstytuanty zostały stłumione przy użyciu broni palnej. Rozpędzenie Konstytuanty, pierwszego demokratycznie wybranego parlamentu Rosji, przez bolszewików było początkiem wojny domowej w Rosji. Rewolucja bolszewicka doprowadziła do powstawania na całym świecie frakcji komunistycznych w partiach socjalistycznych i partii komunistycznych, których strukturą nadrzędną stała się powołana w 1919 z inicjatywy partii bolszewickiej Międzynarodówka Komunistyczna (Komintern) z siedzibą w Moskwie oraz jej aparat wykonawczy (Komitet Wykonawczy Kominternu).
W 1920 roku Armia Czerwona na czele z Lwem Trockim zwyciężyła Białych. Po powstaniu w Kronsztadzie i powstaniu tambowskim bolszewicy byli zmuszeni do wprowadzenia Nowej Polityki Ekonomicznej (NEP). Własność prywatna pozostała w rękach małych i średnich przedsiębiorstw chłopskich, duży przemysł natomiast był kontrolowany przez państwo. W 1922 roku, czwarty kongres Międzynarodówki Komunistycznej podjął politykę zachęcającą komunistów do porozumienia z socjaldemokratami, polityka ta została jednak odrzucona po nieudanym aliansie Komunistycznej Partii Wielkiej Brytanii i Partii Pracy. W 1923 umierający Lenin, widząc rosnący w państwie radzieckim autorytaryzm, stwierdził, że Rosja powróciła do burżuazyjnej maszyny carskiej[34]. Po śmierci Lenina w styczniu 1924 roku, mimo jego woli, ZSRR pozostawał pod coraz większą kontrolą Józefa Stalina. Stalin stworzył biurokratyczno-totalitarny rząd, uznany przez socjalistów za podważenie początkowych ideałów rewolucji październikowej.
Rewolucja rosyjska z października 1917 roku przyniosła ostateczny podział ideologiczny między komunistami a socjalistami.
Australijska Partia Pracy jest uznawana za pierwszą partię socjaldemokratyczną na świecie (powstała w 1891 roku). W 1904 Australijczycy wybrali pierwszego socjalistycznego premiera na świecie, Chrisa Watsona. W 1945 wybory w Wielkiej Brytanii wygrała Partia Pracy, na czele rządu stanął Clement Attle rozpoczynając realizacje programu socjalistycznego w tym kraju. Partie socjaldemokratyczne zdominowały politykę powojenną w krajach takich jak: Francja, Włochy, Czechosłowacja, Belgia i Norwegia. W Szwecji, Szwedzka Socjaldemokratyczna Partia Robotnicza sprawowała władzę w latach 1937–1976, 1982–1991 i 1994–2006. We Francji socjalistyczny rząd znacjonalizował spółki takie jak: Charbonnages de France (CDF), Electricité de France (EDF), Gaz de France (GDF), Air France, Banque de France, i Régie Nationale des Usines Renault[35]. Po II wojnie światowej rządy socjaldemokratyczne wprowadziły reformy społeczne i redystrybucję bogactw przez dobrobyt państwa i podatki.
Na politykę Partii Pracy wpłynęły poglądy reformatora społecznego i ekonomisty, Williama Beveridge’a, który określił pięć punktów zła klasy robotniczej okresu przedwojennego: chcę (bieda), choroby, ignorancja (brak dostępu do edukacji), nędza (złe warunki mieszkaniowe) i bezczynność (bezrobocie)[36]. Po przejęciu władzy w kraju w 1945, rząd laburzystów wprowadził zasiłek dla bezrobotnych, system ubezpieczeń społecznych i państwowych emerytur. Minister zdrowia, Aneurin Bevan, utworzył Narodową Służbę Zdrowia (National Health Service). Rząd laburzystowski znacjonalizował główne obiekty użyteczności publicznej takie jak: kopalnie, złoża węglu, gaz, energie elektryczną, koleje, żelazo i stal[37]. Anthony Crosland przyznał, że rząd Attle znacjonalizował ok. 25% przemysłu krajowego[38]. Kolejny rząd laburzystów w latach 1974–1979 znacjonalizował produkcję samochodów, duże przedsiębiorstwa lotnicze i okrętowe.
W USA również można było dostrzec elementy gospodarki planowej, mianowicie powołane podczas II wojny światowej biuro regulujące ceny i płace istniejące w latach 1941-7[39][40]. Do 1964 roku najwyższa stawka podatkowa wynosiła około 90%, a do 1982 roku – 70%[41].
Specyficzny model państwa nazwany modelem nordyckim, został przyjęty przez partie socjaldemokratyczne w Danii, Islandii, Norwegii, Szwecji i Finlandii. Ta szczególna adaptacja mieszanki gospodarki rynkowej i hojnego (w stosunku do innych krajów rozwiniętych) państwa opiekuńczego różni się od zbliżonych do tego modelu krajach, poprzez nacisk modelu nordyckiego na maksymalizację udziału w rynku pracy, promowania równości płci, egalitarne i rozległe poziomy świadczeń socjalnych, dużą skalę redystrybucji i liberalne stosowanie polityki fiskalnej[42]. Model ten promuje w szczególności ruch związkowy. W 2008 roku w Finlandii do związków zawodowych należało ponad 67,5% pracowników, w Danii – 67,6%, a w Szwecji – 68,3%. W porównaniu przynależność związkowa w USA wynosiła 11,9% a we Francji 7,7%[43]. Model nordycki nie jest jednak jednolitym systemem państwa – każdy z krajów nordyckich ma swoje modele ekonomiczne i społeczne, czasem wyróżniające się od innych państw regionu.
Wiele partii socjaldemokratycznych, zwłaszcza po zakończeniu zimnej wojny, przyjęła neoliberalną polityką rynkową obejmującą prywatyzację i deregulację, skutkując zaprzestaniem prowadzenia gospodarki umiarkowanego socjalizmu na rzecz liberalizmu rynkowego. Pomimo nazwy, prokapitalistyczne tendencje w ruchu socjaldemokratycznym są radykalnie odmienne od wielu niekapitalistycznych rynkowych teorii socjalistycznych obowiązujących w historii. W 1959 Socjaldemokratyczna Partia Niemiec przyjęła nowy program odrzucający walkę klas i marksizm. W roku 1980 wraz z rządami w wielu krajach neoliberalnych polityków konserwatywnych takich jak Ronald Reagan w USA, Margaret Thatcher w Wielkiej Brytanii i Brian Mulroney w Kanadzie, znacznie osłabło znaczenie państwa opiekuńczego i systemu opieki społecznej, które przez polityków liberalnych gospodarczo uważane jest za przeszkodę dla prywatnej przedsiębiorczości. W latach 80. i 90. zachodnioeuropejscy socjaliści zostali zmuszeni do pogodzenia swoich socjalistycznych programów gospodarczych z zasadami wolnego rynku opartego na wspólnej europejskiej gospodarce. W latach 90. lewicowy premier Wielkiej Brytanii z ramienia Partii Pracy, Tony Blair, zakładał politykę opartą na gospodarce wolnorynkowej oraz prowadzenie usług publicznych przez prywatnych przedsiębiorców; system ten został nazwany trzecią drogą.
W 1992 roku powstała Partia Europejskich Socjalistów mająca grupować europejskie partie socjalistyczne i uczestniczyć w polityce Unii Europejskiej[44].
Na przełomie XX i XXI wieku na sile przybrały ruchy antykapitalistyczne i alterglobalistyczne. Ruch socjalistyczny odegrał ważną rolę w tworzeniu nowych ruchów społecznych. Inwazja na Irak w 2003 doprowadziła do wzmożonego ruchu antywojennego w ugrupowaniach lewicowych. Rzecznikiem nowych ruchów lewicowych stał się m.in. amerykański intelektualista Noam Chomsky. Kryzys finansowy rozpoczęty w 2007 doprowadził do dyskusji głównego nurtu socjalistycznego na temat marksizmu[45][46]. W dniu 28 stycznia 2009 gdy miało miejsce spotkanie w Davos. Magazyn Times opublikował artykuł Przemyśleć Marksa[47][48]. Sondaż BBC na dwudziestą rocznicę upadku muru berlińskiego (2009) stwierdził, że 23% respondentów uważa, że kapitalizm jest pełen wad i potrzebny jest inny system ekonomiczny. Liczba Francuzów zgadzających się z tą opinią wyniosła ponad 40%, z kolei 50% Amerykanów podzieliło opinię, że kapitalizm ma problemy, które można rozwiązać dzięki regulacji i reformom. Spośród 27 badanych krajów, 22 wyraziło poparcie dla rządów chcących bardziej równomiernej dystrybucji bogactwa[49].
Afrykański socjalizm był i nadal jest główną ideologią na całym kontynencie. Twórca socjalizmu afrykańskiego, Julius Nyerere, znajdował się pod wpływem socjalizmu fabiańskiego, był zwolennikiem wiejskich tradycji Afrykanów. Opracował model przyszłej gospodarki znany jako Ujamaa, systemu kolektywizacji, który – zgodnie z doktryną Nyerere – był obecny na kontynencie przed kolonializmem. Zasadniczo wierzył w to, że Afrykanie byli już socjalistami. Innymi znanymi afrykańskimi socjalistami byli: Jomo Kenyatta, Kenneth Kaunda, Kwame Nkrumah. Według krytyków[kto?], Afrykański Kongres Narodowy z RPA zrezygnował częściowo z ideałów na rzecz pragmatyzmu i wszedł na neoliberalną drogę. Obecnie wiele krajów afrykańskich rządzonych przez lewicę jest oskarżanych o wykorzystywanie w ekonomii tendencji neoliberalnych. W Azji partie socjalistyczne najbardziej popularne są w Indiach i Nepalu, na większości terenów Dalekiego Wschodu lewica socjalistyczna jest słabsza od antyimperialistycznych ruchów skrajnej lewicy (np. maoizmu).
W Ameryce Południowej ideologia lewicowa staje się coraz bardziej popularna, zjawisko to określa się czasem jako różowa fala[50]. Hugo Chávez przyjął doktrynę znaną jako socjalizm XXI wieku, w ślad za nim poszedł prezydent Rafael Correa. Innymi socjalistycznymi przywódcami w Ameryce Południowej są: Evo Morales i Daniel Ortega[51].
W zamyśle socjalistów własność środków produkcji może być oparta na bezpośredniej własności pracowników spółdzielni, lub całego społeczeństwa. Zarządzanie i kontrola nad działalnością przedsiębiorstw publicznych powinna być oparta na samorządności i autonomii, przy równych relacjach w miejscu pracy. Pierwotne koncepcje ekonomii socjalistycznej zalecały zorganizowanie produkcji w taki sposób, aby bezpośrednio produkcja dóbr i usług była proporcjonalna do ich wartości użytkowej[52]. Rola i wykorzystanie pieniędzy w hipotetycznej gospodarce socjalistycznej jest sporna, pierwotnie utopijni socjaliści często odrzucali pieniądz jako przedmiot niesprawiedliwości społecznej. Socjaliści utopijni, w tym: Karol Marks, Robert Owen i Pierre-Joseph Proudhon, zalecali różne formy zbliżone do pieniądza. Bolszewicki rewolucjonista Lew Trocki twierdził, że po rewolucji socjalistycznej, pieniądze nie mogą być arbitralnie zlikwidowane. Pieniądze miały wyczerpać jedynie „historyczną misję” (nadal stosowane, aż staną się zbędne)[53].
Socjalizm rynkowy odwołuje się do różnych teorii i systemów ekonomicznych, które wykorzystują mechanizm rynkowy do organizowania produkcji i przeznaczania czynników produkcji wśród przedsiębiorstw będących własnością społeczną. Odmiany socjalizmu rynkowego obejmują propozycję, takie jak mutualizm i neoklasyczne modele ekonomii. Socjaliści rynkowi opierają się na gospodarce mieszanej, samorządności pracowniczej i rynku opartego na własności mieszanej z dużym udziałem sektora spółdzielczego i własności pracowniczej.
Gospodarka planowa jest rodzajem gospodarki składającym się z mieszanej własności publicznej środków produkcji i dystrybucji za pośrednictwem centralnego planowania. Enrico Barone przedstawił kompleksowe teoretyczne ramy planowanej gospodarki socjalistycznej. W jego modelu, przy założeniu idealnych technik obliczeniowych, dojdzie do zrównoważenia podaży i popytu[54]. Najbardziej widocznym przykładem gospodarki planowanej jest system gospodarczy Związku Radzieckiego, scentralizowanie zaplanowany model jest zwykle związany z państwami komunistycznymi XX wieku, gdzie zostały połączone z jednopartyjnym systemem politycznym. W gospodarce centralnie planowanej decyzje dotyczące ilości towarów i usług, które mają być produkowane są z góry zaplanowane przez agencję planowania. Chociaż gospodarka radziecka była nominalnie centralnie planowana, w praktyce planowanie opracowane było na podstawie informacji gromadzonych i przekazywanych z państwowych przedsiębiorstw do ministerstw gospodarki. Gospodarka ustalała ceny dla producentów i konsumentów towarów, tym samym gospodarka radziecka była alternatywą dla rynku (popyt i podaż). System ten, jako autorytarny według krytyków, został odrzucony przez główny nurt socjalizmu. Ewenementem w krajach o gospodarce zbliżonej do radzieckiej była Jugosławia, gdzie był praktykowany pewien stopień samorządności.
Socjalistyczna krytyka modelu planowanego podkreśla fakt, że gospodarka radziecka została skonstruowana na akumulacji kapitału i wydobyciu wartości dodatkowej z pracowników przez agencję planowania w celu inwestycji nadwyżki w nowe produkcje. Stąd środowiska lewicowe ukuły nowy termin „państwowa gospodarka kapitalistyczna”. Jeszcze inni socjaliści skupili się na braku samorządu i scentralizowanych uprawnień władzy w modelu radzieckim, co ich zdaniem uczyniło system ZSRR nie socjalistycznym, określanym przez nich mianem biurokratycznego kolektywizmu, kapitalizmu państwowego lub zdeformowanego państwa robotniczego[55].
Zdecentralizowana, samorządna gospodarka planowana opiera się na autonomicznych podmiotach gospodarczych i zdecentralizowanym mechanizmie przydzielania i podejmowania decyzji. Model ten zdobył poparcie wśród przedstawicieli klasycznej i neoklasycznej ekonomii (Alfred Marshall, John Stuart Mill i Jaroslav Vanek). Historycznie model przejawiał się w pracy spółdzielczej, samorządność jako cel stawia sobie zmniejszenie i stopniową eliminacje wyzysku i alienacji. Socjalistyczny rząd Salvadora Allende w Chile, eksperymentował nawet z cybernetyką (projekt Synco), która miała stworzyć sieć informacyjną między rządem, państwowymi przedsiębiorstwami i konsumentami. Innym, nowszym wariantem zdecentralizowanej gospodarki planowanej jest ekonomia uczestnicząca. Charakterystyką ekonomii uczestniczącej są zdecentralizowane rady pracowników i konsumentów.
Michel Bauwens identyfikuje powstanie ruchu otwartego planowania produkcji jako nowej, alternatywnej formacji społecznej do gospodarki kapitalistycznej i gospodarki centralnie planowanej, opartej na współpracy samorządu, własności wspólnej zasobów. Komunizm anarchistyczny jest teorią anarchizmu, który opowiada się za zniesieniem stanu, własności prywatnej i kapitalizmu na rzecz wspólnej własności środków produkcji[56]. Zdecentralizowane planowanie jest związana z ruchami politycznymi społecznego anarchizmu, anarchokomunizmu, trockizmu, komunizmu rad, lewicowego komunizmu i socjalizmu demokratycznego.
Planowanie ekonomiczne to system, w którym spółdzielnie są właścicielami środków produkcji, ale działalność gospodarcza jest kierowana do pewnego stopnia przez agencję rządową i mechanizmy koordynacji. Różni się od scentralizowanej gospodarki planowanej w mikroekonomicznym podejmowaniu decyzji. Model ten realizowany był przez rząd Clementa Attleego. Nacjonalizacja w Wielkiej Brytanii została osiągnięta poprzez uspołecznienie przemysłu (tj. odszkodowania). British Aerospace została połączona z największymi firmami lotniczymi takimi jak: British Aircraft Corporation, Hawker Siddeley. British Shipbuilders również została połączona z innymi spółkami stoczniowymi (Cammell Laird, Govan Shipbuilders, Swan Hunter, i Yarrow Shipbuilders). Kopalnie węgla kamiennego zostały znacjonalizowane w 1947 roku.
Socjalizm rynkowy łączy własność publiczną z mechanizmami rynkowymi. Środki produkcji są własnością publiczną lub są wspólnie eksploatowane w celach zarobkowych. W socjalizmie rynkowym środki produkcji składałyby się z państwowych lub wspólnotowych przedsiębiorstw działających w gospodarce rynkowej. Generowany zysk miałby być używany do bezpośredniego wynagrodzenia pracowników, programów państwowych lub finansowania instytucji publicznych[57]. Miałoby to w założeniu eliminować lub znacznie zmniejszać zapotrzebowanie na różne formy opodatkowania, które istnieją w systemach kapitalistycznych. Najstarsze modele formy socjalizmu rynkowego zostały opracowane przez Enrico Barone w 1908 i Oskara R. Lange (ok. 1936). Zaproponowali oni, aby centralnie planowane poziomy cen ustalane były metodą prób i błędów. Neoklasyczny ekonomista Léon Walras uważał, że gospodarka socjalistyczna oparta na własności państwowej ziemi i zasobów naturalnych stanowi podstawę finansów publicznych, dzięki czemu podatki dochodowe byłyby niepotrzebne[58]. Niektóre koncepcje socjalizmu rynkowego zbliżają się w zakresie postulowanych, rynkowych metod gospodarowania w ramach własności publicznej do modelu kapitalizmu państwowego.
Obecny system gospodarczy Chin jest formalnie zatytułowany jako socjalistyczna gospodarka rynkowa z chińskimi cechami. Łączy on w sobie duży sektor państwowy i sektor prywatny, operujący głównie wokół przemysłu lekkiego. Strefa prywatna w 2005 roku wypracowała między 33% a 70% PKB[59][60]. Chińska gospodarka składa się ze 150 przedsiębiorstw państwowych które podlegają bezpośrednio centralnemu rządowi chińskiemu. W 2008 roku państwowe korporacje stały się bardziej dynamiczne i generowały duży wzrost przychodów dla państwa, w wyniku czego w 2009, mimo kryzysu ekonomicznego na świecie, gospodarka chińska prężnie się rozwijała[61]. Wietnam przyjął podobny model po reformach gospodarczych Đổi Mới, wietnamski rząd zachowuje jednak ścisłą kontrolą sektora państwowego i branż strategicznych, ale pozwala na działalność sektora prywatnego w produkcji towarowej.
Termin socjalizm jest używany w odniesieniu do ruchów politycznych, filozofii politycznej i hipotetycznej formy społeczeństwa do której ruchy te chcą awansować.
Najbardziej wpływową socjalistyczną teorią były prace Karola Marksa i Fryderyka Engelsa. Promowali oni świadomość tych, którzy zarabiają na własnej pracy (klasa robotnicza). Prowadząc tym samym do dalszego kształtowania ich warunków pracy z płatnego niewolnictwa do poszukiwania wolności i emancypacji, a w końcu przejęcia własności środków produkcji na rzecz ogółu społeczeństwa. Według Marksa i Engelsa, emancypacja pracy prowadzi w końcu do stworzenia społeczeństwa bezklasowego. Marksistowska koncepcja socjalizmu zakłada etapy historyczne które zastąpią system kapitalistyczny socjalizmem (stan poprzedzający komunizm). Zakłada ona, że proletariat będzie kontrolował środki produkcyjne na gruncie państwa robotniczego wybudowanego przez pracowników i służącego w ich interesie[62]. Myśl rozwoju historycznego socjalizmu pojawiła się u Marksa i Engelsa po Komunie Paryskiej w 1871 roku, która według nich była pierwszą próbą budowy społeczeństwa opartego na socjalizmie. Przed osiągnięciem komunizmu (socjalizmu), działalność gospodarcza miałaby być organizowana poprzez stosowanie systemów motywacyjnych, a klasy społeczne miałyby dalej istnieć, ale w mniejszym stopniu niż w warunkach kapitalizmu.
Dla ortodoksyjnych marksistów – zgodnie z tezami Karola Marksa – socjalizm jest niższym etapem komunizmu. Komunizm stałby się możliwy dopiero po socjalistycznym etapie rozwijania efektywności ekonomicznej i automatyzacji produkcji, która doprowadziłaby do nadmiaru towarów i usług („każdemu według potrzeb”)[63]. Klasycy filozofii marksistowskiej uznawali że świat jest materialny, opierając się na dialektycznych tezach Hegla, Marks z prac Hegla przyjął trzy podstawowe tezy: alienację, reifikację i rozwiązanie sprzeczności. Rozłam w ruchu marksistowskim nastąpił już po śmierci Karola Marksa. Z tradycyjnego marksizmu wyłoniły się nurty rewolucyjne i reformistyczne.
Dla marksistów (socjalizm naukowy), rozwój kapitalizmu w Europie Zachodniej stanowi podstawę materialną dla możliwości budowy socjalizmu[64]. Thorstein Veblen widział socjalizm jako następny etap procesu ewolucyjnego w ekonomii, który prowadziłby z naturalnego rozpadu systemu rozpadu przedsiębiorstw, w przeciwieństwie do Marksa, nie wierzył aby było to wynikiem walki politycznej czy też rewolucji, uważał socjalizm za oczywisty i ostateczny cel ludzkości[65]. Rewolucyjni socjaliści uważają, że rewolucja jest jedynym środkiem do ustanowienia nowego systemu społeczno-gospodarczego. Socjaliści rewolucyjni są w głównej mierze zwolennikami utworzenie demokratycznej, centralistycznej partii rewolucyjnej pod kierownictwem klasy robotniczej, dążącej do obalenia państwa kapitalistycznego, a ostatecznie – do obalenia instytucji państwa jako takiej. Rewolucja nie musi być określana przez rewolucyjnych socjalistów jako powstanie, lecz jako kompletny demontaż i szybkie przekształcenie wszystkich obszarów społeczeństwa klasowego.
Istnieje wiele odmian socjalizmu i jako taka nie istnieje jedna definicja wszystkich ruchów socjalistycznych.
Nowoczesny socjalizm demokratyczny jest szerokim ruchem politycznym który stara się propagować ideały socjalizmu w ramach systemu demokratycznego. Wielu demokratycznych socjalistów wspiera socjaldemokrację jako drogę do reformy obecnego systemu, jeszcze inni wspierają taktyki rewolucyjne w celu ustanowienia socjalistycznych celów. Nowoczesna socjaldemokracja podkreśla program stopniowej reformy legislacyjnej kapitalizmu w celu uczynienia go bardziej sprawiedliwym i ludzkim, a teoretycznym celem dla socjaldemokracji i demokratycznego socjalizmu jest budowa społeczeństwa socjalistycznego. Te dwa ruchy są bardzo podobne, zarówno w terminologii, jak i ideologii, choć istnieje kilka kluczowych różnic między nimi.
Demokratyczny socjalizm na ogół odnosi się do każdego ruchu politycznego, który dąży do stworzenia gospodarki opartej na demokracji gospodarczej. Demokratyczni socjaliści sprzeciwiają się centralizmowi demokratycznemu i rewolucyjnej partii awangardowej w stylu leninowskim. Demokratyczny socjalizm jest trudny do zdefiniowania i różne grupy uczonych mają diametralnie różne definicje tego pojęcia. Niektóre definicje odnoszą się po prostu do wszystkich form socjalizmu których następstwem jest ewolucyjna, reformistyczna droga do socjalizmu, a nie rewolucja[66].
Leninizm jest teorią opracowaną przez Włodzimierza Lenina. Promuje on utworzenia partii awangardowej, prowadzonej przez zawodowych rewolucjonistów, prowadzących klasę robotniczą do podboju państwa. Teoria leninowska odrzuca naturalny rozpad kapitalizmu, zdaniem leninistów pracownicy w realizacji swoich celów wymagają rewolucyjnej awangardy. Po obaleniu burżuazyjnej dyktatury przez rewolucję socjalistyczną, leniniści dążą do stworzenia państwa socjalistycznego, w którym klasa robotnicza byłaby u władzy. Państwo to miałoby być fundamentem na przejściu do komunizmu (bezklasowego społeczeństwa). W tym stanie, partia awangardowa pełni rolę centralnego jądra w organizacji społeczeństwa socjalistycznego, przewodnicząc jednopartyjnemu systemowi politycznemu[67]. Leninizm odrzuca pluralizm polityczny widząc w nim szkodliwe podziały i destrukcje. Zamiast tego leninizm opowiada się za koncepcją centralizmu demokratycznego[68].
Po śmierci Lenina w 1924 roku, leninizm rozpadł się na wiele przeciwstawnych nurtów, z których głównymi były totalitarny stalinizm i maoizm oraz antytotalitarny trockizm.
Socjalizm wolnościowy jest terminem używanym często jako synonim anarchizmu, w bardziej ogólnym sensie jest stosowany wobec innych nurtów socjalizmu bazujących na innych fundamentach filozoficznych (takich jak marksizm) które odrzucają skostniałe ich zdaniem organizacje partyjne oraz struktury państwa. Socjalizm wolnościowy jest więc nurtem niehierarchicznym, niebiurokratycznym i bezpaństwowym. Wolnościowi socjaliści przeciwstawiają się wszelkim formom przymusu organizacji społecznej, wspierają swobodne zrzeszanie się w miejsce rządu[69]. Prądami wolnościowego socjalizmu są marksistowskie tendencje takie jak: komunizm rad, autonomizm, luksemburgizm, jak również niemarksistowskie ruchy takie jak komunalizm, anarchizm (anarchokomunizm, anarchosyndykalizm, anarchokolektywizm, mutualizm) czy ekonomia uczestnicząca, oraz niektóre warianty socjalizmu utopijnego i anarchoindywidualizmu[70].
Anarchizm uznaje państwo za niepotrzebne i szkodliwe, argumentując że państwo nie może być użyte do budowy gospodarki socjalistycznej i proponuje alternatywę opartą na sfederowanych i zdecentralizowanych wspólnotach autonomicznych. Państwo zdaniem anarchistów jest jednym z elementów aparatu przymusu, dążącego do zniewolenia wolnej jednostki. Obejmuje on zarówno zwolenników anarchoindywidualizmu, jak i ruchów anarchizmu społecznego. Anarchokomuniści postulują bezpośrednie przejście z kapitalizmu do wolnościowego komunizmu, anarchosyndykaliści prowadzą walkę o prawa pracownicze na drodze akcji bezpośredniej i strajków generalnych[71]. Anarchokolektywiści wywodzą się z mutualizmu, który został zreformowany przez Michaiła Bakunina. Anarchokolektywiści postulują spółdzielczość i wynagrodzenie pracy w oparciu o ilość czasu który przyczynił się do produkcji.
Socjalizm chrześcijański jest szerokim pojęciem obejmującym teorię ekonomiczną socjalizmu i wiarę chrześcijańską.
Islamski socjalizm to termin ukuty przez różnych muzułmańskich przywódców do opisu bardziej religijnych form socjalizmu. Muzułmańscy socjaliści uważają, że nauki Koranu i Mahometa są zgodne z zasadami równości. Islamscy socjaliści są bardziej konserwatywni niż ich zachodnich odpowiednicy i odnajdują swoje korzenie w antyimperializmie i antykolonializmie. Islamscy przywódcy socjalistyczni odwołują się do demokracji i publicznej legitymizacji mandatu sprawowania władzy.
Tradycyjni socjaldemokraci zalecają budowę socjalizmu poprzez działanie w ramach istniejącego systemu politycznego kapitalizmu. Ruch socjaldemokratyczny stara się wybierać socjalistów do gabinetów politycznych w celu przeprowadzenia prospołecznych reform. Nowoczesny ruch socjaldemokratyczny natomiast w dużej mierze porzucił cel zmierzania w kierunku gospodarki socjalistycznej, zamiast tego opowiada się za społecznymi reformami w celu poprawy kapitalizmu, poprzez budowę państwa opiekuńczego, zasiłków dla bezrobotnych itd[72]. System taki w ciągu ostatnich kilkudziesięciu lat został wykorzystany w Szwecji, Danii, Norwegii i Finlandii. Takie podejście zostało nazwane modelem skandynawskim.
Ekosocjalizm (inaczej zielony socjalizm) jest ideologią łączącą socjalizm z ekologizmem.
Pierwsze koncepcje ekosocjalistyczne powstały już w latach 80. i 90. XIX wieku w Wielkiej Brytanii. W trakcie rewolucji październikowej próby wszczepienia ekologicznych przekonań do partii bolszewickiej zakończyły się fiaskiem, czego efektem stała się, sprzeczna z zasadami zrównoważonego rozwoju, polityka przemysłowa ZSRR, a następnie całego bloku wschodniego.
Na Zachodzie ekosocjalizm szedł w parze z feminizmem, zaś na globalnym Południu uznany został za ekologię dla ubogich. Ważnymi publikacjami stały się m.in. Ekosocjalizm: od głębokiej ideologii do sprawiedliwości społecznej z 1994 roku autorstwa Davida Peppera, oraz Manifest ekosocjalistyczny z 2001 roku – Joela Kovela i Michaela Lowy’ego[73].
Zieloni socjaliści łączą ze sobą socjalizm, marksizm i ekologię. Są zdeklarowanymi alterglobalistami, uważającymi kapitalizm za przyczynę wszelkich nierówności społecznych, wojen i degradacji środowiska naturalnego. Sprzeciwiają się energetyce jądrowej i akceptują wartości pacyfistyczne.
Ideologia ta jest szczególnie popularna w krajach nordyckich – m.in. Partia Lewicy (Szwecja), Sosialistisk Venstreparti (Norwegia), Socjalistyczna Partia Ludowa (Dania), Sojusz Lewicy (Finlandia), Ruch Zieloni-Lewica (Islandia).
W Polsce nurt ten reprezentuje Polska Partia Socjalistyczna.
Syndykalizm jest ruchem społecznym, który działa poprzez związki zawodowe, odrzucając przy tym socjalizm państwowy lub wykorzystanie instytucji państwa do budowy socjalizmu. Syndykaliści opowiadają się za socjalistyczną gospodarką opartą na związkach federacji lub syndykatów pracowników, którzy posiadają i zarządzają środkami produkcji[74].
Marks nazwał socjalizm „pierwszą” albo niższą fazą społeczeństwa komunistycznego.
Stosując dialektyczną ideę rozwoju do analizy kształtowania się przyszłego społeczeństwa, klasycy marksizmu-leninizmu doszli do wniosku, iż socjalizm i komunizm to dwa zgodne z prawami rozwoju społecznego stadia nowej formacji społeczno-ekonomicznej. [...] Żaden kraj nie może od razu osiągnąć komunizmu, pomijając socjalistyczny stopień rozwoju; socjalizm — to obiektywne i konieczne stadium w rozwoju komunistycznej formacji społeczno-ekonomicznej[75].
Społeczeństwo socjalistyczne nazywane jest pierwszym stopniem komunizmu, ponieważ umocniły się już w nim liczne ważne cechy właściwe całej komunistycznej formacji społeczno-ekonomicznej, przede wszystkim społeczna własność środków produkcji[76]. Rozwój socjalizmu i jego przerastanie w komunizm to jednocześnie proces przygotowywania warunków obumierania państwa.
W ustroju socjalistycznym obowiązuje zasada „od każdego według jego zdolności, każdemu według jego pracy”[77]. Państwa rządzone przez partie komunistyczne oficjalnie określały się jako socjalistyczne, chociaż socjaldemokraci i trockiści uważali to określenie za nieadekwatne.
Według ekonomii marksistowskiej przejście środków produkcji na własność społeczną zmienia w zasadniczy sposób motywy i cel produkcji[78].
Celem socjalizmu jest coraz pełniejsze zaspokojenie rosnących materialnych i kulturalnych potrzeb narodu poprzez nieustanne rozwijanie i doskonalenie produkcji społecznej.
Cel ten nie jest niczym innym, jak świadomym wyrazem obiektywnej prawidłowości ekonomicznej, właściwej produkcji socjalistycznej[79]. Tak właśnie formułuje się w literaturze ekonomicznej podstawowe prawo ekonomiczne socjalizmu, które charakteryzuje istotę socjalistycznego sposobu produkcji: nieprzerwany wzrost i doskonalenie produkcji na bazie przodującej techniki i kolektywnej pracy w celu jak najpełniejszego zaspokojenia stale rosnących potrzeb i wszechstronnego rozwoju wszystkich członków społeczeństwa[79][80].
W latach 1917–1991 wielokrotnie nadużywano terminu, stosując go w odniesieniu do państw rządzonych przez partie komunistyczne[81]. W państwach rządzonych przez partie komunistyczne (tzw. państwa socjalistyczne) socjalizm był systemem, w którym środki produkcji były uspołecznione, a gospodarka według ideologii marksistowskiej miała być nastawiona na sprawiedliwy podział dóbr, a nie na zysk właścicieli kapitału. Socjalizm w tym systemie miał być drogą do komunizmu, który w praktyce nigdy nie został zrealizowany jako społeczeństwo bezklasowe.
Pieniądz – towar uznany w wyniku ogólnej zgody społecznej za środek wymiany gospodarczej, w którym są wyrażone ceny i wartości wszystkich innych towarów. Jako waluta, krąży anonimowo od osoby do osoby i pomiędzy krajami, ułatwiając wymianę handlową. Innymi słowy jest to materialny lub niematerialny środek, który można wymienić na towar lub usługę. Prawnie określony środek płatniczy, który może wyrażać, przechowywać i przekazywać wartość ściśle związaną z realnym produktem społecznym[1][2].
Według współczesnej doktryny na pieniądz składają się trzy elementy[3][4]:
Suma pieniężna jest wyrażana w jednostkach pieniężnych, zmaterializowanych w postaci znaków pieniężnych.
Pieniądz jest powszechnym „środkiem wymiany” w transakcjach sprzedaży. Każdy przyjmuje go za sprzedane towary i usługi, wiedząc o tym, że za pieniądze będzie mógł nabyć inne niezbędne mu dobra (materialne lub usługi). Dzięki pieniądzowi nastąpiło rozdzielenie w czasie transakcji kupna-sprzedaży na dwie odrębne czynności – transakcję kupna oraz transakcję sprzedaży – przez co przestała istnieć konieczność zachowania jedności czasu i miejsca. Towar lub usługa mogą być sprzedane, a za otrzymane pieniądze można dokonać zakupu towaru lub usługi w innym miejscu oraz w innym terminie.
Pieniądz jest „miernikiem wartości”. Przy pomocy pieniądza możliwe jest wyrażenie wartości innych towarów (w postaci cen towarów w jednostkach pieniężnych), ponieważ jest powszechnym ekwiwalentem. Cena jest pieniężnym wyrazem wartości towarów i usług. Wyrażenie wartości towarów i usług w pieniądzu wiąże się z siłą nabywczą pieniądza. Aby określić cenę towaru lub usługi nie trzeba posiadać pieniądza, gdyż pełni on rolę miernika wartości również abstrakcyjnie.
Pieniądz jest „środkiem płatniczym”. Pieniądz stał się środkiem płatniczym poprzez oddzielenie się ruchu towarów i świadczonych usług w czasie od ruchu pieniądza. Dzieje się tak dlatego, że zapłata za towar lub usługę nie musi następować natychmiast po ich dostawie. Zarówno sprzedający, jak i kupujący zazwyczaj umawiają się co do terminu zapłaty należności za dostarczone towary lub usługi. Powstaje więc zobowiązanie odbiorcy wobec dostawcy, które wyrównane będzie w formie pieniężnej w terminie późniejszym. Pieniądz spełnia funkcję środka płatniczego również przy pokrywaniu innych zobowiązań, jak np. z tytułu podatków i opłat, wynagrodzeń pracowników, spłaty kredytów, darowizn itp.
Wraz z rozwojem społeczno-gospodarczym funkcja pieniądza jako środka płatniczego ciągle wzrasta, natomiast jako środka wymiany, maleje. Transakcje gotówkowe występują w obrocie gospodarczym coraz rzadziej, gdyż są zastępowane powszechnie przez obrót bezgotówkowy.
Funkcję „środka przechowywania wartości (tezauryzacji)” pieniądz spełnia wtedy, gdy środki pieniężne uzyskane ze sprzedaży towarów lub usług nie są przeznaczane na zakup innych towarów lub pokrycie zobowiązań, lecz są przechowywane (oszczędzane).
Przechowywanie pieniędzy jest znacznie łatwiejsze niż innych towarów. Ponadto pieniądz może być w każdej chwili zamieniony na potrzebny towar. Jest on najbardziej płynnym aktywem majątkowym każdego podmiotu gospodarczego lub osoby fizycznej. Można go w każdej chwili wykorzystać jako środek płatniczy. Jednak aby pieniądz prawidłowo spełniał funkcję tezauryzacyjną, musi posiadać zaufanie podmiotów gospodarczych i ludności, w szczególności zaś musi przeważać przekonanie, że jego siła nabywcza nie zmniejszy się w znacznym stopniu[5].
Pieniądz jest środkiem determinującym ludzkie zachowania, ponieważ jest ekwiwalentem wszystkiego, co jest człowiekowi niezbędne do życia. Kształtuje pragnienia, sposób myślenia, styl życia i konsumpcji. Stał się przedmiotem pragnień, co nie zawsze prowadzi do pozytywnych zjawisk (np. chciwość, skąpstwo, przekupstwo itp.).
Chęć posiadania pieniądza motywuje ludzi do podejmowania działań w celu jego pozyskania. Działania te mogą mieć dwojaki charakter:
Funkcja motywacyjna pieniądza jest powszechnie stosowana przez pracodawców w celu pobudzenia pracowników do efektywniejszej pracy.
Pieniądze stanowią źródło informacji o sytuacji ekonomicznej państwa, które je emituje. Z kolei wygląd, technika wykonania, oraz materiał z jakiego został wykonany pieniądz informują o poziomie kultury danego społeczeństwa.
Płaca natomiast nie jest jedynie miarą wysiłku pracownika, ale informacją co we własnym i społecznym interesie zrobił.
Pieniądz powoduje rozpad struktur społecznych i powstawanie nowych. Dążenie do wzbogacenia się wywołuje wiele różnic między grupami społecznymi. Uwidacznia się rozwarstwienie społeczeństwa pod względem ilości posiadanego pieniądza, co za tym idzie władzy, wpływów, sławy itp.
Pokusa zdobycia pieniędzy za wszelką cenę determinuje rozwój przestępczości. Środki z nielegalnej działalności wprowadzane do państwowego obiegu wpływają niekorzystnie na stabilność finansową państwa.
Pieniądz wpływa na powstanie i kształtowanie się stosunków między ludźmi. Na ich podłożu powstają instytucje społeczno-finansowe (banki, giełdy, fundacje, zakłady ubezpieczeniowe), które mają za zadanie zaspokajanie społecznych potrzeb. Pieniądz jest też efektywnym narzędziem kontrolowania działalności tych instytucji[6].
W kulturach pierwotnych pojęcie pieniądza jako osobnego środka płatniczego nie istniało. Podstawowym sposobem nabywania dóbr był barter, czyli wymiany na zasadzie przedmiot za przedmiot. Jeden towar wymieniano za inny o niekoniecznie takich samych właściwościach. Szybko jednak zorientowano się, iż nie zawsze taka wymiana towarami była sprawiedliwa, gdyż wartość przedmiotów często nie była taka sama i nie prowadziła do zaspokojenia wszystkich potrzeb. Bezpośrednia wymiana towaru na towar była również uciążliwa. Z czasem więc wprowadzono tzw. pośredników wymiany, którymi zwykle były: sól, zboże, skóry itp. Dobra te były podzielne, trwałe, jednorodne i rzadko występujące. Były to przedmioty ułatwiające zaspokojenie podstawowych potrzeb biologicznych, dlatego też szybko ten sposób wymiany za owe dobra przyjął się w ówczesnym świecie. Podczas tego rodzaju wymiany zachodziły dwie ważne czynności: kupno i sprzedaż, dlatego też możemy powiedzieć, że produkty konsumpcyjne są pierwotną formą pieniądza. Ludzie zaakceptowali taki sposób wymiany, a akceptacja ze strony społeczeństwa jest podstawą by uznać towary konsumpcyjne pieniądzem. Poza tym towary owe można było swobodnie dzielić, a także ich trwałość użytkowa była zadowalająca, co skutecznie wpłynęło na akceptacje „pierwotnego pieniądza”[7].
W procesie rozwoju gospodarki i cywilizacji z biegiem czasu miejsce produktów konsumpcyjnych zajęły metale – najpierw nieszlachetne (brąz, miedź, żelazo) potem szlachetne (srebro, złoto, rzadziej platyna). Metale gwarantowały trwałość użytkową oraz wartość dlatego też stały się formą pieniądza. Od wprowadzenia metali (głównie szlachetnych), jako formy pieniądza wykształciło się pojęcie „pieniądz”. O zastąpieniu towarów konsumpcyjnych, metalami zadecydowały właściwości metali. Były one znacznie bardziej trwałe niż towary konsumpcyjne oraz ich wielkość znacznie mniejsza od towarów co znacznie ułatwiało transport.
Jednak z biegiem czasu, metale zaczęły być uciążliwe. Wymiary metali nie zawsze były takie same, sztaby złota czy srebra zwykle były dużych rozmiarów, a ważenie i dzielenie ich na mniejsze kawałki (pieniądz odliczany) zwykle zajmowało wiele czasu. Pojawiła się więc praktyka by te sztabki złota, czy srebra lub innych metali szlachetnych dzielić, co okazało się trafne. Zaczęto owe metale rozdrabniać na małe kawałki, które zwykle przyjmowały kształt pełnych lub spłaszczonych kulek. Z biegiem czasu zaczęto na nich bić pieczęcie znanych władców, królów by zapobiec fałszowaniu (zmniejszeniu faktycznej ilości danego metalu w kawałku) przy tworzeniu kuleczek metalu. Po nabiciu odpowiednich podobizn, posiadacz nabitych kawałków metalu był świadom i pewien wartości danego „pieniądza”. Przez setki lat kulki metalu, które pełniły funkcje pieniądza były tworzone z kilku metali, jak brąz, srebro, złoto. Z biegiem czasu system bicia metali się wykrystalizował do użytkowania dwóch metali: złota i srebra. Takie kuleczki metalu z nabijanymi podobiznami nazwane zostały monetami[8].
Twórcami monet prawdopodobnie byli Fenicjanie. Ich miasta: Ugarit, Byblos, Sydon i Tyr to symbole bogactwa, którego źródłem był handel prowadzony niemal ze wszystkimi państwami ówczesnego świata. Jako jedni z pierwszych na świecie opanowali wytop i produkcję wyrobów z brązu. Jednak dzisiejszy stan wiedzy nie potwierdza tej tezy, najstarsze bowiem znalezisko, pochodząca z VII wieku p.n.e. bryłka elektrum (w starożytności nazywano tak stop złota i srebra) opatrzona jest stemplem złotnika z Efezu. Gdy pojawiły się takie prywatne pramonety, już tylko krok dzielił ludzi od wprowadzenia na rynek pieniędzy bitych przez władze państwowe. Monetę wynaleziono prawie jednocześnie w VII w. p.n.e. w kręgu cywilizacji greckiej: w Lidii, położonej na zachodnich wybrzeżach Azji Mniejszej (dziś Turcji), oraz w Argolidzie (Peloponez), państwie Fejdona, do którego należała też bogata w pokłady srebra wyspa Egina. Lidyjskie monety z VII i VI wieku p.n.e. są wykonane z elektrum, po jednej stronie widać na nich wizerunki byka i lwa, a po drugiej kwadratowe wgłębienie spowodowane niedoskonałą jeszcze techniką bicia. Natomiast w Argolidzie używano monet srebrnych. Miały one wybity symbol państwowy, którym był żółw morski (zwierzę poświęcone bogini Afrodycie), a kształtem przypominały spłaszczoną baryłkę.
Jednak wytwarzanie pieniądza ze złota i srebra było kosztowne, a ponadto ilość owych metali nie była wystarczająca do zaspokojenia istniejącego zapotrzebowania. Zaczęto więc swoje zasoby pieniężne zostawiać u złotników, którzy wypisywali tzw. kwity depozytowe, które dowodziły ilości posiadanego złota czy też srebra przez osobę posiadającą kwit. Nabywcy kwitów mogli oprocentować swoje złoto czy srebro. Kwity depozytowe były aktywem finansowym pojawiającym się w ewidencji dwóch podmiotów: złotnika oraz posiadacza kwitu. Złotnik zobowiązany był do wypłaty określonej kwitem kwoty okazicielowi dokumentu. Tak to też narodziła się kolejna forma pieniądza – pieniądz papierowy – gdyż ludzie coraz częściej regulowali własne długi oraz należności owymi kwitami depozytowymi.
Po pewnym czasie złotnicy zostali bankierami, gdyż zaczęli emitować coraz więcej owych kwitów. Kwity te coraz częściej nie miały pokrycia w złocie czy srebrze – wprowadzane były do obiegu jako zadłużenie wobec bankierów, a odsetki od nich stanowiły dodatkowy dochód dla złotników. W następstwie kwity zostały zaakceptowane przez państwo, które wprowadziło rozporządzenia, dzięki którym owe „bilety” stały się powszechnym środkiem wymiany handlowej. Następnie ustalono bank, który miał emitować banknoty, które zostały środkiem płatniczym, co zażegnało chaos, który powstał, gdy każdy bank rozpoczął rozprowadzanie swoich banknotów. Później banki emisyjne w większości krajów upaństwowiono, wiążąc je w centralną władzę monetarną[9][10]. Papierowe pieniądze pojawiły się w Chinach, potem w Europie w XVII w.[11]
Kolejne etapy rozwoju pieniądza wiążą się z powstaniem i rozwojem pieniądza bezgotówkowego, w tworzeniu którego największą role odegrali Fenicjanie. Stworzyli oni namiastki bezgotówkowych form rozliczeń w postaci uwierzytelnionych tabliczek służących do dokonywania operacji finansowych. W późniejszym czasie ludzie nie chcieli trzymać banknotów w domu bo było to niewygodne, dlatego oddawali swoją gotówkę do banku w formie wkładu na rachunek bankowy. Można wyróżnić dwa typy rachunku bankowego: (1) na żądanie, gdzie klient mógł w każdej chwili podjąć gotówkę ze swojego rachunku i (2) terminowe, złożone w banku na określony czas, przed upływem którego nie mogli podjąć gotówki. Następnie pojawiły się takie możliwości, jak polecenie przelewu – gdzie bank przekazywał pieniądze na rachunek innej osoby oraz czek bankowy, za pomocą którego okaziciel czeku mógł wypłacić określoną kwotę z banku. Pieniądz bezgotówkowy spowodował kształtowanie się zaufania społeczeństwa do pieniądza oraz instytucji bankowych[12].
Obecnie popularnym środkiem płatniczym staje się pieniądz elektroniczny. Istota tej formy płatniczej polega na zapisie w pamięci komputera rachunków bankowych, a wpłaty, wypłaty, rozliczenia są realizowane za pomocą kart magnetycznych, na których zapisywane są ruchy związane z naszymi finansami. Regulowanie płatności finansowych za pomocą kart magnetycznych może następować wprost z domu, owymi kartami możemy płacić w sklepach, stacjach paliw, a także za pomocą bankomatów możemy wypłacać gotówkę z naszego konta bankowego[13].
Funkcjonowanie pieniądza fiducjarnego jest uzależnione od systemu bankowego. Płatności elektroniczne nie zawsze funkcjonują, mogą być zablokowane. Przelewy międzybankowe trwają od kilku godzin do kilku dni, podlegają kosztom transakcyjnym (szczególnie przelewy międzynarodowe), kontroli organów państwa.
Pieniądz papierowy podlega inflacji, a więc nie jest dobrym środkiem dla tezauryzacji (przechowywania wartości), ponieważ może być dodrukowany przez państwo.
W celu wyeliminowania tych wad walut tradycyjnych wynaleziono w 2009 roku pierwszą kryptowalutę Bitcoin, a wkrótce potem kolejne kryptowaluty. Bitcoin jest dobrem rzadkim (nie powstanie nigdy więcej niż 21 milionów jednostek waluty) oraz bardzo szybkim i łatwym do przesłania (transfer możliwy nawet w ciągu 10 minut). Waluta staje się legalnym środkiem płatniczym w kolejnych krajach[14].
Organizacja Krajów Eksportujących Ropę Naftową[1] (ang. Organization of the Petroleum Exporting Countries, OPEC) – organizacja międzynarodowa krajów producentów ropy naftowej z siedzibą w Wiedniu. Jej celem jest ujednolicenie polityki dotyczącej wydobycia oraz wpływanie na poziom cen ropy naftowej. Członkami OPEC jest 13 państw[2].
Została utworzona w 1960 w Bagdadzie[1]. Największy wzrost znaczenia OPEC przypada na okres kryzysu naftowego w latach 1973–1974, kiedy to organizacja doprowadziła do znaczącego wzrostu cen ropy naftowej.
OPEC podejmuje decyzje przez Konferencję OPEC, na szczeblu ministrów odpowiedzialnych za energetykę[1]. Odbywają się one zwykle co pół roku[1].
Sekretarzem generalnym OPEC (szefem stałego Sekretariatu) od sierpnia 2022 roku  jest pochodzący z Kuwejtu  Haitham Al-Ghais.
Członkowie w 2020 r.[2]:
Byli członkowie[2]:
W 2020 państwa członkowskie OPEC posiadały ok. 40-procentowy udział w rynku sprzedaży ropy naftowej oraz ok. 70 procent światowych rezerw tego surowca[3].
Aborcja (łac. abortus lub abortio[1] „poronienie, wywołanie poronienia”) – zamierzone zakończenie ciąży w wyniku interwencji zewnętrznej, np. działań lekarskich[2][3], w j. łacińskim abortus provocatus[4]. Przeważnie w efekcie dochodzi do śmierci[5] zarodka lub płodu[6] (łac. nasciturus).
Wykonywanie aborcji w wielu krajach jest regulowane prawnie. Aborcja legalna, czyli wykonana w zgodzie z obowiązującymi w danym państwie ustawami i przez dyplomowanego lekarza, nazywana jest po łacinie abortus provocatus lege artis. Aborcja nielegalna, czyli wykonana przez osobę nieuprawnioną lub niezgodnie z regulacjami prawnymi nosi nazwę abortus provocatus criminalis[7]. Ustawodawstwo w tym zakresie różni się od siebie w różnych krajach. Zarówno sama aborcja, jak i metody wprowadzania jej do ustawodawstwa są przedmiotem licznych kontrowersji[8][9].
Synonimy aborcji[10]: sztuczne poronienie, przerwanie ciąży, usunięcie ciąży, spędzenie płodu, zabieg[3], łyżeczkowanie[3], czyszczenie[3], potocznie skrobanka[3][niewiarygodne źródło?]; dla części środowisk synonimem jest także zabicie dziecka w okresie życia prenatalnego[11].
W języku polskim nie używa się słowa aborcja w znaczeniu poronienia samoistnego[3]. Zwraca się uwagę na właściwy dobór słów w rozmowach lekarz-pacjentka i lekarz-rodzice w przypadku, gdy mamy do czynienia z kobietą, która poroniła samoistnie[3]. Nawet neutralne słowa medyczne (łyżeczkowanie, czyszczenie jamy macicy, zabieg) mogą być negatywnie odbierane przez kobietę, która uważa poronienie za osobistą tragedię[3]. Także termin medyczny wyskrobiny używany przez lekarzy dla określenia materiału pobranego podczas łyżeczkowania jamy macicy jest bolesny i nie do zaakceptowania dla kobiety roniącej[3]. Termin medyczny łyżeczkowania jamy macicy o neutralnym charakterze to abrazja[3].
W języku łacińskim brakuje rozróżnienia na poronienie i aborcję – występuje tylko słowo abortus[3]. W związku z tym w oficjalnych dokumentach także w przypadku poronienia może zostać wypisane słowo abortus[3].
Określanie dwóch różnych zagadnień medycznych (poronienia i aborcji) tym samym terminem (aborcja, abortus), mieszanie obu terminów (określanie aborcji jako sztuczne poronienie) oraz używanie tych samych określeń dla czynności medycznych w obu przypadkach (zabieg, łyżeczkowanie, czyszczenie jamy macicy) może prowadzić do mylenia poronienia z aborcją i błędnego nazywania aborcji poronieniem lub przechylenia w odwrotnym kierunku i podawania przekazu, iż poronienie jest aborcją[3].
Aborcja nie jest tym samym co poronienie[3]. Samoistne, przedwczesne zakończenie ciąży nazywane jest poronieniem (jeśli ma miejsce do 22. tygodnia ciąży) lub porodem przedwczesnym (jeśli ma miejsce po 22. tygodniu ciąży)[12]. W przypadku niezupełnego poronienia samoistnego dokonuje się usunięcia obumarłego płodu lub resztek tkankowych, aby uniknąć powikłań zdrowotnych zagrażających życiu kobiety[3]. W przyrodzie występuje tylko poronienie[3]. Około 60% wszystkich poczęć kończy się samoistnie jeszcze przed zakończeniem pierwszego trymestru ciąży, z czego spora liczba jeszcze przed wystąpieniem opóźnionej miesiączki, co określa się terminem ciąża subkliniczna albo aborcja menstruacyjna[12].
W dużej liczbie przypadków kobiety, które decydują się na wykonanie aborcji podają kilka współistniejących powodów, które skłoniły je do podjęcia tej decyzji[14]. Przerywanie ciąży dokonywane jest z następujących przyczyn[14]:
Przyczyny osobiste (52,35% w 2009) i te związane z problemami rodzinnymi i małżeńskimi (25,16% w 2009) należą do najczęstszych w Belgii[14]. Z powodów ekonomicznych wykonano w 2009 roku 15,05% aborcji[14]. Powody medyczne związane z zagrożeniem zdrowia matki lub wadami u dziecka stanowiły mniej niż 4%[18], zaś aborcji ze względu na poczęcie w wyniku gwałtu lub stosunku kazirodczego dokonano w 0,24% przypadków (dane z 2009 dla Belgii)[14].
Wybór procedury obejmuje metody farmakologiczne oraz chirurgiczne (najczęściej próżniowe odessanie zawartości jamy macicy)[19]. Obie metody są efektywne, lecz posiadają zarówno zalety jak i wady – ostateczna decyzja zależy od wieku ciąży oraz opinii samej kobiety[20]. Przed wykonaniem aborcji (w krajach, w których przeprowadzana jest także na życzenie) pacjentka odbywa najpierw rozmowę z lekarzem na temat możliwych powikłań poaborcyjnych oraz dostępnych metod zapobiegania ciąży, które pozwolą jej uniknąć w przyszłości podobnej sytuacji. Zostaje również poinformowana o innych możliwościach, np. urodzeniu i oddaniu dziecka do adopcji[6]. Podczas rozmowy lekarz musi upewnić się, czy kobieta nie została zmuszona do usunięcia ciąży przez osoby trzecie. Następnie kobieta otrzymuje wymagane ustawowo kilka dni do namysłu. Dopiero po upływie tego czasu może zostać wykonana aborcja, a kobieta tuż przed zabiegiem potwierdza swoją wolę na piśmie[6]. W Holandii dziewczęta mające mniej niż 16 lat muszą uzyskać pisemną zgodę rodziców, przy czym istnieją organizacje, które udzielają pomocy i pośredniczą w takich rozmowach[6].
Tzw. pigułka aborcyjna (składnik aktywny: mifepriston) pojawiła się na rynkach farmaceutycznych w latach 1986-1988 pod nazwą Mifegyne[26][27]. Aborcja farmakologiczna jest możliwa na najwcześniejszym etapie (do 9. tygodnia ciąży). Polega na podaniu środków farmakologicznych wymuszających poronienie. Pierwsza dawka zawiera lek, który niszczy trofoblast lub uszkadza zarodek (mifepriston lub metotreksat), a druga dawka zawiera terapeutyk z grupy prostaglandyn (mizoprostol lub gemeprost), wywołujący skurcze macicy i prowadzący do wydalenia zarodka wraz z wyściółką macicy. Najczęściej stosowana jest kombinacja mifepristonu i prostaglandyny.
Mifepriston jest antyhormonem (antagonistą) w stosunku do progesteronu, wiąże się z receptorem progesteronowym dwukrotnie silniej niż naturalny odpowiednik[28]. Prowadzi do:
W 36 do 48 godzin po zażyciu mifepristonu podaje się prostaglandynę, która wywołuje skurcze macicy i przyśpiesza usunięcie zarodka[21].
W 95,3% przypadków dochodzi do całkowitej aborcji bez konieczności interwencji chirurgicznej[21]. U 2,8% kobiet sztuczne poronienie występuje zaraz po podaniu mifepristonu, a przed prostaglandyną[21]. W 2,8% przypadków aborcja jest niecałkowita, a w 1,2% ciąża dalej się rozwija – w tych przypadkach konieczna jest interwencja chirurgiczna[21]. U 0,7% kobiet po aborcji farmakologicznej występują bardzo silne krwawienia[21]. W kwerendzie przeprowadzonej na podstawie danych z amerykańskiej FDA stwierdzono silne krwawienia z dróg rodnych w 39% przypadkach, zaś infekcje w 11%[30]. Odnotowano kilka przypadków śmiertelnego zakażenia bakterią Clostridium sordellii, u osób które zastosowały mifepriston[31].
Przeprowadzone meta-analizy wskazują, że obie metody mają podobną skuteczność, są akceptowane przez pacjentki a bezwzględna ilość powikłań pozabiegowych pozostaje niska[32][33]. Aborcja farmakologiczna może nieść za sobą jednak czterokrotnie większe ryzyko bezpośrednich powikłań niż chirurgiczna[34]. Mizoprostol lub metotreksat mogą być użyte pojedynczo, ale mają niższą skuteczność i na poronienie trzeba czekać do 2 tygodni.
Przez cały okres ciąży stosuje się rozmaite połączenia metody podciśnieniowego opróżniania macicy z mechanicznym zniszczeniem zarodka/płodu i usunięciem jego resztek[24].
W ostatnim trymestrze używana jest zazwyczaj metoda wywołania przedwczesnego porodu lub niszczącego płód odpowiednika metody cesarskiego cięcia. Płód jest najczęściej uśmiercany wewnątrz macicy, przed rozpoczęciem właściwej operacji. Do późnych aborcji zalicza się również wstrzykiwanie roztworu chlorku potasu do układu krwionośnego płodu, powodujące jego śmierć (zatrzymanie akcji serca w rozkurczu). W wyniku aborcji tą metodą może dojść do urodzenia żywego, zdolnego do przeżycia płodu – w Wielkiej Brytanii zdarza się około pięćdziesięciu takich przypadków rocznie (0,67% wszystkich aborcji po 18. tygodniu). Ze względu na późny wiek dopuszczalnego przerwania ciąży (24 tydzień) życie wielu z nich można byłoby zachować przy zastosowaniu intensywnej opieki medycznej[35].
Wszystkie metody aborcji wykonuje się bez znieczulenia płodu. Aktualnie uważa się, że płód ze względu na niekompletny rozwój układu nerwowego odczuwa ból dopiero w III trymestrze ciąży[36], również dopiero wtedy kora mózgowa jest dość rozwinięta, by przynajmniej teoretycznie mógł on posiadać świadomość[37].
Na bezpieczeństwo zabiegu przerywania ciąży oraz obciążenie psychiczne pacjentki największy wpływ ma jego status prawny[38], a w mniejszym stopniu dostępność oraz przyzwolenie społeczne[39]. Przy badaniu niekorzystnych skutków aborcji należy rozróżnić tzw. "aborcję bezpieczną" (wykonywaną w krajach, gdzie jest ona dopuszczalna prawnie, a cała procedura spełnia obowiązujące standardy) od "aborcji niebezpiecznej" (wykonywanej nielegalnie; własnoręcznie lub w tzw. podziemiu aborcyjnym)[40][41]. Brak fachowej pomocy medycznej wiąże się z utrzymywaniem się na wysokim poziomie liczby potencjalnie śmiertelnych powikłań: m.in. krwotoków, infekcji oraz zatruć środkami farmakologicznymi. Zniesienie karalności samej procedury jest koniecznym, choć nie wystarczającym krokiem do odwrócenia tej tendencji[42][43]. Nie istnieją jednoznaczne dowody na to, że legalizacja aborcji wpływa na obniżenie poziomu śmiertelności kobiet. Przykładem kraju, w którym w warunkach legalizacji aborcji zaobserwowano zwiększenie wskaźnika śmiertelności matek jest RPA. W roku 1995 (przed legalizacją aborcji, która nastąpiła w 1996 roku) wskaźnik śmiertelności matek na 100 000 żywych urodzeń wynosił: 260, w 2000 roku – 330, w 2005 roku – 360, w 2010 roku – 300[44]. Podobna sytuacja miała miejsce np. w Gujanie, gdzie legalizacja aborcji w 2000 r. nie wpłynęła na obniżenie wskaźnika śmiertelności kobiet. W 1995 wskaźnik śmiertelności matek wynosił 170 na 100 000 żywych urodzeń, w 2000 – 220, 2005 – 280, 2010 – 280[44]. Aborcja w krajach rozwiniętych jest uznawana za procedurę stosunkowo bezpieczną dla kobiety. Ryzyko śmierci z powodu aborcji w USA jest 12,5 razy mniejsze niż w przypadku urodzenia żywego[45].
Z kolei badania nad śmiertelnością kobiet związaną z ciążą prowadzone w 2004 roku wykazały, że zgony związane z poronieniem lub aborcją były częstsze niż związane z udanym porodem[46]. Analiza porównawcza przypadków z Finlandii i Kalifornii wykazała, że liczba zgonów związanych z ciążą jest 2-4 razy wyższa u kobiet po zabiegu aborcji niż u kobiet, które donosiły ciążę[47]. Wyniki te nagłośniono, ponieważ w sprawie sądowej Roe v. Wade, która doprowadziła do legalizacji aborcji ze względów społecznych w USA jednym z głównych argumentów było to, że aborcja we wczesnej fazie ciąży stwarza kilkunastokrotnie mniejsze ryzyko dla zdrowia i życia kobiety niż jej donoszenie[48].
W 2006 roku aborcja była 4. najczęstszą przyczyną śmierci kobiet w ciąży (średnio 3,9% przypadków w Afryce, 5,7% dla Azji, 12,0% dla Ameryki Łacińskiej w stosunku do 8,2% w krajach rozwiniętych)[49][50]. Rocznie dokonuje się 20 milionów niebezpiecznych aborcji, z czego 97% w krajach rozwijających się[42]. Na ich skutek rocznie umiera 70 tys. kobiet, a 5 milionów doznaje uszczerbku na zdrowiu[51]. Przerwanie pierwszej ciąży w życiu kobiety nie ma znaczącego wpływu na jej późniejszą zdolność rozrodczą i nie prowadzi do bezpłodności, chociaż może nieznacznie podnieść ryzyko przedwczesnego porodu i niskiej wagi dziecka[52].
Karalność aborcji skutkuje gorszą opieką medyczną dla kobiet w ciąży (lekarze boją się je operować ze względu na możliwe oskarżenie o dokonanie aborcji). Stwierdziły to m.in. komitet ds. Eliminacji Dyskryminacji Kobiet i komitet ds. Praw Człowieka[53] (obydwa działające przy ONZ).
Bezpośrednie powikłania po aborcji obejmują takie problemy jak: krwotok, uszkodzenie macicy, uszkodzenie szyjki macicy, infekcja narządu rodnego[54]. Powikłania te występują w kilku procentach przypadków[55][56]. Poważnym powikłaniem jest też niekompletna aborcja wymagająca dalszej interwencji chirurgicznej. Z badań przeprowadzonych w latach 2007-2008 w Nowej Zelandii wynika, że usunięcie pozostałości tkanek w macicy było niezbędne w 10% przypadków po aborcji farmakologicznej i w 1% po aborcji chirurgicznej. U 16% kobiet, które zdecydowały się na aborcję farmakologiczną, konieczne było chirurgiczne usunięcie łożyska[57].
W Szkocji powikłania podczas ciąż po aborcji farmakologicznej są porównywalnie częste z tymi podczas pierwszej ciąży, natomiast rzadsze od tych po aborcji chirurgicznej[58]. Jednym z najczęstszych ubocznych skutków aborcji jest wzrost ryzyka przedwczesnego porodu; z badań EUROPOP wykonanych w 10 europejskich krajach wynika, że w przypadku skrajnego wcześniactwa – aborcja zwiększa to ryzyko o 50%. Natomiast doświadczenie dwóch lub więcej aborcji zwiększa ryzyko przedwczesnego porodu o 63%[59][60]. Przebyta aborcja powoduje wzrost ryzyka wystąpienia ciąży pozamacicznej[61] oraz łożyska przodującego. Badania przeprowadzone na Tajwanie (37 000 przypadków ciąż) wykazały, że łożysko przodujące występuje 1,3-3 razy częściej u tych kobiet, które przeszły aborcję[62]. Zwiększa też ryzyko poronienia w kolejnej ciąży[63] oraz urodzenia martwego dziecka. Wykazaliśmy, że późna aborcja, zwłaszcza wykonana ze względu na wady płodu, zwiększa w sposób oczywisty wskaźnik martwych urodzeń – napisali brytyjscy badacze po przeanalizowaniu rejestrów medycznych za lata 1994-2005, dotyczących zarówno późnych aborcji, jak i martwych urodzeń[64]. Powoduje też wzrost ryzyka wtórnej niepłodności[65]. Z holenderskich badań przeprowadzonych na 6149 parach wynika, że przebyta aborcja zwiększa o 60% ryzyko patologii dotyczącej jajowodów, będącej częstą przyczyną niepłodności[66].
Aborcja farmakologiczna przy pomocy mifepristonu zwiększa ryzyko śmiertelnego zakażenia bakterią Clostridium sordellii. Łącznie w USA odnotowano mniej niż 10 zgonów z tej przyczyny[31]. Aby zapobiec tego typu powikłaniom, przed usunięciem ciąży zaczęto podawać prewencyjnie odpowiednie antybiotyki[67].
Istnieje hipoteza mówiąca, że u kobiet, które poddały się zabiegowi aborcji, częściej występuje rak piersi[68]. Była ona często wykorzystywana jako argument przeciwko legalności zabiegu przez środowiska pro-life, szczególnie w Ameryce[69]. Kontrowersja przyczyniła się do przeprowadzenia szeroko zakrojonych, prospektywnych analiz klinicznych. Ich wyniki jednoznacznie wykazały brak takiej zależności[70][71]. Kilka z najbardziej uznanych ciał medycznych zgadza się z tym wnioskiem; są to m.in. Światowa Organizacja Zdrowia[72] (WHO), amerykański National Cancer Institute[73], American Cancer Society[74], American College of Obstetricians and Gynecologists[75] oraz brytyjski Royal College of Obstetricians and Gynaecologists[76]. Wcześniejsze niejednoznaczne doniesienia wynikały prawdopodobnie z błędu raportowania (ang. response bias) przy ocenie rozmiaru tego zjawiska[77].
Problematyka dopuszczalności i wykonywania aborcji jest wysoce kontrowersyjna w oczach części społeczeństwa i jest przedmiotem różnych ocen moralnych.
W przekonaniu takich religii jak katolicyzm, prawosławie, konserwatywny protestantyzm[potrzebny przypis], Świadkowie Jehowy[78], buddyzm i hinduizm[potrzebny przypis] aborcja jest niedopuszczalna. Opinię tę podzielają również ruchy pro-life (wyznające stanowisko przeciwne dopuszczalności przerywania ciąży), niektórzy etycy świeccy, środowiska konserwatywne i część centrum politycznego i ideologicznego (inne stanowisko dopuszcza aborcję jedynie pod pewnymi określonymi warunkami, jak ciąża w wyniku zgwałcenia czy uszkodzenia lub wady genetyczne płodu).
Wyraźny zakaz dokonywania aborcji znajdował się w Przysiędze Hipokratesa:
W tekście Deklaracji genewskiej fragment ten został zastąpiony następującym zdaniem:
W tekście Przyrzeczenia Lekarskiego, które jest dziś składane przez lekarzy w Polsce, odpowiedni fragment brzmi:
W Polsce problematyka warunków dopuszczalności aborcji jest od lat 90. XX wieku przedmiotem debaty publicznej. Istotny wpływ na społeczne postrzeganie aborcji ma stanowisko Kościoła katolickiego[79]. Problematyka aborcji wywołuje silne emocje, których wyrazem jest między innymi język debat, odnoszący się do "zabijania nienarodzonych" z jednej strony, a do "prawa kobiet do decydowania o swoim ciele" z drugiej. Kwestia aborcji podnoszona jednak bywa często w doraźnych celach politycznych.
W Stanach Zjednoczonych, gdzie szczególnie silne są ruchy antyaborcyjne (a także w Kanadzie i Australii), miały miejsce zamachy bombowe na kliniki aborcyjne oraz zabójstwa lekarzy dokonujących aborcje (np. morderstwo George'a Tillera), dokonywane przez jej radykalnych przeciwników[80] (m.in. organizację Army of God[81]). Przeciw przemocy opowiedziało się wiele organizacji pro-life[82]. Jednocześnie w USA szczególnie silne są ruchy pro-choice, które są zrzeszone w wielu nieformalnych organizacjach i stowarzyszeniach, np. Catholics for Choice (organizacja zrzeszająca katolików opowiadających się za prawem kobiety do podjęcia decyzji o kontynuowaniu lub przerwaniu ciąży).
Aborcja bywa krytykowana z pozycji ateistycznych i humanistycznych przez osoby wychodzące z założenia, że prawo do życia nie wynika jedynie z przekonań religijnych. Opinie takie prezentowali Pier Paolo Pasolini, Oriana Fallaci czy Giuliano Ferrara, znani z ateistycznych (lub agnostycznych) poglądów[83].
Wielu współczesnych etyków nie widzi jednak w przerywaniu ciąży niczego złego. Najczęściej argumentują na dwa sposoby. Część z nich, np. Judith Jarvis Thomson, twierdzi, że nawet akceptacja pełnego statusu moralnego płodów ludzkich nie implikuje istnienia obowiązku udostępniania przez kobietę swojego ciała, by utrzymać płód przy życiu. Inni filozofowie wskazują z kolei na późniejszy niż zapłodnienie moment, od którego płód uzyskuje status moralny. Na przykład Jeff McMahan﻿(inne języki), który sprawuje funkcję White's Professor of Moral Philosophy﻿(inne języki) na Uniwersytecie w Oksfordzie, twierdzi, że płód nabywa status moralny w momencie, kiedy pojawia się świadomość rozumiana jako zdolność do odczuwania[84], która wedle badań naukowych nie może istnieć przed 20-30 tygodniem ciąży[85]. Zgodnie z tym stanowiskiem aborcja przed uzyskaniem zdolności do odczuwania nie krzywdzi płodu i jest dopuszczalna moralnie nawet z bardzo błahych powodów.
W połączeniu ze zwiększającą się dostępnością badań prenatalnych pozwalających na określenie płci płodu rozpowszechniła się aborcja selektywna (zwłaszcza aborcja ze względu na płeć), dotycząca głównie dziewczynek. Jest ona najpowszechniejsza w Indiach i Chinach, a także wśród imigrantów w krajach rozwiniętych takich jak Wielka Brytania. Pomimo formalnego zakazu wykonywania aborcji selektywnych w większości krajów zachodnich (poza Szwecją) jest ona wykonywana dzięki uzasadnianiu jej względami społeczno-ekonomicznymi[86]. W 2012 roku po prowokacji dziennikarskiej organizacja Live Action zarzuciła sieci klinik Planned Parenthood wspieranie selektywnej aborcji ze względu na płeć[87].
W opublikowanym w 2011 roku artykule Alberto Giubilini i Francesca Minerva argumentowali, że uśmiercanie noworodków w praktyce niczym nie różni się od aborcji i powinno być dopuszczalną procedurą, także w stosunku do noworodków pozbawionych wad rozwojowych[88].
W 2012 roku największe stowarzyszenie lekarzy w Kanadzie – Canadian Medical Association – uznało, że życie ludzkie zaczyna się od momentu urodzin, a nie poczęcia[89].
     Legalna na życzenie, bez granicy wieku ciążowego     Legalna na życzenie, z granicą wieku ciążowego po 17. tygodniu     Legalna na życzenie, z granicą wieku ciążowego do 17. tygodnia     Legalna na życzenie, z niejasną granicą wieku ciążowego     Legalna w przypadku zagrożenia życia, zdrowia fizycznego lub psychicznego matki, gwałtu*, wad płodu lub trudności socjoekonomicznych     Legalna w przypadku zagrożenia życia, zdrowia fizycznego lub psychicznego matki, gwałtu* lub wad płodu     Legalna w przypadku zagrożenia życia, zdrowia fizycznego lub psychicznego matki lub wad płodu     Legalna w przypadku zagrożenia życia, zdrowia fizycznego lub psychicznego matki lub gwałtu*     Legalna w przypadku zagrożenia życia, zdrowia fizycznego lub psychicznego matki     Nielegalna z wyjątkiem zagrożenia życia matki     Nielegalna bez względu na okoliczności     Brak informacjiZgodnie z uchwaloną w 1989 r. przez ONZ Konwencją o Prawach Dziecka dziecko oznacza każdą istotę ludzką w wieku poniżej osiemnastu lat (art. 1) oraz każde dziecko ma niezbywalne prawo do życia (art. 6).
W roku 2004 Europejski Trybunał Praw Człowieka w Strasburgu niejednogłośnie orzekł, że "nienarodzone dziecko nie jest uznawane za ‘osobę’ bezpośrednio chronioną art. 2 Konwencji [prawo do życia], a nawet jeśli nienarodzone ma ‘prawo’ do ‘życia’, to jest ono implicite ograniczane przez prawa i interesy matki"[91].
Konwencja o zapobieganiu i zwalczaniu przemocy wobec kobiet i przemocy domowej w art. 39 zabrania dokonywania aborcji lub sterylizacji kobiety bez jej uprzedniej świadomej zgody.
Amerykańska Konwencja Praw Człowieka z 1969 r. głosi Każda osoba ma prawo do poszanowania jej życia. Prawo to będzie chronione ustawą i, w zasadzie, od momentu poczęcia. (art. 4. 1)[92].
W krajach, w których aborcja jest nielegalna lub mocno ograniczona, zdarza się, że kobiety jeżdżą na zabieg za granicę, np. z Polski do Niemiec, Szwecji i Wielkiej Brytanii, z Ameryki Łacińskiej i Karaibów do Stanów Zjednoczonych czy jeszcze do niedawna z Portugalii do Hiszpanii i Francji lub z Irlandii do Wielkiej Brytanii[93].
Polskie ustawodawstwo dopuszcza przerwanie ciąży w następujących przypadkach[94]:
W przypadku zwierząt aborcję wykonuje lekarz weterynarii, zazwyczaj jest to ostateczność, ale lekarz może wykonać aborcję na życzenie właściciela zwierzęcia[95]. W przypadku zwierząt towarzyszących takich jak psy, koty, zajęczaki, gryzonie stosowane są środki farmakologiczne lecz wykorzystuje się też metody chirurgiczne[96]. Najczęściej jednak stawia się na profilaktykę ciąży w postaci kastracji. U zwierząt gospodarskich takich jak bydło i konie wykonuje się specjalny zabieg nazwany fetotomią[97].
Rocznie na całym świecie wykonuje się 56 milionów aborcji[98]. W 2003 na świecie dokonano prawie 42 mln zabiegów aborcji[99][100]. W UE rocznie przeprowadza się około 1,2 mln zabiegów przerwania ciąży[101]. W latach 1973-2003 dokonano na świecie ok. 1 mld aborcji[102].
Po tym jak Sąd Najwyższy Stanów Zjednoczonych uchylił wyrok w sprawie Roe v. Wade, przywrócono kwestię legalności aborcji na poziom parlamentów stanowych[103].
Zgodnie z danymi CDC[104] w rekordowym pod tym względem roku 1990 przeprowadzono w Stanach 1 429 247 legalnych aborcji, w połowie lat 90. XX wieku ta ilość zaczęła spadać kształtując się obecnie na poziomie ok. 850 tys.
W trakcie kampanii na rzecz legalizacji aborcji w Wielkiej Brytanii (lata 1960-1965) organizacje pro-choice podawały, że na jej terenie dokonuje się rocznie 250 tysięcy nielegalnych zabiegów przerywania ciąży. Tymczasem Rada Królewskiego Towarzystwa Ginekologicznego i Położniczego wydała w 1966 roku następujące oświadczenie: Wielokrotnie podawano, że liczba nielegalnych aborcji wynosi 100 tys. rocznie, najnowsze szacunki mówią nawet o 250 tysiącach. Te liczby – podobnie jak wcześniejsze szacunki mówiące o 50 tys. nielegalnych aborcji rocznie – nie mają żadnych rzeczywistych podstaw.
Po legalizacji aborcji w Wielkiej Brytanii w 1968 r. legalnych aborcji było 23,6 tys., w 1969 54,8 tys. Od momentu legalizacji aborcji w tym kraju do chwili obecnej, liczba zabiegów nigdy nie osiągnęła 250 tys. W 2004 roku w samej Anglii i Walii dokonano 185 415 legalnych aborcji[106]. W 2008 roku ponad 20 tys. kobiet poniżej 25. roku życia miało za sobą dwa lub więcej zabiegi aborcji. Około 3,8 tys. kobiet miało za sobą cztery lub więcej zabiegów aborcji[107].
W 2012 w wyniku prowokacji dziennikarskiej wszczęto kontrolę, która ujawniła, że w 50 klinikach (na 250 przebadanych) łamano prawo, a w niektórych emigrantki poddawano aborcji selektywnej[108].
Japonia należała do pierwszych państw, które zalegalizowały aborcję – miało to miejsce w 1948 roku[109].
W lipcu 2010 roku weszła w Hiszpanii w życie ustawa dopuszczająca aborcję na żądanie kobiety do 14. tygodnia ciąży[110].
W Niemczech aborcję zalegalizowano w 1976 r. Przed legalizacją zwolennicy prawa do aborcji podawali liczbę nielegalnych zabiegów, szacowaną nawet na 3 miliony rocznie. Po legalizacji liczba aborcji wynosiła 54 309 w 1977, oraz 73 548 w r. 1978.
Aborcja została zalegalizowana we Włoszech w 1978. W 2008 dokonano tam oficjalnie 121 406 aborcji[111].
Szacuje się obecnie, że w Chinach na skutek zarówno dobrowolnej jak i przymusowej aborcji, rocznie dokonuje się 13 mln zabiegów aborcyjnych[112][113]. Aborcja jest powszechnie wykorzystywana jako środek do ograniczenia urodzeń mniej pożądanych dzieci płci żeńskiej, w rezultacie w 2010 roku występuje ok. 20% przewaga urodzeń dzieci płci męskiej[114].
Aborcja jest tam dopuszczalna od 1970 roku ze względu na zagrożenie życia lub zdrowia matki, gdy ciąża jest wynikiem gwałtu, gdy płód jest ciężko upośledzony oraz ze względu na szereg względów społecznych, takich jak np. posiadanie już czwórki dzieci.
Na terenie archipelagu obowiązuje duńskie prawo aborcyjne z 1956 roku. Zezwala ono na aborcję jedynie gdy ciąża zagraża życiu matki, powstała w wyniku gwałtu lub gdy płód jest poważnie upośledzony. U kobiety zamężnej aborcja może być wykonana jedynie za zgodą jej męża.
Dokładna liczba wykonywanych w Polsce aborcji nigdy nie była znana. Nawet w okresie PRL polskie przepisy były bardziej restrykcyjne niż w innych państwach socjalistycznych, a co za tym idzie – liczba legalnych aborcji była mniejsza niż np. w Czechosłowacji czy ZSRR.
Nie są znane szacunkowe rozmiary podziemia aborcyjnego w dwudziestoleciu międzywojennym ani statystyki legalnych aborcji z lat 1932-1939. Wiadomo jedynie, iż w 1955 roku – ostatnim, w którym obowiązywały przepisy z 1932 roku – miało miejsce w Polsce 1400[potrzebny przypis] legalnych aborcji.
W roku 1997 – kiedy obowiązywała uznana później za niekonstytucyjną nowelizacja ustawy zezwalająca na usunięcie ciąży z przyczyn społecznych – wykonano w Polsce 3173 legalne aborcje.
Statystyki nielegalnych aborcji dokonywanych obecnie w Polsce są trudne do oszacowania: raporty organizacji feministycznych szacują je na ok. 200 tys. rocznie, zaś organizacje działaczy pro-life na 7-13 tys.[115]
We Francji przepisy dotyczące przerywania ciąży reguluje ustawa z 1975 roku, zwana "ustawą Veil". Dopuszcza ona IVG – dobrowolne przerwanie ciąży na życzenie matki do 12. tygodnia ciąży oraz IMG – medyczne przerwanie ciąży na każdym etapie ciąży określonych sytuacjach (zagrożenie życia matki, deformacja płodu).
W Andorze aborcja jest nielegalna z wyjątkiem zagrożenia życia kobiety[116].
Aborcja w Liechtensteinie jest nielegalna i grozi za nią kara więzienia[117].
W San Marino aborcja jest nielegalna, z wyjątkiem przypadków zagrożenia życia kobiety. Dokonanie aborcji wbrew prawu zagrożone jest pozbawieniem wolności[118].
W Watykanie aborcja jest nielegalna bez względu na okoliczności, a za jej dokonanie grozi kara pozbawienia wolności, której podlega zarówno lekarz przeprowadzający zabieg, jak i matka dziecka[119].
Większość wyznań chrześcijańskich potępia aborcję, która jest traktowana jako morderstwo, a jedynym dopuszczalnym warunkiem jej dokonania jest ciąża, która jednoznacznie prowadzi do śmierci matki (np. ciąża pozamaciczna). Kościół katolicki nie dopuszcza aborcji[120], aczkolwiek zwraca się uwagę na zasadę podwójnego skutku[121]. Najbardziej zróżnicowane postawy wobec aborcji mają Kościoły protestanckie (np. Kościół Episkopalny w USA popiera prawo kobiet do aborcji).
Wielu teologów wczesnochrześcijańskich (m.in. w Didache z I w., Atenagoras w II w.) potępiło aborcję na równi z dzieciobójstwem. Była to wyraźna różnica z prawem rzymskim, które pozwalało zarówno na aborcję jak i zabijanie np. słabych i zdeformowanych noworodków. W III w. teolodzy chrześcijańscy nazwali aborcję zbrodnią. W IV w. Grzegorz z Nyssy wskazał, iż embrion jest żywą istotą, a Jan Chryzostom potępił zmuszanie prostytutek do aborcji.
Teologia chrześcijańska na ogół powołuje się na biblijny zakaz „rozlewania krwi niewinnych” jako podstawę zakazu aborcji. Wskazuje również na przykład Jezusa Chrystusa i Maryi, która mimo bardzo trudnych okoliczności i warunków materialnych, zdecydowała się na urodzenie dziecka. Tym niemniej nie wszyscy teolodzy chrześcijańscy uznają zakaz aborcji za nakaz biblijny.
Współcześnie kościół katolicki uznaje, że dokonanie lub pomaganie w aborcji to grzech ciężki[122]. Dokonanie lub pomaganie w aborcji ma następstwo automatycznej[123][124] ekskomuniki[125] . Za odpowiednik aborcji Kościół uznaje także niszczenie embrionów ludzkich poza organizmem matki, np. w laboratoriach[125] .
Prawosławie podziela pogląd, że życie ludzkie zaczyna się od chwili poczęcia i w związku z tym potępia aborcję (w tym także wywołaną stosowaniem antykoncepcji hormonalnej) jako zabójstwo. Ten pogląd odzwierciedla święto Zwiastowania Bogurodzicy, obchodzone 7 kwietnia – na 9 miesięcy przed Bożym Narodzeniem (odpowiednikiem tego święta u katolików jest święto Zwiastowania Pańskiego).
Protestantyzm reprezentuje bardzo różne postawy wobec aborcji. Ogólnie nie potępia antykoncepcji (z wyjątkiem antykoncepcji hormonalnej "po stosunku płciowym" i innych metod, związanych z niszczeniem zapłodnionej komórki).
Nie istnieje żadna ogólna wykładnia tego problemu, każda grupa wyznawców ustala własny pogląd na zasadzie większościowej. W USA przeważa opinia, iż aborcja jest formą dzieciobójstwa, nawet w przypadkach kazirodztwa i zgwałcenia. Jednak wielu amerykańskich protestantów dopuszcza aborcję w takich przypadkach, a także jeśli życie matki jest zagrożone. W większości amerykańskich Kościołów protestanckich istnieją grupy zwolenników i przeciwników prawa do legalnej aborcji. Wyjątkiem jest Południowa Konwencja Baptystów – zadeklarowany i bezwarunkowy przeciwnik aborcji.
Amerykański Kościół Episkopalny (anglikański) popiera legalną aborcję i prawo kobiet do decydowania o dokonaniu aborcji. Świadczy również pomoc i udziela sakramentów kobietom, które miały wykonaną aborcję. Potępiane są wszelkie działania rządu, ograniczające prawo do aborcji[126]. Kościół dopuszcza stosowanie antykoncepcji hormonalnej, ale sprzeciwia się aborcji selektywnej ze względu na płeć płodu.
Kościół Ewangelicko-Augsburski w RP w oświadczeniu z 1991 roku zdecydowanie opowiedział się za ochroną życia od momentu poczęcia oraz sprzeciwił się stosowaniu środków wczesnoporonnych. Kościół czuje się zobowiązany do wzywania społeczeństwa do podejmowania działań mających na celu ochronę życia, które winny się zaznaczyć m.in. w regulacjach prawnych dotyczących ochrony życia poczętego. Jednak w przypadku zagrożenia życia matki decyzję o aborcji pozostawia rodzicom i lekarzom, ponieważ Kościół nie ma kompetencji, by decydować w takich sytuacjach[127]. Z drugiej strony Kościoły luterańskie w Niemczech i państwach skandynawskich nie wyraziły oficjalnego sprzeciwu, kiedy w krajach tych rządy zdecydowały się przed wieloma laty zalegalizować aborcję. Również i dzisiaj znaczna część duchownych luterańskich w Niemczech i w Skandynawii, jak również w Estonii, Łotwie i w Kanadzie nie potępia jednoznacznie aborcji i nie traktuje jej w kategoriach grzechu. Aborcji przeciwne są Kościoły ewangelikalne zrzeszone w Aliansie Ewangelicznym w RP.
Mormoni zdecydowanie potępiają aborcję, jednak dopuszczają okoliczności, które usprawiedliwiają ją (choć nie automatycznie, lecz po starannym rozważeniu): ciąża jako wynik kazirodztwa lub gwałtu, poważne zagrożenie dla życia matki[128].
Świadkowie Jehowy uważają życie jako cenny dar od Boga i dlatego należy dbać o własne i cudze życie i zdrowie, niedopuszczalne ich zdaniem jest pozbawianie życia nienarodzonego dziecka przez aborcję (Ps 36:9; Wj 21:22, 23)[78][129][130].
Chociaż islam[131] podkreśla ważność życia danego przez Boga (Koran 12:85), to nie potępia aborcji tak jednoznacznie, jak większość wyznań chrześcijańskich. Wynika to z przekonania, iż życie (ar. ruh) nie zaczyna się od momentu zapłodnienia. Prawo islamskie (Szariat), m.in. na podstawie hadisów i koranicznego opisu rozwoju embrionu stwierdza, iż najpierw zarodek istnieje jako zapłodnione jajo, następnie jako „skrzep krwi”; większość muzułmanów przyjmuje, że Bóg obdarza embrion duszą dopiero po upływie 120 dni od zapłodnienia. Aborcja jest więc zakazana dopiero od 4. miesiąca ciąży. Do tego czasu islam sunnicki zezwala na dokonanie aborcji w takich przypadkach, jak: zgwałcenie, kazirodztwo, prawdopodobieństwo ojcostwa osoby upośledzonej umysłowo, różnego rodzaju wady i uszkodzenia embrionu (np. zarażenie różyczką). Szariat zawsze zezwala na aborcję, jeśli zagrożone jest życie kobiety. Zakazana jest aborcja z powodów takich, jak trudne warunki materialne lub wybieranie płci dziecka.
Sunniccy interpretatorzy Koranu (szkoła hanaficka: Turcja, Bliski Wschód, Azja Środkowa), dopuszczają aborcję z inicjatywy kobiety, bez zgody męża i także z tzw. „słusznych powodów”, np. ciąża w czasie karmienia piersią dziecka. Szkoły: Hanbali i Szafi (Afryka, Arabia, płd.-wsch. Azja pozwalają na aborcję do 40. dnia ciąży pod warunkiem, że mąż wyrazi zgodę.
Historycznie, aborcja według interpretacji religijnej imamitów, największej grupy muzułmanów szyickich, była całkowicie zakazana. Z biegiem czasu, aborcję dozwolono tylko w przypadku jeśli zagrożone jest życie kobiety. Ajatollahowie zgodnie orzekają, iż usunięcie ciąży ze względu na różnego rodzaju wady i uszkodzenia embrionu, trudne warunki materialne, wybieranie płci dziecka, lub upośledzenie matki, jest całkowicie zakazane[132].
Obecnie Algieria, Egipt, Iran i Pakistan w swoim prawodawstwie dopuszczają aborcję tylko, jeśli życie kobiety jest zagrożone (co nie oznacza, że aborcje nie są tam w praktyce wykonywane także z innych powodów). Ustawodawstwo najłatwiej dopuszczające aborcję ma Tunezja, która dozwala na aborcję przez uprawnionego lekarza z każdego powodu u kobiet zamężnych i niezamężnych; zgoda męża nie jest konieczna.
Religijne prawo żydowskie[133] halacha traktuje aborcję bardziej rygorystycznie niż islam, ale nie zakazuje jej całkowicie jak większość wyznań chrześcijańskich; ogólnie przyjmuje, że aborcja jest dozwolona tylko, jeśli ciąża stanowi zagrożenie dla życia kobiety (w tym przypadku embrion traktowany jest jako potencjalny „morderca” swojej matki, przed którym ona ma prawo bronić się).
Istnieje jednak wiele interpretacji tego prawa: od przyzwolenia na aborcję w większości przypadków do całkowitego zakazu aborcji. Talmud nie traktuje aborcji jako morderstwa, ponieważ na sprawcę aborcji nakłada tylko karę grzywny. Większość rabinów dopuszcza aborcję, jeśli ciąża stanowi zagrożenie dla zdrowia psychicznego kobiety (np. niebezpieczeństwo samobójstwa). Jako zagrożenie dla zdrowia psychicznego traktuje się ciążę będącą wynikiem zgwałcenia lub kazirodztwa. Z reguły negowana jest dopuszczalność aborcji ze względu na wady i choroby wrodzone embrionu. Wyjątkiem jest rabin Eliezar Yehuda Waldenberg, który twierdzi, że można usunąć ciążę, jeśli embrion ma wady lub chorobę, które po urodzeniu mogą mu przysporzyć znacznych cierpień.
Ogólną zasadą w judaizmie jest traktowanie każdej prośby o aborcję jako indywidualnego przypadku, który trzeba oddzielnie rozważyć i który musi być poważnie umotywowany.
Hinduizm uważa aborcję za zbrodnię (jeden z 6 rodzajów morderstwa) i za jeden z najcięższych grzechów. Aborcja stanowi przeszkodę na drodze do połączenia się duszy (atmana) z Brahmanem. Hinduizm głosi, że zarodek ludzki jest świadomą istotą, godną ochrony, ponieważ dusza inkarnuje się w chwili poczęcia. Opisane jest jak bóg Kryszna skazał Aśwattamę na nieśmiertelność (czyli na wieczną niemożliwość wyzwolenia się z cyklu życia i cierpienia) za usiłowanie zabicia embriona[134].
Buddyzm sprzeciwia się odbieraniu każdego życia ludzkiego i to od momentu poczęcia. Aborcja jest jednoznacznie oceniana jako zabójstwo człowieka i ma takie same negatywne skutki karmiczne[135]. W buddyzmie np. japońskim istnieją określone rytuały, mające na celu ułatwienie ponownych narodzin dzieciom nienarodzonym, które zmarły w wyniku aborcji. Przykładem są tu "Ogrody nienarodzonych dzieci" przy świątyniach buddyjskich np. Zōjō-ji w Tokio (ceremonie Mizuko kuyō)[136].
W dwudziestoleciu międzywojennym w Polsce temat poruszali m.in. Zofia Nałkowska w Granicy oraz Tadeusz Boy-Żeleński w Piekle kobiet.
Granicy dopuszczalnego wieku aborcji i eutanazji jest poświęcone opowiadanie Przedludzie (1974) Philipa K. Dicka.
Niemiecka pisarka Karin Struck w książce Widzę moje dziecko we śnie (1992) opisała swoją traumę, związaną z przeprowadzoną aborcją.
Głośnym echem odbiły się współczesne produkcje filmowe, takie jak dokument Niemy krzyk z 1984 roku przedstawiający zabieg aborcji, a także fabularne: brytyjski Vera Drake (2004), amerykański Bella (2006) czy rumuński 4 miesiące, 3 tygodnie i 2 dni (2007).
Temat aborcji jest poruszany przez grupy muzyczne np.: Creation of Death na albumie Purify Your Soul z 1991 roku (Quartering Alive i Nameless Forever), Oceana na albumie Birth.Eater z 2009 roku (The Family Disease, In Birth i The Abortion Plan), Black Uhuru na albumie Black Uhuru z 1980 roku (Abortion), oraz Houk na albumie Extra Pan z 2000 (Holocaust). Wymienione utwory miały charakter sprzeciwu wobec przerywania ciąży[potrzebny przypis][137][138].
 Artykuł uwzględnia ograniczony pod względem terytorialnym stan prawny na 2022-07-27. Zapoznaj się z zastrzeżeniami dotyczącymi pojęć prawnych w Wikipedii.
Przeczytaj ostrzeżenie dotyczące informacji medycznych i pokrewnych zamieszczonych w Wikipedii.
Język grecki, greka (stgr. Ἑλληνικὴ γλῶττα, Hellenikè glõtta; nowogr. ελληνική γλώσσα, ellinikí glóssa lub ελληνικά, elliniká) – język indoeuropejski z grupy helleńskiej[1], w starożytności ważny język basenu Morza Śródziemnego. W kulturze europejskiej zaadaptowany, obok łaciny, jako język terminologii naukowej, wywarł wpływ na wszystkie współczesne języki europejskie, a także część pozaeuropejskich i starożytnych. Od X wieku p.n.e. zapisywany jest alfabetem greckim.
Obecnie, jako język nowogrecki, pełni funkcję języka urzędowego w Grecji i Cyprze. Jest też jednym z języków oficjalnych Unii Europejskiej. Po grecku mówi współcześnie około 15 milionów ludzi. Język grecki jest jedynym żywym językiem helleńskim.
Określenia „język grecki”, „greka” pochodzą od Rzymian, którzy nazywali ten język po łacinie lingua Graeca. Nazwa stosowana przez samych Greków w grece klasycznej brzmiała ἡ Ἑλληνικὴ γλῶττα = he Hellenikè glõtta (nowogr. η Ελληνική γλώσσα = i Ellinikí glóssa lub τα Ελληνικά = ta Elliniká), co można by przetłumaczyć jako „język helleński”.
Termin „język grecki” jest z naukowego punktu widzenia mało precyzyjny, ponieważ może się odnosić do kilku faz rozwojowych języka, z których każda posiada własną nazwę. Może więc odnosić się do następujących bardziej szczegółowych terminów:
Poniżej przedstawiono uproszczony schemat historii rozwoju poszczególnych odmian języka greckiego:
Gramatyka (stgr. γραμματική τέχνη, łac. grammatica) – uporządkowany zbiór reguł językowych rządzących organizacją zdań, dyskursów, tekstów; innymi słowy zespół prawideł umożliwiających tworzenie złożonych jednostek językowych, ich składanie z jednostek elementarnych[1]. Gramatyka to także dział językoznawstwa zajmujący się badaniem tych wzorców[2]. Struktura gramatyczna stanowi element właściwy dla wszystkich języków i dialektów[3] (w tym języków ludów „prymitywnych”[4]), tworzący podstawę ich funkcjonowania; system ten znany jest wszystkim użytkownikom i przyswajany bez potrzeby formalnego nauczania[5].
Gramatyka to część języka najbardziej trwała i odporna na wpływy zewnętrzne[6], choć również podlegająca zmianom[7]. Jest najwyraźniej ujawniającym się elementem organizacji tekstu, choć nie stanowi głównego ani jedynego obrazu systemu językowego. Charakter gramatyki jest uzależniony od typologii danego języka, a na faktyczny sposób wypowiedzi wpływają też aspekty stylistyczne i semantyczne. Faktyczna łączliwość wyrazów może być zatem bardziej restrykcyjna od samych prawideł gramatycznych[1].
W zakres gramatyki wchodzą: fonetyka, fonologia, morfologia oraz składnia, przy czym w ujęciu tradycyjnym nie uwzględnia się fonetyki i fonologii[2]. Za część gramatyki bywa uznawana także semantyka[8]. Pojęcie gramatyki ma zatem charakter bardzo zmienny, często również pod względem rozciągłości jej przedmiotu[9]; ponadto gramatyka uniwersalna nie stanowi gramatyki w żadnym z powszechnych ujęć, lecz raczej koncepcję uniwersalnych właściwości językowych[10].
W postaci stosowanej gramatykę rozumie się przede wszystkim jako opis struktury, tej części systemu językowego, która odnosi się do sposobu łączenia jednostek, zwłaszcza w kontekście tworzenia zdań. Istnieją jednak różne koncepcje pojmowania gramatyki i rozgraniczania jej elementów. Ferdinand de Saussure uznawał, że zasobu słownego nie można ściśle oddzielić od gramatyki; podejście tradycyjne, absolutyzujące poszczególne części gramatyki, traktuje ją zaś nadal jako pewien konglomerat złożony z tych składników. Skrajne stanowisko zajmuje tu Noam Chomsky, który gramatykę w zasadzie utożsamił ze składnią[1].
Poza kontekstem językoznawczym miano gramatyki przypisuje się rozmaitym aspektom praktyki językowej, a sam termin bywa szczególnie kojarzony z regulacjami poprawnościowymi[11]. Stąd wywodzi się właściwe dla kultury internetowej pojęcie grammar nazi, powiązane z potocznym sposobem rozumienia gramatyki[11]. Potocznie pod pojęcie gramatyki podkłada się nie tylko morfologię czy składnię, ale także wymowę, stylistykę[11] czy nawet ortografię[12]. Normy ortograficzne nie są jednak częścią systemu językowego ani gramatyki, lecz odgórnie narzuconymi przepisami, rządzącymi zewnętrzną szatą języka[12][13].
Gramatyczność (poprawność gramatyczna) to własność struktur językowych (przede wszystkim zdań) polegająca na ich zgodności z zasadami gramatycznymi. Wyrażenia spełniające reguły gramatyki nie muszą być jednak akceptowalne pod względem semantycznym (por. zdanie Ta długa głupota szczekała prostopadle, uwydatniające różnicę między gramatyką a semantyką). Obserwuje się priorytetyzację prawideł semantycznych nad gramatycznymi. Podstawowa komunikacja możliwa jest bowiem nawet bez znajomości gramatyki, umożliwia ją proste zestawienie leksemów (Ja ten klucz jutro oddać ten pani)[10].
Istnieją różne sposoby kategoryzacji rodzajów gramatyk. František Čermák proponuje poniższy podział[10]:
Literatura – wszystkie „sensowne twory słowne” (według definicji Stefanii Skwarczyńskiej), czyli dzieła artystyczne, tj. literatura piękna, oraz teksty użytkowe, tj. literatura stosowana, zachowane w formie pisanej lub w przekazie ustnym[1][2].
Literaturę piękną[3] dzieli się zwykle na trzy rodzaje literackie według tradycji, uzupełnianej później, Poetyki Arystotelesa: epikę (zarówno wierszem, jak i prozą), lirykę i dramat. Do gatunków epickich zaliczamy: opowiadanie, nowelę, bajkę, powieść, legendę, baśń. Do gatunków lirycznych zaliczamy: satyrę, hymn, odę, pieśń, sonet, fraszkę, elegię, tren. Do gatunków dramatycznych zaliczamy: tragedię, komedię, dramat właściwy.
W stosunku do literatury europejskiej i literatur związanych z nią stosuje się też podział na epoki (jak barok, renesans, romantyzm). Stosuje się także podziały na style literackie.
Przyroda, in. natura (łac. natura ‘przyroda’)[1] – w najszerszym znaczeniu wszechświat, rzeczywistość. Termin ten obejmuje także zjawiska fizyczne oraz życie – bez uwzględnienia wytworów i oddziaływania ludzi[2][3].
Wszechświat w kosmologii to nazwa oznaczająca czasoprzestrzeń, która zawiera wszystkie obiekty materialne i energię, dostępne do obserwacji metodami bezpośrednimi lub pośrednimi poprzez teleskopy i inne przyrządy obserwacyjne. Według stanu wiedzy na 2008 słowo wszechświat jest dla naukowców dwuznaczne. Zgodnie z ogólnie przyjętą teorią względności i opartej na niej teorii Wielkiego Wybuchu wiek Wszechświata szacuje się na 13,7 mld lat. Dla niektórych kosmologów obserwacyjnych, jest to sfera o promieniu około 4 × 1026 m (50 mld lat świetlnych), z nami w centrum, a pojęcie ogólnej czasoprzestrzeni nie ma sensu. Dla niektórych fizyków teoretycznych, Wszechświat jest czasoprzestrzenią według modelu Wielkiego Wybuchu w małej skali (8 × 1026 m). Obecne pomiary jednorodności promieniowania tła wskazują, że Wszechświat jest prawdopodobnie płaski i będzie rozprzestrzeniał się w nieskończoność.
Powszechnie dziś przyjętym modelem powstania i ewolucji Wszechświata jest model Wielkiego Wybuchu.
Ziemia – trzecia w kolejności, licząc od Słońca, piąta co do wielkości planeta Układu Słonecznego. Jest największą z planet skalistych w Układzie Słonecznym, a także jak dotychczas jedynym znanym miejscem występowania życia. Wokół Ziemi krąży jeden naturalny satelita – Księżyc oraz prawdopodobnie dwa księżyce pyłowe (księżyce Kordylewskiego) i znaczna liczba sztucznych satelitów. Stałą, stabilną orbitę posiada także planetoida (3753) Cruithne, która pozostając w rezonansie z orbitą Ziemi przez niektórych jest uważana za drugi księżyc Ziemi.
Ziemia posiada masę i grawitację odpowiednią dla utrzymania atmosfery, chroniącej przed promieniowaniem jonizującym oraz własne pole magnetyczne chroniące przed wiatrem słonecznym. Oddalenie od Słońca jest właściwe dla utrzymania odpowiedniej temperatury. Uważa się, że czynniki te sprzyjały powstaniu życia na ziemi.
Mars – czwarta od Słońca planeta Układu Słonecznego. Krąży między orbitą Ziemi a pasem planetoid, dzielącym go od orbity Jowisza. Planeta została nazwana od imienia rzymskiego boga wojny – Marsa, zawdzięcza ją barwie, która przy obserwacji z Ziemi wydaje się rdzawo-czerwona i kojarzyła się starożytnym Rzymianom z pożogą wojenną. Odcień ten bierze się od tlenków żelaza pokrywających powierzchnię. Mars jest planetą wewnętrzną z cienką atmosferą, o powierzchni usianej kraterami uderzeniowymi, podobnie jak powierzchnia Księżyca i wielu innych ciał Układu Słonecznego. Występują na nim różne rodzaje terenu, podobne do ziemskich: wulkany, doliny, kaniony, pustynie i polarne czapy lodowe. Okres obrotu wokół własnej osi jest niewiele dłuższy niż ziemski i wynosi 24,6229 godziny (24 h 37 m 22 s). Na Marsie znajduje się najwyższy wulkan w Układzie Słonecznym – Olympus Mons i największy kanion – Valles Marineris. Gładki obszar równinny Vastitas Borealis na półkuli północnej, który obejmuje 40% powierzchni planety, może być pozostałością ogromnego uderzenia[2]. W przeciwieństwie do Ziemi, Mars jest mało aktywny geologicznie i nie ma tektoniki płyt.
Do czasu pierwszego przelotu sondy Mariner 4 obok Marsa w 1965 roku spekulowano na temat obecności ciekłej wody na powierzchni planety. Podstawą spekulacji były obserwowane okresowe zmiany jasności obszarów powierzchni, w szczególności w pobliżu biegunów, które w obserwacjach teleskopowych wydawały się morzami i kontynentami. Długie ciemne linie na powierzchni, nazwane kanałami marsjańskimi, były interpretowane przez niektórych jako kanały nawadniające wybudowane przez istoty rozumne[3]. Ich obserwacje wytłumaczono później złudzeniem optycznym, ale spośród planet Układu Słonecznego poza Ziemią, występowanie na Marsie wody, a tym samym warunków do życia, jest wciąż najbardziej prawdopodobne[4]. Badania geologiczne zebrane przez bezzałogowe misje sugerują, że Mars miał kiedyś duże zasoby wody powierzchniowej, a małe wypływy wód podobne do gejzerów mogły występować w ciągu ostatniej dekady[5]. W roku 2005 dane radarowe wykazały obecność dużych ilości lodu zarówno na biegunach[6][7], jak i na średnich szerokościach geograficznych[8][9]. Lądownik Phoenix 31 lipca 2008 roku stwierdził bezpośrednio obecność wody w próbce regolitu pobranej w okolicach biegunowych[10]. 28 września 2015 roku NASA ogłosiła, że znaleziono dowody na obecność ciekłej słonej wody na powierzchni. W miesiącach letnich woda w stanie ciekłym spływa ze zboczy kanionów i ścian kraterów w postaci strug i pozostawia ciemne plamy, które mogą mieć długość do kilkuset metrów. Badacze uważają, że odkrycie to zwiększa prawdopodobieństwo istnienia życia na Marsie[11].
Mars ma dwa księżyce, Fobosa i Deimosa, które są małe i mają nieregularny kształt. Prawdopodobnie powstały z materii wyrzuconej z Marsa w wyniku wielkich uderzeń w początkach istnienia planety. Mars ma także planetoidy trojańskie, takie jak (5261) Eureka, krążące w pobliżu punktów równowagi grawitacyjnej na orbicie planety wokół Słońca. Wokół Marsa krąży osiem sztucznych satelitów, 2001 Mars Odyssey, Mars Express, Mars Reconnaissance Orbiter, Mars Orbiter Mission, MAVEN, ExoMars Trace Gas Orbiter, Al Amal i Tianwen-1. Na powierzchni znajdują się aktywne łaziki Curiosity, Perseverance i Zhurong, lądownik InSight oraz kilka nieczynnych łazików i lądowników z zarówno udanych, jak i nieudanych misji (stan na maj 2021).
Mars może być łatwo dostrzeżony z Ziemi gołym okiem. W wielkiej opozycji względem Słońca, jego jasność osiąga −2,91 magnitudo[1]; jasnością przewyższają go wówczas tylko Jowisz, Wenus, Księżyc oraz Słońce.
Mars ma promień równy około połowy promienia Ziemi, około 15% objętości Ziemi, a przy tym 11% jej masy, co oznacza, że jego gęstość jest nieco mniejsza niż Ziemi. Jego powierzchnia jest tylko nieznacznie mniejsza niż całkowita powierzchnia ziemskich lądów[1]. Chociaż Mars jest większy i masywniejszy niż Merkury, ma mniejszą gęstość. W efekcie na powierzchni obu planet występuje niemal identyczne natężenie pola grawitacyjnego. Rozmiary Marsa są pośrednie pomiędzy Ziemią a Księżycem. Rdzawoczerwony kolor powierzchni bierze się od hematytu, głównego składnika rdzy[12].
Na podstawie obserwacji orbitalnych oraz badań meteorytów marsjańskich wydaje się, że powierzchnia Marsa jest złożona głównie z bazaltu. Niektóre dowody sugerują, że część powierzchni jest bogatsza w krzemionkę niż bazalt i mogą ją tworzyć skały podobne do ziemskich andezytów, jednak można to także wytłumaczyć obecnością amorficznej krzemionki. Znaczna część powierzchni Marsa jest pokryta pyłem tlenku żelaza[13][14].
Na Marsie nie występuje globalne dipolowe pole magnetyczne podobne do ziemskiego[15]. Planeta ma jednak słabe pole magnetyczne o lokalnym charakterze. Obserwacje wykonane przez sondę Mars Global Surveyor wykazały, że w skorupie planety znajdują się na przemian położone pasma o przeciwnej biegunowości magnetycznej[16] o szerokości przeważnie około 160 km i długości około 1000 km. Podobne struktury (liniowe anomalie magnetyczne) występują na dnie ziemskich oceanów. Istnienie pasm sugeruje, że w przeszłości dochodziło do ruchów płyt tektonicznych przy obecności dipolowego pola magnetycznego. Obecnie we wnętrzu planety nie funkcjonuje mechanizm dynama magnetohydrodynamicznego, który jest odpowiedzialny za generację pola magnetycznego planet[17].
Aktualne modele wnętrza planety zakładają istnienie jądra o promieniu 1480 km, składającego się głównie z żelaza i w około 14–17% z siarki, występującej głównie jako siarczek żelaza, jest ono częściowo płynne i ma dwukrotnie mniejszą gęstość niż materiał jądra Ziemi. Jądro otoczone jest krzemianowym płaszczem, którego aktywność przyczyniła się w przeszłości do powstania wielu obszarów tektonicznych i wulkanicznych na powierzchni. Zewnętrzną warstwę tworzy skorupa, jej średnia grubość to około 50 km, a maksymalnie 125 km[18]. Skorupa ziemska ma średnio 40 km, a w stosunku do rozmiaru planety jest trzy razy cieńsza niż skorupa Marsa.
W okresie powstawania Układu Słonecznego w dysku protoplanetarnym otaczającym Słońce w wyniku procesu akrecji ziaren skalnych i pyłu powstały planety, w tym Mars. Wiele cech jego składu chemicznego wynika z jego położenia w Układzie Słonecznym. Pierwiastki o stosunkowo niskiej temperaturze wrzenia, takie jak chlor, fosfor i siarka są powszechniejsze na Marsie niż na Ziemi; zostały one prawdopodobnie usunięte z obszarów bliższych Słońcu przez wiatr słoneczny[19]. Prawdopodobnie w wyniku tego samego zjawiska pierwotna zawartość tlenu na Marsie była większa niż na Ziemi. Tlen reagował z żelazem, w wyniku czego powierzchnia planety zyskała swój kolor. Mars ma znacznie większą zawartość żelaza w skorupie i płaszczu niż Ziemia, w której większość żelaza skupiła się w jądrze.
Po utworzeniu się planet, w historii Układu Słonecznego miał miejsce epizod Wielkiego Bombardowania. Około 60% powierzchni Marsa tworzą wyżyny noszące liczne ślady uderzeń z tego okresu[20][21][22]. Znaczna część pozostałej powierzchni Marsa powstała prawdopodobnie przez ogromne wypływy lawy po uderzeniach. Największy taki nizinny obszar znajduje się na północnej półkuli, ma wymiary 10600 na 8500 km i jest około cztery razy większy niż Basen Biegun Południowy – Aitken na Księżycu, największy z potwierdzonych basenów uderzeniowych[2]. Jedna z hipotez powstania tego obszaru sugeruje, że Mars został uderzony przez ciało wielkości Plutona około cztery miliardy lat temu. To wydarzenie, uważane za przyczynę dychotomii półkul Marsa, stworzyło basen uderzeniowy Borealis, wygładzony następnie przez wylewy lawy, który obejmuje 40% powierzchni planety[23][24].
Historię geologiczną Marsa można podzielić na kilka sposobów. Najczęściej wykorzystywany jest podział stratygraficzny na trzy podstawowe systemy, zbudowany w oparciu o rozkład kraterów na powierzchni i ich późniejsze modyfikacje (erozję). Wyznaczenie absolutnego wieku skał z różnych systemów jest bardzo niepewne, dlatego granice czasowe okresów geologicznych Marsa są trudne do określenia i zmieniają się[25][26] w miarę rozwoju wiedzy[27]:
Niewielka aktywność geologiczna na Marsie miała miejsce także w niedawnej przeszłości geologicznej. Doliną Athabasca około 200 milionów lat temu płynęła lawa. W kanale Cerberus Fossae woda płynęła mniej niż 20 milionów lat temu, wskazano również niedawne intruzje wulkaniczne[33]. 19 lutego 2008 roku zdjęcia z sondy Mars Reconnaissance Orbiter ukazały zejście lawiny ze zbocza skalnego o wysokości 700 m.
Dane przesłane przez lądownik Phoenix wskazują, że marsjański regolit ma odczyn lekko zasadowy i zawiera pierwiastki takie jak magnez, sód, potas i chlor. Te składniki znajdują się także w glebach na Ziemi i są niezbędnym składnikiem odżywczym dla roślin[34][35]. Doświadczenia przeprowadzone przez lądownik wykazały, że pH marsjańskiego gruntu jest równe 8,3 i może on zawierać śladowe ilości nadchloranów.
Na całej powierzchni Marsa, na stromych zboczach kraterów, kanionów i dolin powszechnie występują wyróżniające się odcieniem smugi; często też pojawiają się nowe. Są one początkowo ciemne, jaśniejąc z biegiem czasu. Czasami takie smugi rozpoczynają się na małym obszarze, a następnie rozprzestrzeniają się na setki metrów. Zaobserwowano również smugi biegnące wzdłuż krawędzi skał oraz innych przeszkód naturalnych, leżących w poprzek ich pierwotnego biegu. Według najpopularniejszej hipotezy smugi te tworzą się, kiedy spodnie warstwy podłoża zostają odsłonięte po zejściu lawin jasnego pyłu, lub przejściu burz pyłowych[36]. Istnieje jednak kilkanaście możliwych wyjaśnień tego zjawiska, w tym tezy zakładające udział wody[37], czy nawet obecność organizmów żywych[38][39].
Z powodu niskiego ciśnienia atmosferycznego woda w stanie ciekłym nie może obecnie istnieć na powierzchni Marsa, z wyjątkiem najniżej położonych terenów w pobliżu równika, gdzie może pojawiać się na krótki czas[40][41]. Duże ilości lodu są uwięzione w kriosferze Marsa, czapy polarne wydają się być w dużej mierze złożone z zamarzniętej wody[42][43]. Dane radarowe z Mars Express i Mars Reconnaissance Orbiter wskazują na duże ilości lodu wodnego zarówno w okolicach biegunów (lipiec 2005)[6][44], jak i na średnich szerokościach geograficznych (listopad 2008)[8]. 31 lipca 2008 roku lądownik Phoenix pobrał próbki lodu bezpośrednio z marsjańskiego regolitu[10]. Objętość lodu w czapach polarnych jest znaczna; gdyby uległy stopieniu, wody wystarczyłoby do pokrycia obszaru równego powierzchni planety do głębokości 11 metrów[45] (w rzeczywistości pokryłaby ona tylko marsjańskie niziny). Wieczna zmarzlina rozciąga się od bieguna do około 60° szerokości planetograficznej[42].
Sądzi się, że w odległej przeszłości występowały na Marsie wielkie przepływy mas wody; największy miał miejsce w okresie, gdy tworzył się obszar Valles Marineris, stając się wielkim systemem kanałów odpływowych dla wód podpowierzchniowych. Mniejsza powódź mogła mieć miejsce około 5 mln lat temu, kiedy powstawały uskoki tworzące Cerberus Fossae, pozostawiając obszar będący prawdopodobnie powierzchnią zamarzniętego morza na równinie Elysium Planitia, którego środek znajduje się w obszarze Cerberus Palus[46]. Morfologię tego regionu można jednak wytłumaczyć wypływami lawy, tworzącymi podobnie spękaną powierzchnię[47], które pokryły teren wcześniej zalany lawą przez erupcję szczelinową w obszarze Athabasca Valles[48]. Nierówności powierzchni o skali decymetrów, bezwładność cieplna zbliżona do równin krateru Gusiewa i obecność stożków freatycznych potwierdzają hipotezę lawowego pochodzenia tego terenu[48]. Ponadto ułamek masowy wody w tym obszarze do głębokości kilkudziesięciu centymetrów to tylko około 4%[49], które można łatwo przypisać minerałom uwodnionym[50], co nie potwierdza obecności lodu w pobliżu powierzchni.
Kamera wysokiej rozdzielczości na orbiterze Mars Global Surveyor dostarczyła zdjęcia, znacznie wzbogacające wiedzę na temat historii wody na powierzchni Marsa. Pomimo istnienia wielu olbrzymich kanałów powodziowych i dendrytycznych sieci dopływów, nie występują mniejsze struktury, które mogłyby wskazywać na pochodzenie wód powodziowych. Być może procesy wietrzenia zatarły je, co wskazywałoby, że kanały te są stare. Na ścianach kraterów i kanionów odnaleziono wiele rys, podobnych do małych ziemskich wąwozów. Znajdują się one głównie na wyżynach półkuli południowej, na zboczach skierowanych w stronę równika; wszystkie znajdują się na południe od 30° szerokości areograficznej[51]. Nie zostały znalezione żadne wąwozy częściowo zdegradowane na skutek wietrzenia, nie ma też nakładających się na nie kraterów, co oznacza, że są bardzo młode.
Dwie fotografie, wykonane w odstępie sześciu lat, ukazują wąwóz z prawdopodobnie nowo powstałymi osadami. Michael Meyer, główny naukowiec programu eksploracji Marsa w NASA, twierdzi, że tylko przepływ materiału o dużej zawartości wody w stanie ciekłym może utworzyć taki układ gruzu skalnego, o takiej barwie. Czy jest to woda z opadów atmosferycznych, wypływów podziemnych, czy też z innego źródła, pozostaje kwestią otwartą[52]. Zostały zasugerowane także inne wyjaśnienia, w tym możliwość tworzenia osadów przez szron dwutlenku węgla lub przez ruch pyłu na powierzchni[53][54].
Dalszymi dowodami, że na powierzchni Marsa występowała ciekła woda, jest wykrycie specyficznych minerałów takich jak hematyt i getyt, które czasem powstają w obecności wody[55]. Część argumentów za istnieniem w przeszłości zbiorników wodnych i przepływów została zanegowana przez dokładną analizę zdjęć o wysokiej rozdzielczości (około 30 cm na piksel), wykonanych przez Mars Reconnaissance Orbiter. Na zdjęciach tych nie obserwuje się form, które powstają na brzegach zbiorników wodnych[56]. Jednakże w 2004 roku łazik Opportunity wykrył obecność minerału jarosytu w skale El Capitan, w odsłonięciu nazwanym Opportunity Ledge. Znaleziony minerał powstaje tylko w obecności kwaśnej wody[57].
We wrześniu 2015 roku analizy danych, które zebrała sonda Mars Reconnaissance Orbiter dają następne dowody na to, że ciekła woda okresowo występuje na Marsie współcześnie. Pracujący na pokładzie sondy spektrometr zarejestrował sygnatury występowania na zboczach uwodnionych minerałów[58].
Mars ma dwie stałe polarne czapy lodowe. Podobnie jak na Ziemi, w czasie polarnej zimy czapa lodowa pozostaje w ciągłej ciemności, co prowadzi do ochłodzenia powierzchni i atmosfery oraz wytrącenia się w grubej warstwy CO2 w postaci suchego lodu[59]. Kiedy biegun zostaje ponownie wystawiony na działanie promieni słonecznych, zamrożony CO2 sublimuje, tworząc silne wiatry (do 400 km/h) wiejące z okolicy biegunów. To sezonowe zjawisko transportuje duże ilości pyłu i pary wodnej, co podobnie jak na Ziemi, tworzy szron i duże chmury typu cirrus. Chmury lodu wodnego były fotografowane m.in. przez łazik Opportunity w 2004 roku[60].
Czapy polarne na obu biegunach składają się głównie z lodu wodnego. Zestalony dwutlenek węgla gromadzi się na północnej czapie polarnej w postaci cienkiej warstwy, o grubości do około jednego metra jedynie w czasie nocy polarnej, podczas gdy południową czapę polarną pokrywa stale suchy lód o grubości około ośmiu metrów[61]. Północna czapa polarna ma średnicę około 1000 kilometrów podczas lata na tej półkuli[62] i zawiera około 1,6 miliona kilometrów sześciennych lodu, który, gdyby go rozprowadzić równomiernie na całej powierzchni czapy, tworzyłby warstwę grubości 2 km[63]. Dla porównania, lądolód na Grenlandii ma 2,85 miliona kilometrów sześciennych. Południowa czapa polarna ma średnicę 350 km i 3 km grubości[64]. Całkowitą objętość lodu w południowej czapie polarnej wraz z sąsiednimi warstwami osadów również szacuje się na 1,6 miliona kilometrów sześciennych[65]. Na obu polarnych czapach widoczne są spiralne kaniony. Uważa się, że formy te są rezultatem różnic w ogrzewaniu słonecznym, sublimacji lodu i kondensacji pary wodnej[66][67].
Sezonowe zamrażanie i rozmrażanie w południowej pokrywie lodowej tworzy pająkowate promieniowe kanały o głębokości 1 metra, wyryte w lodzie przez światło słoneczne. Następnie sublimacja CO2 i prawdopodobnie także wody przyczynia się do wzrostu ciśnienia w ich wnętrzu, co z kolei powoduje erupcje zimnych płynów, często zmieszanych z ciemnym bazaltowym piaskiem lub błotem, podobne do gejzerów[68][69][70][71]. Proces ten zachodzi szybko, w skali kilku dni, tygodni lub miesięcy, co jest szybkością dość nietypową w geologii – w szczególności na Marsie.
Chociaż Johann Heinrich Mädler i Wilhelm Beer zapisali się w historii astronomii raczej jako twórcy map Księżyca, to byli też twórcami pierwszych map Marsa. Zaczęli od stwierdzenia, że większość cech powierzchni Marsa jest stała i określili dokładnie okres rotacji planety. W 1840 roku, po dziesięciu latach obserwacji, Mädler przedstawił pierwszą mapę Marsa. Zamiast nadawać nazwy różnym obiektom, astronomowie po prostu oznaczyli je literami; Sinus Meridiani został wówczas oznaczony literą „a”[72].
Obecne nazwy obiektów i obszarów na powierzchni Marsa pochodzą z wielu źródeł. Duże cechy albedo zachowują zwykle starsze nazwy, choć są one często aktualizowane dla oddania ich natury. Na przykład jasne miejsce nazwane „śniegami Olimpu” (Nix Olympica) okazało się być w rzeczywistości chmurą towarzyszącą olbrzymiej górze Olimp (Olympus Mons)[73]. Powierzchnia Marsa widziana z Ziemi dzieli się na dwa rodzaje obszarów o różnym albedo (jasności). Jaśniejsze równiny pokryte pyłem i piaskiem bogatym w czerwonawe tlenki żelaza były kiedyś uważane za marsjańskie „kontynenty” i nadawano im nazwy takie jak Arabia Terra („ziemia Arabii”) lub Amazonis Planitia („równina Amazonii”). Ciemne obszary uważano za morza, stąd też ich nazwy: Mare Erythraeum („morze Erytrejskie”) i Aurorae Sinus („zatoka zórz”). Największy ciemny obszar widziany z Ziemi to Syrtis Major Planum[74]. Północny płaskowyż polarny nazwano Planum Boreum, podczas gdy południowy – Planum Australe.
Równik Marsa jest definiowany przez jego obrót, ale położenie południka zerowego można określić, tak jak na Ziemi, przez wybór dowolnego punktu. Mädler i Beer wybrali linię południka zerowego w 1830 roku, tworząc pierwsze mapy Marsa. Po analizie zdjęć Marsa z Marinera 9 w 1972 roku za punkt o zerowej długości areograficznej (od greckiej nazwy planety: Ἄρης) wybrano mały krater (nazwany później Airy-0), znajdujący się na Meridiani Planum; wybór ten odpowiada jego pierwotnemu określeniu[75].
Ponieważ Mars nie ma obecnie oceanów, nie ma też „poziomu morza”, który naturalnie można uznać za położony na zerowej wysokości. Poziom odniesienia określono jako wysokość, na której panuje ciśnienie atmosferyczne 6,105 hPa[76]. Ciśnienie to odpowiada punktowi potrójnemu wody i jest równe około 0,6% ciśnienia na poziomie morza na Ziemi (0,006 atm)[77].
Na powierzchni Marsa odnaleziono 43 tys. kraterów uderzeniowych o średnicy co najmniej 5 km[78], nie licząc mniejszych. Największymi potwierdzonymi spośród nich są basen Utopia, stanowiący część północnych nizin oraz basen Hellas, wyraźnie jaśniejszy od otaczających wyżyn i dobrze widoczny z Ziemi[79][80]. Ze względu na mniejszą od ziemskiej masę Marsa, prawdopodobieństwo kolizji obiektu z nim jest o połowę mniejsze niż z Ziemią, jednak Mars znajduje się bliżej pasa planetoid, co z kolei zwiększa szanse na uderzenie przez pochodzące z niego ciała. Jest on również bardziej narażony na uderzenia komet krótkookresowych, które poruszają się wewnątrz orbity Jowisza[81]. Pomimo tego na Marsie jest znacznie mniej kraterów niż na Księżycu, ponieważ atmosfera Marsa zapewnia ochronę przed małymi meteoroidami. Wygląd niektórych kraterów sugeruje, że po uderzeniu meteorytu doszło do wypływu wody[82].
Badania z 2008 roku wsparły hipotezę z 1980 roku, że uderzająca dwudzielność topografii Marsa jest wynikiem wielkiego zderzenia[83]. Północne równiny Vastitas Borealis, wypłaszczone przez wylewy lawy i kontrastujące z południowymi wyżynami, usianymi dawnymi kraterami, miałyby być pozostałością basenu uderzeniowego. Hipoteza ta stwierdza, że cztery miliardy lat temu w północną półkulę Marsa uderzył obiekt o średnicy od 1/10 do 2/3 Księżyca. Zderzenie to utworzyło na północnej półkuli Marsa basen uderzeniowy o 10 600 km długości i 8500 km szerokości, czyli o powierzchni Europy, Azji i Australii razem wziętych, większy niż Basen Biegun Południowy – Aitken[2].
Wulkan tarczowy Olympus Mons (góra Olimp), o wysokości 26 km, jest najwyższą znaną górą w Układzie Słonecznym[84]. Jest to wygasły wulkan położony na rozległym wyżynnym obszarze Tharsis, na którym występuje także kilka innych dużych wulkanów. Olympus Mons jest ponad trzy razy wyższy niż Mount Everest, który ma niewiele ponad 8,8 km[85]. Na Marsie znajduje się jeszcze jedna duża wyżyna wulkaniczna, Elysium, z wulkanami sięgającymi 14 km (Elysium Mons).
Duży kanion, Valles Marineris, określany także na starszych mapach jako kanał Agathadaemon, ma długość 4 tys. km i głębokość do 7 km. Długość Valles Marineris odpowiada rozciągłości Europy, rozciąga się on na jedną piątą obwodu Marsa. Dla porównania, Wielki Kanion Kolorado na Ziemi ma tylko 446 km długości i prawie 2 km głębokości. Valles Marineris powstał w wyniku potężnego wybrzuszenia skorupy w rejonie Tharsis, które spowodowało zapadnięcie skorupy w sąsiadującym obszarze. Kolejnym dużym kanionem jest Ma'adim Vallis (nazwa Ma’adim oznacza Marsa w języku hebrajskim), o długości 700 km, szerokości 20 km i głębokości do 2 km, który również jest większy od Kanionu Kolorado. Jest możliwe, że był on zalany wodą w przeszłości[86].
Obrazy z instrumentu Thermal Emission Imaging System (THEMIS) na pokładzie orbitera 2001 Mars Odyssey, wykonującego zdjęcia w podczerwieni i świetle widzialnym, wskazały siedem prawdopodobnych otworów jaskiń lawowych na zboczach wulkanu Arsia Mons[87]; jaskinie te noszą wspólną nazwę „siedem sióstr”[88].
Wejścia do jaskiń mają od 100 do 252 m szerokości, a ich głębokość jest szacowana na co najmniej 73 do 96 m. Ponieważ światło nie dochodzi do dna większości jaskiń, jest prawdopodobne, że są one znacznie głębsze niż te oszacowania i rozszerzają się pod powierzchnią. Jaskinia Dena jest wyjątkiem, jej dno jest widoczne, a znajduje się na głębokości 130 m. Wnętrza tych komór mogą być chronione przed mikrometeorytami, promieniowaniem UV, rozbłyskami słonecznymi i cząstkami o wysokiej energii, które bombardują powierzchnię planety[89]. Jaskinie odkryto także na zboczach dwóch innych wulkanów pasma Tharsis Montes.
Mars utracił magnetosferę 4 miliardy lat temu[90], od tego czasu cząstki wiatru słonecznego docierają bezpośrednio do jonosfery planety, gdzie zderzają się z cząsteczkami cienkiej atmosfery, nadając im dużą prędkość, umożliwiającą ucieczkę z jej pola grawitacyjnego. Mars Global Surveyor i Mars Express wykryły te zjonizowane cząsteczki, uciekające w przestrzeń kosmiczną[90][91]. W porównaniu do Ziemi, atmosfera Marsa jest bardzo cienka. Ciśnienie na powierzchni osiąga zaledwie 30 Pa (0,30 hPa) na szczycie Olympus Mons, zaś na dnie basenu Hellas sięga 1155 Pa (11,55 hPa); średnie ciśnienie atmosferyczne na Marsie to 600 Pa[92]. Na Ziemi takie ciśnienie panuje na wysokości 35 km nad powierzchnią morza[93]; stanowi to mniej niż 1% ciśnienia przy powierzchni Ziemi (1013 hPa). Ciśnienie w atmosferze Marsa spada wraz z wysokością wolniej niż na Ziemi, względny spadek e razy (w przybliżeniu 2,7 razy) następuje co około 10,8 km[94] (na Ziemi co około 6 km). Wynika to ze znacznie mniejszego przyspieszenia grawitacyjnego na powierzchni Marsa (około 38% ziemskiego). Wpływ tego czynnika jest do pewnego stopnia łagodzony przez niższą temperaturę i wyższą o około 50% średnią masę cząsteczkową atmosfery, które to efekty zwiększają gęstość atmosfery mającą wpływ na ciśnienie.
Atmosfera Marsa zawiera 95% dwutlenku węgla, 3% azotu, 1,6% argonu oraz śladowe ilości tlenu i wody[1]. Jest także silnie zapylona, pył tworzą cząstki o średnicy około 1,5 mikrometra, nadające marsjańskiemu niebu płowy kolor przy obserwacji z powierzchni[95].
Prędkość dźwięku w atmosferze marsjańskiej zmierzona przez próbnik Perseverance wynosi 240 m/s[96].
Metan został wykryty w marsjańskiej atmosferze w stężeniu objętościowym około 30 ppb[97][98]; tworzy on rozszerzone pióropusze, co oznacza, że jest uwalniany z oddzielnych, niewielkich pod względem powierzchni źródeł. Na półkuli północnej główny pióropusz zawiera podczas lata 19 tys. ton metanu, a wydajność źródła szacowana jest na 0,6 kilograma na sekundę[99][100]. Profile sugerują, że mogą istnieć dwa źródła metanu, pierwsze skupione w pobliżu 30°N, 260°W, a drugie w pobliżu 0°, 310°W[99]. Czas życia metanu w atmosferze Marsa może wynosić od 4 lat ziemskich do tylko 0,6 roku ziemskiego[99][101]. Szybka wymiana gazu oznaczałaby istnienie aktywnego źródła gazu na planecie. Wśród możliwych źródeł wymienia się: aktywność wulkaniczną, upadki komet oraz obecność metanogennych form życia. Metan może być również wytwarzany w procesie niebiologicznym zwanym serpentynizacją, z udziałem wody, dwutlenku węgla i oliwinu, które występują na Marsie[102]. W czerwcu roku 2012 opublikowano wyniki testów laboratoryjnych z których wynikało, że metan może być produkowany w procesie naświetlania promieniami UV spadających na planetę bogatych w węgiel meteorytów[103]. Szacuje się, że Mars musi uwalniać 270 ton metanu na rok[99][104].
Spośród wszystkich planet w Układzie Słonecznym, pory roku na Marsie są najbardziej podobne do ziemskich, a to ze względu na podobne nachylenie osi obrotu obydwu planet do płaszczyzny orbity. Jednak z powodu większej odległości Marsa od Słońca, pory roku na nim są około dwa razy dłuższe niż na Ziemi. Temperatura powierzchni Marsa waha się, spadając do około −133 °C podczas zimy na biegunach i dochodząc do +27 °C w ciepłe dni na równiku[105]. Niższe temperatury wynikają z tego, że planeta jest 1,52 razy dalej od Słońca niż Ziemia, w wyniku czego na jego powierzchnię dociera 43 procent energii padającej na taką samą powierzchnię na Ziemi[106]. Duże zmiany z kolei wynikają z małej pojemności cieplnej cienkiej atmosfery (ze względu na niskie ciśnienie) i bezwładności cieplnej marsjańskiego gruntu, który nie może na długo gromadzić ciepła słonecznego[107].
Wpływ na klimat na Marsie ma także stosunkowo duży mimośród jego orbity. Mars znajduje się w pobliżu peryhelium, gdy na półkuli południowej jest lato, a na północnej zima, zaś w pobliżu aphelium na półkuli południowej jest zima, a na północnej lato. W rezultacie, pory roku na półkuli południowej są bardziej surowe niż na półkuli północnej, gdzie różnice między latem a zimą są mniejsze. Temperatura latem na południu może być do 30 °C wyższa niż w lecie na północy, na tej samej szerokości areograficznej[108].
Ponadto marsjańska atmosfera jest tak cienka, że nawet po ciepłym dniu, gdzie temperatura wynosić może nawet 20 °C, w nocy może spaść do −90 °C[109].
Na Marsie występują największe w Układzie Słonecznym burze piaskowe. Mogą one mieć zarówno zasięg lokalny, jak też obejmować całą planetę. W ich trakcie wiatr może osiągać nawet 300 km/h[110]. Burze występują częściej, gdy Mars jest najbliżej Słońca, w wyniku czego jego powierzchnia jest silniej ogrzewana[111]. Ostatnie badania wskazują, że występowanie burz jest związane również ze zmianami pędu planety względem środka masy Układu Słonecznego w trakcie ruchu planety wokół centrum Układu Słonecznego. Inne planety mają wpływ na pęd Marsa, który zmienia się cyklicznie w okresie 2,2 roku (obieg wokół Słońca trwa 1,9 roku), sezon burz pyłowych na Marsie rozpoczyna się gdy pęd rośnie[112][113].
Burze piaskowe to bardzo niebezpieczne zjawisko w kontekście przyszłych lotów na Marsa. Powierzchnia planety zostaje odcięta od promieni słonecznych, a wszystko pokrywa się warstwą drobnego pyłu. Może mieć to zły wpływ na pracujące na planecie urządzenia mechaniczne i elektroniczne, w tym na panele słoneczne, których wydajność znacznie spadnie. Przypuszczenia te potwierdziły się podczas globalnej burzy pyłowej w 2007 roku po której zauważono znaczny spadek energii wytwarzanej przez panele słoneczne zainstalowane na pokładzie aktywnych w tym czasie łazików marsjańskich Spirit oraz Opportunity[112]. Burza pyłowa z 2018 roku zakończyła pracę łazika Opportunity[114].
Średnia odległość Marsa od Słońca to około 230 mln km (1,52 au), a czas obiegu wokół Słońca (rok marsjański) jest równy 687 dni ziemskich, co odpowiada 1,8809 roku ziemskiego (1 rok, 320 dni i 18,2 godziny). Doba słoneczna na Marsie jest niewiele dłuższa niż ziemska i ma 24 godziny, 39 minut i 35,244 sekundy[1].
Nachylenie osi obrotu Marsa (odchylenie od prostej prostopadłej do płaszczyzny orbity) to 25,19°, jest ono podobne do nachylenia osi obrotu Ziemi[1]. W rezultacie pory roku na Marsie są podobne do ziemskich, choć prawie dwa razy dłuższe z uwagi na dłuższy rok. W obecnej orientacji biegun północny Marsa wskazuje kierunek zbliżony do położenia gwiazdy Deneb[115].
Orbita Marsa ma stosunkowo duży mimośród, równy około 0,09; wśród planet Układu Słonecznego tylko orbita Merkurego ma większą ekscentryczność. W przeszłości orbita Marsa była bardziej kołowa niż obecnie, jej mimośród zmienia się w wyniku oddziaływania grawitacyjnego z innymi planetami. 1,35 miliona lat temu (lat ziemskich) ekscentryczność orbity Marsa wynosiła około 0,002, dużo mniej niż dzisiaj ma orbita Ziemi[116]. Okres zmian mimośrodu Marsa to 96 tys. lat ziemskich, w porównaniu analogiczny cykl dla Ziemi ma okres 100 tys. lat[117]. Mars ma też znacznie dłuższy cykl zmian ekscentryczności o okresie 2,2 mln lat ziemskich, ale ma on mniejszą amplitudę i na wykresie zmian ekscentryczności jest zakrywany przez cykl krótszy. Przez ostatnie 35 tys. lat mimośród jego orbity był nieco większy niż dziś, ze względu na efekty grawitacyjne innych planet. Minimalna odległość między Ziemią a Marsem będzie nadal powoli zmniejszała się przez następne 25 tys. lat[118].
Mars ma dwa małe księżyce o nieregularnych kształtach, których orbity są bardzo bliskie planety: Fobosa i Deimosa. Mogą być one ciałami utworzonymi z materii wyrzuconej przez uderzenia z powierzchni planety albo przechwyconymi planetoidami[119][120].
Oba zostały odkryte w 1877 roku przez Asapha Halla. Ich nazwy pochodzą od imion synów greckiego boga wojny Aresa, Fobosa (jego imię znaczy „strach”) i Dejmosa („trwoga”), którzy w mitologii greckiej towarzyszyli ojcu w bitwach. Odpowiednikiem Aresa w mitologii Rzymian był Mars[121][122].
Obserwowany z powierzchni Marsa ruch Fobosa i Deimosa bardzo różni się od ruchu naszego Księżyca. Fobos znajduje się bardzo blisko planety i jego okres obiegu to zaledwie 7,66 godziny, znacznie mniej niż czas obrotu Marsa wokół własnej osi, przez co jego pozorny ruch na niebie jest przeciwny do ruchu Słońca. Fobos wschodzi na zachodzie, a zachodzi na wschodzie, a jego pozorny czas obiegu to około 11 godzin, przez co wschód następuje częściej niż 2 razy na dobę marsjańską. Deimos krąży nieznacznie dalej niż orbita synchroniczna, jego pozorny ruch jest zgodny z ruchem Słońca, ale bardzo powolny. Obiega on planetę w 30 godzin, ale do czasu, gdy ponownie znajdzie się w tym samym miejscu na nieboskłonie, mija aż 5,28 marsjańskiego dnia[123]. Oba satelity wykazują obrót synchroniczny z obiegiem, podobnie jak Księżyc ziemski, przez co z powierzchni Marsa widoczna jest zawsze ta sama ich strona.
Ponieważ orbita Fobosa jest poniżej orbity synchronicznej, siły pływowe pochodzące od planety stopniowo obniżają jego orbitę, obecnie w tempie 1,8 m na wiek. W ciągu najbliższych 50 milionów lat, po przekroczeniu granicy Roche’a, wszystkie niezwiązane skały oderwą się od jego powierzchni, a księżyc może nawet zostać rozerwany, tworząc pierścień wokół planety, lub zderzyć się z powierzchnią Marsa[123]. Deimos znajduje się znacznie dalej od planety, siły pływowe są na nim niewielkie, ale teoretycznie podobnie jak ziemski Księżyc powoli oddala się on od planety.
Pochodzenie obu księżyców nie jest jasne. Do niedawna uznawano, że ze względu na niskie albedo i skład podobny do chondrytów węglistych, są one podobne do planetoid i zapewne zostały one w przeszłości przechwycone przez grawitację planety. Niestabilna orbita Fobosa wydaje się wskazywać, że przechwycenie nastąpiło stosunkowo niedawno. Oba mają jednak orbity położone bardzo blisko równika, co jest bardzo nietypowe dla przechwyconych obiektów, a dynamika przechwycenia jest trudna do wyjaśnienia. Akrecja we wczesnej historii Marsa to inna wiarygodna możliwość, która jednak nie wyjaśnia, dlaczego ich skład wydaje się przypominać planetoidy, a nie Marsa.
Trzecią możliwością jest udział w ich powstaniu dodatkowego ciała[124]. Nowe dowody obserwacyjne wskazują, że Fobos ma silnie porowate wnętrze[125] i sugerują, że zawiera on głównie krzemiany warstwowe i inne minerały znane z Marsa[126], wskazując na jego powstanie z materii wyrzuconej przez uderzenie w powierzchnię planety, która później połączyła się w ciało na orbicie[120]. Podobnie, według teorii wielkiego zderzenia, powstał ziemski Księżyc. Podczas gdy widma powierzchni księżyców w świetle widzialnym i bliskiej podczerwieni przypominają widma planetoid, widmo Fobosa w dalszej podczerwieni nie przypomina żadnych chondrytów[126].
Podobnie jak Jowisz, Mars ma na swojej orbicie planetoidy krążące z okresem obiegu równym okresowi obiegu planety, czyli tzw. planetoidy trojańskie. Są to: (5261) Eureka – pierwsza odkryta planetoida trojańska Marsa, 1998 VF31, 1999 UJ7 oraz 2007 NS2.
Obserwowany gołym okiem z Ziemi Mars ma wyraźnie żółty, pomarańczowy lub czerwony kolor, a jego jasność zmienia się w trakcie ruchu po orbicie silniej niż jakiejkolwiek innej planety. Rzeczywisty kolor powierzchni Marsa jest bliższy karmelowego, a widoczne zaczerwienienie nadaje mu pył w atmosferze planety; uwzględniając to, łazik NASA Spirit przekazał zdjęcia krajobrazu w błotnistym, zielono-brązowym kolorze, z niebiesko-szarymi skałami i plamami jasnoczerwonego piasku[127]. Obserwowana wielkość gwiazdowa Marsa zmienia się od +1,8 w pobliżu koniunkcji, do −2,91 w opozycji w peryhelium[1]. W najdalszym od Ziemi położeniu, planeta znajduje się ponad siedem razy dalej od niej, niż w najbliższym. W warunkach najmniej korzystnych dla obserwacji, potrafi ginąć w blasku Słońca przez miesiące. W najbardziej korzystnych warunkach, zdarzających się co 15 lub 17 lat i zawsze pomiędzy późnym lipcem a końcem września, Mars pokazuje bogactwo szczegółów powierzchni w teleskopach. Szczególnie zauważalne, nawet przy małym powiększeniu, są lodowe czapy polarne[128].
Mars, zbliżając się do opozycji, zaczyna poruszać się pozornie ruchem wstecznym, co oznacza, że zakreśla pętlę na tle gwiazd, cofając się względem swojego zwykłego kierunku ruchu. Ruch wsteczny planety trwa około 72 dni, Mars osiąga maksymalną jasność w połowie tego okresu[129].
Sytuację, w której Mars i Słońce znajdują się dokładnie po przeciwnych stronach Ziemi, nazywamy opozycją. Przypada ona zawsze blisko czasu, gdy odległość między Marsem a Ziemią jest najmniejsza; opozycję od momentu największego zbliżenia może dzielić do 8,5 dnia. Odległość obu ciał przy największym (dla danego cyklu orbitalnego) zbliżeniu zmienia się od około 54[130] do 103 milionów kilometrów, ze względu na eliptyczność obu orbit, co powoduje podobne zmiany średnicy kątowej Marsa[131]. Kolejne opozycje Marsa występują średnio co 780 dni, czas ten nazywany jest synodycznym okresem obiegu; może on zmieniać się od 769 do 812 dni[132].
Największe zbliżenie Marsa do Ziemi w czasie ostatnich 60 tys. lat miało miejsce 27 sierpnia 2003 (o godzinie 9:51:13 uniwersalnego czasu koordynowanego), planety zbliżyły się na odległość 55 758 006 km (0,372719 au), a obserwowana wielkość gwiazdowa Marsa sięgnęła −2,88m. Stało się to, gdy Mars był o jeden dzień od opozycji i około trzech dni od przejścia przez peryhelium. Szacuje się, że ostatnie większe zbliżenie Marsa wydarzyło się 12 września 57 617 roku p.n.e., zaś następne zdarzy się już 29 sierpnia 2287 roku[133]. Jednak ta rekordowo mała odległość była tylko nieznacznie mniejsza, niż przy innych niedawnych zbliżeniach. Na przykład minimalna odległość z 22 sierpnia 1924 to 0,37285 au, a minimalna odległość 24 sierpnia 2208 będzie równa 0,37279 au[117].
Wędrówki Marsa na nocnym niebie zostały zauważone przez starożytnych astronomów egipskich, a przed 1534 rokiem p.n.e. był im znany jego ruch wsteczny[134]. Astronomowie babilońscy w okresie państwa nowobabilońskiego prowadzili regularne zapisy pozycji planet i systematyczne obserwacje ich zachowań. O Marsie wiedzieli, że w czasie 79 lat mieści się jego 37 okresów synodycznych lub 42 obiegi zodiaku przez planetę. Wynaleźli też metody arytmetyczne dokonywania drobnych poprawek do przewidywanych pozycji planet[135][136].
W IV wieku p.n.e. Arystoteles stwierdził, że Mars zniknął za Księżycem podczas okultacji, wykazując tym samym, że planeta jest dalej[137]. Ptolemeusz, Grek żyjący w Aleksandrii, próbował rozwiązać problem ruchu orbitalnego planet, w tym Marsa[138]. Teoria geocentryczna Ptolemeusza i jego praca zbiorowa na temat astronomii, przedstawiona w wielotomowym dziele Almagest, stała się niekwestionowanym fundamentem wiedzy astronomicznej na następne czternaście wieków[139]. Literatura starożytnych Chin potwierdza, że Mars był znany chińskim astronomom nie później niż w czwartym wieku p.n.e.[140] W V wieku naszej ery, hinduscy astronomowie w traktacie astronomicznym Surja Siddhanta oszacowali średnicę Marsa[141]. W VIII wieku perski astronom Yaqub ibn Tariq, w pracy Az-Zīj al-Mahlul min as-Sindhind li-Darajat Daraja, próbował oszacować odległość między Ziemią a Marsem[142].
W XVII wieku, Tycho Brahe zmierzył paralaksę dobową Marsa, a Johannes Kepler użył jej do obliczania orientacyjnej odległości do planety[143]. Gdy teleskop stał się dostępny, ponownie zmierzono dobową paralaksę Marsa, w celu określenia odległości Słońce-Ziemia, po raz pierwszy dokonał tego Giovanni Cassini w 1672. Jednakże te wczesne pomiary były utrudnione przez jakość instrumentów[144]. W 1610 Galileo Galilei po raz pierwszy oglądał Marsa przez teleskop[145]. Holenderski astronom Christiaan Huygens jako pierwszy zaobserwował szczegóły jego powierzchni (Syrtis Major) oraz określił okres obrotu planety[146].
W XIX wieku rozdzielczość teleskopów osiągnęła poziom wystarczający do identyfikacji cech powierzchni Marsa. 5 września 1877 roku miała miejsce opozycja Marsa będącego w peryhelium, gdy planeta była szczególnie blisko Ziemi. Włoski astronom Giovanni Schiaparelli wykorzystał tę sytuację, aby z pomocą swojego 22 cm teleskopu wykonać pierwszą mapę Marsa. Zaobserwował na tej planecie, jak mu się wydawało, długie proste struktury, które nazwał po włosku canali, a które później okazały się być złudzeniem optycznym. Uznał je za naturalne zjawisko i nadał im nazwy znanych rzek na Ziemi. Włoskie słowo canali, oznaczające „rowy”, zostało przetłumaczone na angielski jako „kanały”; doprowadziło to do wielu spekulacji na temat inteligentnych istot budujących kanały na Marsie, co raczej drażniło Schiaparellego[147][148].
Zafascynowany tym odkryciem, orientalista Percival Lowell założył obserwatorium, które miało do dyspozycji teleskopy 300 i 450 mm. Obserwatorium było wykorzystywane do obserwacji Marsa w 1894 roku i przy następnych, mniej korzystnych opozycjach. Opublikował kilka książek o Marsie i życiu na planecie, które miały wielki wpływ na społeczeństwo[149]. Kanały obserwowali również inni astronomowie, tacy jak Henri Perrotin i Louis Thollon w Nicei, używający jednego z największych ówczesnych teleskopów[150][151].
Sezonowe zmiany (zmniejszanie się czap polarnych i ciemnych obszarów podczas marsjańskiego lata) w połączeniu z obserwacjami kanałów doprowadziły do licznych spekulacji na temat życia na Marsie, w tym do długo utrzymującego się przekonania, że na planecie istnieją rozległe morza i roślinność. Ówczesne teleskopy nie miały wystarczającej rozdzielczości, by dostarczyć rozstrzygających dowodów, jednak wraz z budową coraz większych i doskonalszych teleskopów obserwowano coraz mniejszą ilość długich prostych linii. W 1909 roku Flammarion, prowadząc obserwacje przez 840 mm teleskop, stwierdził, że dostrzega na Marsie liczne nieregularne wzory, ale żadnych kanałów[152].
Wiele sond kosmicznych, włączając w to orbitery, lądowniki i łaziki, zostało wysłanych w kierunku Marsa przez ZSRR, USA, Europę oraz Japonię, aby zbadać powierzchnię planety, jej klimat oraz strukturę geologiczną. Aktualny koszt wysłania z powierzchni Ziemi na powierzchnię Marsa 1 kg ładunku oscyluje w pobliżu 309 tys. dolarów[153].
Około połowy wszystkich misji na Marsa nie zostało zakończonych powodzeniem, ulegając awarii przed ukończeniem lub nawet w początkowym etapie lotu. Chociaż wysoki odsetek niepowodzeń wynika z różnych problemów technicznych, liczba drobnych awarii lub przypadków niewyjaśnionej utraty łączności jest tak duża, że w NASA przez wiele lat żartowano o „wielkim galaktycznym upiorze”, który żeruje na statkach kosmicznych lecących na Marsa[154]. Szczególnie złą passę mieli Rosjanie – niemal wszystkie ich sondy zawiodły całkowicie, a pozostałe zebrały bardzo mało danych.
Pierwszego udanego przelotu w pobliżu Marsa dokonał w 1965 roku Mariner 4. W dniu 14 listopada 1971 Mariner 9 stał się pierwszą sondą na orbicie innej planety, wchodząc na orbitę wokół Marsa[155]. Pierwszego udanego lądowania na powierzchni dokonała radziecka sonda Mars 3 z programu Mars, wyniesiona w 1971 roku, ale kontakt z nią został utracony w ciągu 20 sekund po lądowaniu. Na Marsie rozbiły się radzieckie lądowniki Mars 2 i Mars 6. W 1975 NASA wysłała sondy programu Viking: dwa orbitery wyposażone w lądowniki, które pomyślnie wylądowały na planecie w 1976 roku. Viking 1 pozostał operacyjny przez sześć lat, Viking 2 przez trzy. Lądowniki Viking przekazały kolorowe panoramy powierzchni Marsa i wykonały eksperymenty biologiczne, których celem było poszukiwanie śladów życia na planecie, a orbitery wykonały mapy powierzchni na tyle szczegółowe, że nadal pozostają w użyciu[156].
Sondy radzieckiego programu Fobos zostały wysłane na Marsa w 1988 roku, w celu badania planety i jej księżyców. Kontakt z Fobosem 1 został stracony w drodze na Marsa, natomiast Fobos 2 z powodzeniem fotografował Marsa i Fobosa, ale uległ uszkodzeniu zanim odłączono dwa lądowniki, które miały wylądować na powierzchni tego księżyca[157].
Po zakończonej niepowodzeniem misji orbitera Mars Observer wystrzelonego w 1992 roku, NASA wysłała w 1996 roku sondę Mars Global Surveyor. Ta misja zakończyła się pełnym sukcesem, kończąc swoją podstawową misję mapowania powierzchni planety na początku 2001 roku. Kontakt z sondą został utracony w listopadzie 2006 roku, podczas trzeciego programu rozszerzonego, po dokładnie 10 latach w przestrzeni kosmicznej. Wyniesiony w tym samym oknie startowym, miesiąc po Surveyor, lądownik Mars Pathfinder z łazikiem Sojourner wylądował w dolinie Ares Vallis na Marsie latem 1997 roku. Także ta misja była sukcesem i wzbudziła duże zainteresowanie, po części ze względu na znaczną liczbę zdjęć, które zostały przesłane na Ziemię[158].
W 2003 roku NASA rozpoczęła program Mars Exploration Rover, składający się z dwóch łazików o nazwach Spirit (MER-A) i Opportunity (MER-B). Obydwie sondy pomyślnie wylądowały w styczniu 2004 roku i osiągnęły lub przekroczyły wszystkie podstawowe cele misji. Do najważniejszych wyników naukowych należy znalezienie dowodów, że woda istniała na powierzchni Marsa w przeszłości i to w obydwu miejscach lądowań. Wiry marsjańskiego pyłu i wiatr sporadycznie oczyszczały panele słoneczne obu łazików, tym samym zwiększając ich żywotność[159]. Łazik Spirit utracił mobilność w 2009 roku, a w marcu 2010 ostatecznie stracono z nim kontakt[160][161]. Opportunity, po przebadaniu odsłonięć warstw geologicznych w mniejszych kraterach, dotarł w sierpniu 2011 roku do dużego krateru Endeavour, gdzie prowadził dalsze badania[162]. Ostatni kontakt nawiązano w czerwcu 2018 roku, tuż przed rozpoczęciem szczególnie silnej burzy pyłowej[163]. Misję uznano za zakończoną w lutym 2019 roku.
Kolejną sondą NASA, która wylądowała na Marsie, był lądownik Phoenix, który dotarł w okolice północnej czapy polarnej planety 25 maja 2008 roku[164]. Lądownik miał ramię o długości 2,5 m, zdolne wkopać się na metr w marsjański grunt, oraz mikroskopową kamerę o rozdzielczości jednej tysięcznej grubości ludzkiego włosa. 15 czerwca 2008 roku kamery lądownika przekazały obraz jasnej substancji w miejscu lądowania, którą zidentyfikowano jako lód wodny; uległ on sublimacji przed 20 czerwca[165][166]. Misja została uznana za zakończoną 10 listopada 2008 roku, gdyż inżynierowie nie byli już w stanie skontaktować się z lądownikiem[167].
W 2001 roku misję rozpoczął orbiter NASA 2001 Mars Odyssey, który nadal (stan z lutego 2021) pozostaje czynny na orbicie Marsa[168]. Wchodzący w skład wyposażenia sondy spektrometr promieniowania gamma wykrył znaczne ilości wodoru w górnej warstwie regolitu na Marsie. Uważa się, że wodór ten jest związany w postaci cząsteczek wody, tworzących wieloletnią zmarzlinę.
W 2003 roku Europejska Agencja Kosmiczna (ESA) rozpoczęła misję Mars Express, składającą się z orbitera Mars Express i lądownika Beagle 2. Misja Beagle 2 nie powiodła się. Podczas opadania stracono kontakt z lądownikiem i został on uznany za utracony. Na początku 2004 roku zespół odpowiedzialny za urządzenie PFS (Planetary Fourier Spectrometer) na pokładzie orbitera ogłosił, że wykryto metan w marsjańskiej atmosferze. W czerwcu 2006 roku ESA doniosła także o odkryciu lokalnych zorzy na Marsie[169] (związanych z lokalnymi anomaliami magnetycznymi).
Wysłana w 2005 roku sonda NASA Mars Reconnaissance Orbiter, na orbitę planety dotarła 10 marca 2006 roku. Orbiter tworzy mapy terenu i pogody Marsa, aby znaleźć odpowiednie miejsca lądowania dla przyszłych lądowników. Układ transmisji danych z sondy na Ziemię został znacznie ulepszony, przez co ma on większą przepustowość niż wszystkie poprzednie misje razem wzięte. MRO przesłał m.in. pierwsze zdjęcia lawin, schodzących ze zboczy w pobliżu bieguna północnego planety[170] czy dowody na obecność słonej wody na Marsie[171].
26 listopada 2011 r. rozpoczęła się misja Mars Science Laboratory z łazikiem Curiosity, który jest większy, szybszy (do 90 m/h) i wyposażony w doskonalsze przyrządy niż Mars Exploration Rovers. Eksperymenty chemiczne uwzględniają chromatograf, spektrometr gazowy i laser, które mogą analizować skład skał w odległości do 13 m[172]. Lądowanie łazika na powierzchni planety nastąpiło 6 sierpnia 2012 roku.
We wrześniu 2014 roku na orbity wokół planety weszły dwie sondy: amerykańska MAVEN, która ma prowadzić badania atmosfery planety[173] i indyjska sonda Mars Orbiter Mission[174].
W marcu 2016 roku rozpoczęła się misja ExoMars 2016, przygotowana przez ESA we współpracy z Roskosmosem. Składała się z orbitera poszukującego gazów śladowych w atmosferze planety, w tym metanu, oraz z niewielkiego lądownika stacjonarnego. Sonda dotarła w pobliże Czerwonej Planety w październiku 2016 roku gdzie nastąpiło oddzielenie lądownika od sondy. Lądownik Schiaparelli nie zdołał wylądować na planecie, rozbił się 19 października[175]. Na 2022 rok planowany jest start drugiej misji programu ExoMars. W jej ramach zostanie wysłany łazik Rosalind Franklin, który będzie mógł wykonywać wiercenia do głębokości 2 m w poszukiwaniu związków organicznych[176].
W 2018 roku na Marsa wysłano misję InSight, która od 26 listopada 2018 roku prowadzi badania geofizyczne planety, między innymi mierząc jej aktywność sejsmiczną[177].
W oknie startowym w lipcu 2020 roku w stronę Marsa została wysłana emiracka misja Al Amal (Hope), chińska misja Tianwen-1 i amerykańska misja Mars 2020[178]. Sonda Al Amal weszła na orbitę 9 lutego 2021 roku[178], a Tianwen-1 dzień później[179]. W maju 2021 od orbitera Tianwen-1 odłączył się lądownik z łazikiem Zhurong, który wylądował na powierzchni planety[180].
Załogowa wyprawa na Marsa została określona przez Stany Zjednoczone jako główny długoterminowy cel programu Wizji Eksploracji Kosmosu, który ogłosił w 2004 roku ówczesny prezydent USA George W. Bush[181]. NASA i Lockheed Martin rozpoczęły prace nad statkiem kosmicznym Orion (wcześniej Crew Exploration Vehicle), który miał umożliwić ponowne lądowanie na Księżycu do 2020 roku, jako krok w kierunku ekspedycji na Marsa. W 2007 roku administrator NASA Michael D. Griffin stwierdził, że agencja zamierza sprowadzić człowieka na Marsa przed 2037[182]. Ze względu na cięcia budżetowe powrót na Księżyc został wykreślony z najbliższych planów NASA, ale lot na Marsa w latach 30. XXI wieku nadal jest w planach[183].
ESA ma nadzieję, że lądowanie człowieka na Marsie nastąpi między 2030 a 2035[184]. Poprzedzą ją kolejno coraz większe sondy, począwszy od ExoMars[185] i wspólnej misji NASA/ESA Mars Sample Return[186], której data realizacji, ze względu na koszty, pozostaje nieustalona.
Mars Direct jest propozycją stosunkowo taniej misji załogowej na Marsa, zaproponowaną przez założyciela Mars Society Roberta Zubrina. Do wyniesienia w przestrzeń kosmiczną dużych mas wykorzystane miałyby być przyszłe rakiety klasy Saturn V, aby pominąć budowę statku na orbicie okołoziemskiej[187]. Pewien wariant projektu zakłada, że astronauci nie będą natychmiast wracali na Ziemię, jeśli w ogóle mieliby kiedykolwiek powrócić[188].
Dzięki orbiterom, lądownikom i łazikom, jest obecnie możliwa obserwacja zjawisk astronomicznych z Marsa. Podczas gdy Fobos widziany z równika Marsa ma jedną trzecią średnicy kątowej Księżyca widzianego z Ziemi, Deimos oglądany gołym okiem wyglądałby bardziej na gwiazdę niż na księżyc (3′ średnicy kątowej) i byłby nieco jaśniejszy niż Wenus widziana z Ziemi[189].
Różne zjawiska znane z Ziemi są obecnie obserwowane na Marsie, na przykład meteory i zorze[169]. Z Marsa można obserwować przejście Merkurego na tle tarczy Słońca (tranzyt Merkurego), tranzyt Wenus, a także tranzyty Ziemi, z których najbliższy wystąpi 10 listopada 2084[190]. Księżyc Fobos ma na tyle małą średnicę kątową, że może powodować tylko częściowe zaćmienie Słońca; w przypadku Deimosa stopień przesłonięcia tarczy słonecznej jest znikomy, dlatego mówi się o tranzycie Deimosa, a nie o zaćmieniu[191][192]
Pogląd, że Mars zamieszkany jest przez inteligentnych Marsjan był szczególnie popularny pod koniec XIX wieku. Obserwacje Giovanniego Schiaparelliego i książki Percivala Lowella doprowadziły do powstania obrazu Marsa jako wysychającej, ochładzającej się planety, na której starożytna cywilizacja stworzyła wielkie kanały nawadniające[193].
Wiele innych obserwacji i oświadczeń znanych osobistości złożyło się na zjawisko kulturalne, zwane „marsjańską gorączką” (Mars Fever)[194]. W 1899 roku, podczas badania szumu radiowego atmosfery, wykorzystując swoje odbiorniki w laboratorium Colorado Springs, wynalazca Nikola Tesla obserwował powtarzające się sygnały, które następnie uznał za komunikaty radiowe z innej planety, przypuszczalnie z Marsa. W 1901 Tesla w wywiadzie powiedział:
Dopiero jakiś czas później w moim umyśle błysnęła myśl, że obserwowane zaburzenia mogą być spowodowane przez inteligentne działanie. Pomimo że nie mogłem odczytać ich sensu, było dla mnie niemożliwym, aby myśleć o nich jako o czymś zupełnie przypadkowym. Ciągle rośnie we mnie uczucie, że byłem pierwszym, który usłyszał pozdrowienie z innej planety[195].
Teorie Tesli zyskały poparcie Lorda Kelvina, który podczas wizyty w Stanach Zjednoczonych w 1902 roku miał powiedzieć, iż myśli, że Tesla wychwycił sygnały wysyłane z Marsa do Stanów Zjednoczonych[196]. Jednak Kelvin stanowczo zaprzeczył tym doniesieniom niedługo przed wyjazdem z Ameryki: Co naprawdę powiedziałem, to że mieszkańcy Marsa, jeśli istnieją, byliby niewątpliwie w stanie zobaczyć Nowy Jork, w szczególności w blasku energii elektrycznej[197].
W 1901 roku w artykule The New York Timesa dyrektor Harvard College Observatory, Edward Charles Pickering, powiedział, że otrzymał telegram od Lowell Observatory w Arizonie, który zdawał się potwierdzać, że Mars próbował komunikować się z Ziemią:
Na początku grudnia 1900 roku otrzymaliśmy z Lowell Observatory w Arizonie telegram, że zaobserwowano wiązkę światła z kierunku Marsa (obserwatorium Lowella specjalizowało się w obserwacjach Marsa), która trwała siedemdziesiąt minut. Przekazałem tę informację do Europy oraz rozesłałem neostylowe kopie na cały kraj. Tamtejszy obserwator jest ostrożnym i pewnym człowiekiem i nie ma powodu, by wątpić, że to światło istniało. Zostało podane, że pochodził ze znanego punktu geograficznego na Marsie. To wszystko. Teraz historia poszła na cały świat. W Europie twierdzą, że byłem w kontakcie z Marsem, powstają na tym tle wszelkiego rodzaju wyolbrzymienia. Czymkolwiek było to światło, nie mamy możliwości poznania. Czy to była inteligencja, czy nie, nikt tego nie może potwierdzić. To jest absolutnie niewytłumaczalne[198].
Pickering zaproponował później utworzenie zestawu luster w Teksasie z zamiarem komunikacji z Marsjanami[199].
Jeszcze w latach sześćdziesiątych XX wieku publikowano artykuły o biologii Marsa, odrzucające inne wyjaśnienia sezonowych zmian na planecie. Publikacje zawierały nawet szczegółowe scenariusze metabolizmu i cykli chemicznych funkcjonowania ekosystemu[200].
Dopiero po 1960 roku sondy NASA, wysłane na planetę w programie Mariner, rozwiały te mity na temat Marsa. Badania z lat 70. XX w. prowadzone przez lądownik programu Viking nie wykryły żadnych śladów życia, a przedstawiły powierzchnię planety jako surową i niegościnną dla życia[201].
Przy użyciu danych z tych misji sporządzono mapy Marsa, lecz dopiero sonda Mars Global Surveyor, wystrzelona w 1996 i pracująca do końca 2006 roku, sporządziła kompletne i niezwykle szczegółowe mapy topografii, pola magnetycznego i składników mineralnych powierzchni Marsa[202], wykazując całkowity brak śladów zamieszkania Marsa przez inteligentne istoty. Mapy te są obecnie dostępne w Internecie, na przykład w serwisie Google Mars.
Pseudonaukowe spekulacje na temat inteligentnego życia na Marsie trwają jednak do dziś. Niektóre oparte są na drobnych elementach postrzeganych w skali obrazów kosmicznych, takich jak „piramidy” i „Marsjańska Twarz”. Astronom Carl Sagan stwierdził:
Mars stał się swego rodzaju mityczną areną, na której ścierają się nasze ziemskie nadzieje i obawy[148].
Według obecnej wiedzy do powstania i utrzymania się życia niezbędne jest występowanie ciekłej wody na powierzchni planety. Wymaganie to określa strefę, zwaną ekosferą, w jakiej musi krążyć ona wokół gwiazdy; dla Słońca obszar ten rozciąga się obecnie na zewnątrz orbity Wenus, do odległości bliskiej długości półosi wielkiej orbity Marsa[203]. W pobliżu peryhelium Mars znajduje się wewnątrz tego regionu, ale niskie ciśnienie atmosferyczne uniemożliwia istnienie ciekłej wody na większości obszarów – lód nie topnieje, ale sublimuje do postaci pary wodnej z pominięciem cieczy. Występowanie zbiorników wodnych i dużych przepływów wskazuje, że w przeszłości na planecie panowały lepsze warunki do rozwoju życia niż obecnie, ale to nie rozstrzyga, czy żywe organizmy w ogóle na niej istniały. Najnowsze badania sugerują, że wody na powierzchni Marsa mogły być zbyt słone i kwaśne, by mogło się w nich rozwinąć życie podobne do ziemskiego[204].
Brak magnetosfery i bardzo cienka atmosfera Marsa stanowią duże wyzwania dla życia: transport ciepła na powierzchni jest niewielki, ochrona przed bombardowaniem meteorytami i wiatrem słonecznym jest słaba, a ciśnienie atmosferyczne nie wystarcza do utrzymania wody w stanie ciekłym. Prócz tego Mars jest prawie, a być może całkowicie nieaktywny geologicznie; koniec aktywności wulkanicznej sprawił, że nie ma obiegu substancji mineralnych pomiędzy powierzchnią i wnętrzem planety[205], jaki zachodzi na Ziemi.
Sondy Viking w połowie lat 70. prowadziły eksperymenty mające na celu wykrycie drobnoustrojów w marsjańskiej glebie w miejscach lądowania; wyniki zostały początkowo uznane za słabo pozytywne. Stwierdzono m.in. czasowy wzrost zawartości CO2 w próbce wystawionej na działanie wody i substancji odżywczych. Jednak wniosek ten był później kwestionowany przez wielu naukowców, prowadząc do długiej debaty; pracujący dla NASA Gilbert Levin stwierdził, że Viking mógł znaleźć życie. Ponowne analizy obecnie 30-letnich danych z Vikinga, w świetle współczesnej wiedzy o ekstremofilnych formach życia, sugerują, że testy Vikinga nie były wystarczająco zaawansowane, aby wykryć takie formy życia. Badania mogły nawet zabić hipotetyczne organizmy[206]. Testy przeprowadzone przez lądownik Phoenix wykazały, że grunt jest bardzo alkaliczny (ma wysokie pH) i zawiera związki magnezu, sodu, potasu i chloru. Zawartość składników odżywczych w regolicie może być wystarczająca do utrzymania życia, ale musi być ono chronione przed intensywnym promieniowaniem ultrafioletowym[207].
W 1996 roku w Centrum Lotów Kosmicznych imienia Lyndona B. Johnsona, w pochodzącym z Marsa meteorycie ALH 84001 zostały znalezione nietypowe mikroskopijne struktury. Niektórzy naukowcy sugerowali, że kształty te mogą być zachowanymi skamieniałościami bakterii żyjących niegdyś na Marsie, które wraz ze skałą zostały wyrzucone w przestrzeń kosmiczną, a po 15 milionach lat podróży dotarły na Ziemię. Wątpliwości budził od początku rozmiar tych struktur, mniejszych od wszystkich ziemskich bakterii. Wykazano także, że podobne twory mogą powstać w procesie nieorganicznym, bez udziału istot żywych[208].
Małe ilości metanu i formaldehydu niedawno wykryte przez orbitery Marsa mogą wskazywać na istnienie życia, ponieważ związki te szybko rozkładają się w atmosferze Marsa i muszą być na bieżąco uzupełniane[209][210]. Ale możliwe jest, że związki te są uwalniane przez procesy wulkaniczne lub geologiczne, takie jak serpentynizacja[102].
W 2013 roku łazik Curiosity odnalazł na powierzchni Marsa nadchlorany – substancje niszczące organiczne związki węgla. Oznacza to że ewentualne marsjańskie organizmy muszą żyć głębiej pod powierzchnią[211].
Profesor Martin Bizzarro z Uniwersytetu Kopenhaskiego w Danii wraz ze współpracownikami przeanalizował stężenie rzadkiego izotopu chromu, znanego jako chrom-54, w próbkach meteorytów, które przybyły na Ziemię z Marsa, aby oszacować, ile wody osadziło się na Czerwonej Planecie przez asteroidy. Duńscy planetolodzy uważają, że 4,5 mld lat temu Marsa pokrywał ocean o głębokości od 300 m do 1 km. Sugerują też, że na tej planecie warunki wspierające życie mogły istnieć dużo wcześniej, niż na Ziemi. Mars jest znany jako Czerwona Planeta, ale kiedyś mógł być niebieski z powodu pokrywającej go wody – twierdzą eksperci z Uniwersytetu w Kopenhadze[212].
Nazwa Mars pochodzi od rzymskiego boga wojny. W różnych kulturach, Mars reprezentuje męskość i młodość. Jego symbol, koło ze strzałką skierowaną w prawo i do góry, stosowany jest także jako symbol płci męskiej.
W dawnych Chinach Marsowi zgodnie z teorią pięciu elementów przypisywano żywioł ognia, południową część świata i kolor czerwony. Jego pojawienie się na niebie uznawano za zły znak, zapowiadający wojnę[213].
Nazwę Mars nosi baton czekoladowy produkowany przez Mars Incorporated[214].
Mars jest marką papierosów produkowaną przez firmę Mars Incorporated.
Mars odnosi się do typów pistoletów Bergmann i Webley.
Nazwę Mars noszą francuskie miejscowości w regionie Langwedocja-Roussillon, Rodan-Alpach i Limousin.
Jonathan Swift w 19 rozdziale swojej powieści Podróże Guliwera opisał księżyce Marsa, około 150 lat przed ich faktycznym odkryciem przez Asapha Halla, opis dość trafnie opisywał ich orbity. Wyobrażenie, że planeta ma dwa księżyce jest jednak jeszcze starsze i pochodzi od Keplera, który w 1610 roku mylnie odczytał anagram Galileusza i uznał, że ów astronom odkrył satelity Marsa[215].
Wyobraźnię twórców stymulowała intensywna czerwień powierzchni Marsa oraz dziewiętnastowieczne naukowe spekulacje, że warunki na powierzchni nie tylko mogą podtrzymywać życie, ale życie inteligentne[216]. Przypuszczenia te dały początek scenariuszom science fiction, wśród nich opublikowanej w 1898 roku przez H.G. Wellsa powieści Wojna światów, w których Marsjanie szukając ucieczki z umierającej planety, dokonują inwazji na Ziemię. Stworzona przez Orsona Wellesa adaptacja radiowa Wojny światów, została zaprezentowana 30 października 1938 roku w stacji CBS jako relacja na żywo, stając się przyczyną paniki wśród mieszkańców New Jersey, kiedy wielu słuchaczy wzięło ją za prawdę[217][218].
Inne znane dzieła, w których znaczącą rolę odgrywał Mars i Marsjanie to m.in.: zbiór Kroniki marsjańskie Raya Bradbury, w którym ludzcy odkrywcy przypadkowo niszczą cywilizację Marsa, cykl Barsoom Edgara Rice’a Burroughsa, książka Z milczącej planety C.S. Lewisa[219] i szereg książek Roberta Heinleina, pisanych do połowy lat sześćdziesiątych[220].
Arthur C. Clarke swoją pierwszą powieść Piaski Marsa (1951) poświęcił jego zasiedlaniu przez ludzi. Wydarzenia opisane w trylogii Kima Stanleya Robinsona: Czerwony Mars (1992)[221], Zielony Mars (1994)[222] i Błękitny Mars (1996)[223] oparte są na wizji kolonizacji i przystosowywania tej planety do ludzkich potrzeb. Autor koncentrował się jednak nie na aspektach technicznych przedsięwzięcia, lecz na wizji budowy nowego społeczeństwa ludzi – Marsjan. Również znany amerykański pisarz Ben Bova, poruszył tematykę eksploracji Marsa w trylogii Mars (1992), Powrót na Marsa (1999) i Życie na Marsie (2008), zawartej w cyklu Droga przez Układ Słoneczny.
Motyw marsjańskiej kolonii, walczącej o niezależność od Ziemi, jest elementem fabuły powieści Grega Beara i Kima Stanleya Robinsona, a także w filmie Pamięć absolutna na podstawie opowiadania Philipa K. Dicka i w serialu telewizyjnym „Babilon 5”. Niektóre gry wideo również korzystają z tego elementu, w tym Red Faction. Mars (i jego księżyce), były również miejscem akcji popularnych gier serii Doom i Martian Gothic.
Pierwszym polskim pisarzem, który poruszył temat Marsa był Stanisław Lem. W swojej pierwszej powieści Człowiek z Marsa (1947) przedstawił jakby własną wizję wojny światów[224]. Już w tej powieści pojawił się przewodni motyw niemożności porozumienia pomiędzy przedstawicielami odmiennych cywilizacji. W opowiadaniu Ananke (1971) temat kanałów marsjańskich, badań i kolonizacji Marsa jest jednym z ważnych motywów, natomiast sama planeta została nazwana świnią[225].
Kolejnym polskim pisarzem, który poruszył tematykę kolonizacji Marsa i kształtowania się tamtejszego społeczeństwa był Rafał Kosik. W swojej powieści Mars (2003) przedstawił wizję zdegenerowanej marsjańskiej cywilizacji, chylącej się ku upadkowi oraz katastrofy ekologicznej, do której doprowadziło nieumiejętne przeprowadzenie terraformacji[226].
Po tym, jak sondy kosmiczne Mariner i Viking przysłały zdjęcia ukazujące rzeczywiste oblicze Marsa, martwego i pozbawionego kanałów, te wyobrażenia Marsa zostały zarzucone na rzecz realistycznych wyobrażeń przyszłej kolonizacji Marsa przez ludzkość, z których do najbardziej znanych należy trylogia marsjańska (Czerwony Mars, Zielony Mars i Błękitny Mars) Kima Robinsona. Jednak pseudonaukowe spekulacje na temat Twarzy na Marsie i innych tajemniczych obiektów zauważonych przez sondy kosmiczne powodują, że życie na Marsie to nadal popularny motyw w twórczości science fiction, zwłaszcza w filmie[227].
Komiksowa postać inteligentnego Marsjanina Marvina pojawiła się w telewizji w 1948 roku w animowanej serii Zwariowane melodie i nadal jest obecna w kulturze masowej[228].
Kazik Staszewski na płycie Melassa nagrał piosenkę o inwazji Marsjan na Polskę pod tytułem Mars Napada[229]. Brytyjska grupa rockowa Muse skomponowała piosenkę pod tytułem Knights of Cydonia nawiązującą do Cydonii – obszaru znanego z Marsjańskiej Twarzy.
Do tematyki związanej z Marsem (a także Marsjanami) odnosi się wiele filmów – oto niektóre z nich:
Białka, proteiny – wielkocząsteczkowe biopolimery o masie cząsteczkowej od ok. 10 tys. do kilku mln Da, a właściwie biologiczne polikondensaty, zbudowane z aminokwasów połączonych ze sobą wiązaniami peptydowymi. Występują we wszystkich żywych organizmach oraz wirusach. Synteza białek odbywa się przy udziale specjalnych struktur komórkowych, zwanych rybosomami.
Głównymi pierwiastkami wchodzącymi w skład białek są C, O, H, N, S, także P oraz niekiedy kationy metali Mn2+, Zn2+, Mg2+, Fe2+, Cu2+, Co2+ i inne.
Skład ten nie pokrywa się ze składem aminokwasów. Wynika to stąd, że większość białek (są to tzw. białka złożone lub proteidy) ma dołączone do reszt aminokwasowych różne inne cząsteczki. Regułą jest przyłączanie cukrów, a ponadto kowalencyjnie lub za pomocą wiązań wodorowych dołączane może być wiele różnych związków organicznych pełniących funkcje koenzymów oraz jony metali.
Zsyntetyzowany w komórce łańcuch białkowy przypomina unoszącą się swobodnie w roztworze „nitkę”, która może przyjąć dowolny kształt (w biofizyce nazywa się to kłębkiem statystycznym), ale ulega procesowi tzw. zwijania białka (ang. protein folding) tworząc mniej lub bardziej sztywną strukturę przestrzenną, zwaną strukturą lub konformacją białka „natywną”. Zwykle tylko cząsteczki, które uległy zwinięciu do takiej struktury, mogą pełnić właściwą danemu białku rolę biochemiczną; istnieją jednak białka pozbawione struktury trzeciorzędowej stanowiące wyjątek od tej reguły.
Ze względu na skalę przestrzenną pełną strukturę białka można opisać na czterech poziomach:
Najczęściej skład pierwiastkowy białek przedstawiany jest następująco[3]:
Białka nie posiadają charakterystycznej dla siebie temperatury topnienia. Przy ogrzewaniu w roztworze, a tym bardziej w stanie stałym, ulegają, powyżej pewnej temperatury, nieodwracalnej denaturacji (ścinanie się włókien białka) – zmianie struktury, która czyni białko nieaktywnym biologicznie (codziennym przykładem takiej denaturacji jest smażenie lub gotowanie jajka)[2]. Jest to spowodowane nieodwracalną utratą trzeciorzędowej lub czwartorzędowej budowy białka.
Z tej przyczyny dla otrzymania suchej, ale niezdenaturowanej próbki danego białka, stosuje się metodę liofilizacji, czyli odparowywania wody lub innych rozpuszczalników z zamrożonej próbki pod zmniejszonym ciśnieniem.
Denaturacja białek może również zachodzić pod wpływem soli metali ciężkich, mocnych kwasów i zasad, niskocząsteczkowych alkoholi, aldehydów oraz napromieniowania. Wyjątek stanowią proste białka, które mogą ulegać także procesowi odwrotnemu, tzw. renaturacji – po usunięciu czynnika, który tę denaturację wywołał. Niewielka część białek ulega trwałej denaturacji pod wpływem zwiększonego stężenia soli w roztworze, jednak proces wysalania jest w większości przypadków w pełni odwracalny, dzięki czemu umożliwia izolowanie lub rozdzielanie białek.
Białka są na ogół rozpuszczalne w wodzie. Do białek nierozpuszczalnych w wodzie należą tzw. białka fibrylarne, występujące w skórze, ścięgnach, włosach (kolagen, keratyna) lub mięśniach (miozyna). Niektóre z białek mogą rozpuszczać się w rozcieńczonych kwasach lub zasadach, jeszcze inne w rozpuszczalnikach organicznych. Na rozpuszczalność białek ma wpływ stężenie soli nieorganicznych w roztworze, przy czym małe stężenie soli wpływa dodatnio na rozpuszczalność białek.
Jednak przy większym stężeniu następuje uszkodzenie otoczki solwatacyjnej, co powoduje wypadanie białek z roztworu. Proces ten nie narusza struktury białka, więc jest odwracalny i nosi nazwę wysalania białek.
Białka posiadają zdolność wiązania cząsteczek wody. Efekt ten nazywamy hydratacją. Nawet po otrzymaniu próbki suchego białka zawiera ona związane cząsteczki wody.
Białka, ze względu na obecność zasadowych grup NH2 oraz kwasowych COOH mają charakter obojnaczy – w zależności od pH roztworu będą zachowywały się jak kwasy (w roztworze zasadowym) lub jak zasady (w roztworze kwaśnym). Dzięki temu białka mogą pełnić rolę bufora stabilizującego pH, np. krwi[4]. Różnica pH nie może być jednak znaczna, gdyż białko może ulec denaturacji. Wypadkowy ładunek białka zależy od ilości aminokwasów kwaśnych i zasadowych w cząsteczce. Wartość pH, w której ładunki dodatnie i ujemne aminokwasów równoważą się nazywany jest punktem izoelektrycznym białka.
Białka odgrywają zasadniczą rolę we wszystkich procesach biologicznych. Biorą udział w katalizowaniu wielu przemian w układach biologicznych (enzymy są białkami), uczestniczą w transporcie wielu małych cząsteczek i jonów (np. 1 cząsteczka hemoglobiny przenosząca 4 cząsteczki tlenu), służą jako przeciwciała oraz biorą udział w przekazywaniu impulsów nerwowych jako białka receptorowe. Białka pełnią także funkcję mechaniczno-strukturalną. Wszystkie białka zbudowane są z aminokwasów. Niektóre białka zawierają nietypowe, rzadko spotykane aminokwasy, które uzupełniają ich podstawowy zestaw. Wiele aminokwasów (zazwyczaj ponad 100) połączonych ze sobą wiązaniami peptydowymi tworzy łańcuch polipeptydowy, w którym można wyróżnić dwa odmienne końce. Na jednym końcu łańcucha znajduje się niezablokowana grupa aminowa (tzw. N-koniec), na drugim niezablokowana grupa karboksylowa (C-koniec).
Istnieje wiele kryteriów podziału białek.
Ze względu na budowę i skład, dzielimy białka na proste i złożone.
Białka proste (proteiny) zbudowane są wyłącznie z aminokwasów. Dzielimy je na następujące grupy:
Białka złożone (dawniej – proteidy):
Białka dzielimy również ze względu na właściwości odżywcze – wyróżnia się białka doborowe i niedoborowe.
Białka mają następujące funkcje:
U ludzi trawienie białek zaczyna się dopiero w żołądku, gdzie komórki główne komórek gruczołowych żołądka wydzielają nieczynny enzym pepsynogen. Komórki okładzinowe wydzielają kwas solny, w obecności którego pepsynogen przekształca się w postać czynną – pepsynę. W dwunastnicy działają trypsyna i chymotrypsyna, które rozkładają cząsteczki polipeptydów do tripeptydów i dipeptydów. Te z kolei rozkładane są przez peptydazy ściany jelita cienkiego do aminokwasów, które zostają wchłaniane do krwi za pomocą odpowiednich przenośników znajdujących się w rąbku szczoteczkowym i żyłą wrotną wędrują do wątroby. Stamtąd większość aminokwasów dalej dostaje się z krwią do komórek ciała. Nadwyżka pozbawiana jest reszt aminowych, przez co powstaje amoniak i ketokwasy. Amoniak przekształcany jest w mniej toksyczny mocznik, który z krwią odtransportowywany jest do nerek. Natomiast ketokwasy mogą zostać wykorzystane do syntezy cukrów i niektórych aminokwasów, zużyte na cele energetyczne lub przekształcone w tłuszcze zapasowe.
Oddychanie (łac. respiratio – oddychanie[1]) – procesy życiowe związane z uzyskiwaniem przez organizmy energii użytecznej biologicznie:
Procesy te często są sprzężone, jednak nie zawsze muszą być powiązane bezpośrednio, co może prowadzić do niejednoznaczności. Ponadto definicje podawane przez specjalistów z różnych dziedzin kładą nacisk zwykle na jeden z tych aspektów, czasem bagatelizując drugi. Oddychanie zewnętrzne jest konieczne dla organizmów, u których w oddychaniu komórkowym biorą udział gazy (jako substraty lub produkty). Jest ono podkreślane w definicjach spotykanych w zoologii lub medycynie. Oddychanie jest wówczas definiowane jako proces wymiany gazów pomiędzy organizmem a jego środowiskiem, zapewniający dopływ tlenu do tkanek i stałe usuwanie dwutlenku węgla, a celem ostatecznym jest zdobycie podczas procesu utleniania substancji organicznych energii potrzebnej do życia[2]. Istnieją jednak organizmy, u których występuje wyłącznie oddychanie komórkowe, zupełnie niezwiązane z wymianą gazów (np. organizmy beztlenowe). W związku z tym oddychanie często definiowane jest jako ogół procesów prowadzących do uzyskania energii niezbędnej do pozostałych procesów życiowych w drodze utleniania związków organicznych, przy czym u organizmów oddychających tlenowo oddychanie obejmuje też procesy wymiany gazowej[3].
W materiałoznawstwie oddychaniem określa się zdolność materiałów do wybiórczego przepuszczania gazów (w tym pary wodnej)[4][5].
Oddychanie zewnętrzne – inaczej wymiana gazowa między otoczeniem a organizmem żywym[6] – jedna z podstawowych czynności organizmu, złożony proces biologiczny zachodzący bezustannie w organizmie, polegający na dostarczaniu do komórek organizmu gazów niezbędnych do uwolnienia energii użytecznej biologicznie[7][8]. Oddychanie zewnętrzne zachodzi przez powłoki komórki i ciała. Jest procesem związanym z wymianą gazową, jednak nie są to pojęcia tożsame. W przypadku, gdy oddychanie powłokami ciała jest niewystarczające, np. ciało organizmu pokryte jest trudno przepuszczalną warstwą (naskórek, oskórek), zachodzi ono głównie w wybranych miejscach dzięki wentylującym ruchom oddechowym. Umożliwia to kontrolę nad wymienianiem i transportem gazów, nie tylko związanych z oddychaniem.
Oddychanie komórkowe – nazywane oddychaniem[9] – wielostopniowy proces utleniania związany z wytwarzaniem energii użytecznej metabolicznie. Ze względu na środowisko życia organizmu rozróżniamy oddychanie tlenowe (przy udziale tlenu pobieranego z powietrza lub z wody) i oddychanie beztlenowe[7], które odbywa się w środowisku beztlenowym.
U organizmów fotosyntetyzujących wymiana gazowa związana jest nie tylko z wymianą gazów oddechowych, ale również z pobieraniem dwutlenku węgla będącego substratem fotosyntezy i wydaleniem tlenu będącego jej produktem ubocznym. Podobnie jest u organizmów pochłaniających lub wydalających gazy podczas chemosyntezy (np. bakterii metanowych) oraz innych procesów metabolicznych. Dlatego w przypadku roślin, grzybów, protistów i prokariontów stosowanie pojęcia oddychanie w znaczeniu innym niż oddychanie komórkowe jest rzadkie i może mieć charakter potoczny.
Jelito cienkie (łac. intestinum tenue) – najdłuższa część przewodu pokarmowego, położona pomiędzy żołądkiem a jelitem grubym, od którego oddziela się poprzez zastawkę krętniczo-kątniczą. W obrębie jamy brzusznej jelito cienkie zajmuje okolicę pępkową, podbrzuszną i obie okolice biodrowe, a częściowo też miednicę małą. Jego długość jest osobniczo zmienna, zależy też od wieku i od stanu skurczu błony mięśniowej. Średnia długość jelita cienkiego to 5–6 m, natomiast na zwłokach wynosi ona od 6 do 8[1]. Średnica (czyli inaczej światło jelita) ma około 3 cm. U noworodków i dzieci jelito cienkie jest stosunkowo dłuższe niż u dorosłych – przypuszczalnie w związku z większą pojemnością jamy brzusznej.
Jelito cienkie dzielimy na trzy podstawowe części. Są to, kolejno: dwunastnica, jelito czcze oraz jelito kręte. Stosunkowo ściśle daje się odgraniczyć jedynie dwunastnica; jelito czcze przechodzi w kręte bez wyraźnej granicy.
Rurkowaty narząd o długości około 30 cm. Łączy się z żołądkiem, a następnie przechodzi w jelito czcze. Tutaj wpada żółć z wątroby oraz sok trzustkowy z trzustki (o odczynie zasadowym, w ilości około 2,5 litra na dobę). Przechodzi w jelito czcze na wysokości zgięcia dwunastniczo-czczego[2].
Rurkowaty narząd o długości 2–5 m. W błonie śluzowej jelita cienkiego występują liczne gruczoły, wydzielające sok jelitowy lub zasadowy śluz (około 2 litrów na dobę). Błona śluzowa ma również mnóstwo malutkich, unerwionych wypustek, do których dochodzą bardzo cienkie naczynia krwionośne i limfatyczne. Każda wypustka (kosmek) pokryta jest mikrokosmkami. Wchłania mleczko pokarmowe i za pośrednictwem krwi dostarcza pożywienie do każdej żywej komórki organizmu. W jelicie czczym odbywa się zasadnicza część trawienia.
W jelicie krętym zachodzi końcowe trawienie pokarmów. Jelito kręte kończy się zastawką krętniczo-kątniczą i przechodzi w jelito grube.
Jelito czcze różni się od jelita krętego następującymi cechami:
Funkcją jelit jest wchłanianie składników pokarmowych. Wewnątrz jelito pokryte jest wypustkami (kosmkami), które wchłaniają strawiony pokarm, skąd przechodzi on do organizmu.
Unaczynienie tętnicze jelita cienkiego wykazuje dużą zmienność, co powoduje, że w wielu przypadkach jego anatomia jest trudna do przewidzenia. Tętnica krezkowa górna odchodząca od przedniej ściany aorty daje zmienną liczbę odgałęzień tętnic jelitowych, które przebiegają w krezce jelita cienkiego. Początkowo biegną prosto, a dalej tworzą charakterystyczne "arkady" - zespolenia z sąsiednimi naczyniami. Unaczynienie dwunastnicy pochodzi od pnia trzewnego – zapewniają je gałązki tętnicy żołądkowo-dwunastniczej[3]. Unerwienie jest autonomiczne i niezależne od woli. Jelita mają własny system rozrusznikowy, który odpowiada za perystaltykę – komórki Cajala.
Płuco (łac. pulmo) – pojedynczy lub parzysty narząd oddechowy kręgowców. Służy do wymiany gazowej (potocznie oddychania). Do kręgowców tych zaliczane są też – prócz płazów, gadów, ptaków i ssaków – ryby dwudyszne, które w niesprzyjających warunkach atmosferycznych prowadzą wymianę gazową za pomocą workowatego płuca, powstałego z przekształconego pęcherza pławnego.
W ciągu rozwoju rodowego kręgowców z przedniego odcinka jelita powstają dwa różne narządy oddechowe – płuca i skrzela. U ryb ulegają degeneracji płuca (z wyjątkiem wspomnianych już ryb dwudysznych), a u zwierząt lądowych – skrzela. Przypuszczalnie płuca pochodzą z aparatu hydrostatycznego podobnego do pęcherza pławnego ryb (według niektórych pęcherz pławny ryb jest zdegenerowanym płucem). Występują jednak wątpliwości, gdyż pęcherz pławny powstaje po stronie tylnej (grzbietowej) cewy jelitowej, płuca natomiast po stronie przedniej (brzusznej).
Wykształcenie płuc zostało wywołane przez wzmożone zapotrzebowanie na tlen zwierząt lądowych. Również ich rozwój (coraz silniejsze fałdowanie) ma ścisłe powiązanie z zapotrzebowaniami energetycznymi organizmów. Listkiem zarodkowym, z którego powstają płuca, jest entoderma.
U form dorosłych płazów występują workowate płuca, które wspomagane są w wymianie gazowej przez skórę i błony śluzowe jamy gębowo-gardzielowej. Niektóre płazy posiadają płuca gładkie, jednak u większości występują w nich fałdy sterczące do wewnątrz i krzyżujące się ze sobą – co zwiększa powierzchnię wymiany gazowej. Wentylacja płuc płazów odbywa się przez rytmiczne obniżanie i podnoszenie jamy gębowej (brak jest klatki piersiowej). Wśród płazów występują takie (np. salamandra bezpłucna), które nie posiadają płuc, a wymiana gazowa odbywa się powierzchnią całego ciała.
Płuca gadów podzielone są na liczne komory, mają strukturę gąbczastą (tzw. płuca gąbczaste). Gadzie płuca są silniej pofałdowane od tych u płazów, a ich wentylacja odbywa się dzięki mięśniom międzyżebrowym klatki piersiowej.
Płuca ptaków są jednym z bardziej skomplikowanych organów oddechowych zwierząt. W swojej strukturze zawierają miliony cienkich rurek znanych jako parabronchi. Mają one porowatą strukturę umożliwiającą penetrację powietrza poprzez kapilary w głąb sąsiadujących naczyń krwionośnych (blood capilaries). Narządami wspomagającymi płuca są worki powietrzne, dzięki którym możliwy jest stały przepływ powietrza przez płuca w sposób zapobiegający mieszanie się powietrza świeżego z powietrzem zużytym. Wentylacja płuc u ptaków jest dwojaka – inna podczas lotu, a inna podczas chodu. W czasie lotu klatka piersiowa ulega zablokowaniu (mięśnie międzyżebrowe są wyłączone), a powietrze jest wypychane poprzez ucisk skrzydeł na worki powietrzne. Podczas spoczynku wentylacja zachodzi dzięki mięśniom klatki piersiowej oraz mięśniom brzucha. Są to płuca błoniaste lub rzadziej używana nazwa – rurkowate.
Najlepiej rozwinięte, duże płuca o budowie pęcherzykowatej występują u ssaków. Powiększenie powierzchni oddechowej (u ssaków jest ona 40 razy większa niż u płazów[potrzebny przypis]) było możliwe dzięki wytworzeniu pęcherzyków płucnych. Wentylacja płuc odbywa się przez pracę mięśni międzyżebrowych oraz przepony. Przepona i mięśnie międzyżebrowe kurczą się, tworząc w płucach podciśnienie, dzięki czemu powietrze jest zasysane.
Budowa płuca, a dokładniej podział na płaty, jest gatunkowo zmienna. Płuco lewe dzieli się płat doczaszkowy – dodatkowo rozdzielony na część doczaszkową i doogonową – oraz płat doogonowy płuca lewego. U koni nie występuje wtórny podział płata doczaszkowego. Płuco prawe dzieli się na płat doczaszkowy, płat środkowy, płat dodatkowy oraz płat doogonowy płuca prawego. Przeżuwacze, a niekiedy świnie, mają płat doczaszkowy podzielony na część doczaszkową i doogonową. Płuco prawe konia pozbawione jest płata środkowego[1].
Płuca dorosłego człowieka mogą pomieścić do ok. 5 litrów powietrza. Dorosły człowiek robi od 16 do 24 oddechów na minutę. W płucach zachodzi wymiana gazowa. Powietrze do płuc dostarczane jest poprzez drogi oddechowe i drzewo oskrzelowe liczące 16 generacji podziałów i wnikające do płuc. Od tchawicy odchodzą oskrzela główne, następnie (w płucach): oskrzela płatowe, segmentowe, podsegmentowe, zrazikowe. Potem oskrzeliki i oskrzeliki końcowe które doprowadzają powietrze do drzewa oddechowego. Drzewo oddechowe to generacje 17-23, kończące się pęcherzykami płucnymi – wymieniającymi tlen i dwutlenek węgla z krwią. Całe płuca pokryte są opłucną – delikatną tkanką okrywającą. W organizmie człowieka występuje płuco prawe i lewe. Prawe trzypłatowe, lewe dwupłatowe. Związane jest to ze zdecydowaną różnicą miejsca po lewej i prawej stronie klatki piersiowej – w lewej stronie znajduje się serce, co uniemożliwia usytuowanie trzeciego płatu. Swoistym odpowiednikiem środkowego płata w płucu lewym jest języczek płuca lewego, należy on jednak do płata górnego[2]. Płuca są jednymi z ważniejszych organów w ciele człowieka. Dostarczają tlen hemoglobinie, która transportuje go do każdej komórki w organizmie człowieka, umożliwiając oddychanie komórkowe. Wydolność oddechowa zależy od: pojemności płuc, drożności dróg oddechowych, ilości tlenu w powietrzu, ciśnienia krwi oraz ilości czerwonych krwinek we krwi. Powierzchnia obu płuc człowieka, dzięki ich pęcherzykowatej budowie wynosi około 100 m².[potrzebny przypis]
W każdym płucu wyróżniamy szczyt, oraz trzy powierzchnie: żebrową, śródpiersiową i przeponową.
Na szczególną uwagę zasługują wnęki obu płuc znajdujące się na ich powierzchniach śródpiersiowych:
We wnęce płuca lewego znajdują się (licząc od góry):
We wnęce płuca prawego znajdują się (licząc od góry):
Prócz tego w obrębie obu wnęk znajdują się:
Powyższe elementy znajdujące się we wnękach płuc tworzą korzenie płuc.
Płuca w organizmie człowieka składają się z płatów rozdzielonych od siebie szczelinami. Wynika to z podziału anatomicznego widocznego wyraźnie na płucach. Podział segmentowy wynika już z budowy drzewa oskrzelowego i na płucach go nie uświadczymy. 
Płuco lewe:
Płuco prawe:
Zaburzenia psychiczne[1] – wzorce lub zespoły zachowań, sposobów myślenia, czucia, postrzegania oraz innych czynności umysłowych i relacji z innymi ludźmi, będące źródłem cierpienia lub utrudnień w indywidualnym funkcjonowaniu dotkniętej nimi osoby[2][3]. Zaburzenia te mogą być utrwalone, nawracające, mogą stopniowo ustępować (remisja) lub przebiegać jako pojedynczy epizod. 
Zaburzenia psychiczne przebiegać mogą np. z dużymi zaburzeniami emocji i nastroju, zaburzeniami myślenia i złożonej aktywności, zaburzeniami świadomości, a także z objawami wytwórczymi, np. urojeniami i omamami. Do zaburzeń psychicznych zaliczamy zaburzenia lękowe, zaburzenia obsesyjno-kompulsywne, zaburzenia psychotyczne, zaburzenia afektywne, część zaburzeń psychosomatycznych, zaburzenia neurorozwojowe, zaburzenia psychiczne organiczne, zaburzenia osobowości, zaburzenia związane z uzależnieniami, niektóre dewiacje seksualne. Różnorakie podziały chorób psychicznych nie są jednak ścisłe, bowiem zaburzenia psychiczne często łączą w sobie objawy charakterystyczne dla innych jednostek chorobowych (np. ciężki epizod depresji z objawami psychotycznymi), a granica pomiędzy tzw. normą psychiczną a patologią może być płynna i w dużej mierze uwarunkowana czynnikami społeczno-kulturowymi[4].
Wyodrębnienia większości poszczególnych zaburzeń psychicznych w aktualnie stosowanych klasyfikacjach takich, jak DSM-5 czy ICD-10, dokonano głównie ze względów praktycznych, związanych z zasadami postępowania medycznego, społecznego lub prawnego. Co więcej, autorzy owych klasyfikacji posługują się bardziej ogólnym terminem „zaburzenie” (ang. disorder) celem uniknięcia poważniejszych wątpliwości dotyczących terminu „choroba” (ang. disease lub illness). Niemniej jednak niektóre źródła piśmiennicze posługują się zamiennie pojęciami „zaburzenie psychiczne” i „choroba psychiczna”, pomijając różnice pomiędzy konotacjami tych terminów[5].
W polskim systemie prawnym pojęciu „choroby psychicznej” nadano specyficzne znaczenie, odrębne od znaczenia terminu „zaburzenie”. Osoby cierpiące na tak rozumiane choroby psychiczne w Polsce mają większy dostęp do nieodpłatnych świadczeń zdrowotnych, niż ogół osób z zaburzeniami psychicznymi, ale też tylko wobec osób z chorobą psychiczną jest możliwe postępowanie lecznicze bez ich zgody. Jeszcze węższym sposobem definiowania „chorób psychicznych” jest utożsamianie ich z psychozami[5].
Pojęcie zaburzenia wiąże się z pojęciami zdrowia psychicznego, normalności zachowania oraz jego patologii.
Określenie granic normalności jest często zbędne, zwykle też bardzo trudne. Psychiatria biologiczna wiąże zaburzenia psychiczne z przyczynami neurochemicznymi. Psychologia najczęściej łączy większość zaburzeń z czynnikami intrapsychicznymi, doświadczeniami społecznymi i poziomem nasilenia stresu. Skrajne podejścia, jak antypsychiatria czy psychologia systemowa, przerzucają ciężar patologii z procesów intrapsychicznych na interakcje społeczne, zwłaszcza związane z rodziną pochodzenia.
Ponad 1,5 mln osób w ciągu roku trafia do szpitali psychiatrycznych (wzrost o około 900 tys. od 1990 r. do 2004, największy odsetek w Europie). W 2007 roku około 100 tys. osób znalazło się w polskich szpitalach psychiatrycznych z najcięższymi schorzeniami psychotycznymi[8] (dane z 2007 r.).
Antybiotyki (z gr. ἀντί, anti „przeciw” i βίος, bios „życie”) – naturalne wtórne produkty metabolizmu mikroorganizmów, które działając wybiórczo w niskich stężeniach wpływają na struktury komórkowe lub procesy metaboliczne innych mikroorganizmów, hamując ich wzrost i podziały. Antybiotyki są przedmiotem badań auksanografii, stosuje się je jako środki w leczeniu wszelkiego rodzaju zakażeń bakteryjnych. Bywają także używane profilaktycznie w zapobieganiu zakażeniom bakteryjnym w przypadku osłabienia odporności, na przykład neutropenii, a także w profilaktyce bakteryjnego zapalenia wsierdzia.
Nazwa odwołuje się do zabójczego dla żywych bakterii działania antybiotyków. Naukowcy wiedzieli już w XIX wieku, że niektóre organizmy przeciwdziałają rozwojowi bakterii. Zjawisko to nazwano antybiozą.
Odkrycie pierwszego antybiotyku (penicyliny) zostało dokonane w 1928 roku przez Alexandra Fleminga[1], który zauważył, że przypadkowe zanieczyszczenie podłoża pleśnią Penicillium chrysogenum powstrzymuje wzrost kultur bakterii z rodzaju Staphylococcus[2].
Oprócz pleśni zdolnością wytwarzania antybiotyków wyróżniają się promieniowce i niektóre bakterie.
Wkrótce po odkryciu penicyliny pojawiły się następne antybiotyki: naturalne, półsyntetyczne i syntetyczne. Wprowadzenie antybiotyków do lecznictwa było przełomem dającym lekarzom oręż do walki z chorobami zakaźnymi, które do tej pory były przyczyną śmierci i chorób setek milionów osób.
Pod względem budowy chemicznej antybiotyki należą do różnych grup związków organicznych. Z tysięcy naturalnie występujących antybiotyków zaledwie kilkadziesiąt mogło być włączonych do leczenia ludzi i zwierząt. Pozostałe nie znajdują zastosowania w medycynie ze względu na toksyczność lub działania niepożądane.
Działanie antybiotyków polega na powodowaniu śmierci komórki bakteryjnej (działanie bakteriobójcze) lub wpływaniu w taki sposób na jej metabolizm, aby ograniczyć jej możliwości rozmnażania się (działanie bakteriostatyczne).
Leczenie chorób zakaźnych polega na zabiciu mikroorganizmów wywołujących chorobę. Trudność terapii, z którą borykali się lekarze przed erą antybiotyków polegała na tym, żeby znaleźć środek jednocześnie zabójczy dla chorobotwórczych bakterii i bezpieczny dla gospodarza.
Antybiotyki zazwyczaj zakłócają procesy metaboliczne mikroorganizmów. Podstawą terapii antybiotykami jest zasada selektywnej toksyczności Ehrliha, zgodnie z którą antybiotykiem jest substancja, która w organizmie, w stężeniu nie wykazującym większej toksyczności dla ludzi i zwierząt wyższych, powoduje uszkodzenie lub śmierć mikroorganizmów. Można to osiągnąć przez stosowanie substancji oddziałujących na takie struktury, które są obecne w komórkach mikroorganizmów, a których nie ma w organizmie człowieka lub występują w nim w innej formie.
Główne mechanizmy działania antybiotyków to:
Osobnym problemem jest szkodliwość dla naturalnej flory bakteryjnej człowieka.
(podział ze względu na budowę chemiczną)
Antybiotyki
Leki przeciwprątkowe (leczenie zakażeń wywołanych przez prątki – np. gruźlicy):
Leki przeciwgrzybicze:
Antybiotyki można podzielić również według innych kryteriów, mających większe znaczenie ze względu na zastosowanie antybiotyków w terapii. Leki te różnią się między innymi:
Antybiotyki są lekami względnie mało toksycznymi, ich właściwości toksyczne są znacznie większe w stosunku do mikroorganizmu niż do organizmu gospodarza. Niemniej, niektóre antybiotyki mogą wywoływać działania niepożądane. Wyróżniamy trzy główne grupy niepożądanych działań antybiotyków:
Oporność na antybiotyki jest cechą pewnych szczepów bakteryjnych, która umożliwia im przetrwanie w obecności antybiotyku. W zależności od pochodzenia, dzieli się ją na pierwotną (naturalna struktura bakterii uniemożliwiająca działanie leku) lub nabytą – na skutek nabycia genów oporności od innych bakterii lub spontanicznych mutacji. Częsta oporność wśród bakterii wiąże się z nieracjonalną antybiotykoterapią oraz zbyt dużym zużyciem tych leków w przemyśle spożywczym. W wielu krajach (np. USA) legalne jest też stosowanie niewielkich dawek antybiotyków podczas hodowli dla podniesienia masy trzody chlewnej. Dawki te są zbyt małe, aby zabić wszystkie bakterie, świetnie więc stymulują wyrobienie przez bakterie antybiotykooporności[3].
Naturalne antybiotyki są produkowane przez niektóre gatunki grzybów, szczególnie pleśni Penicillium, oraz przez niektóre bakterie, np. z rzędu promieniowców. Syntetyzowane w ich organizmach antybiotyki nie są szkodliwe dla nich samych dzięki licznym mechanizmom obronnym.
Zdolność do wytwarzania antybiotyków jest ewolucyjnym przystosowaniem organizmów do życia w środowisku naturalnym. Daje posiadającym ją mikroorganizmom przewagę nad innymi, utrudniając wrażliwym szczepom wzrost i tym samym uniemożliwia im konkurowanie w walce o dostęp do pożywienia.
Przemysł medyczny koncentruje się na wytwarzaniu antybiotyków półsyntetycznych:
Niektóre antybiotyki uzyskuje się na skalę przemysłową metodami syntezy chemicznej. Wiele z nich jest produkowanych metodami biotechnologicznymi w wielkich fermentorach (bioreaktorach) o pojemności ok. 50 do 300 m³. W bioreaktorach przeprowadza się reakcje biosyntezy antybiotyków naturalnych przez odpowiednie mikroorganizmy oraz reakcje biotransformacji. Biotransformacja (w syntezie antybiotyków) polega na przekształceniu jednych związków chemicznych (substratów) w inne (produkty) za pomocą enzymów formie czystej, mikroorganizmów lub komórek organizmów wyższych.
Produkt, jaki możemy otrzymać z danego substratu, zależy od wielu czynników. Najważniejsze z nich to:
Zwierzęta (Animalia) – królestwo obejmujące wielokomórkowe organizmy cudzożywne o komórkach eukariotycznych, bez ściany komórkowej, w większości zdolne do aktywnego poruszania się. Są najbardziej zróżnicowanym gatunkowo królestwem organizmów. Tradycyjnie dzielone są sztuczne grupy bezkręgowców i kręgowców, wśród których wyróżnia się: ryby, płazy, gady, ptaki i ssaki, włącznie z człowiekiem.
Najstarsze znaleziska kopalne zwierząt – morskie zwierzęta o miękkich ciałach – pochodzą z końca prekambru, neoproterozoiku (Otavia antiqua – 760 mln lat temu[2], fauna ediakarańska – 630 do 542 mln lat temu), natomiast skamieniałości strunowców (w tym pierwszych kręgowców[3]) – z kambru i ordowiku. W kambrze, około 500 mln lat temu, występowali już przedstawiciele wszystkich znanych obecnie typów bezkręgowców.
W nomenklaturze zoologicznej zwierzęta (łac. animal, animalis[4]; gr. ζώον, zōon[5]) klasyfikowane są jako takson w randze królestwa (regnum). Takson ten obejmuje wszystkie gatunki zwierząt, w tym również człowieka (Homo sapiens). Poza terminologią fachową wyraz „zwierzę” określa każde żywe stworzenie z wyjątkiem człowieka[6].
Nauka o zwierzętach to zoologia, ich klasyfikacją zajmuje się systematyka biologiczna, a chorobami zwierząt – medycyna weterynaryjna. Wszystkie gatunki zwierząt występujące na danym obszarze to fauna (np. fauna Polski).
Za cechy odróżniające zwierzęta od innych organizmów przyjmuje się sposób odżywiania, brak ściany komórkowej, gromadzenie glikogenu oraz obecność (u wyżej uorganizowanych zwierząt) układu mięśniowego i nerwowego[7].
Zwierzęta nie potrafią samodzielnie wytwarzać substancji odżywczych. Wszystkie są cudzożywne, tzn. żywią się innymi organizmami, ich szczątkami lub odchodami. W ten sposób żywią się również grzyby. Zwierzęta wyspecjalizowały się w wielu różnych metodach zdobywania i konsumowania pokarmu.
Ciała zwierząt zbudowane są zawsze z komórek eukariotycznych. Taką cechę mają też komórki roślinne. W odróżnieniu od nich komórki zwierzęce nie mają chloroplastów i ścian komórkowych. Są otoczone cienką błoną komórkową. Wytwarzają kolagen. Brakiem ściany komórkowej charakteryzują się, oprócz zwierząt, niektóre grzyby.
Funkcjonalnie zróżnicowane komórki zwierząt zorganizowane są w zespoły zwane tkankami (z wyjątkiem gąbek), a tkanki w narządy, tworzące z kolei wyspecjalizowane układy narządów, pełniące w organizmie zwierzęcia określone funkcje życiowe (np. układ krwionośny, oddechowy, wydalniczy). Wszystkie zwierzęta odbierają bodźce zewnętrzne za pomocą receptorów i reagują na nie odruchami.
Gromadzenie glikogenu, jako materiału zapasowego jest charakterystyczne dla zwierząt i wielu grzybów.
Zwierzęta, choć nie wszystkie ich typy, są jedynymi organizmami, u których występują układy mięśniowy i nerwowy. Konieczność poszukiwania pokarmu doprowadziła u zwierząt do wykształcenia zdolności do aktywnego ruchu. Zdecydowana większość z nich wykształciła wyspecjalizowane tkanki mięśniowe tworzące układ mięśniowy współpracujący z narządami ruchu. Niektóre (gąbki i polipy parzydełkowców) prowadzą wprawdzie osiadły tryb życia, ale komórki gąbek są zdolne do ograniczonego ruchu, u większości z nich występuje swobodnie pływające stadium larwalne, a parzydełkowce są osiadłe jedynie w stadium polipa.
Aktywne poruszanie się wykształciło u zwierząt narządy zmysłów współpracujące z układem nerwowym.
Niemal wszystkie zwierzęta rozmnażają się płciowo, przynajmniej w pewnym stadium swego cyklu życiowego. Niektóre potrafią rozmnażać się bezpłciowo.
Pochodzenie zwierząt nie zostało dotychczas wyjaśnione, głównie z powodu braku szczątków kopalnych pierwotnych tkankowców. Naukowcy rozważają kilka hipotez wywodzących pochodzenie zwierząt od pierwotniaków, m.in. hipotezy cellularyzacji i integracji. Przyjmuje się, że przodkami zwierząt byli magazynujący glikogen i zaopatrzeni w wić przedstawiciele królestwa Protista. Razem z grzybami i niektórymi pierwotniakami łączone są w supergrupę Opisthokonta[8]. Tak jak u przodków roślin, u zwierząt w procesie komplikowania budowy pojawił się okres występowania kolonijnego. Poszczególne organizmy kolonii rozpoczęły różnicować swoje funkcje, co doprowadziło do powstania tkanek.
Odkrycia dokonane w latach 2009–2012 w rejonie Półwyspu Arabskiego oraz południowych Chin sugerują, że najstarsze zwierzęta pojawiły się ok. 600 mln lat temu (Chiny – 570 mln, Oman – 635 mln), należały do typu gąbek i wywodziły się z organizmów jednokomórkowych podobnych do ameby[9][10]. Wiek najstarszych zwierząt jest jednak prawdopodobnie zaniżony – w 2012 roku w Urugwaju zostały znalezione ślady pozostawione przez wędrujące zwierzęta dwubocznie symetryczne, pochodzące sprzed 585 mln lat[11].
W systematyce organizmów zwierzęta klasyfikowane są w randze królestwa Animalia. Jednostka ta obejmuje ponad 1,3 mln[7][12] współczesnych gatunków. Ich liczba, różnorodność, doskonalenie metod badawczych i wyniki prowadzonych badań przyczyniają się do ciągłych modyfikacji (rewizji) systemów klasyfikacji biologicznej.
Jeszcze pod koniec XX wieku do królestwa zwierząt zaliczano eukariotyczne, heterotroficzne organizmy jednokomórkowe określane nazwą pierwotniaki (Protozoa). Obecnie są one klasyfikowane poza królestwem zwierząt – w zależności od ujęcia, w taksonie Protozoa lub Protista.
Królestwo Animalia dzielone jest na dwa podkrólestwa. Pierwsze z nich obejmuje zwierzęta, które nie mają właściwych tkanek, organów (narządów), układu mięśniowego i nerwowego, czyli cech typowych dla tkankowców. We współczesnych klasyfikacjach określane jest naukową nazwą Parazoa, a w języku polskim nazwami zwyczajowymi beztkankowce, przedtkankowce, nietkankowce lub nibytkankowce. Należy do nich jeden typ – gąbki.
Drugie podkrólestwo obejmuje zwierzęta tkankowe grupowane w 35–40 typach. Liczba wyróżnianych typów jest zależna od autora (lub autorów) danej klasyfikacji. Naukowa nazwa tego podkrólestwa to Eumetazoa lub Epitheliozoa. Polskie nazwy zwyczajowe to tkankowce właściwe lub wielokomórkowce właściwe.
Jeden gatunek (Trichoplax adhaerens) ma niejasną pozycję taksonomiczną. Zaliczono go do typu płaskowców (Placozoa), ale nie ma pewności, czy zaliczyć go do zwierząt tkankowych. Klasyfikacje, w których przyjęto takie założenie, wymieniają Epitheliozoa jako podkrólestwo zwierząt obejmujące dwie grupy: Placozoa i Eumetazoa.
W 2019 roku królestwo zwierząt (Animalia) obejmowało następujące typy[13]:
Ryby – tradycyjna nazwa zmiennocieplnych, pierwotnie wodnych kręgowców, oddychających skrzelami i poruszających się za pomocą płetw. Obejmuje bezżuchwowce krągłouste (Cyclostomata) oraz mające szczęki ryby właściwe (Pisces).
Ryby stanowią najliczniejszą i najbardziej zróżnicowaną grupę współcześnie żyjących kręgowców (ponad połowę). Różnią się od siebie pod względem budowy zewnętrznej i wewnętrznej, ubarwienia oraz przystosowania do warunków środowiska. Ponad 32 tysiące współcześnie żyjących gatunków opisano naukowo, a co roku naukowcy opisują 100–150 nowych gatunków morskich i nieco więcej słodkowodnych[1]. Szacuje się, że nie odkryto jeszcze co najmniej 5000 gatunków, głównie ryb głębinowych ze strefy klimatu tropikalnego[1]. W Polsce występuje około 120 gatunków.
Dział zoologii zajmujący się rybami to ichtiologia.
Wszystkie typy zbiorników wodnych na Ziemi z wyjątkiem zbiorników o skrajnie trudnych warunkach. Ryby występują we wszystkich strefach oceanów. Największym bogactwem gatunków ryb wyróżnia się strefa otwartej toni wodnej, liczne gatunki występują również w strefie dennej.
Wśród ryb rozwinęły się różnorodne strategie rozrodcze. Jednym z elementów z tym związanych jest miejsce składania ikry. Z tego względu wyróżniane są ekologiczne grupy rozrodcze[2]:
Ryby przetwarzają bezużyteczne – z punktu widzenia człowieka – rośliny i zwierzęta wodne na wartościowe dla niego mięso. Są wykorzystywane na całym świecie w celach konsumpcyjnych, przemysłowych oraz rekreacyjno-poznawczych:
Odmienne środowiska, w jakich żyją ludzie i ryby nie zmniejszają faktu, że wzajemnie mogą być dla siebie zagrożeniem.
Zagrożenia dla człowieka:
Zagrożenia dla ryb:
Ryby stanowią takson parafiletyczny, ponieważ każdy klad obejmujący ryby obejmuje również czworonogi (Tetrapoda), które nie są rybami. Z tego powodu tradycyjnie rozumiane ryby (Pisces) nie są już traktowane jako jednostka taksonomiczna[3].
Obecnie zwierzęta zaliczane do ryb dzielone są na gromady[3]:
Człowiek rozumny (Homo sapiens) – gatunek ssaka naczelnego, współtworzący z szympansami, gorylami i orangutanami rodzinę człowiekowatych (Hominidae, wielkie małpy). Jedyny występujący współcześnie przedstawiciel rodzaju Homo. Występuje na wszystkich kontynentach. Charakteryzuje się wyprostowaną postawą, dwunożnością, wysoko rozwiniętą sprawnością manualną i umiejętnością używania ciężkich narzędzi w stosunku do innych gatunków zwierząt, używaniem języka bardziej złożonego niż języki zwierzęce, większym i bardziej złożonym mózgiem niż te u innych zwierząt oraz wysoko rozwiniętym instynktem społecznym[2][3].
Pokrewieństwo i ewolucja ludzi badana jest m.in. za pomocą antycznego DNA, wyekstrahowanego z kości wczesnych hominidów[4].
Małżeństwo paleontologów, Louis Leakey i Mary Leakey, w latach 60. XX w. postawiło hipotezę, że przed ok. 1,9 miliona lat temu Homo habilis (człowiek zręczny) dał początek gatunkowi Homo erectus (człowiek wyprostowany), z którego mniej więcej 190 tys. lat temu wyewoluowali obecni ludzie. Korzystali oni z odkryć dokonanych w wykopaliskach prowadzonych przez ich synów, Jonathana i Richarda Leakeyów w Wąwozie Olduvai w Tanzanii. Obecnie większość naukowców zgadza się, że Homo habilis jest przodkiem wszystkich późniejszych gatunków z rodzaju Homo, w tym również – choć nie bezpośrednio – człowieka rozumnego[5][6].
W 2007 roku z Ileret w Kenii opisano niekompletną kość szczękową (KNM-ER 42703) H. habilis liczącą około 1,44 mln lat, a więc pochodzącą z okresu, w którym żył już H. erectus, co najprawdopodobniej oznacza, że H. erectus nie wyewoluował z H. habilis na drodze anagenezy. W tamtym obszarze odnaleziono również drugą czaszkę (KNM-ER 42700), którą Spoor i inni naukowcy przypisali do H. erectus[7]. W związku z tym postawiono tezę, że dymorfizm płciowy jest słabszy u współczesnych ludzi, niż był u H. erectus, co wskazuje, że nie przypominał on H. sapiens w tak dużym stopniu, jak wcześniej sądzono[8]. Przynależność tej skamieniałości do H. erectus została jednak zakwestionowana przez Karen Baab, która uznała ją za należącą do nieokreślonego gatunku z rodzaju Homo[9]. Ponadto szczątki H. erectus z Dmanisi wykazują te same plezjomorfie co skamieniałości H. habilis, co wspiera hipotezę, że pochodzi on od H. habilis[10][5]. H. erectus jest z kolei przez większość naukowców uznawany za przodka H. sapiens[11][5] – prawdopodobnie początek linii ewolucyjnej obejmującej neandertalczyka i człowieka współczesnego dała jedna z populacji H. erectus (sensu lato, przez część naukowców zaliczana do odrębnego gatunku Homo ergaster) żyjących w Afryce. Afrykański H. erectus (czyli H. ergaster) mógł być również przodkiem lub taksonem siostrzanym dla azjatyckich H. erectus[5].
Według teorii R. Leakeya z lat 60. H. erectus tworzył społeczność podobną do tej, jaką tworzą ludzie obecnie. Miała się ona charakteryzować monogamicznymi związkami i zbieracko-łowieckim trybem życia. Jedyne, co miało różnić gatunek Homo erectus od Homo sapiens według paleontologa, to mniejszy mózg. Zapostulowany w 2007 r. przez Koobi Fora Research Project silniejszy niż do tamtej pory sądzono dymorfizm płciowy u H. erectus oznacza, że samce tego gatunku, podobnie jak gatunku H. habilis, posiadały harem mniejszych od siebie samic, tak jak ma to miejsce u goryli[12]. Przynależność czaszki KNM-ER 42700 do H. erectus – mniejszej od czaszek innych przedstawicieli tego gatunku – jest jednak kwestionowana[9], a potwierdzenie występowania u niego wyraźnego dymorfizmu płciowego wymaga odnalezienia większej liczby skamieniałości z różnych stanowisk o podobnym wieku geologicznym[5].
Istnieją też teorie poligeniczne, według których niezależne było powstanie Homo sapiens w różnych miejscach, także w Azji i Europie[13]. Mniej więcej 100 tysięcy lat temu dotarł na Bliski Wschód, ok. 60 tys. lat temu do Australii, a prawie 40 tys. lat temu do Europy (był to tzw. kromaniończyk, czyli człowiek z Cro Magnon – od nazwy stanowiska archeologicznego we Francji).
Do tego gatunku czasami zalicza się następujące podgatunki:
Trwają również spory, czy człowiek neandertalski, który zamieszkiwał Europę podczas ostatniego zlodowacenia, jest podgatunkiem człowieka rozumnego (i należy go nazywać Homo sapiens neanderthalensis), czy osobnym gatunkiem (Homo neanderthalensis). Wstępnie zsekwencjonowany genom neandertalczyka jest bardziej podobny do genomu ludzi żyjących obecnie na terenach Eurazji niż żyjących w Afryce subsaharyjskiej, co sugeruje, że przepływ genów od neandertalczyków do przodka nie-Afrykańczyków nastąpił jeszcze przed zróżnicowaniem się poszczególnych grup Eurazjatów[14].
Wedle badań z 1928 przeprowadzonych przez Martina, dorosły zdrowy człowiek osiąga wzrost w przedziale 1,21–2,00 m; mężczyzna osiąga przeciętnie 1,65 m, kobieta zaś 1,54 m (93% wzrostu mężczyzny), zaznacza się więc dymorfizm płciowy. Maksymalny średni wzrost badacz ten zaobserwował u Szkotów (1,746 m). Spotyka się jednakże osobniki niemieszczące się w tych granicach, określane mianem karłów bądź olbrzymów. Przypadki takie mogą być powodowane zmianami w kościach, chrząstkach lub zaburzeniami hormonalnymi, np. nadczynnością przedniego płata przysadki (tzw. gigantyzm przysadkowy). Wzrost olbrzymi zdarza się częściej niż karłowaty. Najwyższy odnotowany wzrost dorosłego człowieka wynosił 2,72 m (Robert Wadlow), najniższy zaś 0,67 m[15] (Pojawiają się doniesienia o osobnikach niższych, mierzących 0,55 m[16].)
Biorąc pod uwagę ludzką sylwetkę, wyróżnia się kilka typów konstytucjonalnych budowy ciała:
Prócz tego mogą występować też typy patologiczne zwane dysplastycznymi[15].
Budowa ciała ludzkiego przypomina tę spotykaną u innych ssaków, szczególnie zaś u naczelnych, choć występują pewne odrębności niespotykane u innych gatunków. Zwłaszcza w obrębie układu kostnego, ale też np. unerwienia zaznaczają się jeszcze ślady pierwotnej metamerii, zwłaszcza w przypadku zarodka[15].
Za specyficzną cechę ludzką uchodzi postawa spionizowana. Łączy się z nią budowa stopy, a także kończyn górnych, które dzięki zmianie pozycji nabyły zwiększonych możliwości manipulacyjnych. Dzięki temu rozwinęły się chwytne ręce i stopy przystosowane do chodu. Kończyny ludzkie cechują się charakterystyczną proporcją długości, odbiegającą od spotykanej u innych naczelnych. Wskaźnik długości kończyn górnych w odsetkach dolnych wynosi u człowieka jedynie 78 (u szympansa 106, goryla 117). Sam kręgosłup cechują specyficzne dla człowieka krzywizny (kifozy i lordozy). Ich zadanie stanowi zwiększenie sprężystości, a więc amortyzacja i ochrona mózgowia przed wstrząsami[17]. Liczba kręgów ludzkich nie jest stała, wynosi zwykle 33 lub 34. Składają się na nią:
Pierwsze 3 grupy zaliczane są do kręgów przedkrzyżowych, czyli prawdziwych, dwie ostatnie zaś do rzekomych. U człowieka kręgi guziczne są silnie uwstecznione[18]. Pionizacja przyniosła też ze sobą zmiany w budowie klatki piersiowej, spotykane też u małp człekokształtnych. Zmiana położenia środka ciężkości, który pierwotnie (u zwierząt czworonożnych) leżał pod kręgosłupem, a u człowieka znajduje się przed nim, wymusiła skrócenie wymiaru strzałkowego klatki piersiowej, zwiększenie jej wymiaru poprzecznego i wciągnięcie kręgosłupa do przodu, by zbliżyć go do środka ciężkości. W okresie życia płodowego zmiany te nie zachodzą w zauważalnym stopniu, zmiany proporcji wymiarów klatki dokonują się po urodzeniu[19].
Czaszka ludzka inaczej niż małpia łączy się z kręgosłupem. U innych naczelnych spotyka się podtrzymujące ją silnie rozwinięte mięśnie barkowe, u człowieka ich brak i bardziej swobodne ułożenie czaszki na kręgosłupie daje jej szersze możliwości wzrostu[17]. Twarzoczaszka w stosunku do mózgoczaszki uległa skróceniu, a nawet wsunięciu pod większą od niej mózgoczaszkę[17].
Gatunek człowieka rozumnego występuje bardzo szeroko, nie dorównuje mu w tym względzie żaden inny ssak. Zamieszkuje on wszystkie kontynenty, choć na Antarktydzie nie osiedlił się na stałe. Dodatkowo niewielkie grupy ludzi niekiedy podróżują w pojazdach napędzanych silnikami rakietowymi w kosmos, gdzie okresowo zamieszkują Międzynarodową Stację Kosmiczną.
H. sapiens zasiedla różnorodne środowiska. Posiada zdolność przekształcenia siedlisk na bardziej odpowiadające swym potrzebom dzięki użyciu technologii. Tworzy miasta, w których od 2008 roku żyje przeszło połowa osobników. W ujęciu ekologicznym niezrównoważone wykorzystywanie zasobów środowiska klasyfikuje człowieka jako wysokiego szczeblem drapieżnika[20].
Genom jądrowy człowieka rozumnego liczy sobie około 3*109 par zasad, jego masę cząsteczkową szacuje się na 3*1012 D. Jego długość wynosi około 1 m. Geny zajmują w nim około 5%, z czego 3% koduje białka, a pozostałe 2% pełni funkcje regulatorowe. Telomerom przypada w udziale 10%[21].
Występują 22 pary autosomów i jedna heterosomów, oznaczanych symbolami X i Y. Chromosomy człowieka podzielono na grupy w zależności od ich wielkości, położenia centromeru i rozmieszczenia prążków[22].
Występuje 10 organizatorów jąderek[22].
Jego strukturę rozpracował w 1952 James D. Watson. Cały czas trwają prace nad jego poznaniem, w których przodujący wydaje się Projekt Poznania Ludzkiego Genomu (HGP, Human Genome Project), zrzeszający naukowców z 18 państw[21].
Prócz genomu jądrowego ma on także mitochondrialny DNA. Dziedziczy się inaczej niż jądrowy: prawie w całości po matce (udział genów ojca szacuje się 0,1%). W komórce somatycznej człowieka znajduje się przeciętnie około tysiąca mitochondriów. DNA mitochondrialny to cząsteczka naga i kolista, składająca się u człowieka z 16569 par zasad, w związku z czym nie stanowi nawet 1% całkowitego DNA komórki. Znajdują się tu nieliczne geny:
W przeciwieństwie do DNA jądrowego nie występują tu introny. W regionie intercistronowym leży nie więcej niż 87 par zasad, od których rozpoczyna się replikacja. Zdarza się też zjawisko nadpisania. Genom ten, wystawiony na działanie tlenu i wolnych rodników tlenowych, a dysponujący niewielkimi możliwościami naprawy po uszkodzeniach, ulega mutacji dziesięciokrotnie częściej od DNA jądrowego. Zmiany te mogą być przyczyną wielu chorób (encefalomiopatie)[21].
Całkowitą liczebność tego gatunku szacowano 15 listopada 2022 roku na 8 miliardów[23]. Stale się ona zwiększa.
   
Biochemia – nauka zajmująca się chemią w organizmach żywych, a w szczególności biosyntezą, strukturą, stężeniem, funkcjami (w tym skutkami niedoboru oraz nadmiaru) i przemianami substancji chemicznych w organizmach (z uwzględnieniem aspektów energetycznych)[1].
Typowe związki chemiczne będące przedmiotem badań biochemicznych to biopolimery (np. białka (w tym enzymy), polisacharydy i kwasy nukleinowe, tj. RNA i DNA), aminokwasy, węglowodany, lipidy, nukleotydy, hormony i in.
Do dziedzin nauki blisko związanych lub zazębiających się z biochemią należą:
Żelazo (Fe, łac. ferrum) – pierwiastek chemiczny o liczbie atomowej 26, metal z VIII grupy pobocznej układu okresowego, należący do grupy metali przejściowych.
Pod względem masy żelazo jest najczęściej występującym pierwiastkiem chemicznym na Ziemi. Stanowi większość składu jej jądra zewnętrznego i wewnętrznego. Jest także czwartym najbardziej powszechnym pierwiastkiem w skorupie ziemskiej. Dostatek tego pierwiastka w strukturze planet skalistych podobnych do Ziemi wiąże się z obfitą jego produkcją w procesie fuzji jądrowej w gwiazdach o dużej masie, w której żelazo jest ostatnim pierwiastkiem, wytworzenie którego wiąże się z uwolnieniem energii. Pierwiastki o większej liczbie atomowej powstają w wyniku gwałtownego wybuchu supernowej, która rozrzuca w przestrzeń radionuklidy, będące także prekursorem stabilnego żelaza.
Czyste żelazo jest lśniącym, srebrzystym, dość twardym i stosunkowo trudnotopliwym metalem, który ulega pasywacji[7][8]. Domieszka krzemu bądź węgla, związana z procesem otrzymywania metalu z rud żelaza, zwiększa głębokość i szybkość korozji. Od wieków jest stosowane w formie stopów z węglem, czyli żeliwa i stali, oraz stopów z manganem, chromem, molibdenem, wanadem i wieloma innymi (są to tzw. stale stopowe).
Podobnie jak inne pierwiastki chemiczne VIII grupy – ruten i osm – żelazo występuje w szerokim zakresie stopni utlenienia, od −II do VI, z których najpowszechniejsze są II i III stopień. Ma 25 izotopów z przedziału mas 45–69. Trwałe są izotopy 54, 56, 57 i 58, z czego najwięcej jest izotopu 56 (92%). Żelazo w stanie wolnym występuje w meteroidach oraz środowiskach o małej zawartości tlenu, gdyż reaguje z wodą i tlenem. Powierzchnia czystego żelaza jest lśniąca i srebrzystobiała, lecz utlenia się na wolnym powietrzu, tworząc uwodnione tlenki żelaza, potocznie nazywane rdzą. W przeciwieństwie do metali tworzących na swojej powierzchni powłokę pasywną, tlenki żelaza zajmują większą objętość niż metal, w wyniku czego łuszczą się, odsłaniając kolejne dla czynników korozyjnych warstwy nieskorodowanej jeszcze powierzchni.
W literaturze żelazu przypisuje się różną liczbę odmian alotropowych.
Dwie odmiany alotropowe[9]:
Cztery odmiany alotropowe[10]:
Żelazo wykorzystywane jest od czasów prehistorycznych. Wyparło stosowane wcześniej stopy miedzi mające niższe temperatury topnienia. W czystej postaci żelazo jest stosunkowo miękkie, aczkolwiek nie jest możliwe otrzymanie takiej jego formy na drodze wytapiania, gdzie otrzymuje się żelazo znacznie twardsze i wzmocnione przez zanieczyszczenia, a w szczególności przez węgiel. Odpowiednia mieszanina żelaza z węglem, w ilości od 0,002% do 2,1% węgla nazywana jest stalą, która charakteryzuje się nawet 1000 razy większą twardością niż czyste żelazo. Surowe stopy żelaza z węglem wytwarzane są w wielkich piecach, w których ze wsadu składającego się z rudy żelaza z dodatkiem koksu i topników wytapia się surówkę o wysokiej zawartości węgla. W kolejnych etapach produkcji, przy użyciu tlenu zmniejsza się zawartość węgla w surówce do odpowiedniej wartości, aby otrzymać stal. Z uwagi na szereg korzystnych właściwości, a także dostatek złóż rudy żelaza na świecie, stale oraz stopy żelaza utworzone z innymi metalami (stale stopowe) są najbardziej powszechnymi metalami przemysłowymi.
Związki chemiczne żelaza mają wiele zastosowań. Reakcja spalania mieszaniny tlenku żelaza i sproszkowanego glinu, nazywanej termitem, stosowana jest przy spawaniu i oczyszczaniu rud. Pierwiastek ten tworzy również związki dwuskładnikowe z halogenami i tlenowcami.
Oprócz minerałów duże znaczenie technologiczne mają karbonylkowe kompleksy żelaza, które otrzymuje się z chlorków żelaza i które są katalizatorami licznych reakcji organicznych. Zielony chlorek żelaza(II) o kwaskowym smaku jest podawany przy niedokrwistości.
Żelazo jest szeroko rozprzestrzenione w skorupie ziemskiej i jego zawartość wynosi ok. 6,2% (co stawia żelazo na 4. miejscu wśród pierwiastków i 2. miejscu wśród metali).
Żelazo występuje w minerałach takich jak np.:
Podstawowym źródłem produkcji żelaza są rudy żelaza, choć coraz większe znaczenie ma żelazo pozyskiwane z recyklingu[11]. W wydobyciu rud żelaza w 2017 roku, wynoszącym ogółem ok. 2,4 mld ton, przodowały: Australia (880 mln ton), Brazylia (440 mln ton), Chiny (ok. 340 mln ton), Indie (190 mln ton) i Rosja (100 mln ton).
W Polsce zasobów żelaza w okolicach Suwałk nie wydobywa się w związku z groźbą zaistnienia katastrofy ekologicznej oraz z uwagi na głębokie położenie złóż[12].
Żelazo odgrywa ważną rolę w biologii. Mimo znacznego rozpowszechnienia na Ziemi, żelazo należy do mikroelementów – występuje w niewielkich ilościach w składzie organizmów, jest jednak pierwiastkiem niezbędnym do życia dla prawie wszystkich organizmów żywych – drobnoustrojów, roślin i zwierząt, w tym człowieka[14].
Jest metalem występującym w wielu ważnych enzymach redoks, odpowiadających za oddychanie komórkowe, utlenianie i redukcję u roślin i zwierząt. W tym też w centrach aktywnych licznych enzymów takich jak: katalaza, peroksydazy oraz cytochromy. Znajduje się też w grupach prostetycznych wielu innych ważnych białek należących do metaloprotein: hemoglobinie, mioglobinie, będących typowymi białkami wykorzystywanymi do transportu i przechowywania tlenu cząsteczkowego u kręgowców.
Niekiedy podaż żelaza może nie pokrywać zapotrzebowania organizmu na ten pierwiastek, dzieje się tak m.in. w stanach zwiększonego zapotrzebowania, zaburzeń wchłaniania lub zwiększonej utraty żelaza. Po pewnym czasie prowadzi to do jego niedoboru i związanych z nim objawów chorobowych.
U kręgowców jest to głównie niedokrwistość z niedoboru żelaza. Według danych WHO aż 30% światowej populacji może spełniać kryteria niedokrwistości. W krajach rozwijających się ten problem dotyczy co drugiej kobiety w ciąży i blisko 40% dzieci w wieku przedszkolnym[15].
Niedobór żelaza u roślin powoduje zakłócenia przebiegu fotosyntezy i chlorozę młodych liści.
Czasem mimo istniejących mechanizmów regulacyjnych organizmu, może dojść do stanów przeciążenia żelazem. Najważniejszymi schorzeniami związanymi z nadmiarem żelaza w organizmie są hemochromatoza dziedziczna i hemosyderoza. Duże ilości soli żelaza(II) są toksyczne. Związki żelaza(III–VI) są nieszkodliwe, ponieważ się nie wchłaniają.
W organizmie ludzkim żelazo występuje w hemoglobinie, tkankach, mięśniach, szpiku kostnym, białkach krwi, enzymach, ferrytynie, hemosyderynie oraz w osoczu[16]. Przeciętny mężczyzna ma w organizmie 4 gramy żelaza, a kobieta około 3,5 grama.
Żelazo wchłania się w dwunastnicy i jelicie cienkim w postaci Fe2+. Po wchłonięciu wiązane jest przez apoferrytynę w błonie śluzowej przewodu pokarmowego. Powstaje ferrytyna, a żelazo znajduje się wtedy na III stopniu utlenienia. We krwi transportowane jest przez transferrynę. Prawidłowe stężenie żelaza w surowicy krwi[17]: * wartość średnia
Magazynowane jest w wątrobie również w postaci ferrytyny.
Zapotrzebowanie na żelazo u człowieka jest zmienne i zależy od wieku, płci i stanu organizmu. U osób dorosłych wynosi ono od 1 mg/dobę u mężczyzn do 2 mg u kobiet, z zastrzeżeniem, że w okresie ciąży i karmienia powinno to być ok. 3 mg/dobę[18]. Różnice w przyswajalności żelaza z pożywienia są bardzo duże w zależności od diety, od 1–2% dla diety wyłącznie zbożowej, do 25% dla diety mięsnej. Dla średniej, mieszanej diety przyswajalność żelaza wynosi ok. 10%, co oznacza konieczność spożywania ok. 10-krotnie większej ilości żelaza niż wynosi jego zapotrzebowanie przez organizm[19]. Źródła żelaza w diecie człowieka to: mięso (w tym mięso ryb), wątroba, żółtko jaj, twaróg, orzechy, mleko, warzywa strączkowe, brokuły, krewetki[potrzebny przypis]. Szpinak, wbrew obiegowym opiniom, zawiera umiarkowane ilości żelaza[20] na dodatek w formie słabo przyswajalnej przez człowieka[21].
Suplementację preparatami żelaza powinno się stosować m.in. u osób po zabiegach operacyjnych z dużą utratą krwi, u osób z krwawieniami z przewodu pokarmowego, z dróg rodnych, kobiet ciężarnych, karmiących, przy obfitych menstruacjach, u wcześniaków, u dzieci po konflikcie serologicznym, u osób z zaburzeniami wchłaniania żelaza[15]. Część badań wskazuje, że podawanie żelaza może zmniejszać natężenie objawów u dzieci z ADHD mających niedobory tego pierwiastka. Rola suplementacji żelaza w tej chorobie nie jest jednak potwierdzona i wymaga dalszych badań[22].
Lawina – gwałtowna utrata stabilności i przemieszczanie się: spadanie, staczanie lub ześlizgiwanie się ze stoku górskiego mas śniegu, lodu, gruntu, materiału skalnego, bądź ich mieszaniny (ruch jednego typu materiału z reguły powoduje ruch innego typu materiału znajdującego się na zboczu)[1]. Lawina jest najgwałtowniejszą postacią ruchów masowych i stanowi olbrzymie zagrożenie dla ludzi i ich otoczenia oraz infrastruktury. Występowanie lawin można jednak w pewnym stopniu przewidywać, a moment ich uruchomienia monitorować (np. przy wykorzystaniu geofonów jako systemu wczesnego ostrzegania, przy założeniu, że lawina powstaje w pewnej stałej strefie. Ewentualnie można monitorować szlaki schodzenia lawin, które są definiowane rzeźbą terenu. Jest to powszechna praktyka np. na terenach alpejskich).
Między cząstkami materiału leżącego na zboczu lub go budującego oraz między materiałem a podłożem działają siły tarcia oraz siły oddziaływania międzycząsteczkowego generalnie określane jako kohezja, które równoważą ciężar tego materiału. Jeśli ta równowaga zostanie zachwiana, np. przez wzrost masy śniegu wskutek opadów lub zmniejszenia siły tarcia (zmiana parametrów wytrzymałościowych niektórych lub wszystkich warstw tworzących dany masyw), następuje osuwanie się, spełzywanie lub spływanie materiału ze zbocza. Gdy zjawiska te mają gwałtowny przebieg, możemy mówić o lawinie.
Za najprostszą i jednocześnie najbardziej skuteczną metodę można uznać unikanie miejsc zagrożonych zejściem lawiny, zarówno w trakcie poruszania się po górach, jak i np. przy wyborze miejsca na budowę domu. Obszary takie można określić na podstawie analizy ukształtowania terenu, bezpośrednich obserwacji czy innych symptomów oraz historii zdarzeń.
Sztuczne (wykonane ludzką ręką) typy ochrony przed tego typu zjawiskami podzielić można na dwie grupy:
1. Aktywne – zabezpieczające w sposób bezpośredni powierzchnię skarpy, niedopuszczające lub znacznie ograniczające możliwość wystąpienia lawiny. Wśród tego typu metod wyróżnić można między innymi:
2. Pasywne – przyjmujące na siebie siłę uderzenia lawiny bądź pozwalające zmieniać jej trajektorię. Można tutaj wyróżnić:
Pewną metodą mieszaną jest sztuczne wywoływanie lawin przy pomocy umieszczania w zagrożonych połaciach śniegu materiałów wybuchowych lub przez ich ostrzał artyleryjski – metoda kontrowersyjna i raczej mało skuteczna.
Największe szanse uratowania osoby zasypanej w lawinie są w pierwszych 15 minutach. Po dotarciu na miejsce lawiniska ratownicy tyralierą przeszukują śnieg za pomocą sond lawinowych – kilkumetrowych cienkich tyczek. Nieocenioną pomocą służą także specjalnie szkolone psy.
Coraz popularniejsze stają się nowoczesne systemy wspomagające szukanie zasypanych osób:
Pogoda – stan atmosfery w danym miejscu i czasie[1]; w szerszym ujęciu – warunki meteorologiczne na danym obszarze. Ogół zjawisk pogodowych na danym obszarze w okresie wieloletnim (przynajmniej 30 lat) określany jest jako klimat. O ile nie jest podane inaczej termin pogoda jest rozumiany jako pogoda na Ziemi.
Jej stan określają składniki pogody (czyli fizyczne właściwości troposfery)[2]:
Badaniem zjawisk pogodowych zajmuje się meteorologia, ich przewidywaniem dział meteorologii – synoptyka. Dane meteorologiczne zbierane są przez stacje meteorologiczne. W Polsce sieć tych stacji obsługiwana jest przez Instytut Meteorologii i Gospodarki Wodnej.
Pojęcie pogody stosowane jest także do zjawisk zachodzących na innych ciałach niebieskich, np. na Słońcu[3] czy Jowiszu[4].
Promieniowanie elektromagnetyczne (fala elektromagnetyczna) – rozchodzące się w przestrzeni zaburzenie pola elektromagnetycznego[1].
Składowa elektryczna i magnetyczna fali indukują się wzajemnie – zmieniające się pole elektryczne wytwarza zmieniające się pole magnetyczne, a z kolei zmieniające się pole magnetyczne wytwarza zmienne pole elektryczne. Właściwości fal elektromagnetycznych zależą od długości fali. Promieniowaniem elektromagnetycznym o różnej długości fali są fale radiowe, mikrofale, podczerwień, światło widzialne, ultrafiolet, promieniowanie rentgenowskie i promieniowanie gamma. W opisie kwantowym promieniowanie elektromagnetyczne jest traktowane jako strumień nieposiadających masy cząstek elementarnych zwanych fotonami. Energia każdego fotonu zależy od długości fali.
Historia odkryć związanych z promieniowaniem elektromagnetycznym[2] :
Promieniowanie elektromagnetyczne przejawia właściwości falowe ulegając interferencji, dyfrakcji, spełnia prawo odbicia i załamania (polaryzacji). W wyniku superpozycji fal elektromagnetycznych może powstać fala stojąca.
Jednak niektóre właściwości promieniowania elektromagnetycznego (szczególnie jego oddziaływanie z materią) zależą od długości fali (częstotliwości promieniowania) i dlatego dokonano podziału promieniowania elektromagnetycznego na zakresy ze względu na jego częstotliwość. Granice poszczególnych zakresów są umowne i nieostre. Należy je traktować szacunkowo, promieniowanie o tej samej długości może być nazywane falą radiową lub mikrofalą – w zależności od kontekstu. Granice promieniowania gamma i promieniowania rentgenowskiego często rozróżnia się z kolei ze względu na źródło tego promieniowania. Najdokładniej określone są granice dla światła widzialnego, gdyż są one zdeterminowane fizjologią ludzkiego oka.
Fale radiowe znajdują bardzo szerokie zastosowanie w telekomunikacji, radiofonii, telewizji, radioastronomii i wielu innych dziedzinach nauki oraz techniki.
W technice podstawowym źródłem fal radiowych są anteny zasilane prądem przemiennym odpowiedniej częstotliwości. Wiele urządzeń generuje też zakłócenia będące falami radiowymi, wymienić tu można na przykład: zasilacze impulsowe, falowniki i regulatory tyrystorowe, piece indukcyjne, spawarki, zapłon iskrowy silników samochodowych, iskrzące styki urządzeń elektrycznych.
Naturalne źródła fal radiowych to między innymi wyładowania atmosferyczne, zorze polarne, radiogalaktyki.
W atmosferze propagacja fal radiowych jest dosyć skomplikowana, zachodzą różnorodne odbicia i ugięcia fali w niektórych warstwach atmosfery. Przebieg tych zjawisk zależy zarówno od długości fali, jak i własności powietrza zależnych od pory dnia, pogody, położenia geograficznego.
W zależności od metody wytwarzania niekiedy mikrofale są zaliczane do fal radiowych[3], albo do podczerwieni[4].
Podstawowe zastosowania mikrofal to łączność (na przykład telefonia komórkowa, radiolinie, bezprzewodowe sieci komputerowe) oraz technika radarowa. Fale zakresu mikrofalowego są również wykorzystywane w radioastronomii, a odkrycie mikrofalowego promieniowania tła miało ważne znaczenie dla rozwoju i weryfikacji modeli kosmologicznych. Wiele dielektryków mocno absorbuje mikrofale, co powoduje ich rozgrzewanie i jest wykorzystywane w kuchenkach mikrofalowych, przemysłowych urządzeniach grzejnych i w medycynie.
W elektronice mikrofalowej rozmiary elementów i urządzeń są porównywalne z długością fali przenoszonego sygnału. Powoduje to, że przy analizie obwodów nie można stosować elementów o stałych skupionych. Do prowadzenia mikrofal używane są falowody. Do wzmacniania i generacji sygnałów mikrofalowych służą masery, specjalne lampy mikrofalowe oraz mikrofalowe elementy półprzewodnikowe.
Promieniowanie podczerwone jest nazywane również cieplnym, szczególnie gdy jego źródłem są nagrzane ciała. Każde ciało o temperaturze większej od zera bezwzględnego emituje takie promieniowanie, a ciała o temperaturze pokojowej najwięcej promieniowania emitują w zakresie długości fali rzędu 10 μm. Przedmioty o wyższej temperaturze emitują promieniowanie o większym natężeniu i mniejszej długości, co pozwala na zdalny pomiar ich temperatury i obserwację za pomocą urządzeń rejestrujących wysyłane promieniowanie.
Technika rejestracji promieniowania podczerwonego emitowanego przez obiekty o temperaturach spotykanych w codziennych warunkach to termowizja. Umożliwia ona zobrazowanie obiektów w ciemności oraz pomiar temperatury w poszczególnych punktach ich powierzchni. Jest wykorzystywana między innymi w nauce, pożarnictwie, medycynie, wojskowości, w diagnostyce urządzeń mechanicznych i obwodów elektrycznych, oraz do oceny izolacji termicznej budynków.
W paśmie promieniowania podczerwonego są prowadzone obserwacje astronomiczne i meteorologiczne. Promieniowanie to znalazło zastosowanie w technice grzewczej. Jest również stosowane do przekazu informacji – do transmisji danych w światłowodach i układach zdalnego sterowania.
Spektroskopia w podczerwieni umożliwia identyfikację organicznych związków chemicznych i badanie ich struktury.
Światło (promieniowanie widzialne) to ta część widma promieniowania elektromagnetycznego, na którą reaguje zmysł wzroku człowieka. Różne zwierzęta mogą widzieć w nieco różnych zakresach.
Światło jest tylko w niewielkim stopniu absorbowane przez atmosferę ziemską i przez wodę. Ma to duże znaczenie dla organizmów żywych, zarówno wodnych, jak i lądowych.
Światło ma bardzo duże znaczenie w nauce i wiele zastosowań w technice. Dziedziny nauki i techniki zajmujące się światłem noszą nazwę optyki.
Promieniowanie ultrafioletowe jest zaliczane do promieniowania jonizującego, czyli ma zdolność odrywania elektronów od atomów i cząsteczek. W dużym stopniu określa to jego właściwości, szczególnie oddziaływanie z materią i na organizmy żywe.
Słońce emituje ultrafiolet w szerokim zakresie spektralnym, ale górne warstwy atmosfery ziemskiej (warstwa ozonowa) pochłaniają większość promieniowania z krótkofalowej części spektrum. Obserwacje astronomiczne w ultrafiolecie rozwinęły się dopiero po wyniesieniu ponad atmosferę przyrządów astronomicznych.
W technice ultrafiolet stosowany jest powszechnie. Powoduje świecenie (fluorescencję) wielu substancji chemicznych. W świetlówkach ultrafiolet wytworzony na skutek wyładowania jarzeniowego pobudza luminofor do świecenia w zakresie widzialnym. Zjawisko to wykorzystuje się również do zabezpieczania banknotów i w analizie chemicznej (Spektroskopia UV). Ultrafiolet o małej długości fali jest wykorzystywany do sterylizacji (wyjaławiania) pomieszczeń.
Niektóre owady, na przykład pszczoły, widzą w bliskiej światłu widzialnemu części widma promieniowania ultrafioletowego, również rośliny posiadają receptory ultrafioletu.
Promieniowanie rentgenowskie jest promieniowaniem jonizującym.
Technicznie promieniowanie rentgenowskie uzyskuje się przeważnie poprzez wyhamowywanie rozpędzonych cząstek naładowanych.
W lampach rentgenowskich są to rozpędzone za pomocą wysokiego napięcia elektrony hamowane na metalowych anodach. Źródłem wysokoenergetycznego promieniowania rentgenowskiego są również przyspieszane w akceleratorach cząstki naładowane.
Promieniowanie rentgenowskie jest wykorzystywane do wykonywania zdjęć rentgenowskich do celów defektoskopii i diagnostyki medycznej.
W zakresie promieniowania rentgenowskiego są również prowadzone obserwacje astronomiczne.
Promieniowanie gamma jest promieniowaniem jonizującym.
Promieniowanie gamma towarzyszy reakcjom jądrowym, powstaje w wyniku anihilacji – zderzenie cząstki i antycząstki, oraz rozpadów cząstek elementarnych. Otrzymywane w cyklotronach promieniowanie hamowania i synchrotronowe również leży w zakresie długości fali promieniowania gamma, choć niekiedy bywa nazywane wysokoenergetycznym promieniowaniem rentgenowskim.
Promienie gamma mogą służyć do sterylizacji żywności i sprzętu medycznego. W medycynie używa się ich w radioterapii oraz w diagnostyce. Zastosowanie w przemyśle obejmują badania defektoskopowe. Astronomia promieniowania gamma zajmuje się obserwacjami w tym zakresie długości fal.
Płaska fala elektromagnetyczna rozchodząc się w próżni w nieograniczonym obszarze jest falą poprzeczną, w której składowa elektryczna i magnetyczna są prostopadłe do siebie, a obie są prostopadłe do kierunku rozchodzenia się fali. Fala elektromagnetyczna nie będąca falą płaską, lub rozchodząc się w ośrodku, lub w ograniczonym obszarze może mieć inny rozkład pola elektromagnetycznego. Charakterystyczne rozkłady pola elektromagnetycznego w propagującej fali nazywane są modami fali elektromagnetycznej.
Ze źródła punktowego rozchodzą się fale kuliste[5]. Każdą falę rozchodzącą się w nieskończonym bezstratnym ośrodku dielektrycznym, niezbyt blisko źródła, można uważać za kulistą, a dostatecznie mały jej wycinek za płaską[6].
Promieniowanie laserów często ma gaussowski profil wiązki charakteryzujący się rozkładem amplitudy natężenia pola elektrycznego w płaszczyźnie prostopadłej do osi wiązki opisanym funkcją Gaussa[7][8].
Mody fali elektromagnetycznej można podzielić na[9]:
Mod propagującej fali jest zdeterminowany przez rodzaj i kształt ośrodka, w którym rozchodzi się fala i przez jego granice. Charakterystyczne mody drgań występują przy propagacji mikrofal w falowodach i światła w światłowodach.
Polaryzacja fali elektromagnetycznej to charakterystyczne zachowanie się kierunków wektorów pola elektrycznego i magnetycznego. Przyjęto, że polaryzację fali elektromagnetycznej określa się dla jej składowej elektrycznej (składowa magnetyczna jest do niej prostopadła).
W fali elektromagnetycznej jej pola elektryczne i magnetyczne niosą ze sobą energię. W próżni i jednorodnym idealnym dielektryku składowe elektryczne i magnetyczne niesionej energii są sobie równe, natomiast w ośrodku o niezerowym przewodnictwie elektrycznym są różne[11].
Choć w elektrodynamice klasycznej energię promieniowania elektromagnetycznego uważa się za wielkość ciągłą, zależną jedynie od natężenia pola elektrycznego i indukcji pola magnetycznego, to zjawiska zachodzące na poziomie atomowym dowodzą, że jest ona skwantowana[a]. Energia pojedynczego kwantu jest zależna tylko od częstotliwości fali  i wynosi
gdzie  jest stałą Plancka.
Strumień energii przenoszonej przez falę elektromagnetyczną w każdym punkcie przestrzeni określa wektor Poyntinga zdefiniowany jako
gdzie:
Biegnąca fala elektromagnetyczna niesie ze sobą pęd równy
Fala odbita lub pochłonięta przekazuje ten pęd wywierając ciśnienie. Pomiar ciśnienia promieniowania słonecznego przeprowadzony przez Lebiediewa w 1900 roku był pierwszym ilościowym potwierdzeniem teorii fali elektromagnetycznej Maxwella.
Prędkość rozchodzenia się fali elektromagnetycznej w próżni jest stała, nie zależy od jej częstości ani układu odniesienia. Nazywa się ją prędkością światła. Jest ważną stałą fizyczną, a jej wartość wynosi około 3·108 m/s. W ośrodkach materialnych prędkość fali elektromagnetycznej (rozchodzenia się fotonów) jest zawsze mniejsza niż w próżni.
Rozchodzenie się fali w ośrodkach zależy zarówno od właściwości tych ośrodków, jak i częstotliwości fali.
Rozchodzenie się fal elektromagnetycznych opisują równania Maxwella. W pustej przestrzeni (próżni) nie zawierającej ładunków (źródeł) redukują się one do[13]:
Równania te są liniowymi równaniami różniczkowymi fali rozchodzącej się z prędkością
W nieprzewodzącym bezstratnym ośrodku o względnej przenikalności elektrycznej  i względnej przenikalności magnetycznej μr prędkość fali wyniesie
Dla fali płaskiej rozchodzącej się w kierunku  niektóre rozwiązania powyższych równań różniczkowych mają postać:
Równania Maxwella i ich rozwiązanie pozwoliły połączyć pole elektryczne i magnetyczne w jedno pole elektromagnetyczne i pokazać, że ma ono taką samą naturę jak światło.
Fizyka kwantowa opisuje promieniowanie elektromagnetyczne jako strumień fotonów – niepodzielnych paczek falowych. Fotony są nieposiadającymi masy cząstkami elementarnymi, ich energia i pęd zależą od częstotliwości (a co za tym idzie od długości fali ):
Masa – jedna z podstawowych wielkości fizycznych określająca bezwładność (masa bezwładna) i oddziaływanie grawitacyjne (masa grawitacyjna) obiektów fizycznych[1]. Jest wielkością skalarną. Potocznie rozumiana jako miara ilości materii obiektu fizycznego[a]. W szczególnej teorii względności związana z ilością energii zawartej w obiekcie fizycznym. Najczęściej oznaczana literą m.
W układzie jednostek miar SI podstawową jednostką masy jest kilogram (kg).
W nierelatywistycznej fizyce klasycznej pojęcie masy występuje w dwóch znaczeniach[2]:
Fizyka klasyczna nie uzasadnia, dlaczego te dwie wielkości mają być sobie równe, choć równość taką stwierdzono – por. eksperyment Eötvösa i zasada równoważności.
Jest miarą bezwładności ciała, to znaczy miarą zmiany prędkości ciała wywołanej działaniem na nie siły. Druga zasada dynamiki Newtona ma postać:
Jest to wielkość opisująca oddziaływania grawitacyjne dwóch punktowych ciał, występująca w prawie powszechnego ciążenia:
W fizyce relatywistycznej pojęcie masy zależy od teorii, różne określenia i koncepcje masy przedstawiają szczególna teoria względności i ogólna teoria względności.
Wielkością fizyczną charakteryzującą obiekt fizyczny lub układ takich obiektów jest w szczególnej teorii względności masa spoczynkowa, zwana niekiedy po prostu masą. Wielkość ta nie zależy od układu odniesienia (jest niezmiennikiem transformacji Lorentza).
Pomiędzy energią, pędem i masą (spoczynkową) ciała zachodzi związek:
gdzie  oznacza masę spoczynkową ciała (oznaczaną także ).
Masa spoczynkowa jest (z dokładnością do czynnika c−1), wartością bezwzględną czterowektora energii – pędu.
Tradycyjne sformułowanie prawa zachowania masy, które odegrało bardzo ważną rolę w rozwoju fizyki i chemii, mówi, że jeżeli układ fizyczny nie wymienia materii (tzn. cząstek o niezerowej masie spoczynkowej) z otoczeniem (czyli jest układem zamkniętym, choć niekoniecznie izolowanym), to masa (spoczynkowa) materii układu uczestniczącej w dowolnym procesie fizycznym lub chemicznym pozostaje stała, a suma mas produktów jest równa sumie mas substratów (masa jest addytywna). Jednak zgodnie z koncepcją równoważności masy i energii (stanowiącą część szczególnej teorii względności Einsteina) wymiana przez układ energii z otoczeniem (układ nieizolowany) powoduje zmianę masy (spoczynkowej) układu, a suma mas spoczynkowych produktów nie jest równa sumie mas spoczynkowych substratów (masa spoczynkowa nie jest addytywna), nawet w układzie izolowanym (nie wymieniającym energii). Masa spoczynkowa układu nieizolowanego nie jest więc zachowana, a nawet w układzie izolowanym nie jest zachowana suma mas spoczynkowych składników układu. Ta zależność staje się bardzo widoczna w reakcjach jądrowych. Natomiast jest zachowana zarówno całkowita masa spoczynkowa układu izolowanego, jak i jego całkowita energia. Jednak, w przeciwieństwie do wartości masy spoczynkowej (niezmiennika relatywistycznego), wartość energii całkowitej zmienia się przy zmianie układu odniesienia (nie jest ona niezmiennikiem).
Czasem spotykane jest też pojęcie „masy relatywistycznej” oznaczające całkowitą energię obiektu fizycznego wyrażoną w jednostkach masy:
Dla obiektów fizycznych o niezerowej masie spoczynkowej (ciał):
Masa relatywistyczna poruszającego się ciała rośnie wraz z prędkością (aż do nieskończoności, przy zbliżaniu się prędkości do prędkości światła) w próżni.
Fotony o zerowej masie spoczynkowej poruszają się z prędkością  a ich energia (masa relatywistyczna) zależy od długości fali.
Wprowadzenie pojęcia „masy relatywistycznej” to zabieg dostosowujący nierelatywistyczne (klasyczne) wzory fizyczne do zjawisk zachodzących dla dużych prędkości. Na przykład dzięki użyciu pojęcia masy relatywistycznej w miejsce spoczynkowej, równanie pędu newtonowskiego  staje się prawdziwe dla dowolnej prędkości, a nie tylko prędkości małych w porównaniu z prędkością światła w próżni. Podobnie:
W powyższym wzorze pierwszy składnik związany jest z relatywistyczną zmianą masy, a drugi składnik jest klasycznym opisem dynamiki.
W mechanice nierelatywistycznej (dla prędkości dużo mniejszych od prędkości światła w próżni) pierwszy składnik się zeruje (jest bardzo bliski zeru) i wzór przyjmuje postać jak w drugiej zasadzie dynamiki Newtona
Według zasady Macha bezwładność materii nie wynika z własności wewnętrznej materii, ale stanowi miarę jej oddziaływania z całym Wszechświatem.
Jednym z postulatów ogólnej teorii względności jest zasada równoważności mówiąca, że nie można rozróżnić spadku swobodnego od ruchu jednostajnego. Z postulatu tego wynika, że masy bezwładna i grawitacyjna są sobie równoważne.
We współczesnej fizyce cząstek elementarnych, opartej na kwantowej teorii pola uważa się, że masa nie jest fundamentalną własnością cząstek materii, ale jest nabywana przez oddziaływanie z polem Higgsa.
Przy opisie odległych obiektów astronomicznych, w szczególności planet pozasłonecznych, często jest podawana masa minimalna. Niezwykle skuteczna metoda badania zmian prędkości radialnej pozwala wyznaczyć jedynie dolne ograniczenie na rzeczywistą masę obiektu, dokładniej:
gdzie  jest kątem nachylenia orbity ciała do osi obserwacji. Wielkość tę często nazywa się skrótowo masą, choć nie jest to ścisłe określenie. Wiele spośród odkrytych obiektów może mieć znacznie większą rzeczywistą masę i być brązowymi karłami, a nie planetami.
Półprzewodniki – substancje, najczęściej krystaliczne, których konduktywność może być zmieniana w szerokim zakresie (na przykład od 10−8 do 103 S/cm) poprzez domieszkowanie, ogrzewanie, oświetlanie lub inne czynniki. Przewodnictwo typowego półprzewodnika plasuje się między przewodnictwem metali i dielektryków[1].
Wartość oporu półprzewodnika na ogół maleje ze wzrostem temperatury. Półprzewodniki posiadają pasmo wzbronione między pasmem walencyjnym a pasmem przewodzenia w zakresie od 0 do 6 eV (na przykład Ge 0,7 eV, Si 1,1 eV, GaAs 1,4 eV, GaN 3,4 eV, AlN 6,2 eV). Koncentrację nośników ładunku w półprzewodnikach można zmieniać w bardzo szerokich granicach, regulując temperaturę półprzewodnika lub natężenie padającego na niego światła, a nawet przez ściskanie lub rozciąganie.
W przemyśle elektronicznym najczęściej stosowanymi materiałami półprzewodnikowymi są pierwiastki grupy 14. (krzem, german) oraz związki pierwiastków grup 13. i 15. (arsenek galu, azotek galu, antymonek indu) lub 12. i 16. (tellurek kadmu). Materiały półprzewodnikowe są wytwarzane w postaci monokryształu, polikryształu lub proszku. Obecnie produkuje się też półprzewodniki organiczne, na ogół wielocykliczne związki aromatyczne, między innymi poli(p-fenyleno-winylen).
Półprzewodniki dzieli się na:
Półprzewodnik samoistny jest to półprzewodnik, którego materiał jest idealnie czysty, bez żadnych zanieczyszczeń struktury krystalicznej. Koncentracja wolnych elektronów w półprzewodniku samoistnym jest równa koncentracji dziur.
Przyjmuje się, że w temperaturze zera bezwzględnego w paśmie przewodnictwa nie ma elektronów, natomiast w wyższej temperaturze powstają pary elektron–dziura; im większa wartość temperatury, tym więcej takich par jest generowanych.
Półprzewodniki samoistne mają mało ładunków swobodnych (co objawia się dużą rezystywnością), dlatego też stosuje się domieszkowanie. Materiały uzyskane przez domieszkowanie nazywają się półprzewodnikami niesamoistnymi lub półprzewodnikami domieszkowanymi.
Domieszkowanie polega na wprowadzeniu i aktywowaniu atomów domieszek do struktury kryształu. Domieszki są atomami pierwiastków niewchodzących w skład półprzewodnika samoistnego – przykładem niech będzie domieszka krzemu w arsenku galu. Ponieważ w wiązaniach kowalencyjnych bierze udział ustalona liczba elektronów, zamiana któregoś z atomów struktury na odpowiedni atom domieszki powoduje wystąpienie nadmiaru lub niedoboru elektronów.
Wprowadzenie domieszki dającej nadmiar elektronów (w stosunku do półprzewodnika samoistnego) powoduje powstanie półprzewodnika typu n, domieszka taka zaś nazywana jest domieszką donorową („oddaje elektron”). W takim półprzewodniku powstaje dodatkowy poziom energetyczny (poziom donorowy) położony w obszarze pasma wzbronionego niewiele poniżej poziomu przewodnictwa lub w samym paśmie przewodnictwa. Nadmiar elektronów jest uwalniany do pasma przewodnictwa (prawie pustego w przypadku półprzewodników samoistnych) w postaci elektronów swobodnych zdolnych do przewodzenia prądu. Mówimy wtedy o przewodnictwie elektronowym lub przewodnictwie typu n (z ang. negative, ujemny). Dla krzemu typowymi domieszkami donorowymi są atomy 15. grupy układu okresowego (więcej elektronów walencyjnych), głównie fosfor.
Wprowadzenie domieszki dającej niedobór elektronów (w stosunku do półprzewodnika samoistnego) powoduje powstanie półprzewodnika typu p, domieszka taka zaś nazywana jest domieszką akceptorową („przyjmuje elektron”). W takim półprzewodniku powstaje dodatkowy poziom energetyczny (poziom akceptorowy) położony w obszarze pasma wzbronionego niewiele nad poziomem walencyjnym, lub w samym paśmie walencyjnym. Poziomy takie wiążą elektrony znajdujące się w paśmie walencyjnym (prawie zapełnionym w przypadku półprzewodników samoistnych), powodując powstanie w nim wolnych miejsc. Takie wolne miejsce nazwano dziurą elektronową. Zachowuje się ona jak swobodna cząstka o ładunku dodatnim i jest zdolna do przewodzenia prądu. Mówimy wtedy o przewodnictwie dziurowym lub przewodnictwie typu p (z ang. positive, dodatni). Dziury, ze względu na swoją masę efektywną, zwykle większą od masy efektywnej elektronów, mają mniejszą ruchliwość, przez co rezystywność materiałów typu p jest z reguły większa niż materiałów typu n mających ten sam poziom domieszkowania. Typowymi akceptorami dla krzemu są atomy 13. grupy układu okresowego (mniej elektronów na ostatniej powłoce), zwykle bor.
Układ SI, właściwie Międzynarodowy Układ Jednostek Miar (fr. Système international d'unités, SI) – znormalizowany układ jednostek miar zatwierdzony w 1960 (później modyfikowany) przez Generalną Konferencję Miar na XI Generalnej Konferencji Miar. Jest stworzony w oparciu o metryczny system miar. Jednostki w układzie SI dzielą się na podstawowe i pochodne [1].
Układ SI został oficjalnie przyjęty przez wszystkie kraje świata z wyjątkiem Stanów Zjednoczonych, Liberii i Mjanmy (Birmy)[2]; w Polsce układ SI obowiązuje od 31.12.1966 r.[3]
Wzorzec kilograma
Wzorzec metra (numer 27 ulokowany jest w USA)
W 1791 r. we Francji przyjęto definicję metra jako  długości mierzonej wzdłuż południka paryskiego od równika do bieguna. Rok później przyjęto nazwy „metr”, „ar”, „litr” oraz „graw” (jako jednostkę masy)[4]. Układ SI powstał z XIX-wiecznego układu MKS, do którego należał  metr, kilogram i sekunda. W 1954 roku podstawowymi jednostkami zostały: amper, kelwin, kandela. Międzynarodowy Układ Jednostek Miar został zatwierdzony na XI Generalnej Konferencji Miar w 1960 roku. Po obradach XIV Generalnej Konferencji Miar w 1971 r. do klasy jednostek podstawowych został włączony mol określający liczność materii. Natomiast na XX Konferencji, w październiku 1995 roku do klasy jednostek pochodnych włączono jednostki występujące dotychczas jako jednostki uzupełniające – radian i steradian[5][6].
Obecnie układ SI zawiera 7 jednostek podstawowych[7].
Jednostkami pochodnymi nazywa się wszystkie pozostałe jednostki wielkości fizycznych, zarówno te mające własne nazwy jak np. wat (W) czy dioptria, jak i te, które ich nie mają i są wyrażane za pomocą jednostek podstawowych, np. przyspieszenie nie ma swojej nazwy jednostki i wyrażane jest za pomocą metra i sekundy
Przyjęte oficjalnie na XI Generalnej Konferencji Miar w 1960 roku są nazwami (prefiksami) dla przeliczników dziesiętnych jednostek[9]. Poza kilogramem wszystkie jednostki podstawowe nie są spotęgowane i nie mają przedrostków.
Zboża, rośliny zbożowe – grupa roślin uprawnych z rodziny wiechlinowatych (traw, Poaceae). Ich owoce, o wysokiej zawartości skrobi, są wykorzystywane do celów konsumpcyjnych, pastewnych i przemysłowych. Najpopularniejszymi produktami przerobu zbóż są mąki, kasze, oleje i syropy. Zboża są podstawowym surowcem w wielu gałęziach przemysłu takich jak: młynarstwo, piwowarstwo, gorzelnictwo, farmaceutyka.
Według danych GUS za rok 2021, zbiory zbóż w Polsce wyniosły 34,6 mln ton[1].
Ponadto niektórzy zaliczają tu rośliny z innych rodzin nazywane umownie zbożami rzekomymi, np.
Ze względu na sposób uprawy zboża można podzielić na:
Banan – jadalny owoc tropikalny, z botanicznego punktu widzenia – jagoda[1], wytwarzany przez kilka gatunków roślin zielnych z rodzaju banan (Musa) (największe znaczenie użytkowe mają owoce banana zwyczajnego)[1][2].
Słowo banan pochodzi bądź od arabskiego słowa banan, oznaczającego palec[3] lub z afrykańskiego języka wolof, w którym rośliny te określa się mianem banaana[4].
Uprawiane są głównie w strefie międzyzwrotnikowej – w Afryce, Ameryce Południowej i Północnej oraz w Azji i krajach Pacyfiku[1][2].
Najczęściej banany mają od 10 do 15 cm, największe okazy osiągają do 30 cm. Rosną w groniastych owocostanach[2]. Owoce poszczególnych gatunków różnią takie cechy jak stopień mączystości czy zawartość cukrów, co decyduje o ich zastosowaniu kulinarnym.
Często spotykanym mitem na temat bananów jest twierdzenie, że w końcówkach tych owoców mogą znajdować się lamblie – pasożyty wywołujące lambliozę[5]. Dłuższa końcówka owocu jest pozostałością po szypułce, a krótszy, drugi koniec to pozostałość po szyjce słupka kwiatu. Szypułki bananów łączą się w tzw. „rączki” i „kiście” lub „wiązki” – w takiej postaci skupiającej po kilka owoców odciętych z owocostanu banany trafiają do oferty handlowej[6]. Pojedynczy owoc zwany jest „paluszkiem”, „rączkę” tworzy kilka zrośniętych z sobą owoców, a kilka rączek zrasta się następnie w „wiązkę” liczącą zwykle ok. 20 „paluszków”[7].
Należy wziąć pod uwagę, że owoce z importu, często sztucznie doprowadzane do stanu dojrzałości[8], mają inną wartość odżywczą.
W 2014 łączne zbiory bananów wyniosły wg danych FAOSTAT-u 114 130 tysięcy ton, z czego na Azję przypadało 55,8%, na obie Ameryki 24,7%, Afrykę 17,9%, Oceanię 1,3% oraz Europę 0,3%. Największymi producentami były następujące kraje[12]:
Łącznie uprawy bananów zajęły obszar 5 393 811 ha[12]. Głównymi eksporterami owoców w 2014 były Ekwador (26% światowego eksportu), Filipiny (15%), Kostaryka (12%), Kolumbia (7,8%) i Gwatemala (7,2%), a największymi importerami – kraje Unii Europejskiej, Stany Zjednoczone, Japonia, Chiny i Rosja[13].
Przez lata znaczącym producentem bananów była amerykańska korporacja United Fruit Company, założona w 1899 roku. Jej obecnym następcą jest główny amerykański dystrybutor bananów Chiquita Brands International[14]. Firma ta miała trzynastoprocentowy udział w światowym eksporcie bananów w 2013, podczas gdy jej główni konkurenci: Fresh Del Monte Produce (do 1989 część spółki Del Monte Foods) 12% zaś Dole Food Company 11%[15].
W 2014 banany były głównym produktem eksportowym Belize[13].
Banany wymagają dalekiego transportu z miejsca, gdzie są uprawiane do miejsca sprzedaży. Aby osiągnąć jak najdłuższą trwałość skórki, której wygląd decyduje o wartości handlowej owoców, zbiór bananów następuje przed osiągnięciem dojrzałości. Owoce wymagają ostrożnego traktowania, szybkiego transportu do portów, schłodzenia i transportu w chłodniach. Celem jest powstrzymanie bananów przed wydzielaniem etylenu, naturalnego środka powodującego ich dojrzewanie. Takie postępowanie umożliwia przechowanie i transport przez 3-4 tygodnie w temperaturze 13 °C. Po przybyciu na miejsce przeznaczenia banany trafiają do dojrzewalni – miejsca gdzie umieszczane są w temperaturze 17 °C i traktowane etylenem o niskim stężeniu. Po kilku dniach banany zaczynają dojrzewać i rozprowadzane są do docelowej sprzedaży. Niedojrzałe banany nie powinny być przechowywane w domowych lodówkach, ze względu na zbyt niską temperaturę, która szkodzi owocom. Dojrzałe banany mogą być przechowane w domowej chłodziarce przez kilka dni.
Dwutlenek węgla (który banany wytwarzają samoistnie) i sorbenty etylenu mogą wydłużyć trwałość owoców nawet w wyższych temperaturach. Efekt ten można wykorzystać pakując owoce w szczelny worek polietylenowy z dodatkiem absorbentu etylenu (np. nadmanganianu potasu). Zabieg ten wydłuża okres przechowywania do 3-4 tygodni bez potrzeby chłodzenia[16][17][18].
Herbata – napar przyrządzany z liści i pąków grupy roślin, nazywanych tą samą nazwą, należących do rodzaju kamelia (Camellia). Rośliny te są do siebie podobne, traktowane jako odrębne gatunki lub odmiany jednego gatunku – herbaty chińskiej (Camellia sinensis). Dawniej zaliczano je do rodzaju Thea, różnią się od innych kamelii zawartością substancji swoistych i kilkoma drobnymi cechami morfologicznymi[1].
Polska nazwa herbata to zbitka pochodząca od łac. herba thea (gdzie pierwszy wyraz herba oznacza „zioło”, a drugi – thea – jest zlatynizowaną postacią chińskiej nazwy tejże rośliny[2]). W Chinach znak określający herbatę: 茶 jest różnie czytany w zależności od dialektu: tê – dialekt hokkien (skąd pochodzą określenia w językach zachodniej Europy) oraz chá, w dialektach kantońskim i mandaryńskim (z odmianami tej wymowy popularnymi na Wschodzie – w państwach takich jak Indie, Iran, Turcja, Rosja, Czechy).  
Nazwą herbata określa się również napary z różnych ziół, suszu owocowego[3][4], dlatego w użyciu są określenia: lipowa, miętowa, rumiankowa itp. herbata, herbata z malin, dzikiej róży, bzu czarnego itp., herbata paragwajska, rooibos.
Smak i aromat herbaty w dużym stopniu zależą od warunków, w jakich ona dojrzewała oraz sposobu jej późniejszej obróbki. Herbata uprawiana jest między zwrotnikami Raka i Koziorożca, głównie w południowej i południowo–wschodniej części Azji. Najlepiej rośnie w klimacie ciepłym, łagodnym z regularnymi opadami, sprzyjający jest klimat monsunowy odznaczający się dużym nasłonecznieniem i wysokimi sumami opadów. Herbata swoje optimum osiąga na obszarach o średnich rocznych temperaturach od 10 do 30 °C. Wysokość jej upraw najczęściej sięga do 2400 m n.p.m., krzewy uprawiane w wyższych partiach ze względu na chłodniejsze warunki dojrzewają dłużej, ale dzięki temu mają mocniejszy aromat.
Sposób zbierania herbaty również ma wpływ na jej walory smakowe i zapachowe. Najczęściej zbiera się ją ręcznie. Zrywane są jedynie szczytowe pączki oraz dwa listki pod nim. Największe plony dają rośliny 6–7-letnie. Aby krzewy były mocne i dawały dobre owoce ogranicza się liczbę zbiorów w ciągu roku. W Chinach i Japonii herbatę zbiera się 3–5 razy w roku, natomiast w Indiach i Indonezji ze względu na szybszą regenerację rośliny zbioru dokonuje się nawet 15 razy do roku[5]. Pierwszy zbiór w roku określa się z angielskiego first flush, często jest to herbata uznawana za lepszą niż z pozostałych zbiorów i osiąga wyższą cenę.
Zazwyczaj dzieli się herbaty według sposobu wytwarzania.
Herbata mocno przetworzona, dająca ostatecznie liście o czarnym kolorze. Otrzymywana jest w wyniku czterech procesów – więdnięcia, skręcania, utleniania i suszenia.
Popularne gatunki czarnej herbaty to: assam, darjeeling (Indie), ceylon (Sri Lanka), yunnan (Chiny).
Herbata zielona (绿茶 lǜchá) powstaje z liści, w których wcześnie zatrzymano proces utleniania – po zerwaniu są suszone, a następnie poddaje się je działaniu wysokiej temperatury, by zatrzymać działanie enzymów. Po prawidłowym zaparzeniu (wodą o odpowiednio niższej temperaturze) ma mniej intensywny smak, a zwłaszcza goryczkę, niż herbata czarna, aromat często jest "trawiasty" lub warzywny. Zdecydowana większość herbat produkowanych w Chinach i Japonii należy do tej kategorii.
Popularne gatunki zielonej herbaty: gunpowder, longjing (lung ching), sencha, matcha.
Herbata, w której następuje utlenianie, zatrzymane przez utratę wilgoci. Przetwarzanie jest minimalne: liście więdną i następnie są suszone. Herbata biała ma odcień lekko srebrnawy, a po zaparzeniu ma kolor jasnosłomkowy. Jest to jeden z najdroższych rodzajów herbaty – ceny zależą od gatunku, niemniej w Polsce przeciętnie kosztuje ona od 20[6] zł do ponad 100 zł za 100 g. Droższe herbaty powstają głównie z młodych pączków, które jeszcze nie zdążyły się rozwinąć. Jedną z tańszych odmian jest Bai Mudan, gdzie oprócz pączków używane są 2 kolejne liście.
Proces produkcji tej herbaty zbliżony jest w fazie początkowej do procesu wytwarzania herbaty zielonej, ale kończy się fermentacją liści w gorącym i wilgotnym środowisku. Kolor naparu jest zielonkawo-żółty. Obecnie najrzadziej produkowany typ herbaty.
Herbata pu-erh (普洱茶 pǔ'ěr chá, nazywana czasem w Polsce herbatą czerwoną) przechodzi dodatkowy proces leżakowania i fermentacji z udziałem mikroorganizmów. Pochodzi z prowincji Yunnan w południowych Chinach. Nadaje się do wieloletniego przechowywania, zmieniając w trakcie aromat. Jej charakterystyczną cechą jest bardzo silny aromat, choć z czasem zanika gorycz. W Chinach fermentowane herbaty są uważane za korzystnie wpływające na trawienie. Nazwa "pǔ'ěr" jest tam zastrzeżona dla określonego miejsca pochodzenia. W innych prowincjach istnieje tradycja produkowania podobnych fermentowanych herbat, obejmuje je zbiorcze określenie hēichá (黑茶).
Herbata ulung[7] (lub oolong, wersje fonetyczne od chińskiej nazwy wulong, czyli „czarny smok”[8] 烏龍茶 wūlóngchá), nazywana też niebieską lub turkusową. Wywodzi się z Chin kontynentalnych i Tajwanu. Po zebraniu liście pozostawiane są na słońcu. Kiedy zwiędną, wytrząsa się je w bambusowych koszach lub odpowiednich maszynach w celu obtarcia brzegów liści. Czynności te (więdnięcie i wytrząsanie) powtarza się kilkakrotnie na przemian. Stopień utlenienia jest większy niż w herbatach zielonych, ale mniejszy niż w herbatach czarnych. Często herbaty tego typu podpraża się nad węglem drzewnym.
Herbaty aromatyzowane, zwane inaczej herbatami perfumowanymi, powstają przez mieszanie dodatkowych aromatów z liśćmi herbat – zielonej, czarnej, pu-erh, lub ulung. Czynność tę wykonuje się tuż przed pakowaniem, czyli po dokonaniu się wszystkich procesów związanych z produkcją herbaty. Najczęściej do herbat dodaje się płatki kwiatów (róży, jaśminu) lub owocowych esencji zapachowych (pomarańczowych i cytrynowych). Nie należy mylić herbat aromatyzowanych z naparami ziołowymi, zwanymi „herbatkami ziołowymi”, które nie powstają na bazie herbaty i nazwę herbata noszą niesłusznie.
Najbardziej popularne gatunki herbat aromatyzowanych: Earl Grey, lapsang souchong.
Zwana też herbatą ekspresową. Mocno rozdrobniona herbata zaparza się w krótkim czasie.
Herbaty prasowane znane są od czasów panowania w Chinach dynastii Tang. Wytwarzano wtedy herbatę w formie twardych kulek, wystawiając liście pod działanie pary, następnie prasując je i dopiero wtedy poddając suszeniu. W takiej właśnie formie herbata dotarła do Japonii. Produkowane są z herbat należących do różnych gatunków. Do ich produkcji mogą być stosowane całe liście, liście łamane oraz pył herbaciany. Obecnie herbata jest prasowana hydraulicznie na kształt płytek, niewielkich kostek, kulek, gniazd lub miseczek.
Według jednej z legend początki herbaty sięgają 2737 roku p.n.e., w którym mityczny cesarz Shennong przypadkowo zaparzył pierwszy napar z liści herbaty. Prawda jest trudna do ustalenia; wiadomo, że herbatę od dawna znali Chińczycy, jednak pierwsze zapiski na jej temat pochodzą dopiero z X wieku p.n.e.[9]
W VIII wieku chiński poeta Lu Yu napisał Księgę herbaty, zawierającą opis krzewu herbacianego, narzędzi do zbioru i selekcji liści, przyborów do ceremonii przyrządzania i picia herbaty. Zawierała również spis plantacji i wielbicieli tego napoju[10].
Dopiero ok. 803 roku n.e. mnich Dengyō Daishi przywiózł z Chin pierwsze nasiona krzewu herbacianego do Japonii, gdzie rozpoczęto jej uprawę. Kultura związana z piciem herbaty przyjęła w obu krajach zupełnie odmienne postaci. Różnice wynikają głównie z odmiennego podejścia filozoficznego. W Japonii narodziła się ceremonia parzenia herbaty związana z ascetyczną filozofią i estetyką buddyzmu zen, zwana cha-no-yu (dosł.: „wrzątek na herbatę”), do dzisiaj kultywowana jest w tym kraju. Jest to sztuka, która odznacza się umiłowaniem piękna. Kakuzō Okakura (Tenshin), autor monografii Księga herbaty (Cha no hon, 1906)[11], określa tę ceremonię jako kult oparty na adoracji piękna istniejącego pośród przyziemnych realiów codziennej egzystencji. 
Obrzędy chińskie natomiast, dużo starsze od japońskich, związane z taoizmem i konfucjanizmem oraz wierzeniami i praktykami różnych ludów zamieszkujących kontynentalne Chiny ulegały na przestrzeni wieków silnej regionalizacji. Dziś w całych Chinach można napotkać rozliczne ceremonie picia herbaty charakterystyczne np. dla różnych mniejszości narodowych. Herbata w Chinach darzona jest tak dużą estymą, że stanowi nawet kluczowy element tradycyjnych chińskich zaślubin.
Herbata pojawiła się w Mongolii pod koniec V wieku n.e., a w Tybecie w 620 n.e., w Tybecie bardzo szybko stała się podstawowym (obok kumysu) napojem, a w Mongolii upowszechniła się w XII wieku. Jednakże, w odróżnieniu od Chin i Japonii nie wykształcono tam tak rozbudowanych ceremonii picia herbaty, zaś sam proces przyrządzania i smak herbaty znacząco się różniły od chińskiego wzoru.
Po raz pierwszy z herbatą zetknęli się, w pierwszej połowie XVI wieku, Rosjanie, w czasie podboju Syberii i kontaktów dyplomatycznych z Chinami. W Rosji herbatę tradycyjnie nazywa się czajem (ros. чай) i przygotowuje się w samowarze, bez mleka, za to w postaci bardzo mocnego, intensywnego naparu.
Do Indii trafiła z Chin jako masala czaj (चाय). W Indiach czaj jest dostępny również u ulicznych sprzedawców zwanych ćajwala. Noszą oni czajniki z gorącą herbatą i podają ją w świeżo palonych glinianych kubkach, wyrzucanych zaraz po spożyciu.
Z kupcami hinduskimi herbata trafiła do krajów arabskich. W języku arabskim herbatę określa się jako شاي (šāī, szaj).
Pierwsze w zachodniej Europie informacje o herbacie pochodzą z końca XVI wieku (Marco Polo o niej nie wspomina). Podał je jezuita J.P. Maffei (1589) w dziele Historia Indii. Po nim o herbacie pisali Mikołaj Trigault (1615) i Alvaro Samedo (1643).
Po nich, znacznie większy zakres informacji podał polski misjonarz, podróżnik i przyrodnik Michał Boym, zauważając dwa rodzaje herbaty: zieloną i żółtą (tzw. kwiatową), oraz że liście suszy się również na ogniu, nie tylko na słońcu. W rękopisie Rerum Sinensium Compendiosa Descriptio, napisanym przed rokiem 1656, opisał herbatę w sposób następujący:
Krzew, z którego liści przygotowany jest słynny w całych Chinach napój, nazywany jest Cia. Zbierają liście na wzgórzach, suszą je na małym ogniu na żelaznych siatkach i zalewają wrzącą wodą, przez co jest zielony lub żółty. Zwykle rozkoszują się pijąc go w stanie gorzkim. Dodając do niego słodyczy, często częstują nim gości. Japończycy mieszają proszek z liści z wodą i piją. Jednostka wagi najlepszej herbaty kosztuje często bardzo drogo. Orzeźwia przy upałach, zapobiega tworzeniu się kamieni i senności[12].
Do Europy herbatę przywieźli Holendrzy na początku XVII wieku. Do Anglii herbata trafiła w 1658 za sprawą kupca Thomasa Garrawaya. W 1618 herbata pojawiła się w Rosji jako dar cesarza Chin dla cara Michała I Romanowa.
W połowie XIX w. Brytyjska Kompania Wschodnioindyjska zapoczątkowała na szeroką skalę uprawy w Indiach z sadzonek i nasion sprowadzonych z Chin przez Roberta Fortune'a.
Do Polski dotarła w 1664 roku z Francji[13]. Pierwsze znane wzmianki o herbacie pojawiają się w liście króla Jana II Kazimierza do żony Ludwiki Marii. Początkowo była traktowana jako ziele lecznicze i zwyczaj picia herbaty rozpowszechnił się dopiero w drugiej połowie XVIII wieku, pomogły w tym również dobre stosunki handlowe pomiędzy Polską i Anglią. Ze względu na wysoką cenę napój ten pity był jedynie na dworze królewskim, dworach magnackich oraz bogatej szlachty i mieszczan. Przez liczne kontakty z Rosją i ZSRR w Polsce, zwłaszcza na utraconych wschodnich terenach, a później wśród repatriantów, często na określenie herbaty używano określenia czaj. Na tych terenach również popularne było urządzenie służące do jej parzenia zwane samowarem[14].
W gwarze więziennej używa się pochodzącej od czaju formy czajura jako synonimu bardzo mocnego naparu mieszanki herbaty, kawy i tytoniu (np. 6 łyżek herbaty, 6 łyżek kawy, tytoń z jednego papierosa) zalewanego wrzącą wodą gotowaną za pomocą buzały.
W związku z piciem herbaty zrodziło się wiele zwyczajów, m.in. japońska ceremonia picia herbaty czy angielski tea time.
W krajach zachodnich tradycyjnie preferuje się długie zaparzanie niewielkiej ilości liści w dużej ilości wody:
Herbata zawiera znaczne ilości katechin, głównie epikatechinę, epigallokatechinę, galusan epikatechiny i galusan epigallokatechiny – są to związki spełniające rolę przeciwutleniaczy. W świeżo zebranych liściach zawartość tych związków może dochodzić do 30% suchej masy. Najwyższe stężenie katechin występuje w herbatachbiałej i zielonej, czarna zawiera mniej związków z tej grupy gdyż ulegają one rozkładowi podczas procesu utleniania. Herbata zawiera także taniny, teaninę, znaczne ilości jonów fluorkowych (do 0,03%) oraz alkaloidy purynowe, jak kofeina (do 4,5%), teobromina (do 0,5%) oraz teofilina[16] (do 0,04%). Herbata jest bogata także w niektóre witaminy: A, B1, B2, C, E i K. Ponadto zielona herbata zawiera niebiałkowy aminokwas L-teaninę, który wykazuje działanie uspokajające, zmniejszające stres i niepokój oraz stabilizujące nastrój osób pijących zieloną herbatę[17].
Napar herbaciany zawiera 100–594 mg kofeiny w 1 l płynu uzyskanego z czarnej herbaty. W przypadku zielonej herbaty wartość ta wynosi 160–450 mg/l[18]. Zawartość kofeiny w zaparzonym napoju zależy od gatunku i ilości herbaty oraz długości parzenia. Dla porównania, kawa parzona zawiera 386–652 mg na 1 l, a kawa espresso – 1691–2254 mg/l.
Badania wykazują negatywne skutki picia herbaty, wiążące się głównie z wysoką zawartością fluoru (do 9 mg/L i stawia w niebezpiecznej sytuacji osoby pijące herbatę w dużych ilościach[19], kiedy rekomendowane przez WHO stężenie fluoru w wodzie pitnej wynosi 0,5 - 1 mg/L[20]), jak i wysoką ilością glinu[19] w herbatach utlenionych, którego uwalnianie z liści i dostępność rośnie wraz ze spadkiem pH herbaty (np. po dodaniu soku z cytryny do naparu z fusami)[21]. 
Negatywne skutki może wywoływać u niektórych osób zawarta w herbacie kofeina (teina), która hamuje neurogenezę w hipokampie[22][23][24],
uczula receptory dopaminy, blokuje receptory adenozynowe, osłabia pamięć, zwiększa uczucie niepokoju i lęku[25] oraz powoduje uzależnienie fizyczne[26]. Zawarte w herbacie szczawiany mogą przy większym spożyciu prowadzić do uszkodzenia nerek[27].
Właściwości herbaty wykorzystywane są głównie w medycynie oraz kosmetologii. Działanie pobudzające herbata zawdzięcza teinie (kofeina). Ma ona również działanie moczopędne, powoduje rozszerzenie naczyń krwionośnych, a tym samym zapobiega chorobom układu krążenia. Ułatwia oddychanie oraz akcję serca zapobiegając także chorobom serca. Zapobiega rozwojowi paradontozy oraz powstawaniu ognisk miażdżycy. Niektóre badania stwierdziły jej działanie przeciwnowotworowe. 
W kosmetologii największe zastosowanie zyskała zielona herbata. Produkowanych jest wiele produktów na bazie jej wyciągu. Są to głównie toniki, balsamy nawilżające, preparaty do włosów, żele pod prysznic oraz kremy do skóry odwodnionej lub suchej. Ekstrakt z zielonej herbaty hamuje nadmierną aktywność gruczołów łojowych, a także ma właściwości przeciwdrobnoustrojowe, łagodzące stany zapalne, przyśpieszające gojenie się ran i zmniejszające obrzęki. Składnik środków stosowanych do włosów przetłuszczających się, leczenia łupieżu oraz środków nawilżających pod oczy. Przez swój subtelny zapach zielona herbata stosowana jest ponadto w perfumach.
W grudniu 2017 „British Journal of Ophthalmology” opublikowało badanie naukowców z Uniwersytetu Browna oraz Uniwersytetu Kalifornijskiego (USA), z którego wynika, że codzienne picie gorącej herbaty zmniejsza ryzyko zachorowania na jaskrę[29][30]. U badanych, którzy codziennie pili gorącą herbatę, ryzyko zachorowania na jaskrę było o 74% mniejsze, niż u pozostałych[29][30]. Herbata taka musi zawierać kofeinę[30].
Herbata kupowana w specjalistycznych sklepach posiada unikalne oznaczenia. Są to skróty angielskich terminów, określających klasę herbaty ze względu na rodzaj i wygląd liści. Występująca za oznaczeniem gradacji cyfra „1”, oznacza najlepszą partię w danym zbiorze. Poniższe oznaczenia odnoszą się do herbat czarnych pochodzących z Indii i Cejlonu.
W przypadku herbat łamanych, do ich oznaczenia, dodaje się literkę B (ang. broken – łamane). Znaczenie pozostałych symboli nie zmienia się.
Są to okruchy liści (ang. fannings/fines – odsiewy/miał, dust – pył), pozostałość po produkcji herbat łamanych. Uważana za herbatę o gorszej jakości. Jednak przez wzgląd na dostatecznie dużą powierzchnię parzenia oraz szybki czas zaparzania, są one stosowane w herbatach ekspresowych.
Głównymi producentami herbaty na świecie są: Indie, Sri Lanka, Chiny, Japonia, Indonezja, Bangladesz, Kenia, Uganda, Turcja, Malawi, Iran i Argentyna. Średni roczny plon wynosi 990 kg/ha (suchej masy liści). Najwięcej herbaty pozyskuje się w Azji oraz Afryce.
Polska importuje herbatę od głównych producentów na świecie (Kenii, Sri Lanki, Indii). Sprowadza się liście nieutlenione lub częściowo utlenione, następnie przetwarzane są w polskich zakładach, pakowane i zostają eksportowane za granicę bądź do polskich dystrybutorów. 
Według przeprowadzonego w styczniu 2017 r. rankingu Euromonitora oraz Banku Światowego, Polska zajmuje 4. miejsce w Europie w piciu herbaty (rocznie na osobę). Natomiast na świecie Polska lokuje się na miejscu 9., wyprzedzając takie państwa jak: Japonia, Chiny, Arabia Saudyjska. 
Całkowita wielkość zbiorów herbaty na świecie wyniosła w 2012 r. 4 818 tys. ton, co stanowi wzrost o 32% względem roku 2005. W poniższej tabeli przedstawiono wielkość zbiorów herbaty przypadających na poszczególne państwa[35].
Funkcja (łac. functio, -onis „odbywanie, wykonywanie, czynność”[a]) – dla danych dwóch zbiorów  i  przyporządkowanie[b] każdemu elementowi zbioru  dokładnie jednego elementu zbioru [1][2]. Oznacza się ją na ogół  itd.
Jeśli funkcja  przyporządkowuje elementom zbioru  elementy zbioru  to zapisujemy to następująco:
Zbiór  nazywa się dziedziną, a zbiór  – przeciwdziedziną funkcji  Zbiór wszystkich funkcji ze zbioru  do zbioru  oznacza się często [3].
Ponadto:
Wykresem funkcji  nazywa się zbiór  Z definicji funkcji wynika, że dla każdego  istnieje dokładnie jeden taki  że  Jeśli  jest funkcją ciągłą, to jej wykres jest krzywą w układzie współrzędnych na płaszczyźnie.
Wykres funkcji jednoznacznie ją określa. Jeśli  to  przy czym  jest jedynym takim elementem.
W teorii mnogości często stosuje się następującą definicję funkcji, pochodzącą od Peana[3]:
Faktycznie utożsamia się w niej funkcję z jej wykresem. Jest użyteczna w tworzeniu systemów aksjomatycznych pewnych teorii, bowiem funkcja jest wtedy pojęciem pochodnym względem aksjomatyki teorii mnogości.
Ważną klasą funkcji są funkcje
nazywane funkcjami o wartościach liczbowych[6].
W zbiorze funkcji liczbowych określonych na ustalonym zbiorze  można zdefiniować działania arytmetyczne:
Funkcja  jest ograniczona, jeśli istnieje taka liczba rzeczywista dodatnia  że dla każdego  spełniona jest nierówność 
Jeśli funkcja liczbowa  przyjmuje jedynie wartości rzeczywiste
to nazywa się ją funkcją o wartościach rzeczywistych[6].
Dla funkcji o wartościach rzeczywistych wyniki powyżej zdefiniowanych czterech działań arytmetycznych są funkcjami o wartościach rzeczywistych. Wyjątkiem jest mnożenie przez stałą, która powinna być rzeczywista, aby w wyniku mnożenia funkcji o wartościach rzeczywistych przez tę stałą uzyskać funkcję o wartościach rzeczywistych.
Funkcjami liczbowymi nazywamy:
Można także mówić o funkcjach liczbowych wielu zmiennych (rzeczywistych lub zespolonych):
których dziedzina jest podzbiorem iloczynu kartezjańskiego zbioru liczb rzeczywistych lub zbioru liczb zespolonych, które zapisuje się:
Jeżeli dziedzina  jest skończona, wystarczy wymienić wszystkie pary (argument, wartość). Można to zrobić za pomocą grafu (przykład obok).
Funkcje liczbowe można definiować za pomocą wzorów. Jest to sposób analityczny. W tym celu wykorzystuje się pewien zasób funkcji (wielomiany, funkcje elementarne itp.), działania algebraiczne, złożenie funkcji i operację przejścia do granicy (w tym operacje analizy matematycznej, takie jak różniczkowanie, całkowanie i sumowanie szeregów)[7].
Klasa funkcji, które można przedstawić za pomocą szeregu (potęgowego, trygonometrycznego itp.) jest bardzo szeroka. Każdą funkcję elementarną można przedstawić za pomocą szeregu potęgowego zwanego szeregiem Taylora.
Przedstawić analitycznie funkcję można w sposób jawny, tzn. jako  lub jako tak zwaną funkcję uwikłaną, tzn. za pomocą równania [7].
Czasem funkcja jest dana kilkoma wzorami, na przykład:
Do określenia funkcji można też stosować metodę opisową. Na przykład funkcja Dirichleta jest funkcją, która dla argumentów wymiernych przyjmuje wartość 1, a dla argumentów niewymiernych 0.
Funkcja może na ogół być określona na wiele sposobów. Na przykład funkcję sgn (x) można określić w taki sposób:
albo w taki:
Dla funkcji rzeczywistych o wartościach rzeczywistych stosowano tabelaryczny sposób określania funkcji. Obecnie w dobie kalkulatorów i arkuszy kalkulacyjnych tabele wartości funkcji logarytmicznych i trygonometrycznych i innych nie są już niezbędne, ale bywają wykorzystywane[8].
Ważnym sposobem przedstawiania i badania funkcji jest jej wykres, który dla funkcji  w przypadku funkcji ciągłej jest krzywą na płaszczyźnie[8].
Zamiast mówić o funkcji jako o relacji między zbiorami, można też mówić o zależności (związku) między dwiema zmiennymi  i  gdzie pierwsza z nich przyjmuje wartości ze zbioru  a druga przyjmuje wartości ze zbioru  wtedy  nazywa się zmienną niezależną, a  – zmienną zależną[9][10]. Taka interpretacja funkcji jest często używana w analizie matematycznej i zastosowaniach matematyki w innych naukach. W tym wypadku niezależność zmiennej  oznacza, że może się ona zmieniać w dowolny sposób, a zależność zmiennej  oznacza, że jej zmiany są zależne od zmian zmiennej  Na przykład droga  w ruchu jednostajnym o prędkości  jest zależna od czasu  ruchu i wyraża się wzorem:
W praktyce często się zdarza, że zbiór  jest opisywany przez kilka zmiennych niezależnych  Mówimy wtedy, że zmienna  jest funkcją zmiennych  Na przykład siła  działająca na ciało jest zależna od masy  ciała i jego przyspieszenia 
Definicję funkcji spełniają na przykład:
Wszystkie wielkości fizyczne rozpatruje się jako funkcje innych zmiennych:
Funkcja może wyrażać własność pewnego obiektu, dlatego obejmuje bardzo wiele pojęć z nauk empirycznych. Jako funkcję można też traktować każdą relację równoważności zachodzącą między dokładnie dwoma obiektami – jest to tzw. inwolucja.
Astronomia:
Chemia:
Biologia:
Medycyna i fizjologia:
Geografia fizyczna, geodezja i inne nauki o Ziemi:
Geografia społeczna, demografia i socjologia:
Ekonomia:
Psychologia:
Mając dwie funkcje  i  można utworzyć funkcję złożoną  określoną wzorem 
Wielokrotne złożenie funkcji  nosi nazwę iteracji. Ściśle: -tą iteracją funkcji  nazywa się funkcję
Funkcję  nazywa się funkcją różnowartościową lub iniekcją, gdy dla każdych dwóch różnych argumentów przyjmuje różne wartości, tzn. dla dowolnych dwóch  zachodzi warunek
Przykładem funkcji różnowartościowej jest funkcja określona wzorem 
Funkcję  nazywa się funkcją „na” lub suriekcją, jeżeli jej przeciwdziedzina  jest równocześnie jej zbiorem wartości funkcji. Oznacza to, że dla każdego  istnieje co najmniej jeden taki  że 
Funkcję będącą jednocześnie różnowartościową i „na” nazywa się funkcją wzajemnie jednoznaczną lub bijekcją. Innymi słowy, bijekcja przyporządkowuje każdemu  dokładnie jedno  (i na odwrót). Bijekcja  może istnieć tylko wtedy, gdy zbiory  i  mają tyle samo elementów (są równej mocy). Bijekcję  nazywa się permutacją.
Dla każdej funkcji wzajemnie jednoznacznej można określić funkcję  taką, że  którą nazywa się wówczas funkcją odwrotną.
Dla funkcji  można określić jej zawężenie, nazywane też obcięciem lub ograniczeniem, do zbioru  Jest to funkcja  taka, że  dla każdego  Nazywa się ją też funkcją częściową dla funkcji f[11].
Jeżeli  jest funkcją, a  jest jej zawężeniem do zbioru  to dla dowolnego zbioru  mamy 
Z drugiej strony, dla  można przedłużyć funkcję  zachowawszy często pewną regułę, otrzymując w ten sposób funkcję  Można np. wymagać, by przedłużenie  funkcji  było ciągłe, różniczkowalne lub okresowe.
Poszukiwaniem wzajemnych zależności między różnymi wielkościami zajmowali się już starożytni Grecy, którzy badali dość szeroki krąg zależności funkcyjnych. Pojęcie funkcji w postaci początkowej pojawiało się w średniowieczu, lecz dopiero w pracach matematyków XVII wieku, Fermata, Kartezjusza, Newtona i Leibniza, zaczęło być traktowane jako obiekt badań. Newton używał terminu fluenta[d]. Terminu funkcja użył po raz pierwszy[12] Leibniz w pracy Odwrotna metoda stycznych lub o funkcjach[13]. Po raz drugi Leibniz użył tego terminu w dość wąskim znaczeniu w pracy opublikowanej w czasopiśmie „Acta Eruditorum” w 1692 roku i dwa lata później w „Journal des Sçavans”. Następnie w tym samym 1694 roku Johann Bernoulli w „Acta Eruditorum”, nie używając co prawda słowa funkcja, oznaczył mimochodem literą n „dowolną wielkość utworzoną z nieoznaczonych i stałych”[e][14]. Po trzech latach, w tym samym piśmie, Bernoulli wielkości te oznaczał przez X i  a w liście do Leibniza z 26 kwietnia 1698 roku stwierdził, że symbole te są lepsze, bo „od razu jest widoczne, od jakiej zmiennej jest funkcja”. Jeszcze w 1698 roku w korespondencji między oboma uczonymi funkcja była rozumiana jako wyrażenie analityczne i weszły do użytku terminy wielkość zmienna i wielkość stała.
Określenie funkcji jako wyrażenia analitycznego było po raz pierwszy sformułowane w druku w artykule Johanna Bernoulli opublikowanym w 1718 roku. Napisał on:
Definicja. Funkcją wielkości zmiennej nazywa się tutaj wielkość utworzoną w jakikolwiek sposób z tej wielkości zmiennej i stałych[15].
W tym samym artykule zaproponował on jako „charakterystykę” funkcji grecką literę  zapisując argument jeszcze bez nawiasów  Zarówno nawiasy, jak literę f wprowadził Leonhard Euler w 1734 roku.
W języku potocznym używa się słów symetria (gr. συμμετρια) oraz symetryczny w odniesieniu do przedmiotu, obrazu itp. składającego się z dwóch części, z których każda jest  lustrzanym odbiciem drugiej (w poziomie lub pionie), np. litery A, H, I, M, T, B, C, D, O oraz pary liter pq, bd są symetryczne w tym sensie.
W terminologii matematycznej termin symetria ma znaczenie istotnie szersze. Obejmuje też inne własności figur, np. symetria liter N, S, Z nie jest wprawdzie lustrzana, ale po obrocie o 180º figura wygląda identycznie. Ponadto symetrie w matematyce są ujmowane jako pewnego typu przekształcenia figur geometrycznych. Do symetrii zalicza się obroty o wielokrotności danego kąta (np. o 30º, 60º, 90º,…) oraz wielkie bogactwo symetrii ornamentów, np. rozet w gotyckich katedrach[1]
Symetria jest to więc właściwość figury, bryły lub ogólnie dowolnego obiektu matematycznego (można mówić np. o symetrii równań), polegająca na tym, iż istnieje należące do pewnej zadanej klasy przekształcenie niebędące identycznością, które odwzorowuje dany obiekt na niego samego. Brak takiej właściwości nazywany jest asymetrią. W zależności od klasy dopuszczalnych przekształceń wyróżnia się rozmaite rodzaje symetrii. Tym samym terminem określa się nie tylko obiekty, ale też same przekształcenia.
Dla figur płaskich i przestrzennych w zależności od rodzaju przekształcenia wyróżniana jest m.in.:
Symetria – przekształcenie  przestrzeni euklidesowej E na siebie, mające pewną hiperpłaszczyznę H punktów stałych i spełniające warunek:
W zależności od wymiaru hiperpłaszczyzny H otrzymujemy trzy osobno określone wyżej pojęcia:
Grupa symetrii kwadratu składa się z czterech obrotów o kąty 90º, 180º, 270º i o kąt 0º (czyli przekształcenie tożsamościowe) oraz czterech symetrii osiowych (względem osi poziomej, pionowej i dwóch przekątnych). Złożenie dowolnych dwóch z tych ośmiu przekształceń też należy do tej grupy, ale wynik złożenia zależy od kolejności wykonywania tych przekształceń, tzn. działanie składania ich nie jest przemienne, więc grupa symetrii kwadratu jest nieprzemienna[2].
W ogólnym ujęciu „symetryczność” może odnosić się także do obiektów niegeometrycznych, jak np. równania, czy macierze i dotyczyć innych własności niż relacje usytuowania w przestrzeni.
Przykłady: liczby palindromiczne, niektóre kwadraty magiczne, trójkąt Pascala, bliźniacze krzyżówki tautogramowe.
Poniższa macierz jest symetryczna (względem głównej przekątnej):
Zbliżonym do symetrii pojęciem jest „samopodobieństwo”, które zakłada istnienie wzajemnie jednoznacznego przekształcenia części zbioru na cały zbiór. Najprostszy przykład to odwzorowanie zbioru liczb parzystych (dodatnich) w zbiór liczb naturalnych  Własność tę jednak mają również bardzo złożone zbiory, np. trójkąt Sierpińskiego, dywan Sierpińskiego i inne fraktale.
Pług – narzędzie uprawowe do wykonywania orki.
Najstarszy rysunek pługa (sprzed 5500 lat) znaleziono w starożytnym mieście Ur (obecnie Irak).
Pług składa się z jednego lub kilku korpusów płużnych. Każdy korpus płużny posiada lemiesz, który odcina pas roli (skibę) od calizny, odkładnicę odwracającą i wrzucającą odciętą skibę w bruzdę wykonaną przez poprzedni korpus płużny.
W zależności od przeznaczenia głębokość orki może wahać się od około 5 cm (podorywka) do około 30 cm (orka przedzimowa). Zależnie od rodzaju orki wyróżnia się pługi podorywkowe i do orki głębokiej.
Pierwsze stosowane pługi jednoskibowe były ciągnięte przez zwierzęta pociągowe – woły lub konie. Pług taki mógł być z przodu prowadzony na dwukołowym wózku zwanym koleśnicą lub z nią zintegrowany pług koleśny. Pług pozbawiony koleśnicy nazywano bezkoleśnym. Pługi wieloskibowe konne były zawsze zintegrowane z koleśnicami.
Pług może być uważany za rozwinięcie radła i sochy.
Element pługa w heraldyce:
Herb Ustronia
Herb Lubszy
Herb Gozdowa
Herb Płużnicy
Herb Wąsewa
Herb gminy Popielów
Herb powiatu łobeskiego
Prąd elektryczny – uporządkowany ruch ładunków elektrycznych[1].
W naturze przykładami są wyładowania atmosferyczne, wiatr słoneczny czy czynność komórek nerwowych, którym również towarzyszy przepływ prądu. W technice obwody prądu elektrycznego są masowo wykorzystywane w elektrotechnice i elektronice.
Pod wpływem pola elektrycznego (przyłożonego napięcia) w materiałach, w których istnieją ruchliwe nośniki ładunku dochodzi do zjawiska przewodzenia prądu elektrycznego.
Materiały, które dobrze przewodzą prąd elektryczny to przewodniki. Oporność właściwa dobrych przewodników jest rzędu od 10−8 do 10−6 Ω·m.
Dielektryk (izolator elektryczny) to materiał, w którym bardzo słabo przewodzony jest prąd elektryczny. Może to być rezultatem niskiej koncentracji ładunków swobodnych, niskiej ich ruchliwości lub obu tych czynników równocześnie. Oporność właściwa dielektryków jest większa od 106 Ω·m.
Półprzewodniki mają oporność właściwą pośrednią między metalami a izolatorami. Ich przewodnictwo zwykle mocno rośnie ze wzrostem temperatury.
Specyficzną formą przewodnictwa jest nadprzewodnictwo – występujący w niektórych materiałach efekt sprawiający, że w odpowiednio niskiej temperaturze ma on zerową rezystancję. W nadprzewodnikach zachodzą również inne zjawiska, na przykład efekt Meissnera. Większość materiałów wykazuje nadprzewodnictwo dopiero w bardzo niskiej temperaturze.
Prąd elektryczny jest w istocie ruchem cząstek obdarzonych ładunkiem, zwanych nośnikami ładunku. Umownie przyjęło się określać kierunek przepływu prądu poprzez opisanie ruchu ładunków dodatnich, niezależnie od tego jaki jest rzeczywisty znak i kierunek ruchu nośników w danym materiale.
W metalach (zarówno stałych, jak i w stanie ciekłym) nośnikami ładunku są elektrony. Elektrony, znajdujące się w paśmie przewodnictwa, mogą swobodnie się przemieszczać w objętości metalu. Dlatego wszystkie metale są dobrymi przewodnikami, a prąd elektryczny w metalach jest ruchem elektronów przewodnictwa.
W konwencjonalnych półprzewodnikach (takich jak krzem czy german) w temperaturze zera bezwzględnego nie ma elektronów w paśmie przewodnictwa. Przewodzenie prądu wymaga przeniesienia elektronów z pasma walencyjnego do przewodnictwa (poprzez dostarczenie im energii, na przykład termicznej lub w postaci promieniowania).
W elektrolitach, zarówno ciekłych, jak i stałych, nośnikami ładunku są ruchliwe jony – ujemne aniony i dodatnie kationy. W niektórych elektrolitach występują ruchliwe jony obu znaków, w innych tylko jednego. Istnieją przewodniki jonowe, wykazujące bardzo dobre przewodnictwo elektryczne nawet w stanie stałym (przewodniki superjonowe).
W gazach nośnikami prądu są jony, zarówno dodatnie, jak i ujemne. W próżni i rozrzedzonych gazach można wytworzyć wolne elektrony, których ruch jest prądem elektrycznym.
Przewodnictwo mieszane to przewodnictwo, w którym występuje zarówno przewodnictwo elektronowe, jak i jonowe. Tego typu przewodnictwo jest istotne na przykład w materiałach, z których wykonywane są elektrody ogniw paliwowych.
W wielu przypadkach wygodny jest opis procesu przewodzenia za pomocą kwazicząstek. Czyni się tak w przypadku półprzewodników, gdzie ruch elektronów w paśmie walencyjnym opisuje się raczej za pomocą ruchu „pustego miejsca po elektronie”, czyli dziury. Podobnie ruch elektronów w niektórych materiałach amorficznych i skompensowanych półprzewodnikach opisuje się za pomocą polaronów, czy nośniki ładunku w nadprzewodnikach za pomocą par Coopera.
Ruch naładowanego ciała jako całości jest również prądem elektrycznym.
Podstawowymi parametrami nośników prądu elektrycznego, determinującymi przewodnictwo materiału są ich koncentracja i ruchliwość.
Prąd elektryczny w przewodnikach płynie od potencjału wyższego do potencjału niższego. By było to możliwe, w obwodzie zamkniętym musi znajdować się element, który zapewni dostarczenie nośników ładunku z punktów o niższym potencjale do punktów o wyższym potencjale, czyli w kierunku przeciwnym do działającego na nie pola elektrycznego. Wymaga to dostarczenia energii i dzieje się w elementach nazywanych źródłami prądu. Rolę chwilowego źródła energii w obwodzie może pełnić również element inercyjny (mający zdolność gromadzenia energii) – uprzednio naładowany kondensator albo cewka indukcyjna z energią zgromadzoną w jej polu magnetycznym.
Wielkością opisującą prąd elektryczny jest natężenie prądu elektrycznego I, które definiuje się jako stosunek ładunku, który przepływa przez poprzeczny przekrój przewodnika do czasu przepływu tego ładunku t:
Definicja ta określa średnią wartość prądu w czasie t. By określić wartość chwilową, należy posłużyć się pochodną ładunku elektrycznego q po czasie t:
Jednostką natężenia prądu elektrycznego w układzie SI jest amper [A].
Natężenie prądu I można wyrazić też przez liczbę ładunków przepływających z prędkością v przez powierzchnię S
Często zamiast 'natężenie prądu elektrycznego I’ lub 'prąd elektryczny o natężeniu I’ mówi się krótko ‘prąd elektryczny I’.
W ośrodkach ciągłych parametrem najlepiej charakteryzującym prąd elektryczny jest gęstość prądu, opisująca przepływ ładunku przez jednostkową powierzchnię. W odróżnieniu od natężenia prądu, które jest skalarem i nie jest przypisana do punktu przestrzeni, gęstość prądu jest wektorem, a rozkład przestrzenny gęstości prądu nazywa się polem gęstości prądu.
Prąd stały charakteryzuje się stałą wartością natężenia oraz kierunkiem przepływu. Większość układów elektronicznych zasilana jest prądem stałym. Mogą być zasilane bezpośrednio z baterii lub akumulatorów. Dla urządzeń, które są zasilane z sieci energetycznej stosuje się zasilanie prądem stałym wytwarzanym przez zasilacze sieciowe.
Prąd zmienny to prąd elektryczny, którego wartość natężenia zmienia się w czasie. Prąd zmienny nieokresowy może reprezentować prąd o dowolnej zmienności w czasie (czarna krzywa na rysunku), może też prąd zmieniający się zgodnie z określoną funkcją matematyczną lub w sposób zdeterminowany zjawiskiem fizycznym. Potocznie termin prąd zmienny stosowany jest często do prądu okresowego o przebiegu sinusoidalnym.
Prąd przemienny to charakterystyczny przypadek prądu elektrycznego okresowo zmiennego, w którym wartości chwilowe podlegają zmianom w powtarzalny, okresowy sposób, z określoną częstotliwością. Wartości chwilowe natężenia prądu przemiennego przyjmują naprzemiennie wartości dodatnie i ujemne. Stosunkowo największe znaczenie praktyczne mają prąd i napięcie o przebiegu sinusoidalnym.
Teleinformatyka – równoważna ściśle powiązanym: informatyce technicznej z telekomunikacją, jako specjalność w dyscyplinie nauk inżynieryjno-technicznych oraz kierunek studiów, obejmująca inżynierię komputerów i oprogramowania w projektowaniu i eksploatacji systemów teleinformatycznych funkcjonujących z wykorzystaniem sieci internetowej.
W latach 78/79 dyskutowano w Polsce o dwóch rozumieniach pojęcia teleinformatyka (nazwa pochodząca ze złożenia nazwy telekomunikacja i informatyka) stwierdzając[1]:
Przez teleinformatykę rozumiemy dział informatyki, w którym przynajmniej niektóre zadania elementarne są wykonywane zdalnie za pomocą środków telekomunikacyjnych[2] oraz dział telekomunikacji zajmujący się problematyką szeroko rozumianego zdalnego dostępu komputerów nosi nazwę teleinformatyka[3].
a było to jeszcze w czasach mechanicznych i elektronicznych central PSTN.
Termin teleinformatyka istniał nieco w zapomnieniu przez następne lata. Jednakże w tych latach coraz więcej urządzeń sieci telekomunikacyjnych było projektowanych w technice cyfrowej, aż do opracowania infrastruktury sieciowej całkowicie zarządzanej przez systemy informatyczne. Równocześnie w tych latach, systemy informatyczne w znaczącym stopniu uzależniły swoje działania od stałego dostępu do publicznej sieci teleinformatycznej – internetu, stając się systemami teleinformatycznymi[4]. Pojawiły się też nowe wymagania na zapewnienie bezpieczeństwa systemów i sieci – bezpieczeństwa teleinformatycznego. Symbioza informatyki i telekomunikacji stała się faktem[5].
Coraz więcej uczelni wprowadza kierunki studiów dziennych oraz podyplomowych: „informatyka techniczna i telekomunikacja oraz studiów „teleinformatyka”[6]. Istnieje też zawód „technik teleinformatyk”.
Obecnie termin teleinformatyka oraz teleinformatyczny jest już powszechnie używany i dobrze rozumiany. Występuje też w szerszym znaczeniu opisu produktów i usług teleinformatycznych społeczeństwa cyfrowego.
W ramach obowiązującej od 2018 w Polsce Klasyfikacji dziedzin i dyscyplin naukowych teleinformatyka jest specjalnością w ramach szerokiej dyscypliny informatyka techniczna i telekomunikacja w dziedzinie nauk inżynieryjno-technicznych.
Zakres badań i przedmioty programu obejmują praktycznie cały obszar wiedzy informatycznej i telekomunikacyjnej, stąd konieczne jest formowanie specjalności i specjalizacji. W tym obszarze znajdują się działy:
Silnik parowy – silnik napędzany parą, najczęściej parą wodną.
Są nimi:
Pierwszym znanym silnikiem parowym była bania Herona z 60 n.e., a pierwszym znanym urządzeniem napędzanym silnikiem parowym był mechanizm otwierania drzwi świątyni z ok. 120 p.n.e. prawdopodobnie działający na zasadzie fontanny parowej.
25 sierpnia 2009 Charles Burnett III ustanowił nowy rekord prędkości dla pojazdu napędzanego parą, po ponad 100 latach pobił poprzedni rekord, osiągając prędkość 224 kilometrów na godzinę[1][2].
W czasach nowożytnych rozwój silnika parowego przebiegał zgodnie z chronologią wynalazków bezpośrednio z nim związanych:
Broń – termin występuje w dwóch znaczeniach:
Definicja broni nie została do dziś ustalona i wymyka się próbom jednoznacznego określenia[2]. Łatwiej natomiast definiowalne jest pojęcie "systemu broni", który tworzy kombinacja broni oraz wyposażenia używanego do dostarczenia jej destrukcyjnej siły do celu[3]. System broni składa się z:
Prowadzone są badania nad nowymi rodzajami broni np. laserową (zob. laser), innymi broniami energetycznymi, działami magnetycznymi, bronią wykorzystującą impuls elektromagnetyczny (EMP), środkami walki elektronicznej (WRE, ang. EW) itd. Rozwijane są projekty robotów bojowych.[potrzebny przypis]
Rzeźbiarstwo –  dziedzina sztuk plastycznych związana z tworzeniem trójwymiarowych form z różnych materiałów, co odróżnia ją od malarstwa i grafiki, zwykle dwuwymiarowych.
Efektem rzeźbiarstwa są rzeźby, przeznaczone zwykle oglądania dookolnego. Cechami rzeźb jest zwykle faktura (zależna od materiału) oraz kolor (polichromia), a także skala (miniaturowa lub monumentalna).
Istnieje wiele sposobów klasyfikacji rzeźb.
Ze względu na kształt:
Ze względu na liczbę przedstawionych postaci:
Ze względu na temat dzieli się je z kolei na:
Ze względu na funkcję można wyodrębnić następujące grupy:
Sposób wykonania rzeźby zależy od użytego materiału. Najczęściej używa się drewna, kamienia, metali szlachetnych, tworzywa sztucznego, kości słoniowej, gipsu, gliny, brązu, wosku i in.
Rzeźby powstają w wyniku sukcesywnego wybierania materiału. Najczęściej spotykanym gatunkiem drewna, z którego wykonywane są rzeźby na terenie Polski, jest lipa. Przyjmuje się, że wszystkie drzewa liściaste mogą stanowić materiał dla rzeźbiarza. Niektóre gatunki, bardziej szlachetne, charakteryzują się większą trwałością i twardością, są to dąb, jesion, orzech, kasztanowiec. Gatunki te ze względu na swoją cenę zazwyczaj wykorzystywane są do tworzenia rzeźb przeznaczonych do ekspozycji pod zadaszeniem. Rzeźby plenerowe o znacznych gabarytach najczęściej wykonywane są z topoli świeżo ściętej, ponieważ po wysuszeniu ten gatunek drewna jest bardzo trudny w obróbce. Do mniej popularnych gatunków drewna, z których można wykonywać rzeźby, należą: brzoza, jabłoń, śliwa, wiśnia; te ostatnie ze względu na przebarwienia dają bardzo interesujące efekty. Do drzew liściastych, z których raczej nie wykonuje się rzeźb, należą: grusza (ze względu na spiralny charakter słojów i nieprzewidywalność) oraz robinia akacjowa (ze względu na dużą twardość w połączeniu ze stosunkowo znaczną nieprzewidywalnością). Drzewa iglaste poza nielicznymi przypadkami nie są wykorzystywane do tworzenia rzeźb, ze względu na łykowatość, dużą żywiczność oraz tendencję do rozwarstwiania się materiału pod wpływem uderzeń dłuta.
Powstają najczęściej z marmuru lub piaskowca. Rzadziej, ze względu na trudności w obróbce, spotyka się rzeźby powstałe z granitu i innych rodzajów kamienia. Rzeźby w kamieniu ze względu na stosunkowo długi proces obróbki i koszt materiału najpierw poprzedzane są wykonaniem modelu gipsowego, niekiedy na podstawie modelu glinianego.
Jedną z metod wykonania rzeźby w metalu jest technika odlewnicza „na wosk tracony”. Rzeźba powstaje w wyniku zastąpienia modelu wykonanego w dowolnym materiale przez jego replikę z brązu (czasem mniej szlachetnych stopów metali także bazujących na mosiądzu albo odwrotnie, z bardziej szlachetnych, czyli srebra i złota).
Model wykonuje się przeważnie z gliny pokrywanej formą zwaną negatywem z gipsu. Gdy model jest wyjątkowo skomplikowany, stosuje się czasami specjalny silikon. Silikon stosuje się też przy wielokrotnym odlewaniu rzeźby z tej samej formy. Poprzez swoją elastyczną strukturę łatwiej zdjąć negatyw silikonowy ze skomplikowanego modelu niż sztywny negatyw gipsowy, który podatny jest na pęknięcia i uszkodzenia. Gdy negatyw gipsowy zastygnie, a gips stanie się twardy, rozbiera się tę strukturę w celu wyjęcia z wnętrza, modelu glinianego. W powstałą formę negatywową z gipsu bądź z silikonu wlewa się płynny wosk, który dokładnie odwzorowuje kształt negatywu.
Następnie model woskowy z doczepionym systemem wlewowym, też z wosku, jest obudowywany (zalewany) kolejną „formą”, ze specjalnej masy formierskiej. Podczas procesu wypalania wosk zostaje wytopiony lub ulega wyparowaniu. Stąd nazwa całego procesu „na wosk tracony”. W powstałe miejsce, przestrzeń, zostaje wlany płynny metal: aluminium, mosiądz, brąz, złoto, srebro. Po wystygnięciu forma jest tłuczona i rzeźba jest wyciągana. Następnie rzeźba poddawana jest procesowi obróbki wykańczającej np. szlifowaniu, spawania, patynowaniu itd.
Jest to forma z metalu uzyskana przy użyciu technik i narzędzi kowalskich. Metal nagrzewany jest przy pomocy paleniska kowalskiego lub pieca, a następnie poddawany jest obróbce kucia swobodnego lub matrycowego.
Rzeźba powstaje podczas obróbki bloków lodu za pomocą piły łańcuchowej. W użyciu są także mniejsze piłki, pilniki, dłuta, a nawet żelazka lub woda. Rzeźba może składać się z łączonych ze sobą obrobionych bloków lodowych. Rzeźby mogą powstawać na specjalnych chłodzonych postumentach. Dla osiągnięcia dodatkowych efektów artystycznych rzeźby podświetla się różnobarwnym światłem.
Abakany powstają w wyniku utwardzenia dzianiny pokrytej żywicą syntetyczną, który termin ten został użyty po raz pierwszy przez jednego z krytyków sztuki opisującego prace Magdaleny Abakanowicz, która jest uważana za prekursora tej techniki.
Tama Trzech Przełomów (chin. upr.: 三峡大坝; chin. trad.: 三峽大壩; pinyin: Sānxiá Dàbà) – zapora wodna wzniesiona na rzece Jangcy w centralnej prowincji Chin – Hubei. Budowa rozpoczęła się w roku 1993. Napełnianie zbiornika zakończono 26 października 2010, uzyskując poziom wody wynoszący 175 m, który umożliwia elektrowni wodnej działanie z pełną mocą. Zapora jest najdroższym pojedynczym projektem budowlanym na świecie. Koszt przedsięwzięcia ocenia się na 37 mld USD.
Inwestycja stała się obiektem krytyki licznych środowisk: historyków (w wyniku powstania zapory zalano liczne zabytki), ekologów (zagroziła ona egzystencji niektórych gatunków ryb i ssaków wodnych), geologów (skala sztucznego zbiornika jest tak wielka, że ciężar nagromadzonej wody może wpłynąć na tektonikę) oraz ekonomistów (inwestycja może przynieść więcej strat niż korzyści ekonomicznych).
Według planów z 2007 roku 26 generatorów o łącznej mocy 18,2 GW miało produkować 84,7 TWh energii rocznie oraz chronić przed powodziami, a także zwiększyć żeglowność rzeki i sprawić, że przez sześć miesięcy w roku statki oceaniczne o nośności 10 tys. ton docierać będą 2,4 tys. km w głąb lądu. Ostatnie osoby z miasteczka Gaoyang w prowincji Hubei, które było przeznaczone do zalania wodą, zostały ewakuowane 15 lipca 2008[1].
Tama Trzech Przełomów jest największą hydroelektrownią na świecie pod względem mocy. Moc elektrowni wynosi 22,5 GW (w roku 2007: 18,2 GW). Posiada 32 generatory, każdy o mocy 700 MW. Średnie zużycie węgla do wyprodukowania 1 kWh energii w Chinach to 366 gramów. Zatem tama Trzech Przełomów przyczynia się do ograniczenia zużycia węgla o (szacunki z roku 2007) 31 milionów ton rocznie. Co ogranicza emisje (szacunki z roku 2007) 100 milionów ton gazów cieplarnianych, milionów ton pyłów, miliona ton dwutlenku siarki, 370 tys. ton tlenku azotu, 10 tys. ton tlenku węgla oraz znaczących ilości rtęci do atmosfery.
Konsekwencją powstania tamy Trzech Przełomów stało się przymusowe przesiedlenie ponad 1,26 mln osób[2]. Całkowitemu zatopieniu uległ obszar 17 dużych miast, 140 miasteczek i ponad 3000 wsi. Powstanie zapory doprowadziło ponadto do zatopienia śródmieścia trzech dużych aglomeracji (Wanxian, Fuling, Chongqing).
W 2012 roku chińskie władze podjęły decyzję o sukcesywnym przesiedleniu kolejnych 100 tysięcy mieszkańców okolic zapory w związku ze wzmożoną aktywnością sejsmiczną i niebezpieczeństwem zatopienia przyległych obszarów w przypadku niekontrolowanej awarii. Powstanie tamy Trzech Przełomów jest największym w historii przykładem przesiedlenia ludności wywołanego realizacją pojedynczej inwestycji. Konsekwencją powstania zapory stało się także zatopienie ponad 1600 dotychczas istniejących fabryk i kopalń, oraz 1300 stanowisk archeologicznych.
Przemieszczenie po uruchomieniu zapory 40 mld ton wody spowodowało mierzalne, choć nieistotne w praktyce, skutki dla obrotu Ziemi: oś obrotu przechyliła się nieco, przesuwając biegun geograficzny o 2 cm, a doba wydłużyła się o 0,06 mikrosekundy[3].
Wieża – konstrukcja o wymiarach poprzecznych znacznie mniejszych od wysokości, pracująca jako wspornik utwierdzony w fundamencie, obciążony działaniem wiatru w kierunku poziomym oraz obciążeniem grawitacyjnym[1]. Wieże miały różne funkcje, szczególnie popularne były w architekturze średniowiecznej, przede wszystkim jako wieże obronne i kościelne – zarówno wolno stojące (kampanile, dzwonnice), jak i stanowiące część budynku kościoła. W średniowiecznym zamku występowały charakterystyczne formy mieszkalno-obronne (donżony) i tzw. wieże ostatniej obrony (stołpy).
Zasady obliczeń statycznych stalowych wież oraz masztów regulują normy PN-B-03204:2002 Konstrukcje stalowe. Wieże i maszty. Projektowanie i wykonanie[2] oraz PN-EN 1993-3-1:2006 Eurokod 3 – Projektowanie konstrukcji stalowych – Część 3 -1: Wieże, maszty i kominy – Wieże i maszty.
Ze względu na przeznaczenie wyróżniamy kilkanaście rodzajów wież:
Wieża ratuszowa w Krakowie
Krzywa Wieża w Pizie
Krzywa Wieża w Toruniu
Krzywa wieża w Ząbkowicach Śląskich
Mysia Wieża w Kruszwicy
Wieża Eiffla
Wieża w Tokio
Wieża nadawcza radiostacji gliwickiej
Szklana wieża polskiego pawilonu na Wystawie Światowej w Paryżu w 1925
Stalowa wieża z tarczami z pozłacanej miedzi, polskiego pawilonu na Wystawie Światowej w Nowym Jorku w 1939
Wieża ratuszowa w Bielsku-Białej
RTCN Święty Krzyż
Wieża Bazyliki Matki Bożej Bolesnej Królowej Polski w Licheniu
Wieża spadochronowa w Katowicach
Wieża Ziębicka w Nysie
Wieża bazyliki jasnogórskiej
Wieża wentylacyjna tunelu pod Mozą w Rotterdamie (1937) stanowi przykład art déco
Flamenco – zjawisko kulturowe, związane z folklorem andaluzyjskich Romów, obejmujące muzykę, śpiew, taniec, strój i zachowania. Jego tradycja pochodzi z Andaluzji, i tam do dziś można oglądać jego wykonanie.
W 2010 roku flamenco zostało wpisane na listę niematerialnego dziedzictwa UNESCO[1].
Taniec flamenco posiada swe korzenie w dawnych religijnych tańcach orientalnych. W dzisiejszej jego postaci dopatrzyć się można wielu elementów tańca hinduskiego, takich jak ruchy ramion, dłoni i palców, a także używanie nóg jako instrumentu perkusyjnego. Prawdopodobnie taniec ten pochodzi bezpośrednio od hinduskiego tańca katak[2].
Opowiada się tańcem historię i gestom nie przypisuje się konkretnych znaczeń. We flamenco ruchy ciała i gesty wyrażają pewne stany emocjonalne tancerza lub podkreślają znaczenie słów i charakteru melodii, która im towarzyszy.
Jest to taniec wykonywany solo, w duecie, grupowo lub składa się z kolejnych „solówek”, wykonywanych przez poszczególnych tancerzy.
Strój tancerzy – przeważnie czarny, granatowy lub ciemnobrązowy – to obcisłe spodnie, biała koszula, obcisła kamizelka, mała apaszka pod szyją, buty wzorowane na butach do jazdy konnej z wysokimi obcasami i charakterystyczny płaskodenny kapelusz. Kamizelka i kapelusz bywają wykorzystywane jako element choreografii.
Strój tancerek to przede wszystkim szeroka, kolorowa falbaniasta spódnica lub suknia (wykorzystywana w tańcu), falbaniasty gorset i kolorowa chusta, uzupełnieniem czasem bywa wachlarz, grzebień lub kwiat.
Muzyka wykonywana jest przede wszystkim na gitarze (choć zdarzają się przypadki użycia fletu, skrzypiec, wiolonczeli). Rytm wybiją zarówno tańczący jak i śpiewacy przez: uderzenia dłońmi (klaskanie), pstrykanie palcami, uderzenia otwartymi dłońmi o pudła, na których siedzą śpiewacy (tzw. cajón) i za pomocą kastanietów.
Z punktu widzenia charakteru wykonywanego śpiewu (muzyki) rozróżnia się następujące rodzaje flamenco:
Dawniej gitara flamenco była jedynie instrumentem towarzyszącym, ale na początku XX wieku zaczęła się powoli przekształcać w instrument solowy.  Największym gitarzystą tamtego okresu był Ramón Montoya (1880–1949), który wprowadził elementy zapożyczone z gry na gitarze klasycznej. W latach 60.  zadanie kontynuował Manolo Sanlucar.
Początki szerokiej popularności flamenco sięgają końca lat 60. XX wieku. Wśród wykonawców nowoczesnego flamenco najważniejszą postacią ostatnich lat był zmarły w 1992 roku Camarón de la Isla. Towarzyszył mu wybitny gitarzysta Paco de Lucía, oraz jego brat – Ramón de Algeciras (zm. 2009), a później inny wspaniały gitarzysta – Tomatito.  Po nagraniu 10 płyt z Paco de Lucią Camarón rozpoczął karierę solową nagrywając w 1979 rewolucyjną płytę La Leyenda del tiempo, o której gazety pisały, że dla flamenco jest tym samym, czym dla muzyki pop Sgt. Pepper’s Lonely Hearts Club Band Beatlesów.
Jednym z pierwszych nowatorów, który odniósł sukces komercyjny był Paco de Lucia, a wkrótce dołączyli do niego Lole y Manuel oraz inni, którzy unowocześnili brzmienie flamenco i poszerzyli klasyczny repertuar o nowe pieśni. I tak, na przykład Enrique Morente i Juan Peña – o pseudonimie „El Lebrijano” – pierwsi współpracowali z grupami andaluzyjskimi z Maroka.
Kolejni muzycy wprowadzali często do swych nagrań elementy rocka, salsy, bluesa i jazzu (w ten sposób powstał ruch muzyczny nazywany „nuevo flamenco”). Narodziły się grupy: Ketama, Pata Negra, La Barbiera del Sur, Navajita Platea, sukcesy odnosiła wokalistka Niña Pastori. Zespół Pata Negra, prowadzony przez braci Raimundo i Rafaela Amadora, wydał w roku 1987 album Blues de la Frontera, który stał się wielkim wydarzeniem na hiszpańskim rynku muzycznym.
Ważnym krokiem w rozwoju „nuevo flamenco” było nagranie przez Paco Peñę w 1991 Misa Flamenca – mszy z udziałem pieśniarzy flamenco oraz chóru muzyki poważnej z Londynu.
Pod koniec lat 90. udany powrót na scenę mieli tacy artyści jak Enrique Morente i José Mercé. Morente wydał w roku 1996 z grupa rockową Lagartija Nick ciekawy album Omega, natomiast José Mercé wspólnie z Vincente Amigo wydali płytę Del Amanecer.
Mimo stałego unowocześniania flamenco, nadal zasadnicze znaczenie ma fakt przechodzenia tradycji flamenco z pokolenia na pokolenie. Jedna z najlepszych wykonawczyń „czystego” flamenco Fernanda de Utrera (1923–2006) oraz jej młodsza siostra Bernarda były wnuczkami legendarnego cygańskiejlgo pieśniarza „Pininiego”. Nawet członkowie zespołu Ketama wywodzą się z dwóch cygańskich klanów – Carmonas i Sotos. Na początku XXI wieku imponującą karierę zaczęła robić Estrella Morente (ur. 1980), córka śpiewaka flamenco Enrique Morente i tancerki Aurory Carbonell.
Obecnie, coraz bardziej widoczne są wpływy innych gatunków muzyki na flamenco. Efektem jest powstanie takich hybryd muzycznych jak flamenco chill, albo flamenco lounge. Tacy artyści jak m.in. Almasala, bracia Sotomayor, Freddy Marquez czy Chambao czerpią inspirację z muzyki elektronicznej, chilloutowej osadzając ją na gruncie muzyki flamenco.
Tryktrak (ang. backgammon) lub Nardy (od pal. nēw-ardaxšīr — dosłownie: odważny Ardaszir[1]) – gra planszowa dla dwóch graczy. Każdy z graczy dysponuje 15 pionami w odrębnych kolorach, które przesuwa po planszy, składającej się z 24 pól (trójkątów), nazywanych liniami, zgodnie z liczbą oczek wyrzuconych na dwóch kostkach. Celem gry jest zdjęcie wszystkich swoich pionów z planszy. Wygrywa gracz, który uczyni to pierwszy.
Różne gry podobne do tryktraka były znane zarówno w Azji, jak i w Europie już w czasach starożytnych. Grano w nie głównie w basenie Morza Śródziemnego, na Bliskim Wschodzie i Azji Mniejszej. Możliwe, że słowa Alea iacta est odnosiły się właśnie do tej gry, a nie do klasycznych kości określanych jako „cubus”. Tryktrak do dzisiaj jest ludową grą Gruzinów, Ormian, Arabów i Turków, znany jest także w krajach Europy Południowej i Wschodniej, m.in. w Grecji, Bułgarii, na terenach byłej Jugosławii i ZSRR[2].
Angielska nazwa backgammon powstała w XVII wieku i pochodzi prawdopodobnie od średnioangielskiego słowa gamen oznaczającego grę: back game, czyli gra do tyłu, odnosi się do zasad: powrotu do domu swoimi pionami, a także możliwości zbicia i zaczynania od początku. Być może źródłosłowem jest też częste umieszczanie planszy do gry w tryktrak na odwrocie planszy do szachów.
Początkowe ustawienie pionów na planszy przedstawia poniższy obrazek.
Gracz posługujący się pionkami czarnymi porusza się po polach oznaczonych na rysunku od liczby 1 do 24. Gracz posiadający piony czerwone porusza się w przeciwnym kierunku – od 24 do 1. 
Po rzucie może przesunąć swe dwa pionki o liczby wskazane na kostkach (lub jednym pionem dwa razy). Piony można przestawiać wyłącznie na pola, na których nie znajdują się co najmniej dwa piony przeciwnika.
Wyjątkowym rzutem jest tak zwany „dubel” (ta sama liczba oczek na obu kostkach), który daje łącznie cztery ruchy o wskazaną liczbę oczek.
W ostatniej fazie gracz musi doprowadzić wszystkie swe pionki do „domu”. Dla gracza z czarnymi pionkami są to pola oznaczone numerami 19–24. Dopiero wtedy może zdejmować pionki z planszy. Odbywa się to poprzez wyrzucenie kostką liczby oczek potrzebnej pionkowi do opuszczenia planszy. Jeżeli gracz wyrzuci 6, a nie posiada pionków na szóstym polu (licząc od krawędzi planszy), to może zdjąć pion z kolejnego pola.
W czasie gry można zbijać. Zbicie polega na umieszczeniu swego pionka na polu, na którym znajduje się pojedynczy pionek przeciwnika (tzw. „blotka”). Po biciu pionek zbity jest odkładany na bandę i przeciwnik nie może kontynuować gry, dopóki jego pionek najpierw nie wróci na planszę. By pionek wrócił na planszę, trzeba rzucić kostką i rozpocząć grę tym pionkiem tak, jak gdyby znajdował się on na polu zerowym.
Linia znajdująca się na planszy nosi nazwę banda, a miejsce, do którego gracz musi włożyć piony – dwór.
W grze podstawową umiejętnością jest takie poruszanie pionkami, aby nigdy nie pozostawały samotne (samotne piony przeciwnik może zbijać).
Gracz stara się zablokować jak najwięcej pól w swym „domu”, a następnie zbić pion przeciwnika – wtedy graczowi trudno będzie odzyskać pion (w najlepszej sytuacji można zablokować cały dom – wtedy przeciwnik musi czekać, aż gracz zacznie opuszczać planszę).
Krykiet (ang. cricket) – sport drużynowy, w którym mecze rozgrywane są między dwiema drużynami po jedenastu zawodników. Pochodzi z Anglii, gdzie podobna gra była znana już na przełomie XIII i XIV wieku.
Mecze krykietowe rozgrywane są na trawiastym boisku o kształcie zbliżonym do owalnego, w środku którego znajduje się płaski pas (ang. pitch – dosłownie: "murawa") długości 20,12 m. Na końcach pasa ustawione są bramki (ang. wicket) – trzy wbite w ziemię drewniane słupki (ang. stump – dosłownie: "pniak", "kijek"). Gracz jednej z drużyn rzuca twardą piłkę z korka obłożonego skórą, o obwodzie 224–229 mm, w kierunku przeciwległej bramki, tak aby w nią trafić, a gracz drugiej drużyny (ang. batsman) broni jej przed uderzeniem za pomocą drewnianego kija.
Jeszcze jeden gracz z kijem (ang. non-striker) stoi w pobliżu gracza rzucającego (ang. bowler). Pozostali gracze z drużyny bowlera zajmują pozycje w owalu jako gracze z pola (ang. fielder). Celem gry jest zdobywanie runów, krykietowego odpowiednika punktów, przez batsmana i non-strikera.
Gra popularna jest w wielu innych krajach na świecie – w tym prawie we wszystkich należących do Brytyjskiej Wspólnoty Narodów, a także w krajach azjatyckich, i niektórych krajach europejskich. Kraje, których drużyny mają tzw. Test status – mogą rozgrywać między sobą mecze krykieta w najwyższej klasie rozgrywkowej – to:
Mecze krykietowe rozgrywa się w kilku odmianach. Najpopularniejsze z nich to:
Ślady istnienia prostej wersji krykieta pochodzą z XIII wieku, ale gra mogła powstać jeszcze wcześniej. Najprawdopodobniej stworzyły ją dzieci w społecznościach chłopskich i trudniących się metalurgią żyjących w rejonie zwanym The Weald pomiędzy Kentem i Sussex. Istnieją pisane źródła potwierdzające, iż w grę creag grał w roku 1300 w Newenden sam książę Edward, syn króla Anglii Edwarda I.
W 1598 w sprawie sądowej o ustalenie prawa własności do ziemi pojawiło się odniesienie do sportu zwanego kreckett – koroner nazwiskiem John Derrick zeznał, iż kilkadziesiąt lat wcześniej na obszarze, którego dotyczyła sprawa (były to tereny Royal Grammar School w Guildford), grał z kolegami w sport o tej właśnie nazwie. Oxford English Dictionary podaje, iż był to pierwszy zarejestrowany przypadek wystąpienia nazwy krykieta w języku angielskim.
Istnieje wiele słów podawanych jako etymologiczne źródło nazwy cricket. Jedną z teorii jest wywodzenie się jej od jednej z nazw kija krykietowego: starofrancuskiego criquet (rodzaj maczugi), flamandzkiego krick(e) ("kijek") lub staroangielskiego crycc ("kostur", "laska"; ta teoria oznaczałaby wywodzenie się nazwy cricket z północnej Anglii, gdyż w pozostałych rejonach cc było spółgłoską podniebienną – stąd mogło się ze słowa crycc wykształcić crutch, "kostur").
Jeszcze przed angielską wojną domową sport ten stał się rozrywką ludzi dorosłych, którzy grali przeciw sobie w drużynach reprezentujących poszczególne parafie. Po niej z kolei nowy purytański rząd zwalczał "nielegalne zgromadzenia", jak nazywano zbieranie się w celu uprawiania sportów, zwłaszcza tych głośniejszych, takich jak piłka nożna. Kładziono także większy nacisk na przestrzeganie szabatu, a jako że był to jedyny czas wolny, jakim dysponowały klasy niższe, popularność krykieta okresowo spadła praktycznie wszędzie, poza niektórymi szkołami płatnymi.
Po Restauracji w 1660 krykiet znów się rozwijał i pod koniec stulecia osiągnął kształty zorganizowanej, wręcz profesjonalnej rozgrywki, w którą stawką były pieniądze. Wiadomo, że wielki mecz krykietowy o wysoką stawkę (50 gwinei od drużyny) z jedenastką zawodników w każdej ekipie rozegrano w Sussex w 1697; ten właśnie mecz stał się pierwszym, o którym wspomniały angielskie gazety, co stało się w dużej mierze dzięki wprowadzeniu rok wcześniej wolności prasy.
W XVIII wieku krykiet przestał być jedynie "wiejską rozrywką" i urósł do rangi narodowego sportu Anglików – w dużej mierze dzięki zakładom na pieniądze i zestawianiu przez bogatych własnych "jedenastek" – tak właśnie powstały słynne kluby ze Slindon (założyciel: Charles Lennox, 2. książę Richmond) i Hambledon. W Londynie z kolei, gdzie sport ten zyskał wielką popularność już na początku stulecia – już w 1707 roku na mecze krykieta na Artillery Ground w Finsbury przychodziły tłumy – w 1787 założono Marylebone Cricket Club, który przez długie lata miał prawo samodzielnego ustalania zasad krykieta. Angielski Stadion Narodowy to Lord's Cricket Ground w Londynie.
W XIX wieku, mimo że na kilka lat krykietowi przeszkodził wybuch wojen napoleońskich, rozwój trwał: nastąpiły zmiany w technice gry – underarm bowling, posyłanie piłki przez bowlera w kierunku batsmana, tak aby toczyła się po ziemi, zostało zastąpione najpierw przez roundarm, a następnie (w 1864) overarm bowling, w których piłka jest rzucana, a nie toczona, przez co trudniej ją odbić. Zmiany te uznano za bardzo kontrowersyjne i spotkały się one ze zdecydowanym sprzeciwem wielu konserwatywnych ludzi krykieta, ale ostatecznie zostały zaakceptowane przez klub z Marylebone.
Począwszy od 1839 zakładano kluby będące reprezentacjami hrabstw (pierwszym był Sussex County Cricket Club), które w grudniu 1889 ustanowiły, a w następnym roku rozpoczęły własne oficjalne rozgrywki – County Championship.
XIX wiek to również początki meczów międzynarodowych. Pierwsze takie spotkanie miało miejsce w 1844 roku w Elysian Fields w New Jersey, a zmierzyły się ze sobą reprezentacje USA i Kanady. W 1859 drużyna angielska wybrała się na pierwsze w historii zamorskie tournée do Ameryki Północnej, a osiemnaście lat później, w 1877, odbył się pierwszy w historii Test match – w Melbourne Anglia zmierzyła się z Australią.
Trzynaście lat wcześniej, w 1863 roku, karierę rozpoczął piętnastoletni wówczas W. G. Grace. Przez 44 lata gry zrewolucjonizował ten sport – nie tylko praktycznie w pojedynkę przyczyniając się do jeszcze większej popularyzacji krykieta, ale też samodzielnie opracowując większość do dziś stosowanych technik odbijania piłki.
Ostatnie dwie dekady przed wybuchem I wojny światowej nazwane zostały "Złotą Erą Krykieta" pod wpływem tęsknoty za tym, co zabrał tej dyscyplinie ogólnoświatowy konflikt. Z kolei okres międzywojenny to czas dominacji jednego gracza, australijskiego batsmana Donalda Bradmana. To właśnie jako broń przeciw jego wielkim umiejętnościom Anglicy opracowali kontrowersyjną taktykę zwaną bodyline, polegającą na celowaniu piłką w odbijającego.
W roku 1961 rozpoczął się największy kryzys w historii krykieta, skutek południowoafrykańskiej polityki segregacji rasowej – apartheidu. Gdy RPA została zmuszona do wystąpienia z Commonwealthu, jej federacja krykieta musiała – na mocy ówczesnych przepisów – opuścić International Cricket Conference (ICC). Siedem lat później kryzys pogłębił się, gdy południowoafrykańskie władze odwołały tournée angielskiej reprezentacji do RPA ze względu na obecność w składzie Anglików czarnoskórego Basila D'Oliveiry. W 1970 członkowie ICC zawiesili do odwołania występy bardzo silnej wówczas reprezentacji RPA w międzynarodowych rozgrywkach. Południowi Afrykańczycy organizowali więc "buntownicze tournée" (ang. rebel tours) – płacili krykiecistom z innych państw, by ci formowali "jednorazowe" drużyny i grali mecze przeciw RPA. Mimo że ICC zadecydowała, by gracze, którzy przyjmą taką ofertę, również byli zawieszani, wielu, zwłaszcza starszych i kończących kariery, dawało się skusić, gdyż krykiet nie był wówczas sportem, w którym gracze zarabiali znaczne sumy pieniędzy. Sytuacja ta trwała aż do 1991 roku, kiedy do władzy doszedł Nelson Mandela, a apartheid został zniesiony.
Nowa era w historii krykieta zaczęła się w 1963 roku, kiedy angielskie hrabstwa zmodyfikowały zasady, umożliwiając rozgrywanie nowego typu meczów – meczów o odgórnie ustalonej liczbie overów. Osiem lat później na podstawie tego przepisu powstały mecze jednodniowe – One-day International (ODI). International Cricket Council szybko zaakceptowała tę formę gry i w 1975 zorganizowała pierwsze mistrzostwa świata w krykiecie w meczach jednodniowych. Od tego momentu ODI stopniowo zdobywały coraz większą popularność kosztem meczów wielodniowych.
Na początku XXI wieku dłuższa odmiana krykieta zaczęła co prawda odzyskiwać popularność, ale w tym samym okresie pojawiła się kolejna odmiana z ograniczoną liczbą overów – Twenty20.
Najstarsza wzmianka o żeńskim krykiecie pojawiła się w The Reading Mercury 26 lipca 1745 roku i dotyczyła meczu drużyn z miejscowości Bramley i Hambledon nieopodal Guildford. Spotkanie zakończyło się zwycięstwem pań z Hambledon, a autor tekstu w Mercurym chwalił zawodniczki obu drużyn za umiejętności nieodstające od męskich.
Pierwszy klub kobiecy założono w 1887 w Nun Appleton w Yorkshire. Nadano mu nazwę White Heather Club. Trzy lata później drużyna pod nazwą Original English Lady Cricketers objechała Anglię, grając liczne mecze pokazowe. W 1926 powstał Związek Krykieta Kobiet, a w latach 1934-35 odbyło się pierwsze zamorskie tournée do Australii i Nowej Zelandii, w ramach którego w grudniu 1934 miał miejsce pierwszy w historii mecz testowy kobiet pomiędzy Australijkami i Angielkami.
W 1958 powstała International Women's Cricket Council, od 2005 roku działająca jako część International Cricket Council pod nazwą Women's Cricket Committee, koordynująca rozgrywki żeńskiego krykieta, w który gra się obecnie w Anglii, Australii, Nowej Zelandii, RPA, Indiach Zachodnich i Holandii.
Pierwsze Mistrzostwa Świata w Krykiecie Kobiet rozegrano w 1973 w Anglii. Tytuł zdobyła reprezentacja gospodarzy.
Podstawowy sprzęt to wyposażenie zawodników:
Ubiór zawodników to koszulka polo, długie spodnie, buty z kolcami (dla uzyskania lepszej przyczepności), nakrycie głowy chroniące oczy przed słońcem, a w razie potrzeby także wełniany sweter. Batsmani i wicket-keeperzy, którzy narażeni są na częste uderzenia szybko lecących piłek, zakładają także ochraniacze na nogi oraz podbrzusze, kaski i rękawice. Na wyposażenie pola gry składają się:
Przebiegiem gry rządzą 42 zasady opracowane przez Marylebone Cricket Club w porozumieniu z innymi państwami, w których gra się w krykieta. Poza różnicami istniejącymi pomiędzy poszczególnymi odmianami krykieta dopuszczalne jest jednak również zmienianie zasad w poszczególnych meczach za zgodą obu drużyn. Można też stosować dodatkowe zasady, w zależności od okoliczności, w których rozgrywany jest mecz.
Krykiet powszechnie uważany jest za „grę dżentelmenów”, charakteryzującą się wysokimi standardami etycznymi. Nie należą do rzadkości sytuacje, w których decyzja sędziego zostaje zakwestionowana przez któregoś z kapitanów jako krzywdząca i zmieniona na korzyść rywala. Zachowanie kapitanów i innych graczy określa Preambuła Przepisów Krykieta. W przypadku niestosownego zachowania któregoś z graczy, za zwrócenie mu uwagi odpowiada kapitan drużyny, który jest absolutnym liderem zespołu na boisku i poza nim.
Drużyna składa się z jedenastu graczy, z których część wyspecjalizowana jest jako rzucający (ang. bowler; sam rzut nosi po angielsku nazwę delivery), a część jako odbijający (ang. batsman). Wyrównana drużyna ma zazwyczaj pięciu lub sześciu specjalistów od odbijania i czterech lub pięciu od rzucania, a także jednego gracza wyspecjalizowanego w grze na kluczowej pozycji łapiącego (ang. wicket-keeper, co można przetłumaczyć jako "bramkarz").
Zawodnicy osiągający znakomite wyniki zarówno jako rzucający jak i jako odbijający, tacy jak Australijczyk Adam Gilchrist (łączy role wicket-keepera i batsmana) czy Anglik Andrew Flintoff (batsman i bowler) znani są jako wszechstronni (ang. all-rounder) i są wysoce cenieni.
Rolą każdego gracza z pola (ang. fielder) jest uniemożliwienie batsmanom drużyny przeciwnej zdobywania runów, a by tego dokonać przede wszystkim muszą złapać piłkę. Kluczową rolę odgrywa wśród nich wicket-keeper, zawodnik ustawiony za wicketem, który ma za zadanie łapać piłki, które miną batsmana, nie wchodząc w kontakt z jego kijem.
Od właściwych decyzji podejmowanych przez kapitana odnośnie do strategii drużyny zależy jej zwycięstwo. To właśnie on podejmuje decyzje w kwestii ustalania pozycji graczy w polu czy bowlerów. On także wybiera stronę monety przy przedmeczowym rzucie monetą, decydującym, która drużyna będzie odbijała jako pierwsza; decyzja ta, podejmowana na podstawie aktualnych i przewidywanych warunków pogodowych, stanu pitchu oraz umiejętności własnych i przeciwnych zawodników, może mieć znaczący wpływ na przebieg i wynik meczu.
Oznacza to również, iż to właśnie kapitan narażony jest na najostrzejszą krytykę w razie przegranej, co sprawia, że musi być człowiekiem o dużej odporności psychicznej, ale też jest zazwyczaj najbardziej chwalony, gdy jego ekipa wygra.
Jeżeli batsman jest w stanie odbijać, ale kontuzja nie pozwala mu biegać, sędziowie oraz kapitan drużyny przeciwnej mogą wyrazić zgodę, by odbijającemu pomagał biegacz (ang. runner) – jeśli to możliwe, powinien tę funkcję pełnić zawodnik, który już odbijał. Jedyną rolą biegacza jest bieganie między wicketami w zastępstwie kontuzjowanego kolegi. Musi wówczas mieć na sobie dokładnie takie samo ubranie i sprzęt jak unieruchomiony odbijający. Zajmuje miejsce po lewej stronie praworęcznego odbijającego (lub po prawej leworęcznego) i w odległości około 20 metrów od niego. Od 2011 roku zabronione jest korzystanie z biegacza w meczach międzynarodowych.
Jeżeli batsman odniesie kontuzję, która uniemożliwia mu odbijanie, ma prawo zejść z boiska i powrócić na nie, gdy jego stan się poprawi (jednak tylko podczas przerwy spowodowanej wyautowaniem któregoś z graczy jego drużyny). W przypadku zejścia niedysponowanego batsmana, gra jest kontynuowana, a na boisko wchodzi kolejny batsman. Jeśli batsman nie powróci już do gry, przy jego nazwisku odnotowuje się Retired - not out (pl. zszedł, niewyautowany). Jeżeli batsman zejdzie z boiska z powodów innych niż kontuzja (np. zmęczenie), ma prawo powrócić do gry, ale tylko za zgodą kapitana przeciwnej drużyny. Jeśli nie powróci na boisko lub kapitan nie wyrazi zgody, przy nazwisku batsmana zaznacza się Retired - out (zszedł, wyautowany).
Ponadto, we wszystkich formach krykieta, jeśli gracz odniesie kontuzję lub rozchoruje się w trakcie meczu, jego miejsce może zająć gracz rezerwowy. Zawodnikowi temu nie wolno jednak rzucać, odbijać, być kapitanem czy wicket-keeperem, a gdy tylko gracz z podstawowego składu znów będzie gotów do gry, wraca na boisko, a rezerwowy z niego schodzi.
Przez dziewięć miesięcy – od lipca 2005 do marca 2006 – ICC testowała pomysł "superrezerwowego" (ang. Super Sub) w meczach jednodniowych. Byłby to gracz, który na stałe (do końca meczu) zastępowałby kolegę z drużyny i mógłby zajmować wszystkie pozycje na boisku. Pomysł porzucono.
Mecz składający się z dwóch inningsów i niemający odgórnie ograniczonej liczby overów rozgrywany jest na przestrzeni trzech do pięciu dni z około sześcioma godzinami gry każdego dnia. W trakcie gry są ogłaszane oficjalne przerwy na obiad i herbatę, a w razie potrzeby także krótsze przerwy, w trakcie których zawodnicy mogą się napić.
Mecze rozgrywane są jedynie przy bezdeszczowej pogodzie, a w przypadku meczów profesjonalnych, gdy piłka może lecieć z prędkością ponad 140 km/h, gra się tylko w dzień, aby było wystarczająco dużo światła, by batsman mógł zobaczyć piłkę. Dlatego też mecze przerywane są w czasie opadów (z wyjątkiem mżawki) oraz silnego zachmurzenia. Niektóre mecze jednodniowe rozgrywane są przy sztucznym oświetleniu, ale – poza kilkoma eksperymentami w Australii – nie próbowano tego w przypadku dłuższych gier.
Ze względu na to, iż profesjonalne mecze krykietowe rozgrywane są "na powietrzu", w Anglii, Australii, Nowej Zelandii, RPA i Zimbabwe gra się latem, zaś w Indiach Zachodnich, Indiach, Pakistanie, Sri Lance i Bangladeszu zimą; latem uniemożliwiają to huragany i monsuny.
Mecz podzielony jest na dwie części (ang. innings; uwaga: słowo to występuje w tej samej formie w liczbie pojedynczej i mnogiej). Poprzez rzut monetą decydowane jest, która drużyna zacznie mecz jako rzucająca, a która jako odbijająca – wyboru dokonuje ten kapitan, który wybierze właściwą stronę monety. W drugim inningsie następuje zamiana ról.
Innings jest zakończony, jeżeli:
Innings podzielony jest na części (ang. over), każda składająca się z sześciu legalnych rzutów (ang. delivery). Rzuty niezgodne z przepisami (wide lub no ball) są powtarzane, a do wyniku strony odbijającej dopisuje się jeden bieg jako tzw. "extras". Po każdym overze dokonywana jest zmiana stron pitchu, a rzucający zajmuje pozycję w polu, skąd z kolei jeden zawodnik zostaje rzucającym; także sędziowie zajmują pozycje po przeciwnych stronach pitchu.
Celem batsmana jest zaliczenie możliwie największej liczby biegów (ang. run), które są krykietowym odpowiednikiem punktów. Bieg zaliczany jest, jeśli dwóch batsmanów – ten, który odbijał i stojący po przeciwnej stronie pitchu czyli non-striker – zamieni się miejscami, lub piłka znajdzie się poza granicą pola gry. Celem drużyny bowlera jest wyeliminowanie (ang. out) wszystkich batsmanów rywala w możliwie jak najmniejszej liczbie rzutów, co można uzyskać na wiele sposobów, które opisano niżej.
Gdy odbijający zostanie wyeliminowany, jego miejsce zajmuje następny. Innings kończy się, kiedy wyeliminowany zostanie dziesiąty batsman (w drużynie jest jedenastu zawodników, ale oprócz odbijającego w grze w danej chwili bierze udział jeszcze zawodnik biegający). W tym momencie drużyna jest "cała wyeliminowana" (ang. all out). Po zakończeniu inningsu drużyny zamieniają się rolami.
Zwycięzcą zostaje ta drużyna, która w chwili zakończenia meczu ma na koncie więcej runów. W różnych wariantach krykieta można spotkać w meczu różną liczbę overów i inningsów.
Bowler rzuca piłkę w kierunku batsmana w określony przepisami sposób (ang. bowling action) – oryginalna definicja mówiła, że ręka nie może się wyprostowywać podczas rzutu. Rzut z wyprostowującą się ręką był (i jest) traktowany jako throwning, który w krykiecie jest zabroniony. Zginanie podczas rzutu uznano za dozwolone, ale wyprostowanie w trakcie rzutu powodowało ogłoszenie no ball.
W związku z tym bowlerzy rzucają (bowling) piłkę mając rękę wyprostowaną i obracając dookoła ramienia.
Jednak w 2005 roku definicja została uznana za fizycznie niemożliwą, więc ją zmieniono. Badania biomechaniczne pokazały, że każdy bowler jakoś rozprostowuje rękę w trakcie akcji bowlingowej, gdyż obrót ręki dookoła ramienia powoduje automatyczne rozprostowanie lekko zgiętego łokcia. Uznano więc, że można uznać rozprostowywanie łokcia do 15 stopni. Powyżej tej granicy akcja jest uznawana za no ball.
Kolejne ograniczenie, jakiemu podlega rzucający, dotyczy ustawienia na pitchu – przynajmniej część jego przedniej (tzn. bliższej odbijającemu) stopy w czasie tego kroku, podczas którego oddawany jest rzut, znajdować się musi za popping crease, choć nie musi dotykać w tym momencie ziemi. Złamanie tego przepisu również oznacza ogłoszenie no ball.
Najczęściej bowler rzuca tak, by piłka odbiła się od ziemi w drodze do batsmana. Musi to jednak zrobić tak, aby nie minęła go poza jego zasięgiem, gdyż wówczas ogłaszany jest wide. Zarówno wide jak i no ball oznaczają dodanie do wyniku odbijającej drużyny jednego runa i konieczność powtórzenia nieprzepisowego rzutu.
Podstawowym zadaniem bowlera jest wyeliminowanie batsmana rywali (ang. out lub dismissal). W dalszej kolejności musi maksymalnie ograniczyć liczbę runów zdobytych przez batsmana (liczba runów na over określana jest jako economy rate).
Wyróżnia się dwa podstawowe typy bowlerów: rzucających piłki szybkie (ang. fast bowler) i podkręcone (ang. spin bowler).
Batsman odbija piłkę z batting crease używając kija. Jeżeli uda mu się czysto trafić płaską powierzchnią, uderzenie takie nosi nazwę shot (ang. "strzał") lub stroke (ang. "cios", "uderzenie"), a jeśli krawędzią – edge (ang. "krawędź") lub snick (ang. "nacięcie"). Strzały noszą nazwę zależną od kierunku, w jaki posyłają piłkę. W zależności od obranej przez drużynę strategii, można odbijać defensywnie, zbijając piłkę w dół lub ofensywnie, mocno w kierunku pustych sektorów boiska. Nie ma zasady nakazującej batsmanowi bieg i walkę o runa po odbiciu piłki.
Jeżelli batsmanowi uda się posłać piłkę poza linię graniczną pola gry, automatycznie zdobywa runy – sześć, jeśli piłka w drodze nie dotknie ziemi lub cztery, jeśli dotknie.
Batsmani przystępują do odbijania w kolejności (ang. batting order) wyznaczonej przez kapitana. Z reguły pierwsza para (ang. openers) musi stawić czoła najbardziej agresywnym rzutom oddawanym przez niezmęczonych fast bowlerów dysponujących nową, niezużytą piłką. Zazwyczaj jako ostatni wyznaczani są najsłabsi odbijający, gracze specjalizujący się w grze na innych pozycjach, którym rzadko udaje się zdobyć runy.
Aby zdobyć runa, odbijający musi uderzyć piłkę i pobiec na przeciwległy koniec pitchu, zaś jego nieodbijający partner biegnie w przeciwnym kierunku. By run został zaliczony, obaj zawodnicy muszą dotknąć ziemi za popping crease albo kijem albo częścią ciała. Jeśli piłka zostanie uderzona dostatecznie mocno, zawodnicy mogą zawrócić i przebiec tę samą drogę z powrotem, by zdobyć kolejnego (albo nawet kolejne) runa. Jeżeli batsmani zdobędą nieparzystą liczbę runów, a tym samym zamienią się końcami pitchu, będzie to również oznaczało, że odbijający i nieodbijający zamienią się rolami.
Jeśli gracz z pola (fielder) strąci poprzeczki wicketu w chwili, gdy żaden batsman nie jest bezpieczny za popping crease, odbijający bliższy tego wicketa jest wyeliminowany.
Jeśli piłka przekroczy granicę pola gry, drużyna odbijająca zdobywa cztery runy, a jeśli przed jej przekroczeniem nie dotknie murawy – sześć.
Odbijający może zostać wyeliminowany (ang. dismissed) na dziesięć sposobów. Gdy to się stanie, opuszcza pole gry, a jego miejsce zajmuje następny batsman. Kiedy wyeliminowany zostanie dziesiąty, a tym samym ostatni, jedenasty, pozostanie bez partnera, następuje zakończenie inningsu.
Wiele trybów eliminowania batsmana wymaga, by wicket został "położony" (ang. put down), co oznacza strącenie przynajmniej jednego baila ze stumpów, względnie wyrwanie stumpa z ziemi przez piłkę lub rękę trzymającą piłkę. Z poniżej wymienionych metod eliminowania batsmana, pierwsze sześć występuje powszechnie, podczas gdy pozostała czwórka to raczej regulaminowe ciekawostki rzadko znajdujące zastosowanie.
Nad przebiegiem meczu czuwa dwóch sędziów (ang umpire). Jeden z nich, field umpire, stoi za tą bramką, od której następuje rzut, i podejmuje większość rozstrzygnięć. Drugi, square leg umpire, stoi na pozycji zwanej square leg, z której ma dobry widok z boku na batsmana i pomaga w podejmowaniu decyzji odnośnie do tych sytuacji, które widzi lepiej. Od sezonu 1992/1993 w niektórych meczach zawodowych obaj sędziowie mogą zwrócić się do trzeciego, pozaboiskowego arbitra (zwanego też sędzią telewizyjnym), który ma możliwość korzystania z powtórek telewizyjnych.
Podczas meczu pracuje także dwóch wynikowych (ang. scorer), zwykle po jednym wyznaczonym przez każdą z drużyn. Ich zadaniem jest odnotowywanie wszystkich zaliczonych runów i outów. Robią to w oparciu o sygnały od sędziego, z którym w przerwach meczu (a na bieżąco także ze sobą nawzajem) muszą kontrolować poprawność wyniku. W praktyce wynikowi zajmują się także wszelkimi statystykami związanymi z meczem.
Krykiet jest grą bardzo bogatą w statystyki, gromadzone są one odnośnie do praktycznie każdego aspektu meczu. Najczęściej są to statystyki liczbowe lub wykresy graficzne, pokazujące na przykład, gdzie dany batsman odbił piłkę, zdobywając punkty.
Pole gry ma kształt okrągły lub zbliżony do owalnego i jest pokryte trawą. Nie ma ustalonych wymiarów, ale jego średnica to zazwyczaj 137-150 metrów. Na ogół otoczone jest liną graniczną (ang. boundary – "granica") wzdłuż obrzeża. Centralna część pola – prostokąt o wymiarach 3,05 × 20,12 m z gliny pokrytej krótką trawą – nosi nazwę pitch.
To właśnie w pitchu rozgrywa się większość akcji. Na każdym jego końcu wbite są w ziemię wickety, czyli bramki. Jeden koniec pitchu wyznaczany jest jako koniec odbicia (to tam pozycję zajmuje batsman), a drugi jako koniec rzutu (stamtąd rzuca bowler). Linia łącząca wickety dzieli pole na dwie części; zależnie od tego, jak batsman trzyma kij, prawa (dla odbijającego praworęcznego) lub lewa (dla leworęcznego) nosi nazwę off side, a przeciwna on side lub leg side ("strona nogi").
Linie narysowane lub namalowane na pitchu noszą nazwę crease (ang. "fałda", "zagięcie").
Krykiet testowy to forma krykieta zapoczątkowana w roku 1877 podczas tournée reprezentacji Anglii do Australii. Pierwszy mecz testowy rozpoczął się 15 marca 1877, bez ograniczeń czasowych i z czterema piłkami na over. Zakończył się 19 marca zwycięstwem Australijczyków z przewagą 45 runów.
Od tamtej chwili rozegrano już ponad 1800 testów, a liczba państw dopuszczonych do tych meczów w 2000 roku wzrosła do dziesięciu. Mecz testowy składa się z dwóch inningsów dla każdej drużyny i trwa zazwyczaj pięć dni. Jeżeli nie uda się go dokończyć w wyznaczonym czasie, ogłaszany jest remis.
Mecze testowe Anglii z Australią noszą nazwę The Ashes – od satyrycznego nekrologu opublikowanego w 1882 w The Sporting Times po porażce Anglików na stadionie The Oval, w którym stwierdzono, że angielski krykiet zmarł, a jego ciało zostanie skremowane, zaś prochy zabrane do Australii. Następne tournée, na przełomie 1882 i 1883, nazwano w angielskich medich "wyprawą po odzyskanie popiołów" (ang. ashes). Po zwycięstwie Anglików grupa kobiet z Melbourne wręczyła ich kapitanowi, Ivo Blighowi, małą urnę, mającą zawierać popiół po spaleniu baila użytego podczas drugiego w historii meczu testowego między Anglią a Australią. Wbrew obiegowej opinii, nie jest to jednak ta sama urna, która obecnie stanowi trofeum w The Ashes. Oryginał stanowi własność prywatną i jest bardzo rzadko prezentowany publicznie, nagrodą zaś jest szklana, powiększona kopia.
Inne trofea w meczach testowych to trofeum Basila D'Oliveiry (mecze Anglii z RPA grane w Południowej Afryce), trofeum Wisdena (Anglia – Indie Zachodnie), trofeum Franka Worrella (Australia – Indie Zachodnie), trofeum Transtasmańskie (ang. Trans-Tasman; Australia – Nowa Zelandia), trofeum Bordera-Gavaskara (Australia – Indie) i trofeum sir Viviana Richardsa (RPA – Indie Zachodnie).
Innymi cechami charakterystycznymi meczów testowych - poza tym, że trwają kilka dni - są wyłącznie białe stroje noszone przez zawodników obu drużyn oraz piłka, która jest zawsze czerwonego koloru.
Mecze jednodniowe (ang. One-day International lub w skrócie ODI) to powstała pod koniec XX wieku forma międzynarodowych meczów krykietowych z liczbą overów ograniczoną do 50 na drużynę. W tym formacie rozgrywane są mistrzostwa świata w krykiecie. ODI nazywane są też "Limited Overs Internationals" (LOI) (ang.: "z ograniczoną liczbą overów"), gdyż są to mecze międzynarodowe z ograniczoną liczbą overów, a jeśli pogoda przeszkodzi, nie zawsze udaje się je rozegrać w jeden dzień.
Pierwszy mecz jednodniowy rozegrano 5 stycznia 1971 na Melbourne Cricket Ground; wystąpiły w nim reprezentacje Australii i Anglii. Stało się to w sposób dość przypadkowy – kiedy przez trzy dni deszcz uniemożliwiał rozegranie standardowego meczu testowego między tymi drużynami, sędziowie postanowili, iż zostanie rozegrany mecz jednodniowy, z czterdziestoma ośmiopiłkowymi overami dla każdej drużyny. Australia wygrała mecz pięcioma wicketami.
Pod koniec lat 70. Kerry Packer powołał konkurencyjną wobec tradycyjnych rozgrywek World Series Cricket, w której wprowadzono wiele zasad stosowanych dziś w ODI, między innymi kolorowe uniformy, rozgrywanie meczów w nocy przy świetle reflektorów i z użyciem białej piłki, a także – na potrzeby transmisji telewizyjnych – liczne kamery i mikrofony. Pierwszy mecz z użyciem kolorowych strojów rozegrano 17 stycznia 1979.
Jest to najnowsza odmiana krykieta. Pierwszy mecz w formule Twenty20 – wprowadzonej, aby spopularyzować krykiet pierwszoklasowy i przyciągnąć na trybuny więcej kibiców – rozegrano w Anglii w 2003 roku. W Twenty20 każda drużyna rozgrywa dwadzieścia overów, a zasady opracowane są tak, by uczynić grę żywszą, a tym samym bardziej atrakcyjną dla ludzi, którym nie chce się śledzić na stadionie wolniejszych standardowych meczów jednodniowych czy kilkudniowych test matches; mecz Twenty20 trwa około trzech godzin.
Pierwszy międzynarodowy mecz mężczyzn według reguł Twenty20 rozegrano w 2005 (zmierzyły się Australia z Nową Zelandią), kobiety po raz pierwszy zagrały w tej formule już w 2004 (Anglia – Nowa Zelandia). ICC ogłosiła, iż w latach 2007-2015 będą co dwa lata rozgrywane Mistrzostwa Świata w Krykiecie Twenty20.
Zasadniczo mecz pierwszej klasy to mecz na wysokim szczeblu, mecz międzynarodowy lub wewnątrzpaństwowy, trwający przynajmniej trzy dni i toczony na naturalnej murawie, w którym każda drużyna gra dwa inningsy. W związku z tym jednodniowe mecze międzynarodowe, w którym każda z drużyn gra tylko jednego inningsa, nie są zaliczane do testowych. Przez "wysoki szczebel" rozumie się udział w meczu dwóch drużyn mających status pierwszoklasowy (ang. first-class status). Są to więc mecze testowe oraz mecze drużyn niebędących reprezentacjami narodowymi (względnie jednej takiej drużyny przeciwko reprezentacji), a mających first class status w kraju, z którego się wywodzą, o ile ten jest "pełnym członkiem" ICC. Dlatego też o ile mecze drużyn z Kenii – wiodącego "państwa współpracującego" ICC – z drużynami państw mających first-class status są zazwyczaj uznawane za pierwszoklasowe, o tyle mecze wewnątrzpaństwowe w tym kraju już nie.
Historycy tej dyscypliny początek pierwszoklasowego krykieta datują na 1660, 1772, 1801, 1815 lub 1864.
Modyfikacje zasad krykieta zdarzają się na wszystkich szczeblach. Na szczeblu międzynarodowym zmiany, takie jak wprowadzenie odmiany Twenty20, mają na celu uczynienie gry bardziej atrakcyjną dla widza, ale w meczach rozgrywanych przez niezawodowców celem takich zmian jest często jedynie umożliwienie rozegrania meczu przy ograniczonych środkach techniczno-finansowych – często więc można zobaczyć ludzi grających na przykład przy użyciu piłek tenisowych, albo mecze, w których de facto gra jedna drużyna, która w sposób rotacyjny wyłania ze swojego składu dwójkę odbijających.
W wersji zwanej kwik cricket bowler nie musi czekać, aż batsman będzie gotów do odbicia, co znacząco zwiększa tempo gry. Dzięki temu jest to odmiana popularna w brytyjskich szkołach, gdzie uprawia się ją na lekcjach wychowania fizycznego. Występuje tam także zasada "Tipsy Run" czyli "Podchmielony bieg", która stanowi, iż odbijający musi wykonać bieg zawsze, gdy piłka dotknie kija, nawet jeśli sam batsman nie zamierzał jej dotknąć.
Często spotyka się mecze krykieta halowego – pole gry jest tam otoczone siatką, a drużyny liczą tylko ośmiu graczy. W Samoa z kolei istnieje odmiana krykieta zwana kilikiti, w którą gra się przy użyciu kija o kształcie zbliżonym do kija hokejowego.
     Państwa o najwyższej randze – testowej     Państwa współpracujące     Państwa stowarzyszoneZorganizowana międzynarodowa struktura krykieta jest tworem stosunkowo nowym, który wyewoluował samoistnie z tradycyjnych metod organizowania meczów międzynarodowych.
Założona w 1909 roku International Cricket Council (ICC; początkowo Imperial Cricket Conference, następnie International Cricket Conference, pod dzisiejszą nazwą od 1989) jest obecnie międzynarodowym organem zarządzającym krykietem. W jej skład wchodzą reprezentanci każdego z dziesięciu państw mających drużyny narodowe o test statusie oraz obieralny zespół przedstawicieli państw nie grających w meczach testowych.
Kraje, w których gra się w krykieta, podzielone są w ramach ICC na trzy grupy, zależnie od infrastruktury. Najwyższy poziom to państwa grające w meczach testowych, które automatycznie kwalifikują się do mistrzostw świata. Drugi poziom to państwa współpracujące (Associate Members), a trzeci, najniższy – państwa stowarzyszone (Affiliate Members).
Każdy kraj ma własną radę zarządzającą meczami rozgrywanymi na terenie tego państwa. To od niej zależy też skład reprezentacji narodowej i ustalanie terminów meczów.
Ze względu na rozgrywanie od 2005 roku meczów międzynarodowych w formule Twenty20 spodziewać się można kolejnych zmian w sposobie funkcjonowania ICC.
W Polsce istnieje siedem aktywnych drużyn krykietowych: Warsaw Hussars Cricket Club, Warsaw Cricket Club, Warsaw United Cricket Club, Warsaw Kings Cricket Club, Lodz Cricket Club, Krakow Cricket Club oraz Lublin Cricket Club.
W nieoficjalnych rozgrywkach ligowych w sezonie 2014 wzięło udział 5 drużyn: Warsaw Cricket Club (zespół A i B), Warsaw Kings XI, Warsaw United Cricket Team oraz Łódź Cricket Club. Mistrzem został Warsaw Cricket Club, który zdobywał trofeum także w latach 2012 i 2013.
Podejmowane są jednocześnie starania o przyjęcie Polski w poczet członków Międzynarodowej Rady Krykieta (International Cricket Council).
Starożytna Grecja – cywilizacja, która w starożytności rozwijała się w południowej części Półwyspu Bałkańskiego, na wyspach mórz Egejskiego i Jońskiego, wybrzeżach Azji Mniejszej, a później także w innych rejonach Morza Śródziemnego. Starożytna Grecja uważana jest za kolebkę cywilizacji zachodniej. Grecka kultura, sztuka, mitologia, filozofia i nauka zostały za pośrednictwem Rzymian przekazane Europie i wywierały na jej mieszkańców ogromny wpływ w różnych okresach dziejów.
W epoce brązu powstały na tym obszarze wysoko rozwinięte kultury: minojska na wyspie Krecie, a w Grecji właściwej[1] mykeńska, która później zdominowała Kretę. Wraz z końcem epoki brązu nastąpił gwałtowny upadek cywilizacji mykeńskiej i nastały tzw. wieki ciemne, z których wyłoniła się (ok. VIII w. p.n.e.) kultura Grecji archaicznej z ludnością żyjącą w miastach-państwach zwanych polis, z których najważniejsze były Ateny, Teby, Sparta i Korynt. Za okres szczytowego rozwoju cywilizacji greckiej uznaje się okres klasyczny[2] (V-IV w. p.n.e.), kiedy to Grecy odepchnęli zagrożenie perskie, a najpotężniejsze polis toczyły wojny o hegemonię. W drugiej połowie IV w. p.n.e. dominację nad Grekami uzyskało zhellenizowane państwo macedońskie, a jego władca – Aleksander Wielki – podbijając imperium perskie rozprzestrzenił kulturę grecką na ogromne obszary Azji i zapoczątkował okres hellenistyczny. W wiekach II i I p.n.e. państwa greckie dostały się pod panowanie imperium rzymskiego.
Obszar starożytnej Grecji, czyli tereny zamieszkiwane przez Greków w tym okresie, to przede wszystkim południowa część Półwyspu Bałkańskiego, wyspy Morza Egejskiego i Jońskiego oraz zachodnie wybrzeża Azji Mniejszej. W wyniku wielkiej kolonizacji i później podbojów Aleksandra Wielkiego Grecy założyli nowe polis na zachodzie i wschodzie, ale nie zalicza się ich do geograficznego obszaru określanego mianem starożytnej Grecji[potrzebny przypis]. Większość terenów starożytnej Grecji znajdowała się w strefie klimatu śródziemnomorskiego, którego wpływy na lądzie europejskim ograniczały się do Peloponezu, Grecji Środkowej i Tesalii, w Azji Mniejszej do Jonii[3]. W obszarze jego oddziaływania znajdowały się też wyspy – Kreta, Cyklady, Sporady (dawny Dodekanez). Tereny te są poprzecinane licznymi łańcuchami górskimi, które wyznaczały granice między równinami, dzieląc w ten sposób kraj na krainy historyczne. W centrum Peloponezu znajdowała się Arkadia – wysoki płaskowyż, z którą graniczyły przedzielone górami, sięgające do morza równiny: Argolida, Lakonia, Messenia, Elida i Achaja. Głównymi krainami Grecji Środkowej były: (wymieniając od zachodu): Etolia, Lokryda, Fokida, Beocja, Attyka oraz oddzielona wąskim kanałem duża wyspa Eubea. Tesalia była rozległą równiną otoczoną górami, Jonia obejmowała nadbrzeżne równiny Lydii i Karii[3].
Równiny były zwykle niewielkie (z wyjątkiem Tesalii i Jonii), gleby łatwo erodowały, ale pozwalały wyżywić znaczną liczbę mieszkańców, zwłaszcza dzięki wykorzystywaniu pod uprawę zboczy gór oraz wprowadzeniu odpowiednich do warunków gatunków upraw: drzewa oliwnego, winorośli, a także pszenicy – stanowiących razem tzw. triadę śródziemnomorską, podstawę żywieniową miejscowej ludności[3]. Bogactwa naturalne Grecji były ograniczone. Srebro, miedź i ołów znajdowano w Attyce (góra Laureion) i Cykladach, zasoby drewna były niewielkie[3]. Ważnym elementem życia Greków było morze, stanowiło szlak komunikacyjny na Bliski Wschód, ale też stanowiło zaporę dla armii tamtejszych państw; ryby (obok sera) stanowiły podstawowe źródło białka zwierzęcego[3]. Liczne wyspy ułatwiały nawigację, stanowiły też miejsce postoju podczas nocy, czyniąc z Morza Egejskiego niezwykle przyjazny do żeglugi akwen (z wyjątkiem pory zimowej, kiedy morze było wzburzone i często występowały sztormy)[3].
Z ok. 18 000 p.n.e. pochodzą najstarsze znalezione ślady obecności człowieka na terenie dzisiejszej Grecji – w jaskini Franchthi na Peloponezie[4]. Około VIII/VII tysiąclecia p.n.e. docierały na tereny Grecji owoce rewolucji neolitycznej przebiegającej na Bliskim Wschodzie – rolnictwo, hodowla zwierząt, a później ceramika[4][5]. Droga ekspansji nowych odkryć i będących ich nosicielami migrujących grup ludności biegła z Azji Mniejszej przez będący wtedy lądowym pomostem Bosfor[5][6]. W Grecji, na terenach Tesalii i Macedonii odkryto ślady pierwszej kultury neolitycznej w Europie – Sesklo[potrzebny przypis]. Na wyspach ludy posługujące się neolitycznymi technologiami pojawiły się w V tysiącleciu p.n.e., przybywając zapewne z Azji Mniejszej, choć w IV tysiącleciu p.n.e. oprócz migracji z Anatolii pojawili się także przybysze z Attyki i Eubei[7]. Na przełomie neolitu i epoki brązu nastąpił wyraźny wzrost zaludnienia[7].
Na obszarze starożytnej Grecji rozwinęła się w tym okresie kultura egejska, którą współtworzyły trzy obszary kulturowe: Kreta (kultura minojska), Cyklady (kultura cykladzka), oraz Grecja właściwa (kultura helladzka i mykeńska).
W tym okresie trzy regiony świata egejskiego rozwijały się w miarę równolegle[9]. Z rozwijającej się szybciej i wszechstronniej Azji Mniejszej przybyły (zapewne z niewielkimi migrującymi grupami ludności) i upowszechniły się umiejętności obróbki metali[10]. Ich użycie często pozostawało ograniczone do produkcji broni, ozdób i rzadziej narzędzi, ze względu na trudności w zdobyciu odpowiednich surowców, jednak same technologie produkcji (wytwarzano brąz arsenowy – stop miedzi i arsenu, później także brąz klasyczny – stop miedzi i cyny) i obróbki rozwijały się stosunkowo szybko[10]. Na Krecie z okresu przełomu neolitu i epoki brązu odnajduje się ślady rosnących osad (Knossos, Fajstos), dowody wpływów z Azji Mniejszej i Cyklad w postaci nowych technik zdobywania żywności, nowych form ceramiki i budynków, a także nowych upraw, charakterystycznych dla świata śródziemnomorskiego: oliwki, winorośli i pszenicy[7]. Wraz z upływem czasu rozpowszechniało się na wyspie wykorzystanie metali, nasilały się kontakty z kontynentem i innymi wyspami, wyodrębniły się różniące się między sobą regiony[7]. Kultura innych wysp Morza Egejskiego, z wyjątkiem może Cyklad, związana była często z pobliskimi kontynentami[11]. Najsilniejsze kontakty region miał z Azją Mniejszą, także z Kretą, słabsze z Europą. Ożywiły się one zwłaszcza w połowie tysiąclecia, kiedy nastąpił też wzrost zaludnienia, a wyspy zaczęły pełnić rolę pomostu między różnymi regionami świata egejskiego; stanowiły też źródło metali (miedzi, srebra, ołowiu), także dla Krety[11]. W Grecji właściwej początki epoki brązu pozostawiły niewiele śladów archeologicznych. Z późniejszego okresu – połowy III tysiąclecia p.n.e. – pochodzą ślady małych, ale licznych osad (w Argolidzie, Beocji, Attyce i na Eubei), także kontaktów z wyspami i Kretą[12]. W drugiej połowie III tysiąclecia p.n.e. na Cykladach i w Grecji pojawili się przybysze z Azji Mniejszej, reprezentowani w znaleziskach przez podobne do siebie kultury archeologiczne: Kastri (wyspy) i Lefkandi I (kontynent, a konkretnie Eubea i Beocja), którzy osiedlili się obok rodzimych mieszkańców, co doprowadziło zapewne do jakichś napięć[11][12]. Koniec wczesnej epoki brązu to okres regresu w całym regionie Morza Egejskiego, podobnie jak reszcie świata śródziemnomorskiego[13]. Zniszczona została część osad, społeczności zubożały, osłabły kontakty ze światem zewnętrznym[12][13][14]. Na ogół przemiany te wiąże się z przybyciem grupy ludów indoeuropejskich, które wymieszały się z miejscowymi, co zapoczątkowało formowanie się nowej kultury[13]. Na Krecie, gdzie migracje Indoeuropejczyków nie dotarły[potrzebny przypis], także nastąpiło zmniejszenie liczby osad w niektórych rejonach (choć być może związane z koncentracją ludności w większych ośrodkach), a także osłabienie kontaktów zewnętrznych[15].
Na przełomie III i II tysiąclecia p.n.e. rozwój cywilizacyjny Krety znacznie przyspieszył, powstała cywilizacja minojska (od imienia mitycznego władcy Knossos – Minosa). Wprawdzie wyspę także dotknął kryzys końca wczesnej epoki brązu, ale zdołała ona obronić swój dorobek, podczas gdy regres w Grecji kontynentalnej i na Archipelagu został pogłębiony (lub spowodowany) przez migracje z Azji (kultury Lefkandi-I i Kastri)[14]. Ponownie nawiązano kontakty zewnętrzne, po raz pierwszy na dużą skalę z Lewantem i Egiptem, co zaowocowało poznaniem nowych technologii i wymianą kulturalną[14]. Osady przekształciły się w miasta o zaawansowanej architekturze i infrastrukturze, których centrum były wielkie pałace – siedziba władcy, ośrodek gospodarczy i religijny, którego mieszkańcy posługiwali się pismem (hieroglificznym i linearnym A)[14]. Szczyt rozwoju cywilizacji minojskiej to tzw. okres młodych pałaców (ok. 1675-1450 p.n.e.) – na wyspie wzrosła liczba miast, pałaców i willi, dominującym ośrodkiem było Knossos, brak fortyfikacji wskazuje na brak konfliktów na wyspie, rozwinęła się wyrafinowana sztuka (będąca pod wielkim wpływem Egiptu i innych cywilizacji Bliskiego Wschodu) i rzemiosło[16][17]. Ekspansja Krety na inne wyspy sprawiła, że niektóre z nich stały się częścią kultury minojskiej w drugiej połowie średniej epoki brązu, wpływy minojskie promieniowały też na zacofaną w tym okresie Grecję kontynentalną[14][18]. XVI wiek p.n.e. to początek upadku Krety – najpierw trzęsienie ziemi, potem erupcja wulkanu na Thirze[19] zachwiały gospodarką i stabilnością społeczną wyspy[20]. Osłabiona cywilizacja padła ofiarą inwazji (ok. 1450 roku p.n.e.) z Grecji kontynentalnej, dokonanej przez przedstawicieli kultury mykeńskiej, którzy zniszczyli wszystkie pałace poza Knossos, gdzie zasiedli jako władcy wyspy[21].
Kwestia przybycia na tereny obecnej Grecji ludów, które możemy uznać za Greków jest obiektem sporów i dyskusji[22][23]. Za pewne uznaje się, że u schyłku III tysiąclecia p.n.e. lub w ciągu II tysiąclecia p.n.e. na południe Bałkanów napływały ludy mówiące językami z grupy indoeuropejskiej (do niej zalicza się język grecki), jednak nie udało się wypracować porozumienia na temat charakteru tego wydarzenia, jego czasu i ilości fal migracyjnych[23]. Tradycyjnie historycy zakładali, że mapa dialektów języka greckiego z czasów klasycznych świadczy o wystąpieniu w przeszłości trzech fal migracji grup ludności, z których każda posługiwała się danym dialektem w ukształtowanej formie[23]. Badania archeologiczne wskazywały, że w latach ok. 2000-1900, 1600 i 1200 p.n.e. miały w Grecji miejsce okresy przełomowe (przy czym środkowy był mniej wyraźny niż pozostałe), które powiązano z danymi o dialektach i ukuto teorię o wystąpieniu wtedy trzech inwazji ludów greckich: Jonów, Achajów i Dorów[23]. Nowsze badania podały w wątpliwość ten pogląd – uznano za błędną koncepcję przybycia Greków już ukształtowanych, z gotowym językiem, zakładając, że formowanie się kultury i mowy greckiej miało miejsce już na terenie Grecji i przebiegało na skutek mieszania się elementów religii, języka i kultury różnych grup.[23] Odrzucono więc jako zbędną teorię trzech migracji, z kolei pierwszy okres przełomowy zaczęto traktować nie jako gwałtowną, krótką inwazję, ale długotrwały proces (trwający wiek lub dwa), w czasie którego w różnych fazach napływało wiele grup ludności[23]. Niektórzy badacze wskazują, że pojawienie się w Grecji języka, z którego wyewoluowała Greka, należy wiązać z pojawieniem się w Mykenach pierwszych grobów szybowych (XVII w. p.n.e.), co mogło być spowodowane przybyciem ludu, który narzucił się miejscowym jako elita władzy[22].
Osobną kwestią jest pytanie skąd przodkowie Greków przybyli do Grecji. Najprawdopodobniej dotarli tam przez Bałkany i północną Grecję, ale brak jest dowodów archeologicznych potwierdzających tę tezę[23]. Niezbyt pomocne są też badania lingwistyczne – ze znanych języków Grekę najbardziej przypominały staromacedoński i frygijski (uznawane niekiedy za dialekty wspólnego protojęzyka), trochę także staroarmeński[22]. Znane miejsce, gdzie najwcześniej posługiwano się tymi językami (greckim, macedońskim, frygijskim, w przypadku staroarmeńskiego pewność jest mniejsza) to południowe Bałkany, więc być może tutaj należałoby szukać siedzib przodków Greków; brak jednak na razie wystarczających dowodów archeologicznych[22].
Regres jaki przyniósł koniec III tysiąclecia trwał w Grecji kontynentalnej długo – poza wyjątkowym przypadkiem prosperującej Eginy zaludnienie było słabe, a poziom kultury materialnej niski[24]. Sytuacja zaczęła zmieniać się w XVII wieku p.n.e. – w Argolidzie, Beocji, Attyce, Messenii i Lakonii nastąpił przyrost ludności, pojawiły się tam też bogato wyposażone groby książęce (tzw. groby szybowe), przypominające te z Eginy[25]. Cywilizację jaka zaczęła się wtedy formować nazwano mykeńską od jej najważniejszego ośrodka – Myken w Argolidzie, jej przedstawicieli identyfikuje się z opisanymi w dziełach Homera Achajami[22]. Wcześni Mykeńczycy utrzymywali szerokie kontakty handlowe – ich wyroby znajdowano m.in. w Italii czy Macedonii[26], lecz źródeł bogactwa widocznego w zawartości grobów szybowych dopatruje się raczej w wyprawach rabunkowych lub działalności najemniczej[27]. Około 1450 r. p.n.e. Mykeńczycy dokonali inwazji Krety, zniszczyli wszystkie pałace z wyjątkiem tego w Knossos, którym władali do około 1365 r. p.n.e., kiedy i on uległ zniszczeniu[28]. Panowanie mykeńskie w Knossos było okresem fuzji tradycji minojskiej i kultury najeźdźców – wykształciła się sztuka, którą potem odnajdujemy w Grecji, powstało też pismo linearne B, czyli wersja kreteńskiego pisma linearnego A dostosowana do Greki jaką posługiwali się Achajowie[28]. W XIV wieku p.n.e. w takich ośrodkach jak Mykeny, Tyryns, Pylos powstały pałace, budowle będące podobnie jak ich minojskie odpowiedniki, siedzibą władcy, centrum administracyjnym, miejscem magazynowania i redystrybucji dóbr.[29] Pałace były centrami niezależnych państw, rządzonych przez królów tytułowanych wanax, mających na swe rozkazy arystokrację walczącą w czasie wojny na rydwanach[30]. Mykeńczycy czcili wiele bóstw znanych z późniejszych dziejów Grecji, m.in. Zeusa, Posejdona, Aresa czy Hermesa[31]. Achajowie zajęli miejsce Kreteńczyków na wyspach Morza Egejskiego, ślady ich osadnictwa lub przyjmowania kultury mykeńskiej odnajdujemy w Azji Mniejszej[28], handlowali z zachodnim basenem Morza Śródziemnego (Italia, Sycylia, Sardynia), a na wschodzie, zdominowanym przez wielkie cywilizacje zajmowali się kupiectwem lub piractwem[32]. Źródła hetyckie, mówią o potężnym państwie Ahhijawa, które jest zwykle identyfikowane z Achajami, co może świadczyć o tym, że Mykeńczycy nie tworzyli tylko niezależnych, niewielkich państw, ale także jakąś formę imperium rządzonego przez pojedynczego władcę[33].
Pod koniec XIII wieku p.n.e. rozpoczął się upadek cywilizacji mykeńskiej – pałace zostały zniszczone, niektóre wiele razy. W XII wieku p.n.e. miały miejsce kolejne fazy zniszczeń, doszło do ruchów ludnościowych, nastąpiło wyludnienie niektórych obszarów, napłynęła ludność mówiąca doryckim dialektem języka greckiego, a jednocześnie grupy Mykeńczyków pojawiły się wśród Ludów Morza atakujących ważne ośrodki wschodniego basenu Morza Śródziemnego[34]. Przyczyny tych zjawisk są przedmiotem dyskusji, mówi się o inwazji z zewnątrz, walkach wewnętrznych, zapaści społecznej, niemniej skutkiem wydarzeń z XIII i XII wieku był koniec cywilizacji mykeńskiej, regres i poważne przemiany – w Grecji nastały tzw. wieki ciemne[35].
Po upadku cywilizacji mykeńskiej w dziejach Grecji występuje okres około trzech stuleci, który historycy, ze względu na brak źródeł pisanych oraz nieliczne znaleziska archeologiczne, nazywają wiekami ciemnymi[36]. Około dwa stulecia po upadku kultury mykeńskiej trwały migracje i najazdy – ludność mykeńska przenosiła się na wschód (Cypr, wybrzeża Azji Mniejszej), niektóre obszary niemal się wyludniły (Lakonia, Messenia), na Peloponez prawdopodobnie wkroczyli Dorowie, którzy potem zajęli Kretę, część wysp Morza Egejskiego i fragment wybrzeża małoazjatyckiego, inne ludy pojawiły się w Grecji Środkowej i Tesalii[37]. Dialekt achajski, którym posługiwano się w czasach mykeńskich, przetrwał tylko w niedostępnej Arkadii, na Cyprze i Pamfilii, a wywodzące się z niego joński i attycki w Attyce, Eubei i Jonii oraz na niektórych wyspach, w wielu miejscach (m.in. Peoloponez, Kreta, Rodos) pojawił się dorycki i pokrewne mu dialekty północno-zachodnie[38]. Mieszkańcy Grecji zachowali język, ale tylko w mowie, umiejętność jego zapisu zanikła, podobnie jak przepadły pałace i niemal cała kultura materialna epoki mykeńskiej, zerwanie z przeszłością wydaje się niemal całkowite[39]. Dziedzictwo mykeńskie przetrwało jednak w sferze duchowej – w mitach o czasach herosów, w przekazywanej ustnie poezji epickiej (w której odnaleźć można dalekie echa prawdziwych wydarzeń), w bogatym w pojęcia języku[40].
W okresie wieków ciemnych sieć osadnicza była niezwykle rzadka (obszary gęstszego zaludnienia w tym okresie to: Attyka, Argolida, Beocja, Kreta), ludność mieszkała w niewielkich domach z suszonej cegły, wyposażenie grobów było ubogie (ale pojawiają się w nich produkty z żelaza), kontakty zewnętrzne zostały mocno ograniczone[41]. Nowe odkrycia archeologiczne wskazują jednak, że ten obraz nie był prawdziwy dla całej Grecji – znalezienie w Lefkandi na Eubei grobów szybowych z bogatym wyposażeniem (pochodzącym także z Bliskiego Wschodu), zlokalizowanych wewnątrz okazałego budynku, sugeruje, że już w X wieku p.n.e. Eubea była miejscem bogatszym i znacznie mniej odizolowanym niż reszta świata greckiego[42]. Od ok. 900 roku p.n.e. produkty z Bliskiego Wschodu pojawiają się na innych obszarach (Attyka, Argolida, niektóre wyspy), a ceramika grecka w Fenicji. Kontakty ze wschodem ulegały dalszej intensyfikacji, rósł wpływ Fenicjan na kulturę grecką, Grecy coraz częściej podróżowali na Bliski Wschód (ok. 800 roku p.n.e. w Al-Mina w Syrii istniała prawdopodobnie grecka faktoria), rozpoczęło się wychodzenie z okresu regresu[42]. W IX wieku p.n.e. zaczyna kształtować się polis[43] – podstawa greckiej organizacji życia politycznego i społecznego w następnych okresach ich dziejów, w IX-VIII wieku p.n.e. Grecy do zapisu własnej mowy zaczynają wykorzystywać zapożyczony od Fenicjan i nieco zmieniony alfabet[44].
Kultura grecka jaka wyłoniła się (niemal dosłownie, gdyż w porównaniu z wiekami ciemnymi znacząco rośnie ilość znalezisk archeologicznych, pojawiają się źródła pisane[45]) z wieków ciemnych, posiadała większość cech, które we współczesnym świecie przypisuje się pojęciu starożytna Grecja. Grecy tego okresu posługiwali się nowym alfabetem, żyli we w miarę egalitarnych społecznościach zorganizowanych w, zwykle niewielkie, poleis – miasta-państwa, walczyli w szyku falangi w uzbrojeniu ciężkiego piechura – hoplity, oddawali cześć herosom, zarówno lokalnym, jak i znanym w całej Grecji, korzystali z wyroczni (zwłaszcza w Dodonie, Delfach, Olimpii), czcili bóstwa takie jak Zeus, Apollo[46], znali dzieła Homera – Iliadę i Odyseję. Wszyscy obywatele polis (tzw. demos), czyli dorośli mężczyźni, miejscowego pochodzenia, dysponujący majątkiem pozwalającym na zakup uzbrojenia hoplity mieli wpływ na decyzje dotyczące całej społeczności, brali udział w zgromadzeniach, gdzie dyskutowano ważne sprawy czy obierano przywódców. W społeczności istniała wprawdzie elita, mająca większy wpływ na podejmowanie decyzji, ale dostęp do niej nie był zamknięty – przynależność do arystokracji uzależniona była praktycznie od posiadanego majątku i przedsiębiorczy, szeregowi obywatele mogli do niej awansować (podobnie jak zubożali arystokraci mogli stracić swój status)[47][48]. Niżej na drabinie społecznej, poza grupą obywateli, znajdowali się wolni ludzie nie posiadający ziemi, a najniżej niewolnicy lub niewolni chłopi[49]
Od początku epoki archaicznej trwał proces, który znacząco rozszerzył granice świata greckiego. Polis zakładały w różnych miejscach basenu Morza Śródziemnego kolonie (gr. apoikiai), które często przekształcały się w niezależne miasta, poczuwające się jednak do pewnej wspólnoty z macierzystą polis tzw. metropolią. Początkowo w działalności kolonizacyjnej przodowały miasta z Eubei: Chalkis i Eretria, zakładając nowe ośrodki w Italii, potem dołączyły się Korynt, Megara, Milet i inne. Oprócz Italii kolonizowano Sycylię, wyspy Morza Jońskiego (Kerkyrę), północne wybrzeża Morza Egejskiego (zwłaszcza Półwysep Chalkidiki), później także południowe wybrzeża dzisiejszej Francji (u ujścia Rodanu powstała Massalia, kolonia Fokaji), sięgając nawet Hiszpanii. Liczne kolonie zakładano na wybrzeżach Morza Czarnego (tu aktywnie był zwłaszcza Milet), w rejonie Propontydy, także w części północnej Afryki (Kyrene); w Egipcie faraon przydzielił Grekom Naukratis. Przyczynami osiedlania się Greków na nowych obszarach były przede wszystkim chęć uzyskania dostępu do cennych surowców lub produktów, oraz pragnienie części obywateli poprawienia swojego statusu przez pozyskanie nowej ziemi uprawnej[50]. Wysoki przyrost naturalny w Grecji począwszy od VIII wieku p.n.e. pozwolił na stały wzrost liczby ludności, mimo odpływu jej części do kolonii[50]. Kolonizacja znacząco wpłynęła na rozwój handlu, zarówno między koloniami i greckimi polis, jak i między Grekami i tubylcami z kolonizowanych ziem. Lokalne elity podatne na wpływy greckiej kultury stawały się też odbiorcami greckich towarów, za które często płaciły niewolnikami[51]. Kolonizacja straciła na impecie w połowie VI wieku p.n.e. (nowe ośrodki wciąż powstawały, ale było ich mniej i ich fundacje miały inny charakter), ale do tego czasu pozwoliła Grekom stać się wielkim narodem basenu Morza Śródziemnego silnie oddziałującym swoją kulturą na inne ludy[52].
W późnej epoce archaicznej wiele greckich poleis znalazło się pod władzą tyranów – arystokratów, którzy sięgali po pełnię władzy nad zbiorowością niezgodnie z przyjętymi regułami, najczęściej w wyniku zamachu (zwykle bezkrwawego)[53]. Pierwszym znanym tyranem był Fejdon z Argos, okres świetności Koryntu przypadał na czas panowania Kypselosa i jego syna Periandra, Ortagoras zapoczątkował trwałą tyranię w Sykionie, w VI w. p.n.e. Atenami władali Pizystratydzi. Tyrani zwykle posiadali poparcie ludu, które zyskiwali działaniami mającymi poparcie zbiorowości, bądź zapewnieniem polis spokoju i porządku (np. poprzez eliminację konkurentów do władzy), więc po przewrocie zazwyczaj nie rządzili z użyciem siły[54]. Arystokracja często dążyła do obalenia tyranii we własnym polis, jednak rezultatem takiego przewrotu, było czasami zajęcie miejsca tyrana przez innego arystokratę. Zdarzało się, że walki wewnętrzne w polis (tzw. stasis) przybierały na sile na tyle, że obywatele sami obierali sobie tyrana[55].
W VI w. p.n.e. coraz popularniejsza stawała się idea sprawiedliwego ustroju (gr. eunomia), zakładająca, że swobody obywatela polis powinno ograniczać tylko prawo, nie inna osoba (np. tyran)[56]. Idea prawa jako zespołu norm, który obowiązywał (i mógł być, w przeciwieństwie do obyczaju, zmieniany) w wyniku decyzji wspólnoty, była wynalazkiem greckim[56]. Opracowywanie praw poleis powierzały prawodawcom, którym zapewniano dużą swobodę zmian stosunków politycznych i społecznych, ci jednak ograniczali się zwykle do reform mających na celu niwelowanie napięć społecznych (np. wzmacniając instytucje obywatelskie mające równoważyć siłę arystokratów)[57]. Działalność prawodawców, a także ustawy zatwierdzane przez zgromadzenie obywateli prowadziły do zmian w ustroju poleis greckich – powstawała m.in. rada wybierana spośród obywateli, powiększał się demos (poprzez uwalnianie poddanych) i zmieniało się jego znaczenie[57][58]. Procesy te doprowadziły do wykształcenia się takich ustrojów jak demokracja (prawa polityczne mieli wszyscy obywatele) czy oligarchia (prawa przysługiwały części obywateli, decydował majątek)[58].
W okresie archaicznym na znaczeniu zyskała Sparta, polis położone w Lakonii, na południu Peloponezu. W wyniku dwóch wojen messeńskich (toczonych pod koniec VIII i w VII w. p.n.e.) Spartanie stali się panami żyznej Messenii, część jej mieszkańców stała się przypisanymi do ziemi helotami[59]. Wydarzenie to miało znaczący wpływ na ukształtowanie się specyficznego systemu społeczno-politycznego: każdy wolny obywatel Sparty otrzymał dziedziczną działkę ziemi wraz z uprawiającymi ją helotami, uwalniając się w ten sposób od potrzeby pracy[59]. Charakterystycznym elementem ustroju spartańskiego był rygorystyczny system wychowania, mający na celu podporządkowanie jednostki państwu i zacieranie różnic między obywatelami, którzy powinni być posłusznymi zwierzchnikom, doskonałymi żołnierzami (odegrało tu zapewne rolę stałe zagrożenie rebelią w brutalnie zniewolonej Messenii)[60]. Zabijający indywidualizm i niezależne myślenie system wychowania skutkował upadkiem kulturalnym Sparty od VI w. p.n.e.[61], ale zapewnił jej pozycję potęgi militarnej[62]. Stopniowo pod jej wpływem (czy to na skutek wojen czy sojuszy) znalazło się większość polis Peloponezu (poza Argos, które w okresie archaicznym było największym przeciwnikiem Sparty), które tworzyły tzw. Związek Peloponeski[63].
Innym polis epoki, które osiągnęło znaczącą pozycję były Ateny. Jej chora (czyli obszar podporządkowany miastu) obejmował całą Attykę. Z tego też powodu archaiczne Ateny nie prowadziły zaawansowanej akcji kolonizacyjnej, ani ekspansji zewnętrznej, a mimo to potrafiły zagospodarować znaczący przyrost ludności dzięki kolonizacji wewnętrznej[64][65]. Władzę w polis sprawowało zgromadzenie, rada zwana areopagiem oraz urzędnicy z archontem na czele[64]. Na początku VI w. p.n.e. w Atenach zapanował kryzys, którego skutkiem było zadłużenie biedniejszych warstw społeczeństwa, zastawianie przez nich ziemi, a nawet niewola ich oraz rodzin[65]. Problemy rozwiązały reformy archonta Solona, który przeprowadził kasatę długów i zakazał sprzedaży w niewolę zadłużonych, a także nadał wolność ateńskiej ludności poddanej, zmienił sądownictwo, poszerzył uprawnienia zgromadzenia oraz wprowadził cenzus majątkowy dla obywateli[65][66]. W połowie VI w. p.n.e. władzę w Atenach przejął tyran Pizystrat, a czas jego panowania był dla polis niezwykle pomyślny, jednak już za panowania jego synów nasiliły się walki wewnętrzne, w które wciągnięto także Spartę – jej interwencja obaliła tyranię w Atenach w 510/509 p.n.e.[67] W nowej sytuacji wpływy zyskał Klejstenes, którego reformy, mimo wrogich działań Sparty, doprowadziły do powstania pierwszej demokracji – poszerzono znacznie krąg obywateli, wzmocniono uprawnienia ich zgromadzenia, a obradami obywateli zarządzała tzw. Rada Pięciuset[68]. Arystokracja zachowała jednak silną pozycję, to przede wszystkim jej przedstawiciele obsadzali urzędy i stanowiska w radzie[69].
W VI wieku p.n.e. poleis małoazjatyckie zostały podbite przez ekspansywne Imperium perskie (wcześniej podlegały królom Lydii, a z kolei w VII w. p.n.e. musiały walczyć z najazdami Kimmerów), które następnie zajęło wyspy w pobliżu Azji Mniejszej i grecką Cyrenajkę, a później Trację[69]. W tym czasie w Grecji ekspansję prowadziły: Sparta na Peloponezie i Tessalia w Grecji Środkowej[69]. Sparta pokonała Argos w 494 p.n.e. i tylko dramatyczny przewrót społeczny ocalił miasto przed Związkiem Peloponeskim obejmującym już większość półwyspu[69]. Tessalia w wyniku zwycięskiej tzw. I wojny świętej (ok. 590 p.n.e.) umocniła swoją pozycję i uczyniła Delfy miastem autonomicznym znajdującym pod ochroną Amfiktionii Delfickiej[70]. Postępy Tessalii załamały się pod koniec VI w. p.n.e., m.in. w wyniku porażki z nowo powstałym Związkiem Beockim kierowanym przez silne Teby i skupiającym poleis Beocji[70].
W 499 p.n.e. miasta greckie w Azji Mniejszej zbuntowały się przeciw władzy perskiej. Powstańcy liczyli m.in. na pomoc poleis z Grecji właściwej, ale tylko Ateny i Eretria wysłały niewielkie wsparcie, szybko zresztą wycofane[71]. Po kilku latach walk powstanie upadło, Milet został zniszczony, część ludności została deportowana w głąb imperium (część wyemigrowała na zachód), ale sytuacja wkrótce wróciła do normy, zwłaszcza że Persowie wprowadzili w podległym im poleis demokrację[72]. Od 492 p.n.e. roku perski wódz Mardonios opanował ponownie Trację, a także Macedonię, opierając granice imperium o Tessalię. Dariusz, król perski, zażądał od greckich poleis uznania jego władzy. Wiele się zgodziło, jednak Ateny i Sparta odmówiły, dlatego władca podjął decyzję o działaniach zbrojnych. W 490 p.n.e. Perska ekspedycja wysłana morzem zajęła wyspy Morza Egejskiego, zdobyła Eretrię, ale po lądowaniu w Attyce została pokonana przez Ateńczyków w bitwie pod Maratonem i była zmuszona wycofać się do Azji. Kolejna inwazja perska wydawała się nieunikniona, jednak w międzyczasie wybuchł bunt w Egipcie, a w 486 roku zmarł Dariusz; zadanie dokończenia podboju Grecji przypadło jego następcy – Kserksesowi I. Tymczasem Ateny, zdając sobie sprawę z zagrożenia, na wniosek Temistoklesa zainicjowały program budowy wielkiej floty, a także nasiliły walkę polityczną z ludźmi podejrzewanymi o sprzyjanie Persom (czyli o tzw. medyzm)[73]. W 481 p.n.e. w Grecji zawiązał się tzw. Związek Hellenów, sojusz poleis które miały walczyć z Persją pod przewodnictwem Sparty[74]. W 480 p.n.e. ogromna armia perska przeszła przez Hellespont i wkroczyła do Grecji. Grecy próbowali zagrodzić jej drogę na przesmyku termopilskim, ale niewielkie siły opóźniły marsz wojsk Kserksesa jedynie o trzy dni. Persowie zajęli i spalili Ateny, jednak mieszkańcy miasta ewakuowali się na pobliska wyspę Salaminę, w pobliżu której flota grecka (w dużej części ateńska) pokonała perską w wielkiej bitwie stoczonej 23 września[75]. Kserkses z częścią armii wrócił do Azji (ulokował się w Sardes), w Grecji pozostawiając kontyngent pod dowództwem Mardoniosa. W 479 p.n.e. armia grecka (głównie spartańsko-ateńska) pokonała siły Mardoniosa w bitwie pod Platejami, a mniej więcej w tym samym czasie w Azji Mniejszej grecka ekspedycja rozbiła perską armię i zniszczyła flotę pod Mykale. Do Związku Hellenów przyłączyły się wyspy Morza Egejskiego, Grecy przejęli inicjatywę, ale wojna toczyła się jeszcze przez 30 lat, teraz już pod kierownictwem Aten[76].
Sparta wycofała się z wojny, a z inicjatywy Aten powstał Związek Morski – organizacja skupiająca poleis wyspiarskie, wybrzeża trackiego i małoazjatyckie, które pod przewodnictwem Aten miały kontynuować walkę z Persją[77][78]. Armia związku dowodzona przez ateńskiego polityka, Kimona, działała skutecznie, stopniowo wyrzucając Persów z Europy i części Azji Mniejszej, zdobywając kontrolę na morzu i odnosząc wielkie zwycięstwa (m.in. nad Eurymedonem i pod cypryjską Salaminą)[79][80][81]. W 449 p.n.e. doszło do porozumienia (być może podpisano formalny pokój tzw. Pokój Kalliasa), w którym Persowie uznali wolność miast greckich w Azji i ich członkostwo w ateńskim Związku[82][83].
Ateńczycy stopniowo przekształcali Związek Morski we własne imperium – zajmowali część zdobytych na Persach miejsc i zbrojnie występowali przeciwko nieposłusznym członkom[84][85]. Sparta nie reagowała na wzrost potęgi Aten zajęta własnymi problemami[86][87]. Kiedy w 464 p.n.e. Spartę dotknęło wielkie trzęsienie ziemi i bunt helotów, a miasto odrzuciło ateńską pomoc doszło do zerwania formalnego sojuszu między dwiema poleis[87][88]. W Atenach w tym czasie Efialtes (z pomocą swego późniejszego następcy i kontynuatora reform Peryklesa) przeprowadził reformy demokratyczne, osłabiając arystokrację i poszerzając uprawnienia Zgromadzenia[89][90].
W latach 460–446/445 p.n.e. trwała wojna między ateńskim Związkiem Morskim i sojusznikami Sparty (m.in. Koryntem i Tebami)[91][92][93]. Mimo początkowych sukcesów Aten zakończyła się ona uznaniem status quo i podziałem Grecji na strefy wpływów[94][95]. Po wojnie Ateny nadal umacniały swoją pozycję hegemona w związku, prowadziły politykę na Zachodzie (w Italii i Sycylii), były obecne na Morzu Czarnym i wybrzeżu trackim[96][97].
Kolejna wojna między Atenami i Spartą wybuchła na skutek konfliktu Koryntu (sojusznika Sparty) i Korkyry (sprzymierzeńca Aten)[98][99]. Armia związku peloponeskiego miała przewagę na lądzie, Ateny kontrolowały morze i wojna przez większość czasu była wyrównana[100][101][102]. Jednak zaraza jaka wybuchła w Atenach w 430 r. p.n.e. poważnie osłabiła miasto[103][104], a późniejsza katastrofalna wyprawa sycylijska z lat 415–413 p.n.e. kosztowała je utratę sporej części armii i floty[105][106]. Mimo tego dopiero znacząca pomoc finansowa ze strony Persji pozwoliła Sparcie zwyciężyć odbudowaną ateńską flotę i osiągnąć ostateczny sukces w 404 r. p.n.e.[107][108] Skutkiem wojny był rozpad imperium ateńskiego, hegemonia Sparty w Grecji i powrót poleis małoazjatyckich pod władzę perską. Wielki król Persji i jego satrapowie w Azji Mniejszej stali się w następnych latach ważnym, jeśli nie decydującym graczem w polityce greckiej[109][110].
Sparta nie miała wystarczających środków i pomysłu jak zarządzać Grecją, jej polityka często budziła opory[111][112]. Kiedy zaangażowała się w wojnę z Persją w Azji Mniejszej w 396 r. p.n.e., w Grecji zawiązała się (pod perskim patronatem) koalicja dawnych wrogów: Aten, Teb, Koryntu i Argos[111][113]. Wojna, jaka wybuchła, pozwoliła Atenom z perską pomocą odbudować swoją flotę, jednak zakończyła się zwycięstwem Sparty (Persja pod koniec konfliktu stanęła znowu po jej stronie) i miasto powróciło do polityki podporządkowywania sobie Grecji[114][115]. Pokój, jaki wtedy zawarto, podyktował król perski (stąd nazwa królewski), co potwierdziło jego rolę w Grecji[116][117]. Kolejne bezceremonialne poczynania Sparty (m.in. konflikt z Tebami) doprowadziły do otwartego sojuszu Teb z Atenami, a także stworzenia przez Ateny Drugiego Związku Morskiego[116]. Wojnę jaka wybuchła w 378 p.n.e. zakończyła Persja narzucając stronom pokój, ale trwał on krótko[118]. Wkrótce Ateny wycofały się z konfliktu, obawiając się wzrostu Teb, które konsolidowały swoją władzę nad Związkiem Beockim[118]. Pod demokratycznym rządami Pelopidasa i Epaminondasa Teby wyrosły na potęgę, która mogła równać się ze Spartą[118][119]. W 371 p.n.e. pokonały ją pod Leuktrami i w następnych latach rozmontowały Związek Peloponeski i uwolniły po wiekach spartańskiej władzy Messenię, łamiąc na zawsze potęgę Sparty[120]. Hegemonię Teb zakończyła śmierć Epaminondasa w 362 p.n.e.[121] Liczącym się graczem w Grecji pozostały Ateny, ale nowy Związek Morski był słabszy od pierwszego, a wkrótce wśród jego członków pojawiły się nastroje anty-ateńskie[121]. Podsycone przez perskich satrapów powstanie zakończyło się klęską Aten w 355 r. p.n.e. i osłabieniem jej pozycji[121].
Macedonia do połowy IV wieku p.n.e. pozostawała na peryferiach świata greckiego[122]. Jej mieszkańcy etnicznie i językowo pokrewni Grekom, byli przez nich uznawani za po części barbarzyńców, jednak kraj stopniowo upodabniał się kulturowo do reszty Hellady[123]. Decydującą zmianą w historii Macedonii było wstąpienia na tron w 360 p.n.e. Filipa II, wcześniej zakładnika w Tebach podczas szczytu potęgi tego polis. Nowy król zreformował armię, tworząc formację falangi macedońskiej, a także kawalerii tzw. hetajrów[123]. Pokonał zagrażających państwu z północy Dardanów, potem dzięki zręcznej polityce i podbojom (m.in. w Tracji gdzie zajął greckie poleis oraz kopalnie złota) znacząco poszerzył granice i zasoby Macedonii[124]. Interwencja podczas tzw. III wojny świętej w Grecji Środkowej dała mu możliwość przyłączenia w 352 p.n.e. Tesalii, później anektował poleis Związku Chalcydyckiego oraz dalsze tereny w Tracji aż po Morze Czarne[125]. Rosnąca pozycja Macedonii doprowadziła do wojny z sojuszem Aten i Teb zakończonej sukcesem Filipa[126]. W 337 r. p.n.e. narzucił on całej Grecji (z wyjątkiem Sparty) swoją hegemonię, tworząc Związek Koryncki – formalnie sojusz mający walczyć z Persją, w rzeczywistości narzędzie dominacji macedońskiej[127]. Kolejnym etapem podbojów Filipa była wojna z Persją, rozpoczęta w 336 r. p.n.e., jednak tego samego roku król padł ofiarą zamachu, a na tronie macedońskim zasiadł jego dwudziestoletni syn, Aleksander, później nazwany Wielkim[128].
Młody król szybko stłumił opór miast greckich i został wybrany hegemonem Związku Korynckiego. Jednak gdy ruszył na wyprawę w głąb Bałkanów w Tebach wybuchło powstanie[129]. Aleksander szybko wrócił do Beocji, zdobył i zrównał z ziemią miasto[129]. Czyn ten ostudził w Grecji nastroje antymacedońskie i pozwolił władcy na kontynuowanie rozpoczętej przez ojca rozprawy z Persją[129]. W 334 p.n.e. z wielką armią przekroczył Hellespont i rozpoczął kampanię, która po kilku latach walk uczyniła go panem całego rozległo państwa perskiego (obejmującego Azję Mniejszą, Lewant i Syrię, Egipt, Mezopotamię oraz Iran), którego stolicą uczynił Babilon[130]. W 327 r. p.n.e. przekroczył Hindukusz i z sukcesami walczył w Indiach, lecz opór znużonej armii zmusił go do powrotu do Babilonu w 324 r. p.n.e., gdzie zajął się organizowaniem swojego imperium oraz planami kolejnych podbojów[131]. Latem 323 r. p.n.e. niespełna 33-letni władca zmarł pozostawiając ogromne państwo dowódcom swojej armii[132]. Jego podboje i włączenie pod panowanie macedońsko-greckie wielkich obszarów na Wschodzie zapoczątkowało nowy okres w dziejach Grecji[potrzebny przypis].
     Królestwo Ptolemeusza     Królestwo Kassandra     Królestwo Lizymacha     Królestwo Seleukosa     Epir     Kolonie greckie     Kartagina (poza światem hellenistycznym)     Republika rzymska (poza światem hellenistycznym)Ogromne państwo Aleksandra szybko stało się polem walki pomiędzy jego dowódcami starającymi się wykroić dla siebie własne królestwa – nazwano ich diadochami, a toczone przez nich walki wojnami diadochów[133]. Także część greckich poleis pod przywództwem Aten spróbowała uzyskać niezależność w tzw. wojnie lamijskiej z lat 323–321 p.n.e., ale uległy armii macedońskiej[134]. Wojny diadochów trwające blisko 40 lat po śmierci Aleksandra doprowadziły do powstania trzech królestw, rządzonych przez dynastie wywodzące się od ich założycieli[135]:
Scena polityczna świata greckiego uzupełniana była przez mniejsze, mniej lub bardziej niezależne państwa i poleis jak np. Epir, Pergamon (później lokalna potęga w Azji Mniejszej[136]), Rodos czy poleis zachodniego basenu Morza Śródziemnego.
Zakończenie wojen diadochów nie spowodowało końca rywalizacji między państwami hellenistycznymi. Seleucydzi i Ptolemeusze stoczyli w III i II w. p.n.e. szereg wojen tzw. syryjskich o kontrolę nad Syrią i Palestyną[137]. Ptolemeusze prowadzili ekspansję na wyspach Morza Egejskiego, także w części Azji Mniejszej[134]. Antygonidzi w Macedonii starali się utrzymać kontrolę nad resztą Grecji kontynentalnej, ale musieli stawić czoło nowym tworom politycznym – Związkom Achajskiemu i Etolskiemu[138]. W przeciągu III w. p.n.e. w Azji Mniejszej rosła pozycja Pergamonu, który przeciwstawiał się głównie Seleucydom, walczył także z Galatami – Celtami, którzy po spustoszeniu północnej Grecji na początku III w. p.n.e. osiedli w Anatolii[136][139].
Rozległe państwo Seleucydów borykało się także z tendencjami odśrodkowymi[140] – w III w. p.n.e. uniezależniły się jego wschodnie rubieże – królestwo Greko-Baktryjskie rządzone przez grecką elitę[141], które potem rozpoczęło ekspansję w Indiach[142]. Inna uniezależniona prowincja – Partia, zdobyta przez koczowników ze stepu Środkowo-Azjatyckiego[141], stała się poważnym zagrożeniem dla Seleucydów, przejmując później inne wschodnie satrapie[143][144].
Na Zachodzie szybko rosła potęga italskiego miasta – Rzymu. Na początku III w. p.n.e. było już panem prawie całej Italii (w tym znajdujących się tam greckich poleis)[145]. Seria wojen z Kartaginą dała mu pozycję hegemona na Zachodzie i kontrolę, w większości greckiej, Sycylii[146]. Wojny macedońskie toczone przez Rzym z Antygonidami pozwoliły mu zaangażować się w politykę w Grecji, gdzie został wkrótce głównym arbitrem[147]. Po zwycięskiej wojnie z Seleucydami z lat 192–188 p.n.e. mógł dyktować swoje warunki tak wrogom, jak i sojusznikom w regionie[148]. Stopniowo kolejne części świata greckiego stawały się rzymskimi prowincjami[149]. Próbę przeciwstawienia się tej ekspansji podjął w latach 88–66 p.n.e. helleński Pont pod rządami Mitrydatesa VI, zyskując sojuszników na terenie Grecji europejskiej i w Azji Mniejszej, ale w końcu uległ rzymskim armiom[150]. Znacznie okrojone państwo Seleucydów stało się prowincją w latach 60 I w. p.n.e.[151], ostatnie z państw hellenistycznych – Egipt w 30 r. p.n.e.[152] Wschodnie tereny państwa Seleucydów (Mezopotamia i Iran) trafiły w ręce Partów[153].
Suwerenne, greckie i hellenistyczne państwa i poleis zniknęły z mapy politycznej, jednak ich kultura wciąż trwała, silnie oddziałując także na Rzymian. Wschodnia połowa Imperium Romanum pozostała greckojęzyczna, a część elementów greckiej kultury przyjmowały także dalsze państwa Wschodu[potrzebny przypis].
Dynastia Ming (1368–1644) (chiń. 明朝; pinyin Míng Cháo; Wade-Giles Ming Ch’ao; wym. [mǐŋ tʂʰɑ̌ʊ̯]) – dynastia cesarska Chin, panująca po upadku mongolskiej dynastii Yuan. Była to ostatnia narodowa dynastia chińska.
Założycielem dynastii był Zhu Yuanzhang, jeden z przywódców powstania przeciwko dynastii Yuan. W 1368 ogłosił się cesarzem i przyjął jako nazwę swojej ery panowania Hongwu, a założoną przez siebie dynastię nazwał Ming, czyli Wspaniała. W tym samym roku została zdobyta stolica dynastii Yuan: Dadu (obecnie Pekin). Cesarz Hongwu próbował stworzyć społeczeństwo oparte na samowystarczalnych wspólnotach wiejskich oraz ograniczyć pozycję kupców. W wyniku jego działań zostało odbudowane rolnictwo oraz zbudowana sieć dróg dla celów wojskowych i administracyjnych. Utrzymywano stałą armię liczącą co najmniej milion żołnierzy[1].
Za panowania cesarza Yongle (1402–1424) został odbudowany Wielki Kanał, a także wybudowano nową stolicę Pekin, a w nim Zakazane Miasto, pałacową rezydencję cesarza i jego rodziny.  W latach 1405–1433 nowo wybudowana, olbrzymia flota pod dowództwem admirała Zheng He (1371–1433) odbyła siedem wypraw. W trakcie tych międzynarodowych misji trybutarnych docierała do wybrzeży Azji Południowo-Wschodniej oraz do wybrzeży Oceanu Indyjskiego, aż po Egipt i Mozambik. W 1449 wojska chińskie zostały pokonane przez Mongołów w bitwie pod Tumu. Do niewoli dostał się cesarz Zhu Qizhen. Wydarzenie to traktowane jest jako koniec przewagi militarnej Chin nad koczownikami z północy. Dla przeciwdziałania zagrożeniu z ich strony, po 1474 zaczęto ogromnym kosztem rozbudowywać Wielki Mur Chiński.
Od XVI w. rozwijał się handel z Europejczykami i Japonią. Chiny eksportowały głównie jedwab i porcelanę, a importowały srebro, które zastąpiło jako środek wymiany gospodarczej cesarstwa monety miedziane i banknoty. W XVII w. zmiany klimatyczne i nieumiejętna polityka gospodarcza powodowały klęski głodu i towarzyszące im epidemie. Następował upadek autorytetu władz i wybuchały liczne powstania. W 1644 armia powstańcza zajęła Pekin, a ostatni cesarz dynastii Ming, Chongzhen (1627–1644) popełnił samobójstwo. Przywódca powstania, Li Zicheng, ogłosił się cesarzem nowej dynastii Shun. Po miesiącu Pekin został zajęty przez wojska mandżurskie, co oznaczało początek panowania w Chinach mandżurskiej dynastii Qing. Mingowie utrzymali się na południu Chin do 1662.
Panowanie dynastii Ming było okresem rozkwitu sztuki, zarówno malarstwa, poezji, muzyki, literatury, jak i przedstawień scenicznych. Wzorowanie się na osiągnięciach poprzednich epok powodowało, że nie czyniono wysiłków wykroczenia poza to co już istniało. Szczególnie było to widoczne w malarstwie i poezji. Rozwój społeczeństwa konsumpcyjnego w tym okresie wpływał również na kulturę i sztukę. Zaczęła się rozwijać literatura w języku potocznym. Postęp naukowy i technologiczny za panowania dynastii Ming był nieznaczny, szczególnie w porównaniu do tempa rozwoju w krajach cywilizacji zachodniej. Najważniejsze osiągnięcia z drugiej połowy panowania dynastii Ming były zainspirowane kontaktami z Europą. Liczba mieszkańców cesarstwa wzrosła w tym okresie z około 60 milionów do prawie 200 milionów.
W latach czterdziestych XIV wieku panowanie rządzącej w Chinach mongolskiej dynastii Yuan (1271–1368), już wcześniej osłabione wskutek korupcji urzędników, nadmiernego fiskalizmu i inflacji wywołanej niekontrolowaną emisją papierowego pieniądza, zaczęło chylić się ku upadkowi w wyniku katastrof naturalnych, epidemii i klęsk głodu, które były częścią trapiącej społeczeństwa od Islandii po Japonię eurazjatyckiej serii „plag, klęsk głodu, upadku rolnictwa, depopulacji i społecznych zaburzeń”[2]. Wybuchały liczne powstania, z których największe było, rozpoczęte w 1351, powstanie Czerwonych Turbanów. Czerwone Turbany były związane z tajnym buddyjskim Stowarzyszeniem Białego Lotosu[3][4]. Do rekrutacji zwolenników Stowarzyszenie Białego Lotosu wykorzystało zgromadzenie w jednym miejscu 150 tysięcy okolicznych chłopów, wezwanych na przymusowe roboty przy regulacji Rzeki Żółtej[3][5].
Zhu Yuanzhang był ubogim chłopem i mnichem buddyjskim, który dołączył do Czerwonych Turbanów w 1352[6][7]. Szybko zyskał uznanie w ich szeregach i poślubił przybraną córkę jednego z przywódców powstania[6][7]. W 1356 powstańcy pod przywództwem Zhu Yuanzhanga zdobyli Nankin[8], który później został stolicą Chin, założonej przez niego dynastii Ming.
Wraz z postępującym osłabieniem dynastii Yuan, poszczególni przywódcy powstania zaczęli walczyć o kontrolę nad Chinami i tym samym o prawo do ustanowienia własnej dynastii cesarskiej[9]. W 1363 w bitwie na jeziorze Poyang Zhu Yuanzhang pokonał swojego głównego rywala i przywódcę rywalizującej z nim grupy Han, Chen Youlianga. Dowodzona przez niego flota Ming, licząca 200 tysięcy marynarzy zdołała pokonać ponad trzykrotnie liczniejszą flotę Han, która miała liczyć 650 tysięcy marynarzy. Zwycięstwo osiągnęła m.in. dzięki masowemu użyciu w bitwie branderów. Zwycięstwo nad ostatnią rywalizującą z nim grupą powstańców dało Zhu Yuanzhangowi kontrolę nad basenem rzeki Jangcy i umocniło jego władzę nad południowymi Chinami[10]. Po tym jak w podejrzanych okolicznościach zginął w 1367 syn pierwszego przywódcy powstania Czerwonych Turbanów, w czasie gdy przebywał na dworze Zhu Yuanzhanga, nie pozostał nikt, kto mógłby mu przeszkodzić w zdobyciu tronu cesarskiego. W 1368, po ogłoszeniu się przez Zhu Yuanzhanga cesarzem, armia Ming zdobyła stolicę dynastii Yuan Dadu (大都, Wielka stolica – obecnie Pekin)[11]. Ostatni cesarz z dynastii Yuan uciekł na północ do Xanadu, a pałace dynastii Yuan w Dadu zostały zniszczone[11]; w tym samym roku miasto zmieniło nazwę na Beiping[12]. Zamiast tradycyjnego sposobu nazywania dynastii od nazwy miejsca pochodzenia jej założyciela, Zhu Yuanzhang przyjął nazwę Ming, czyli Wspaniała, wykorzystując precedens stworzony przez dynastię Yuan[8]. Zhu Yuanzhang jest znany jako cesarz Hongwu, od wybranej przez niego nazwy ery panowania[8]. Chociaż Stowarzyszenie Białego Lotosu umożliwiło mu dojście do władzy, Hongwu później kwestionował jego zasługi i zwrócił się przeciwko niemu[8][13].
Cesarz Hongwu natychmiast podjął działania mające na celu odbudowę infrastruktury państwa. Wybudowano mury obronne wokół Nankinu o długości 48 km oraz pałace i budynki rządowe[11]. Historia dynastii Ming (Mingshi) stwierdza, że już w 1364 Zhu Yuanzhang zaczął opracowywać nowy konfucjański kodeks prawny, Da Ming Lü (chiń. upr. 大明律; chiń. trad. 大明律; pinyin Dàmíng lǜ), który został ukończony w 1397 i zawierał część przepisów pochodzących z kodeksu prawnego z czasów dynastii Tang z 653[14]. Hongwu zorganizował system służby wojskowej znany jako system weisuo (chiń. upr. 卫所制; chiń. trad. 衛所制; pinyin Wèi suǒ zhì), który był podobny do systemu fubing z czasów dynastii Tang (618–907). Jego podstawowym założeniem było to, że żołnierze, kiedy nie walczą lub nie ćwiczą, będą uprawiać ziemię, po to, aby mogli sami się utrzymać[15]. System samowystarczalnych żołnierzy-rolników był jednak nieefektywny; skąpe racje żywnościowe i nieregularne nagrody nie wystarczały do utrzymania oddziałów i następowały liczne dezercje. Z wyjątkiem dobrze zaopatrywanych wojsk stacjonujących na granicach państwa, pozostałe jednostki wojskowe miały po 10% teoretycznego stanu osobowego[16].
Chociaż był konfucjanistą, Hongwu nie miał zaufania do urzędników-uczonych wywodzących się z wyższej klasy społecznej gentry i nie wahał się przed skazywaniem ich na chłostę za przestępstwa[17]. W 1373 wstrzymał egzaminy urzędnicze po stwierdzeniu, że 120 urzędników-uczonych, którzy zdali egzamin i uzyskali tytuł jinshi nie miało odpowiednich kompetencji[18][19]. W 1384 wznowiono egzaminy urzędnicze[19]. Wkrótce został stracony główny egzaminator po tym, jak odkryto, że egzaminy zdają i otrzymują tytuł jinshi tylko kandydaci z południa Chin[18].
W 1380 został stracony kanclerz Hu Weiyong pod zarzutem zorganizowania spisku w celu obalenia cesarza; po tym wydarzeniu Hongwu zlikwidował urząd kanclerza i przejął jego obowiązki jako cesarz i szef rządu[20][21]. Z powodu rosnącej nieufności i podejrzliwości w stosunku do ministrów i poddanych, Hongwu utworzył Jinyi Wei – tajną policję wywodzącą się z jego straży pałacowej. W wyniku jej działań torturowanych było i poniosło śmierć około 100 tysięcy osób w czasie kilku czystek w ciągu 30 lat jego rządów[20][22].
Według historyka Timothego Brooka, cesarz Hongwu próbował utrudnić mobilność społeczną poprzez stworzenie sztywnych, regulowanych przez państwo granic pomiędzy okręgami wiejskimi i większymi miastami oraz utrudnianie handlu i podróży bez zgody władz[23]. Hongwu próbował również wpoić surowe zasady moralne poprzez narzucenie jednolitych ubiorów oraz sposobu mówienia i pisania, które miały nie dawać przewagi osobom lepiej wykształconym[24]. Jego podejrzliwość wobec wykształconych elit dorównywała pogardzie wobec najbogatszych kupców, prowadząc do nałożenia wyjątkowo wysokich podatków na siedzibę wpływowych rodzin kupieckich w rejonie Suzhou w Jiangsu[18]. Wysiedlono tysiące zamożnych rodzin z południowego wschodu i osadzono je w okolicach Nankinu, zabraniając im opuszczać miejsce przymusowego osiedlenia[18][25]. Aby kontrolować ich działalność handlową, Hongwu zmusił kupców do spisywania co miesiąc wszystkich posiadanych dóbr[26]. Jednym z jego głównych celów było trwałe ograniczenie wpływów kupców i właścicieli ziemskich, jednak niektóre z jego przedsięwzięć umożliwiły im powiększenie majątków.
Stworzony przez Hongwu system masowych przesiedleń i chęć uniknięcia wysokich podatków, skłoniły wielu ludzi do stania się wędrownymi handlarzami, domokrążcami lub pracownikami, których zatrudniali lub dzierżawili im ziemię właściciele ziemscy lub kupcy[27]. W połowie panowania dynastii Ming zrezygnowano z systemu przymusowego osiedlenia i zamiast tego powierzono lokalnym urzędnikom obowiązek rejestracji pracowników sezonowych w celu zwiększenia dochodów podatkowych[28]. Elita bogatych właścicieli ziemskich i kupców panowała nad dzierżawcami, pracownikami najemnymi, służbą domową i robotnikami sezonowymi – miało to mało wspólnego z wizją cesarza Hongwu: ścisłego przestrzegania hierarchicznego systemu podziału na cztery grupy społeczne: urzędników, rolników, rzemieślników i kupców[29].
Reformy Hongwu wspierały rolnictwo i produkcję rolną w celu stworzenia samowystarczalnych wspólnot, które nie będą opierać się na handlu, który – jak sądził cesarz – ograniczy się tylko do miast[31]. Jednak wytwarzane w wyniku reform nadwyżki żywności, zachęciły rolników do jej sprzedaży[32]. Początkowo handel miał miejsce przy szlakach komunikacyjnych; przed połową panowania dynastii Ming rolnicy zaczęli sprzedawać swoje produkty również w pobliskich miastach[32]. Gdy miasta i wsie połączyła już sieć powiązań handlowych, wiejskie gospodarstwa domowe zaczęły zajmować się również tradycyjnymi miejskimi rzemiosłami, takimi jak tkactwo oraz wytwarzanie odzieży bawełnianej i jedwabnej[33]. Pod koniec panowania dynastii Ming konserwatywni konfucjaniści coraz głośniej wyrażali zaniepokojenie, że tradycyjny porządek społeczny jest podkopywany przez chłopów przejmujących miejskie zwyczaje i związaną z nimi dekadencję[34].
Rolnicy nie byli jedyną grupą społeczną, na którą wpłynęła rosnąca komercjalizacja stosunków społecznych w Chinach; wywarła ona również silny wpływ na grupę właścicieli ziemskich, z której to grupy społecznej zazwyczaj pochodzili urzędnicy administracji państwowej. Zgodnie z tradycją urzędników traktowano jako skromne osoby, które nie wykorzystują swojego stanowiska jako okazji do wzbogacenia; byli znani z tego, że chodzili pieszo ze swoich domów na wsi, gdzie mieszkali, do miast, gdzie pracowali. Za panowania cesarza Zhengde (1505–21), urzędnicy byli już noszeni w luksusowych lektykach i zaczęli nabywać obszerne domy w prestiżowych dzielnicach miast, zamiast mieszkać na wsi[35]. Pod koniec panowania dynastii Ming, zgromadzone bogactwa stały się głównym wyznacznikiem społecznego prestiżu, większym nawet niż posiadanie wysokiej rangi urzędniczej[36].
W pierwszej połowie panowania dynastii Ming, mandaryni rzadko wspominali w swoich sprawozdaniach o wkładzie kupców w życie lokalnych społeczności[38]. Urzędnicy byli wtedy w stanie sami finansować publiczne projekty budowlane, będące symbolem ich dominującej pozycji politycznej[39]. Jednak w drugiej połowie panowania dynastii Ming urzędnicy powszechnie korzystali ze wsparcia finansowego kupców dla realizacji różnego rodzaju przedsięwzięć, takich jak budowa mostów lub tworzenie nowych szkół konfucjańskich dla członków gentry[40]. Od tego czasu w swoich sprawozdaniach zaczęli pisać o kupcach i często wyrażali się o nich z wielkim szacunkiem, ponieważ bogactwo wynikające z ich działalności gospodarczej zwiększało zasoby państwa, jak również zwiększało druk ksiąg niezbędnych dla edukacji gentry[41]. Kupcy zaczęli przejmować zwyczaje i zainteresowania kulturalne gentry, zacierając granice pomiędzy kupcami a gentry i torując drogę członkom rodzin kupieckich do stanowisk urzędniczych[42]. Początki tych zmian można znaleźć już w czasach dynastii Song (960–1279)[43], jednak stało się to bardziej widoczne za panowania dynastii Ming. Źródła z końca epoki Ming pokazują, że nie był przestrzegany ścisły hierarchiczny podział społeczeństwa na cztery grupy społeczne: urzędników (shi (士)), rolników (nong (农)), rzemieślników (gong (工)) i kupców (shang (商))[44].
Cesarz Hongwu uważał, że tylko kurierzy rządowi i drobni handlarze detaliczni powinni mieć prawo do podróżowania daleko poza ich rodzinne miejscowości[26]. Pomimo jego starań aby narzucić ten punkt widzenia, budowa sprawnej sieci komunikacyjnej przeznaczonej dla administracji i wojska, spowodowała powstanie nieoficjalnej sieci połączeń handlowych równoległej do oficjalnej[45]. Koreański urzędnik Choe Bu (1454–1504), którego statek rozbił się u wybrzeży Chin w 1488, zauważył, że miejscowa ludność nie znała odległości między różnymi miejscowościami kraju, która to wiedza była zastrzeżona dla wojska i administracji[46]. To się diametralnie zmieniło pod koniec panowania dynastii Ming, kiedy to kupcy nie tylko odbywali dalekie podróże wraz ze swoimi towarami, ale także przekupywali urzędników, aby móc korzystać z rządowych szlaków komunikacyjnych, a nawet drukowano przewodniki, które naśladowały oficjalne mapy rządowe[47].
W połowie panowania dynastii Ming aktywność gospodarcza kupców została częściowo zinstytucjonalizowana przez państwo, co uwidoczniło zależność od niej mandarynów. Qiu Jun (1420–95), mandaryn z wyspy Hajnan, argumentował, że państwo powinno ograniczać działanie rynku jedynie w czasie kryzysu i że kupcy stanowią najlepszy wskaźnik mierzący zasoby gospodarcze kraju[49]. Rząd zastosował się do tych wskazówek w połowie panowania dynastii Ming, kiedy pozwolono na przejęcie przez kupców państwowego monopolu na produkcję soli. Był to stopniowy proces, gdy państwo zdobywało środki na zaopatrzenie armii stacjonującej na północnej granicy poprzez przyznawanie kupcom licencji na handel solą w zamian za dostawy zaopatrzenia. Rząd zrozumiał, że kupcy mogliby płacić srebrem za licencje na produkcję i handel solą i w ten sposób zwiększyć dochody państwa do tego stopnia, że zakup zaopatrzenia nie byłby problemem[50]. Za panowania cesarza Hongwu i w erze Zhengtong cesarza Zhu Qizhena (1435–49) władze usiłowały ograniczyć rolę srebra jako środka płatniczego na rzecz pieniądza papierowego, jednak nielegalne wydobycie metali szlachetnych odbywało się na dużą skalę[51]. Hongwu nie rozumiał, że rozdawnictwo olbrzymiej ilości banknotów jako nagród wywołuje inflację; w 1425, pieniądz papierowy był warty realnie tylko między 0,025% a 0,014% swojej pierwotnej wartości z XIV wieku[16]. Wartość miedzianych monet również znacznie się zmniejszyła, także w wyniku ich podrabiania na dużą skalę[52]. W XVI wieku, nawiązanie kontaktów handlowych z Europą spowodowało napływ dużych ilości srebra, które w coraz większym stopniu zaczęło zdobywać pozycję środka płatniczego[52]. Już w 1436 w prowincjach południowych podatek płacony w zbożu został częściowo zamieniony na podatek płacony w srebrze[53]. Od 1581, od reformy zainicjowanej przez Wielkiego Sekretarza Zhang Juzhenga (1525–82), podatki od ilości posiadanej ziemi były płacone w srebrze[54].
Po śmierci cesarza Hongwu w 1398, tron objął jego wnuk Zhu Yunwen jako cesarz Jianwen (1398–1402). W 1399 rozpoczęła się trzyletnia wojna domowa[55], gdy przeciwko niemu wystąpił jego wuj Zhu Di, książę Yan. Jianwen znał ambicje braci swojego zmarłego ojca i podjął kroki w celu ograniczenia ich wpływów. Zhu Di, dowodzący wojskami stacjonującymi w okolicach Pekinu, ochraniającymi północną granicę, był jego najgroźniejszym przeciwnikiem. Po aresztowaniu na rozkaz Jianwena wielu współpracowników Zhu Di, wzniecił on bunt przeciwko cesarzowi. Pod pozorem ratowania młodego Jianwena przed jego skorumpowanymi urzędnikami, Zhu Di osobiście dowodził zbuntowanymi wojskami; pałac cesarski w Nankinie spłonął wraz cesarzem Jianwenem, jego żoną, matką i dworem. Zhu Di wstąpił na tron jako cesarz Yongle (1402–1424); jego panowanie jest powszechnie postrzegane przez uczonych jako ponowne założenie dynastii Ming, ponieważ anulował wiele zmian wprowadzonych przez swojego ojca, cesarza Hongwu[56].
Yongle w 1403 ogłosił, że nową stolicą Chin będzie jego dotychczasowa siedziba: Pekin. Budowa nowej stolicy trwała od 1407 do 1420, a przy jej budowie pracowało setki tysięcy robotników dziennie[57]. W centrum stolicy znajdowało się Miasto Cesarskie, a w nim z kolei Zakazane Miasto, pałacowa rezydencja cesarza i jego rodziny. Do 1553, zewnętrzne miasto zostało poszerzone w kierunku południowym, co zwiększyło powierzchnię stolicy do ponad 60 km²[58].
Za panowania cesarza Yongle, po dziesięcioleciach popadania w ruinę, w latach 1411–1415 został odbudowany Wielki Kanał. Impulsem do jego odbudowy była chęć rozwiązania problemu transportu ziarna na północ Chin i do Pekinu. Transport 4 mln shi rocznie (1 shi ≈ 107 litrów) był utrudniony ze względu na nieefektywny system transportu przez Morze Wschodniochińskie lub przez kilka kanałów śródlądowych, który wymuszał kilkakrotne przenoszenie ładunku między różnego typu barkami[59]. Około 165 tysięcy robotników odbudowywało kanał w zachodnim Szantungu i zbudowało 15 śluz[58][60]. Ponowne otwarcie Wielkiego Kanału miało również wpływ na Nankin, ponieważ większego od niego znaczenia gospodarczego nabrało miasto Suzhou, które stało się najważniejszym ośrodkiem handlowym w Chinach[61].
Chociaż nakazał przeprowadzenie kilku krwawych czystek jak jego ojciec − w tym egzekucję Fang Xiaoru, który odmówił napisania jego mowy inauguracyjnej − Yongle miał inny stosunek do urzędników-uczonych. W celu ułatwienia nauki dla tych, którzy przygotowywali się do egzaminów urzędniczych, nakazał sporządzić kompilację tekstów konfucjańskiej szkoły zasad Cheng Yi i Zhu Xi[57]. Yongle zatrudnił ponad dwa tysiące uczonych, którzy sporządzili zawierającą 50 mln słów (22 938 rozdziałów) encyklopedię, skompilowaną z siedmiu tysięcy ksiąg[57]. Przewyższyła ona zakresem tematyki i rozmiarami wszystkie poprzednie encyklopedie włącznie z XI-wieczną kompilacją Songsi Dashu (Cztery wielkie księgi ery Song). Jednak mandaryni nie byli jedyną grupą społeczną, z którą Yongle musiał współpracować i pozyskać. Historyk Michael Chang wskazuje, że Yongle był „cesarzem w siodle”, który często przemieszczał się między obiema stolicami (tak jak to czynili cesarze dynastii Yuan) i stale prowadził wyprawy wojskowe do Mongolii[62]. Było to sprzeczne z konfucjańskimi zasadami, natomiast wzmacniało pozycję eunuchów i wojskowych, których pozycja zależała od cesarza[62].
W 1405 cesarz Yongle mianował swojego zaufanego dowódcę, eunucha Zheng He (1371–1433), admirałem nowo wybudowanej, olbrzymiej floty, przeznaczonej do międzynarodowych misji trybutarnych. Chińczycy wysyłali drogą lądową na zachód misje dyplomatyczne od czasów dynastii Han (206 p.n.e. – 220 n.e.) i byli zaangażowani od stuleci w prywatny handel zamorski sięgający aż do Afryki Wschodniej, szczególnie w czasach dynastii Song i Yuan. Jednak do tej pory nie wysłali żadnej misji porównywalnej wielkości i rozmiarów. Dla celów kolejnych misji zagranicznych, stocznie w Nankinie wybudowały w latach 1403-19 dwa tysiące statków, w tym ogromne baochuany (chiń. upr. 宝船; chiń. trad. 寶船; pinyin Bǎochuán, tłum.: okręt skarbowy), które miały mieć 112–134  m długości i 45–54 m szerokości[63]. Jednak rozmiary tych statków zostały zakwestionowane przez niektórych badaczy, którzy szacują faktyczną długość tych statków na 59 m (200 stóp)[64][65].
W pierwszej wyprawie w latach 1405-07 uczestniczyło 317 statków z załogą liczącą 26 800 osób oraz 70 eunuchów, 180 osób personelu medycznego, 5 astrologów i 300 dowódców wojskowych[66]. Wyprawy nie były kontynuowane po śmierci Zheng He. Jednak jego śmierć była tylko jednym z czynników, które spowodowały zaprzestanie tych misji. Yongle podbił Wietnam w 1407 i ustanowił tam prowincję chińską, jednak wojska chińskie zostały wyparte z tego kraju w 1428; w 1431 nowa wietnamska dynastia Hậu Lê została uznana przez Chiny za władców odrębnego państwa wasalnego[67]. Istniało także zagrożenie ze strony rosnących w siłę Mongołów, co odwracało uwagę dworu od innych spraw[68]. Dla przeciwdziałania temu zagrożeniu zaczęto po 1474 ogromnym kosztem rozbudowywać Wielki Mur Chiński[68]. Przeniesienie przez cesarza Yongle stolicy z Nankinu do Pekinu było w dużej mierze spowodowane koniecznością szybkiej reakcji na zagrożenie ze strony Mongołów[69]. Mandaryni wiązali ogromne wydatki na flotę z silną pozycją eunuchów na dworze cesarskim, więc wstrzymanie finansowania tych wypraw traktowali jako jeden ze środków osłabienia wpływów eunuchów[70].
Po śmierci cesarza Yongle w 1424 na tron wstąpił jego syn, który przyjął tytuł Hongxi (1424–25)[71][72]. Wstrzymał on ekspansjonistyczną politykę ojca, w tym wyprawy przeciwko Mongołom oraz zakazał dalszych wypraw trybutarnych Zheng He[72][73]. Po cesarzu Hongxi panował jego syn, cesarz Xuande (1425–1435)[74]. Po niepowodzeniu ekspedycji wojskowej wysłanej w 1426 w celu stłumienia powstania w Wietnamie, w 1427 podjął decyzję o wycofaniu stamtąd wojsk chińskich[75][76]. Za jego panowania miała miejsce siódma, ostatnia wyprawa Zheng He[77][78]. Jego panowanie przebiegało bez większych problemów wewnętrznych i zewnętrznych i jest uznawane za szczytowy okres świetności dynastii Ming[79].
W 1435 cesarzem został najstarszy syn cesarza Xuande – Zhu Qizhen. W chwili objęcia tronu miał 8 lat, co stwarzało poważne problemy proceduralne, gdyż zgodnie z zasadami ustalonymi przez cesarza Hongwu, decyzje mógł podejmować tylko cesarz i nie była dozwolona żadna regencja[80][81][82]. Nominalnie rząd funkcjonował pod kontrolą niepełnoletniego cesarza, a nieformalną regentką stała się jego babka, cesarzowa Zhang (zm. 1442) – wdowa po cesarzu Hongxi i matka cesarza Xuande[80][81][82]. Wielki wpływ na cesarza miał również jego wychowawca, nauczyciel i powiernik – eunuch Wang Zhen[83]. W 1442 Zhu Qizhen objął osobiste rządy, a wkrótce zmarła jego babka[84]. Pierwszy okres panowania cesarza Zhu Qizhena – era Zhengtong – jest uważany za jeden z najpomyślniejszych w dziejach dynastii Ming[80]. W lipcu 1449 wódz Ojratów Esen rozpoczął inwazję na Chiny. Naczelny eunuch Wang Zhen zachęcił cesarza Zhu Qizhena, aby osobiście poprowadził wojsko przeciwko Mongołom, Cesarz wyruszył ze stolicy z 50-tysięczną armią i mianował swojego przyrodniego brata Zhu Qiyu tymczasowym regentem. 8 września w bitwie pod Tumu Esen pokonał armię cesarską, a sam cesarz dostał się do niewoli. Wydarzenie to oznaczało koniec supremacji militarnej Chin i jest określane jako Katastrofa Tumu[85]. Wojska Esena spustoszyły tereny Chin, aż po przedmieścia Pekinu[86]. Okolice Pekinu zostały ponownie spustoszone w listopadzie przez miejscowych bandytów i Mongołów – dezerterów z armii cesarskiej, udających żołnierzy Esena[87]. Również wielu Chińczyków zajęło się grabieżami po Katastrofie Tumu[88].
Mongołowie zażądali okupu za pojmanego cesarza. Jednak plan się nie powiódł, ponieważ na tron wstąpił jego przyrodni brat Zhu Qiyu, jako cesarz Jingtai (1449-57); a atak mongolski został odparty przez ministra wojny Yu Qiana (1398–1457), który zdołał opanować panikę w Chinach po Katastrofie Tumu. Ponieważ ktoś inny zasiadał na tronie, Zhu Qizhen stał się bezużyteczny dla Mongołów, więc uwolnili go i pozwolili powrócić do Chin[85]. Zhu Qizhen został umieszczony w areszcie domowym, w którym przebywał aż do przewrotu pałacowego w 1457, znanego pod nazwą „tomen” – „wyważenie bram pałacowych”[89]. Po przewrocie ponownie objął tron, przyjmując nową nazwę ery panowania Tianshun[90].
Era Tianshun przebiegała pod znakiem niepokojów, jak również problemów związanych z kwestią służby Mongołów w wojsku cesarskim. 7 sierpnia 1461, chiński generał Cao Qin, dowodzący wojskami złożonymi z żołnierzy pochodzenia mongolskiego i pochodzenia chińskiego, próbował dokonać przewrotu wojskowego, obawiając się, że może zostać kolejną ofiarą czystki prowadzonej przez cesarza[91]. Mongołowie służący w armii cesarskiej również stawali się coraz bardziej zaniepokojeni, ponieważ Chińczycy przestali ufać swoim mongolskim podwładnym po Katastrofie Tumu[92]. Rebelianci dowodzeni przez Cao Qina zdołali podłożyć ogień pod wschodnie i zachodnie bramy Miasta Cesarskiego w Pekinie (który został zgaszony przez ulewny deszcz padający podczas walk) i zabić kilku wysokich urzędników, zanim zostali pokonani, a Cao Qin popełnił samobójstwo, aby nie dostać się w ręce wojsk cesarskich[93]. Cesarz Zhu Qizhen zmarł w 1464, po krótkiej chorobie, w wieku 37 lat[94].
W lutym 1464 rządy objął 16-letni cesarz Chenghua (1464-1487)[95]. W pierwszej połowie swoich rządów cesarz opierał się na urzędnikach z Wielkiego Sekretariatu[96][97]. Przez cały okres panowania pozostawał pod wpływem swojej konkubiny, Pani Wan[98]. Promowała ona eunuchów i swoją rodzinę, przyczyniła się do rozpowszechnienia się na wielką skalę korupcji[99][100]. W ostatnich latach panowania faktyczna władza wpadła w ręce eunuchów Wang Zhi i Liang Zhi[99][100]. W całym kraju wybuchały powstania chłopskie, które były brutalnie tłumione[101]. Za jego panowania rozpoczęto rozbudowę Wielkiego Muru Chińskiego[102]. 1 września 1487 cesarz poczuł się źle, 4 września nakazał swojemu synowi przewodniczyć posiedzeniom rządu, a 9 września zmarł[103].
W 1487 rządy objął syn cesarza Chenghua – cesarz Hongzhi (1487-1505)[104][103]. Po wstąpieniu na tron jego rządy były prowadzone w duchu ideologii konfucjańskiej, a on sam stał się pilnym i pracowitym cesarzem[105]. Ściśle nadzorował sprawy państwowe, obniżał podatki i zbędne wydatki rządowe, na stanowiska rządowe wybierał zdolnych urzędników[106][107]. Cesarz harmonijnie współpracował z Wielkimi Sekretarzami i ministrami, co było rzadkością w drugiej połowie panowania dynastii[108]. Ograniczył wszechwładzę eunuchów i wpływ intryg pałacowych na rządy[109][110]. Oficjalna historiografia chińska opisuje go jako jednego z pięciu pozytywnie ocenianych władców dynastii Ming, obok cesarzy Hongwu, Yongle, Hongxi i Xuande[111]. W przeciwieństwie do swojego ojca, cesarz Zhengde (1505-1521) lekceważył sprawy państwa i zajmował się przede wszystkim rozrywkami[112][113]. Faktyczna władza znalazła się w rękach eunucha Liu Jina, a następnie faworytów cesarza: Qian Ninga i Jiang Bina[114]. O ile pierwszy był obeznany z zasadami działania administracji cesarstwa, to działania pozostałych wprowadzały chaos w państwie[114]. Dodatkowo rozrzutność cesarza pogłębiała problemy finansowe cesarstwa[115]. W okresie jego panowania nasiliły się najazdy Mongołów pod wodzą Dajan-chana. W 1517 wojska chińskie pod dowództwem cesarza pokonały wojska mongolskie. Było to jedyne zwycięstwo wojsk chińskich nad głównymi siłami mongolskimi w XVI wieku[116]. Cesarz Zhengde zmarł bezpotomnie w 1521[117]. Jego panowanie jest oceniane jednoznacznie negatywnie przez tradycyjną historiografię chińską, a za nią przez historyków zachodnich[113].
Wielki Sekretarz Yang Tinghe zdołał przeforsować, że następnym cesarzem został kuzyn cesarza Zhengde – Zhu Houcong, który jako nazwę swojej ery przyjął Jiajing (1521-1567)[118][119]. Wielki Sekretarz ograniczył wpływy eunuchów, odzyskał część nieruchomości rozdanych przez poprzedniego cesarza i pozbył się tysięcy jego popleczników[120]. W marcu 1524 cesarz zmusił Yang Tinghe do złożenia rezygnacji ze stanowiska[120]. W czerwcu tego roku kazał aresztować i wychłostać ponad 200 urzędników-uczonych, którzy błagali go, aby nie nakazywał oddawania czci cesarskiej swojemu zmarłemu ojcu (który nie był cesarzem). W wyniku chłosty 17 z nich zmarło, pozostałych przy życiu zdymisjonowano i skazano na banicję[121][122]. W 1528 69-letni Yang Tinghe został skazany na śmierć, co cesarz w drodze łaski zamienił na degradację[123]. Odtąd cesarz rządził despotycznie, urzędnicy zachowywali swoje stanowiska tak długo, jak bezwarunkowo spełniali wszystkie jego zachcianki[124]. Stał się jedną z najbardziej antypatycznych postaci wśród cesarzy dynastii. Wiele jego braków jako cesarza jest związane z jego wiarą w magię i taoizm[125]. Od lat 30. zaczął się wycofywać z bieżącego prowadzenia spraw państwa, skoncentrował się na poszukiwaniu środków zwiększających długość życia i sprawność seksualną[126]. Np. wierząc, że może do tego przyczynić się stosunek z czternastoletnią dziewicą, kazał przyjąć do pałacu prawie tysiąc młodszych dziewcząt[127]. Na początku panowania jego syna, cesarza Longqinga (1567-1572), zostały podjęte reformy mające wzmocnić państwo (ograniczenie korupcji, władzy eunuchów, zniesienie zakazu handlu zagranicznego). Po obiecujących początkach, cesarz porzucił obowiązki rządowe i oddał się rozrywkom pałacowym[128][129].
Za rządów Dajan-chana (zm. 1543) i jego następców, mongolskie zagrożenie dla Chin było największe od XV wieku, chociaż sporadyczne najazdy odbywały się przez cały okres panowania dynastii Ming. Jak w czasie Katastrofy Tumu, mongolski wódz, wnuk Dajan-chana, Altan-chan (zm. 1582) najechał Chiny i spustoszył okolice Pekinu[130][131]. Chińczycy używali wojsk pochodzenia mongolskiego do odpierania najazdów Altan-chana, tak jak poprzednio mongolskich wojskowych do stłumienia rebelii Cao Qina[132]. Podczas gdy cesarz Yongle podjął pięć dużych wypraw wojskowych na północ od Wielkiego Muru, idąc w ślady cesarza Hongwu walczącego z ostatnimi przedstawicielami dynastii Yuan, stałe zagrożenie najazdami mongolskimi skłoniło jego następców do odbudowy Wielkiego Muru Chińskiego od końca XV wieku. Jak zauważył jednak John Fairbank „okazało się to daremne z wojskowego punktu widzenia, lecz doskonale oddawało chińską mentalność”[68]. Wielki Mur Chiński nie był jedynie czysto obronną budowlą; jego wieże służyły raczej jako system wież strażniczych i stacji sygnalizacyjnych pozwalających na szybkie ostrzeżenie własnych jednostek o zbliżaniu się nieprzyjaciela[133].
W 1381, dynastia Ming podbiła południowo-wschodnie obszary dawnego Królestwa Dali (w obecnej prowincji Junnan). Wojska składające się z muzułmanów Hui pokonały Mongołów i muzułmanów Hui lojalnych wobec dynastii Yuan. Muzułmanie Hui dowodzeni przez generała Mu Yinga, który został mianowany gubernatorem prowincji Junnan, zostali przesiedleni na te tereny w ramach kolonizacji podbitej prowincji[134]. Do końca XIV wieku, około 200 tysięcy osadników wojskowych otrzymało co najmniej 2 miliony mǔ (około 140 tys. ha) ziemi na terenach dzisiejszych prowincji Junnan i Kuejczou. Około pół miliona chińskich osadników zostało osiedlonych w późniejszym okresie; te migracje spowodowały znaczące zmiany w składzie etnicznym regionu, ponieważ na początku panowania dynastii Ming ponad połowa z około 3 mln mieszkańców nie była pochodzenia chińskiego. W regionie istniały dwa różne rodzaje administracji. Obszary, na których większość ludności stanowili Chińczycy Han były zarządzane bezpośrednio przez urzędników chińskich i obowiązywało na nich prawo chińskie; na obszarach zamieszkanych w większości przez ludność tubylczą obowiązywał pośredni system rządów (system tusi). Władzę sprawowali lokalni wodzowie rządzący się miejscowymi prawami, którzy musieli utrzymywać porządek i płacić podatki, a w zamian otrzymywali wynagrodzenie[135]. W latach 1464-66 wybuchały powstania ludów Miao i Mien przeciwko rządom chińskim. Dla stłumienia jednego z tych powstań rząd wysłał 30-tysięczną armię (w tym tysiąc Mongołów), która wraz ze 160-tysięcznymi miejscowymi siłami z Kuangsi stłumiła powstanie. Po tym, jak uczony i filozof Wang Yangming (1472–1529) stłumił kolejne powstanie w tym regionie, zalecał wprowadzenie jednolitej administracji dla wszystkich grup etnicznych, w celu doprowadzenia do sinizacji miejscowej ludności[136].
Na pograniczu północno-zachodnim (w Gansu, Turfanie i Hami) dynastia Ming była zaangażowana w walki z ujgurskim królestwem Turfanu i Ojratami (Mongołami Zachodnimi). W 1404 zostało podbite Hami i utworzono na tych terenach nową prefekturę[137]. W 1406 został pokonany władca Turfanu[138].
W 1472 mongolski władca Turfanu Junus-chan, znany również jako Hadżdżi Ali (pan. 1462–78), zjednoczył pod swoją władzą Mogolistan (w przybliżeniu obejmujący tereny dzisiejszego wschodniego Sinciangu). W tym czasie rozpoczął się konflikt z Chinami związany z kwestią składania trybutu: Turfańczycy odnosili korzyści związane z wysyłaniem do Chin poselstw z darami dla cesarza, co pozwalało im w zamian otrzymywać od cesarza wartościowe podarunki i prowadzić wymianę handlową; Chińczycy uważali jednak, że przyjmowanie i goszczenie tych poselstw było zbyt kosztowne. Junus-chan był rozgniewany nałożonymi w 1465 przez rząd chiński ograniczeniami co do częstotliwości i wielkości poselstw z Turfanu (nie więcej niż jedno poselstwo w ciągu pięciu lat, liczące nie więcej niż 10 członków), a także odmową obdarowania jego posłów kosztownymi podarkami w 1469. W 1473 rozpoczął wojnę z Chinami i w tym samym roku zdobył miasto Hami, znajdujące się w rękach ojrackiego władcy Henshena (Hanshana). Wkrótce został wyparty przez wojska chińskie, jednak po ich wycofaniu ponownie zajął miasto. Mongołowie pod dowództwem Henshena dwukrotnie odbijali Hami (w 1482 i 1483). Jednak syn Junus-chana, Ahmed, ponownie zdobył miasto w 1493, a także pojmał władcę Hami i przedstawiciela Chin w tym mieście (które było lennem cesarstwa Ming). Cesarstwo Ming odpowiedziało nałożeniem blokady gospodarczej na Turfan i wygnaniem wszystkich Ujgurów z Gansu. Spowodowało to tak znaczne pogorszenie warunków bytowych w Turfanie, że Ahmed był zmuszony opuścić Hami. Syn Ahmeda, Mansur, zajął Hami w 1517[139][140]. Te starcia są nazywane „wojną graniczną pomiędzy dynastią Ming a Turfanem”. Po zajęciu Hami, Mansur kilka razy próbował zaatakować Chiny. W 1524 z 20-tysięczną armią dokonał najazdu, ale został pokonany przez wojska chińskie. W 1528 Mansur w przymierzu z Ojratami próbował najechać Jiuquan w Gansu, ale został pokonany przez wojska chińskie i poniósł ciężkie straty[141]. Chińczycy odmówili zniesienia blokady gospodarczej i ograniczeń, które spowodowały wybuch wojny, natomiast Turfan zaanektował Hami[142].
Poza Chinami generalnie uznaje się, że Tybet był niezależny w czasie panowania w Chinach dynastii Ming, podczas gdy historycy w Chinach reprezentują odmienny punkt widzenia. Historia dynastii Ming (Mingshi) – oficjalna historia Chin okresu panowania dynastii Ming, sporządzona w 1739, w czasie panowania dynastii Qing  – stwierdza, że w czasach dynastii Ming utworzono objazdową komanderię nadzorującą tybetańską administrację, jednocześnie odnawiając tytuły urzędnikom poprzedniej dynastii Yuan z Tybetu i nadając nowe tytuły książęce przywódcom szkół buddyjskich[145]. Turrell V. Wylie stwierdza jednak, że cenzura w Mingshi, działająca na rzecz wzmocnienia za wszelką cenę prestiżu i reputacji cesarzy z dynastii Ming, uprościła złożony obraz chińsko-tybetańskich relacji w tym okresie[146].
Współcześni uczeni nadal dyskutują nad tym, czy dynastia Ming faktycznie sprawowała władzę zwierzchnią nad Tybetem, czy też nie, lub też jak uważają niektórzy, Tybet uznawał pewną formę zwierzchności cesarzy dynastii Ming, co jednak zostało w dużym stopniu przerwane, kiedy cesarz Jiajing (1521-67) prześladował buddystów na dworze cesarskim faworyzując taoizm[146][147][148]. Helmut Hoffman twierdzi, że Mingowie podtrzymywali pozory rządów nad Tybetem poprzez okresowe przyjmowanie poselstw z darami dla cesarza i przez nadawanie chińskich tytułów rządzącym lamom, ale faktycznie nie ingerowali w zarządzanie Tybetem[149]. Wang Jiawei i Nyima Gyaincain nie zgadzają się z tym podejściem, stwierdzając, że Chiny w czasach dynastii Ming miały władzę zwierzchnią nad Tybetańczykami, którzy nie dziedziczyli tytułów chińskich, ale byli zmuszeni do podróży do Pekinu, aby je uzyskać[150]. Melvyn C. Goldstein pisze, że Mingowie nie mieli realnej władzy administracyjnej nad Tybetem, odkąd różne tytuły nadawane już rządzącym tybetańskim przywódcom nie dawały władzy, w przeciwieństwie do czasów dynastii Yuan; według niego „cesarze dynastii Ming jedynie uznawali stan faktyczny”[151].  Niektórzy badacze twierdzą, że istotny, religijny charakter relacji mingowskiego dworu z tybetańskimi lamami jest niedoceniany przez współczesną naukę[152][153]. Inni podkreślają handlowy aspekt tych związków, wskazując na niewystarczającą ilość koni w Chinach w czasach dynastii Ming i potrzebę utrzymania wymiany koni i herbaty między Chinami i Tybetem[154][155][156][157][158]. Uczeni dyskutują również nad tym jak dużą władzę i wpływy – jeżeli w ogóle – miała dynastia Ming nad kolejnymi, rządzącymi de facto Tybetem rodami: Phagmodrupa (1354–1436), Rinbungpa (1436–1565) i Cangpa (1565–1642)[159][160][161][162][163][164].
Dynastia Ming dokonywała sporadycznych interwencji zbrojnych w Tybecie w XIV wieku, którym Tybetańczycy najczęściej z sukcesem się przeciwstawiali[165][166]. Patricia Ebrey, Thomas Laird, Wang Jiawei i Nyima Gyaincain podkreślają, że Chiny za czasów dynastii Ming nie utrzymywały w Tybecie stałych garnizonów wojskowych[162][167][168], w przeciwieństwie do poprzedniej, mongolskiej dynastii Yuan[162]. Cesarz Wanli (1572–1620) próbował przywrócić związki chińsko-tybetańskie w obliczu przymierza mongolsko-tybetańskiego zapoczątkowanego w 1578, które to przymierze miało wpływ również na politykę rządzącej w Chinach dynastii Qing (1644–1912) i jej poparcie dla dalajlamów ze szkoły gelug (żółtych czapek)[146][169][170][171][172]. Od końca XVI wieku Mongołowie byli protektorami dalajlamów, co znalazło najpełniejszy wyraz w 1642, w podboju Tybetu przez Guszri-chana (1582–1655) i przekazaniu władzy nad Tybetem dalajlamie[146][173][174][175].
Około 1479, wiceminister wojny zniszczył dokumenty rządowe opisujące wyprawy Zheng He, co było jednym ze zdarzeń sygnalizujących narastanie tendencji izolacjonistycznych w Chinach[67]. Zostało wprowadzone prawo pozwalające budować jedynie małe statki. Postępujący zanik floty wojennej pozwolił na rozwój piractwa u brzegów Chin. Japońscy piraci – wakō – zaczęli atakować chińskie statki i miejscowości nadmorskie, chociaż wkrótce większość piratów stanowili Chińczycy[68].
Zamiast działań ofensywnych, władze wybrały opuszczenie miejscowości nadmorskich, aby zmniejszyć opłacalność piractwa. Cały handel zagraniczny miał być prowadzony przez państwo pod pozorem misji trybutarnych[68]. Został wprowadzony zakaz prywatnych podróży morskich i handlu; polityka ta znana pod nazwą prawo Haijin obowiązywała formalnie do roku 1567[67]. W tym okresie państwowy handel zagraniczny z Japonią był prowadzony w Ningbo, z Filipinami w Fuzhou, a z terenami obecnej Indonezji w Kantonie[176]. Nawet wtedy zezwolono przybywać Japończykom raz na dziesięć lat, a ich liczbę ograniczono do trzystu osób i dwóch statków; zachęciło to chińskich kupców do prowadzenia zakrojonego na szeroką skalę nielegalnego handlu i przemytu[176].
Najgorsze stosunki między Chinami a Japonią panowały za rządów japońskiego przywódcy Toyotomi Hideyoshiego, który w 1592 ogłosił, że zamierza podbić Chiny. W czasie wojny japońsko-koreańskiej w latach 1592-98, wojska japońskie walczyły z wojskami koreańskimi i chińskimi. Wojna ze zmiennym szczęściem toczyła się niemal w całości w Korei i na otaczających ją wodach. Ostatecznie szala zwycięstwa przechyliła się na stronę chińsko-koreańską i po śmierci Hideyoshiego w 1598, Japończycy opuścili ostatnie punkty oporu w Korei i wycofali się do Japonii. Jednak udział w wojnie wiązał się z ogromnymi wydatkami dla skarbca cesarskiego – rzędu 26 milionów uncji srebra[177].
Chociaż Jorge Álvares jako pierwszy Europejczyk wylądował w maju 1513 na wyspie Lintin (chiń. upr. 内伶仃岛; chiń. trad. 內伶仃島; pinyin Nèi língdīng dǎo) w delcie Rzeki Perłowej[178], to Rafael Perestrello był pierwszym europejskim odkrywcą, który wylądował na południowym wybrzeżu Chin i handlował w Kantonie w 1516. Dowodził on dżonką z malajską załogą, wysłaną z Malakki przez władze portugalskie[179][180][181]. W 1517 Portugalczycy wysłali kolejną ekspedycję w celu otwarcia dla nich portu w Kantonie i rozpoczęcia oficjalnych stosunków handlowych z Chinami. W trakcie tej ekspedycji zamierzali wysłać w imieniu króla Manuela I Szczęśliwego poselstwo na dwór cesarza Zhengde; członkowie poselstwa wraz z ambasadorem Tomé Piresem zostali jednak uwięzieni i zmarli w niewoli[179][178]. Po śmierci cesarza Zhengde w kwietniu 1521, konserwatywna frakcja dworska, która była przeciwna rozszerzaniu kontaktów handlowych, zdecydowała, że podbój przez Portugalczyków Malakki (lojalnego wasala Chin) jest wystarczającym powodem odmowy przyjęcia poselstwa[182]. Simão de Andrade, brat Fernão Piresa de Andrade, dowódcy wyprawy, z którą przybyło poselstwo, był podejrzewany przez Chińczyków, że porywa chińskie dzieci aby je zjadać[183]. Simão kupował jako niewolników porwane dzieci, które zostały następnie przewiezione do Diu w Indiach[183]. W 1521 chińska flota wyparła portugalskie statki z Tuen Mun (chiń. upr. 屯门; chiń. trad. 屯門; pinyin Tún mén)[184]. Flota chińska pokonała również flotę portugalską dowodzoną przez Martima Afonso w 1522 w bitwie pod Xicaowan (koło Hongkongu)[185].
Pomimo początkowej wrogości, już w 1549 Portugalczycy wysyłali co roku misje handlowe na wyspę Shangchuan (chiń. upr. 上川岛; chiń. trad. 上川島; pinyin Shàngchuān dǎo)[179]. W 1557 Portugalczycy zdołali przekonać dwór cesarski do zgody na założenie stałego portu i osady w Makau, jako oficjalnej portugalskiej placówki handlowej na wybrzeżu Morza Południowochińskiego[179]. Portugalski zakonnik Gaspar da Cruz (ok. 1520-1570), który przybył do Kantonu w 1556, napisał pierwszą książkę o Chinach i dynastii Ming opublikowaną w Europie (15 lat po jego śmierci)[186]. Zawierała ona informacje o geografii, prowincjach, rodzinie cesarskiej, mandarynach, biurokracji, żegludze, architekturze, rolnictwie, rzemiośle, handlu, odzieży, religii i obyczajach, muzyce i instrumentach muzycznych, piśmiennictwie, edukacji i wymiarze sprawiedliwości[186].
Chiny eksportowały głównie jedwab i porcelanę. Sama Holenderska Kompania Wschodnioindyjska sprzedała w Europie w latach 1602-82 6 milionów chińskich wyrobów z porcelany[187]. Antonio de Morga (1559-1636), hiszpański urzędnik w Manili, sporządził obszerną listę towarów oferowanych przez Chiny na początku XVII wieku, zauważając, że wśród nich znajdowały się różne „niezwykłe rzeczy, które, gdybym chciał je opisać, nigdy bym nie skończył, ani nie wystarczyłoby mi na to papieru”[188]. Po zwróceniu uwagi na różnorodność oferowanych przez Chiny wyrobów jedwabnych, Ebrey pisze o znacznych rozmiarach prowadzonych transakcji handlowych:
„Jeden tylko galeon płynący do posiadłości hiszpańskich w Ameryce wiózł ponad 50 tysięcy par jedwabnych pończoch. W zamian Chiny importowały głównie srebro z kopalni w Peru i Meksyku, za pośrednictwem portu w Manili. Chińscy kupcy angażowali się w te przedsięwzięcia handlowe, a wielu z nich wyemigrowało na Filipiny i Borneo, aby skorzystać z nowych możliwości handlowych[176]”.
Po wprowadzonym zakazie handlu pomiędzy Chinami i Japonią, Portugalczycy wypełnili tę lukę, występując jako pośrednik w handlu pomiędzy tymi krajami[189]. Portugalczycy kupowali chiński jedwab i sprzedawali w Japonii, w zamian otrzymując wydobywane w Japonii srebro[189]. Ponieważ srebro miało większą wartość w Chinach, Portugalczycy mogli użyć japońskiego srebra do kupna większych ilości chińskich wyrobów jedwabnych[189]. Jednak od 1573 – po założeniu przez Hiszpanię w 1571 Manili – portugalskie pośrednictwo handlowe zostało zastąpione przez bezpośredni import srebra z hiszpańskich kolonii w Ameryce[190][191].
Chociaż większość chińskiego importu stanowiło srebro, Chiny kupowały również żywność z Ameryki. Import obejmował bataty, kukurydzę, orzeszki ziemne; rośliny, które mogły być uprawiane na terenach, gdzie tradycyjne chińskie główne źródła pożywienia – pszenica, proso i ryż – nie mogły być uprawiane, pozwalając wyżywić rosnącą ludność Chin[176][192]. Za panowania dynastii Song (960-1279), ryż stał się głównym źródłem pożywienia ubogich[193]; po sprowadzeniu batatów do Chin około 1560, stopniowo stały się one podstawowym pożywieniem niższych klas[194].
Olbrzymie wydatki związane z wojną japońsko-koreańską (1592-98) były jednym z wielu problemów, z którymi musiały się zmierzyć Chiny za panowania cesarza Wanli (1572-1620). Na początku panowania Wanli otoczył się zdolnymi doradcami i sumiennie zajmował się sprawami państwa. Jego Wielki Sekretarz Zhang Juzheng (sprawujący urząd w latach 1572-82) zdołał zapewnić harmonijną współpracę wyższych urzędników. Jednak po nim nie było wystarczająco zdolnej osoby, która zdołałaby utrzymać ich współdziałanie[195]. Urzędnicy wkrótce podzielili się na zwalczające się wzajemnie frakcje. Z czasem cesarz zaczął mieć dosyć spraw państwowych i częstych kłótni między ministrami, woląc pozostawać za murami Zakazanego Miasta, nieosiągalny dla swoich urzędników[196].
Urzędnicy spierali się o to, który z jego synów powinien zostać następcą tronu; cesarz był także zdegustowany swoimi doradcami ciągle spierającymi się o to jak zarządzać państwem[196]. Narastały podziały w rządzie i wśród osób wykształconych, wynikające z debaty nad poglądami Wang Yangminga (1472–1529); jego przeciwnicy kwestionowali niektóre z zasad neokonfucjanizmu[197][198]. Poirytowany tym wszystkim cesarz zaczął zaniedbywać swoje obowiązki, nie uczestniczył w posiedzeniach urzędników omawiających politykę państwa, stracił zainteresowanie klasycznymi tekstami chińskimi, odmawiał czytania petycji i innych dokumentów państwowych, a także przestał zapełniać wakaty na wyższych stanowiskach urzędniczych[196][199]. Pozycja mandarynów w administracji uległa osłabieniu na rzecz eunuchów, którzy stali się pośrednikami pomiędzy stojącym na uboczu cesarzem a jego urzędnikami[200]. Każdy wyższy urzędnik, który chciał omówić sprawy państwowe z cesarzem, musiał przekupywać eunuchów tylko po to, aby jego sprawy zostały przedłożone cesarzowi[200].
Mówiono, że cesarz Hongwu zabronił eunuchom uczyć się czytania i angażowania się w politykę. Niezależnie od tego, czy te zakazy były przestrzegane przez cały okres jego panowania, eunuchowie za panowania cesarza Yongle i jego następców zarządzali olbrzymimi warsztatami cesarskimi, dowodzili armiami oraz mieli wpływ na mianowanie i awanse urzędników. Eunuchowie stworzyli niezależną administrację, która nie podlegała administracji cesarskiej, lecz była do niej równoległa[58]. Chociaż już wcześniej kilku eunuchów w czasach dynastii Ming jak Wang Zhen, Wang Zhi i Liu Jin faktycznie dyktatorsko rządziło krajem, nadmierna, tyranizująca kraj wszechwładza eunuchów zaczęła być widoczna od lat 90. XVI wieku, kiedy cesarz Wanli dał im większe uprawnienia niż administracji cesarskiej, a także przyznał im prawo do zbierania podatków w prowincjach[199][200][201].
Eunuch Wei Zhongxian (1568–1627) sprawował faktyczną władzę za panowania cesarza Tianqi (1620–27) i kazał prześladować, torturować i skazywać na śmierć swoich oponentów, przede wszystkim jego krytyków ze sfery wyższych urzędników związanych z Akademią Donglin. Kazał budować świątynie na swoją cześć w całym cesarstwie i budował dla siebie pałace ze środków przeznaczonych na budowę grobowca poprzedniego cesarza. Jego rodzina i znajomi zdobywali ważne stanowiska pomimo braku kwalifikacji. Wei kazał także publikować prace historyczne przedstawiające w złym świetle jego przeciwników[202]. Nieustanne zmiany na dworze stały się regułą, nastąpiło również nasilenie kataklizmów, zarazy, buntów i obcych najazdów. Chociaż cesarz Chongzhen (1627–44) zdymisjonował Wei Zhongxiana (który wkrótce popełnił samobójstwo)[203], problemy z pałacowymi eunuchami trwały aż do upadku dynastii, który nastąpił niecałe dwadzieścia lat później[204].
W ostatnich latach panowania cesarza Wanli i za panowania jego następców, nastąpił kryzys gospodarczy spowodowany nagłym niedoborem głównego środka wymiany gospodarczej cesarstwa: srebra. Protestanckie państwa: Holandia i Anglia, zainicjowały częste najazdy i akty piractwa skierowane przeciwko katolickim królestwom Hiszpanii i Portugalii, w celu ich osłabienia[205]. W tym samym czasie Filip IV Habsburg (1621–65) rozpoczął walkę z przemytem srebra z Meksyku i Peru przez Pacyfik do Chin, wspierając transport amerykańskiego srebra z Hiszpanii do Manili. W 1639, nowy siogunat Tokugawa znacząco ograniczył kontakty handlowe z zagranicą, powodując ograniczenie eksportu srebra z Japonii. Jednak największy wpływ na zmniejszenie napływu srebra do Chin miało ograniczenie handlu z Ameryką, podczas gdy japońskie srebro w niewielkich ilościach nadal napływało do Chin[206]. Niektórzy badacze twierdzą nawet, że cena srebra wzrastała w XVII wieku nie w wyniku zmniejszenia zapasów srebra, lecz w wyniku spadku popytu na towary[207].
Te zdarzenia spowodowały gwałtowny wzrost wartości srebra, co z kolei spowodowało wzrost realnych obciążeń podatkowych i w większości prowincji zapłata podatków okazała się prawie niemożliwa. Ludzie zaczęli gromadzić srebro co powodowało, że było go mniej w obrocie, co z kolei powodowało szybki spadek wartości monet miedzianych w relacji do srebrnych. W latach trzydziestych XVII wieku, tysiąc miedzianych monet stanowiło równowartość uncji srebra; do 1640 ich wartość spadła do pół uncji, a w 1643 do 1/3 uncji[190]. Dla chłopów oznaczało to katastrofę gospodarczą, ponieważ płacili podatki w srebrze, a za swoje plony otrzymywali monety miedziane[208].
Na początku XVII wieku północne Chiny dotykały klęski głodu spowodowane suszami i oziębieniem klimatu, co skróciło okres wegetacyjny. Zmiana klimatu była częścią okresu ochłodzenia znanego jako mała epoka lodowa[209]. Głód, obok podwyżki podatków, powszechnych dezercji, upadku systemu opieki społecznej i klęsk żywiołowych takich jak powodzie, a także niezdolność rządów do budowy i naprawy systemów nawadniających i przeciwpowodziowych, był przyczyną śmierci wielu osób i rozpadu struktury społecznej[209]. Władze centralne nie dysponowały wystarczającymi zasobami i mogły jedynie w niewielkim stopniu łagodzić skutki tych nieszczęść. Co gorsza w Chinach wybuchła epidemia, obejmując obszar od Zhejiangu do Henanu i powodując śmierć dużej liczby ludności[210]. W 1556, w Shaanxi, za panowania cesarza Jiajinga, miało miejsce najbardziej katastrofalne w skutkach trzęsienie ziemi jakie odnotowano w historii, w wyniku którego zginęło około 830 tysięcy osób[211].
Nurhaczy (1559–1626), wódz jednego z plemion dżurdżeńskich, skonsolidował wszystkie dżurdżeńskie plemiona (mongolskiej i tungusko-mandżurskiej grupy językowej) przyczyniając się walnie do wyodrębnienia narodowości mandżurskiej i powstania silnego państwa w Mandżurii opartego na związkach rodowo-plemiennych. Podczas japońskiej inwazji na Koreę (1592-98) zaoferował wsparcie wojsk koreańskich i chińskich przeciwko Japończykom. Oferta została odrzucona, lecz w zamian za ten gest został nagrodzony przyznaniem honorowych tytułów chińskich. Wykorzystując osłabienie Chin, podbił plemiona sąsiadujące z jego państwem, na północ od Wielkiego Muru[212]. W 1616 ogłosił niezależność od cesarstwa Ming; w 1618 ogłosił manifest Siedem krzywd (chiń. 七大恨; pinyin Qī Dà Hèn mandż. nadan koro), w którym opisał działania prowadzone przez cesarstwo przeciwko Mandżurom, co oznaczało de facto wypowiedzenie wojny[213][214].
Pod dowództwem Yuan Chonghuana (1584–1630) wojska cesarskie skutecznie walczyły z Mandżurami, zwłaszcza w 1626, w bitwie pod Ningyuan (w której Nurhaczy został śmiertelnie ranny) i w 1628. Wojska Yuan Chonghuana ochraniały przełęcz Shanhai, uniemożliwiając Mandżurom jej przejście i zaatakowanie Pekinu. Umiejętnie wykorzystując broń palną, Yuan Chonghuan zdołał zapobiec kolejnym zdobyczom Nurhaczego w basenie rzeki Liao[215]. Chociaż w 1628 został mianowany dowódcą wszystkich wojsk chińskich na pograniczu północno-wschodnim, został stracony w 1630 na podstawie sfingowanych zarzutów o współpracy z Mandżurami[216].
Nie mogąc przekroczyć Wielkiego Muru, Mandżurowie czekali na stosowny moment, rozwijając artylerię i pozyskując sojuszników. Zdołali pozyskać jako swoich doradców szereg chińskich generałów i urzędników. Część żołnierzy chińskich zdezerterowała do Mandżurów. W 1632 Mandżurowie podbili większość południowej Mongolii[215], co spowodowało zakrojony na szeroką skalę zaciąg Mongołów do wojsk mandżurskich i dało im szerszy dostęp do Chin właściwych.
W 1636 mandżurski władca Hong Taiji zmienił nazwę dynastii z „Późniejsza Jin” na „Qing” w Mukdenie, który został zdobyty przez Mandżurów w 1621 i został ich stolicą w 1625[217][218]. Hong Taiji przyjął również chiński tytuł cesarza i imię Chongde (chiń. 崇德; pinyin Chóngdé) oraz zmienił nazwę swojego ludu z Dżurdżenów na Mandżurów[218][219]. W 1638, przy pomocy 100-tysięcznej armii, pokonał i narzucił zwierzchnictwo koreańskiej dynastii Joseon, tradycyjnemu sojusznikowi Chin. Wkrótce potem Koreańczycy wypowiedzieli posłuszeństwo dynastii Ming[219].
Li Zicheng (1606–45), żołnierz będący z pochodzenia chłopem, we wczesnych latach trzydziestych XVII wieku zbuntował się wraz ze swoimi towarzyszami broni w zachodnim Shaanxi po tym, jak żołnierze nie otrzymywali niezbędnego zaopatrzenia[209]. W 1634 został schwytany przez wojsko cesarskie i ułaskawiony pod warunkiem, że wróci do służby[220]. Porozumienie zostało wkrótce złamane po tym, jak władze lokalne straciły jego 36 żołnierzy; w odwecie oddziały Li Zichenga zabiły urzędników i kontynuowały rebelię w Henanie w 1635[221]. Do lat czterdziestych, były żołnierz i rywal Li Zichenga – Zhang Xianzhong (1606–47) – stworzył stałą bazę powstańczą w Chengdu, w prowincji Syczuan, podczas gdy bazą Li Zichenga była prowincja Hubei, a jego wpływy rozciągały się na prowincje Shaanxi i Henan[221].
W 1640 głodujący chińscy chłopi nie byli w stanie płacić podatków i przestali bać się ponoszących często klęski wojsk cesarskich, zaczęli masowo przyłączać się do powstania. Wojska cesarskie próbujące bezskutecznie pokonać wojska mandżurskie na północy i ogromną armię zbuntowanych chłopów na południu, dosłownie się rozpadły. Nieopłacana i głodna armia cesarska została pokonana przez Li Zichenga – obecnie samozwańczego księcia Shun – i opuściła stolicę bez większych walk[222]. Oddziały Li Zichenga wtargnęły do miasta po tym, jak jego bramy zostały zdradziecko otwarte od wewnątrz[222]. Pekin został zajęty przez wojska Li Zichenga, a ostatni cesarz dynastii Ming popełnił samobójstwo[222].
Wykorzystując sprzyjające im okoliczności, Mandżurowie przekroczyli Wielki Mur Chiński po tym, jak generał armii cesarskiej, Wu Sangui (1612–1678), otworzył im bramy w Shanhaiguan. Stało się to wkrótce po tym, jak dowiedział się o losie stolicy i otrzymał informację, że wojska Li Zichenga zbliżają się w jego kierunku. Odrzucił ofertę Li Zichenga i sprzymierzył się z Mandżurami[223]. Po zniszczeniu wysłanej przez Li Zichenga armii w bitwie na przełęczy Shanhai Mandżurowie pod wodzą księcia Dorgona (1612–50) i wojska Wu Sangui wyruszyły w kierunku stolicy. Armia Li Zichenga uciekła ze stolicy, a dwa dni później wkroczyły do niej wojska Dorgona i Wu Sangui. Mandżurski cesarz Shunzhi został ogłoszony cesarzem Chin. Po ucieczce przed Mandżurami z Xi’anu, ścigany wzdłuż rzeki Han, aż do Wuchangu i w końcu wzdłuż północnych granic prowincji Jiangxi, Li Zicheng zmarł tam w lecie 1645; w ten sposób nastąpił koniec dynastii Shun. Jedne świadectwa mówią, że popełnił samobójstwo; inne, że został pobity na śmierć przez chłopów po tym, jak został przyłapany na kradzieży pożywienia[224]. Zhang Xianzhong został zabity w styczniu 1647 przez wojska mandżurskie, po ucieczce z Chengdu i stosowaniu taktyki spalonej ziemi[225].
Pomimo utraty stolicy (Pekinu) i śmierci cesarza, siły lojalne wobec dynastii Ming nie zostały całkowicie zniszczone. Koncentrowały się one głównie na południu kraju. Jednak było kilku pretendentów do tronu i ich siły były podzielone. Wojska dynastii Qing po kolei pokonywały każdego z nich, aż do 1662, kiedy zginął ostatni przedstawiciel dynastii Ming pretendujący do tytułu cesarskiego: Yongli[226][227]. W 1683 wojska dynastii Qing podbiły Tajwan, likwidując Królestwo Dongning założone przez Zheng Chenggonga, stanowiące ostatni punkt oporu sił lojalnych wobec dynastii Ming[228][229].
Cesarze dynastii Ming przejęli podział kraju na prowincje z czasów dynastii Yuan, a 13 prowincji z czasów dynastii Ming jest prekursorami współczesnych prowincji chińskich. Za czasów dynastii Song największą jednostką podziału administracyjnego był okręg (chiń. 路; pinyin Lù)[230]. Po zajęciu północnych Chin przez Dżurdżenów w 1127, dynastia Song panująca na południu kraju utworzyła cztery na wpół autonomiczne okręgi wojskowe, oparte na podziale terytorialnym i odrębne od nich władze cywilne, które stały się prekursorem administracji prowincji w czasach dynastii Yuan, Ming i Qing[231]. Wzorując się na podziale z czasów dynastii Yuan, w czasach dynastii Ming w każdej z prowincji była oddzielna administracja cywilna, wojskowa i kontrolna (cenzorat). Prowincje (chiń. 省; pinyin Shěng) dzieliły się na prefektury (chiń. 府; pinyin Fǔ), na czele których stali prefekci (chiń. 知府; pinyin Zhīfǔ), które z kolei dzieliły się na podprefektury (chiń. 州; pinyin Zhōu), na czele których stali podprefekci. Najmniejszą jednostką podziału administracyjnego był powiat (chiń. upr. 县; chiń. trad. 縣; pinyin Xiàn). Istniały również dwa obszary metropolitalne (chiń. 亰; pinyin Jīng), zwane też Zhili ("[obszar] bezpośrednio rządzony"), wokół Pekinu i Nankinu, które nie należały do żadnej z prowincji[232].
Odchodząc od podstawowego systemu administracji centralnej, znanego jako Trzy departamenty i sześć ministerstw, który istniał od czasów dynastii Han, dynastia Ming miała tylko jeden departament, Sekretariat, który kontrolował sześć ministerstw. Po egzekucji kanclerza Hu Weiyonga w 1380, cesarz Hongwu zlikwidował Sekretariat, Cenzorat i Centralną Komisję Wojskową i przejął osobisty nadzór nad sześcioma ministerstwami i pięcioma regionalnymi okręgami wojskowymi[233][234]. W ten sposób został zlikwidowany jeden ze szczebli administracji rządowej, tylko częściowo przywrócony za panowania jego następców[233]. Wielki Sekretariat (chiń. upr. 内阁; chiń. trad. 內閣; pinyin Nèigé) na początku był zwykłym biurem, które wspomagało cesarza w pracach administracyjnych, lecz bez zatrudniania wielkiego sekretarza, czy kanclerza. Ministerstwa, kierowane przez ministrów i podlegających im dyrektorów pozostawały pod bezpośrednim nadzorem cesarza aż do upadku dynastii Ming[235].
Cesarz Hongwu wysłał swojego następcę w 1391 do Shaanxi, żeby „podróżował i nadzorował” prowincję (chiń. upr. 巡抚; chiń. trad. 巡撫; pinyin Xúnfǔ); w 1421 cesarz Yongle powołał 26 urzędników, aby podróżowali po kraju i kontrolowali oraz nadzorowali administrację. Do 1430 funkcja xunfu została zinstytucjonalizowana. Odtąd zaczął działać ponownie Cenzorat[b] zatrudniający kontrolerów (cenzorów), a później naczelnych kontrolerów. W 1453, „wielkim koordynatorom”, czyli „kontrolerom terenowym” jak odnotował Michael Chang – przyznano tytuł zastępcy lub asystenta naczelnego kontrolera i mieli oni bezpośredni dostęp do cesarza[236]. Podobnie jak za rządów poprzednich dynastii, administracja terenowa była kontrolowana przez podróżujących kontrolerów z Cenzoratu. Kontrolerzy mieli prawo w każdej chwili postawić w stan oskarżenia i odwołać urzędników (w przeciwieństwie do nich przełożeni oceniali podległych urzędników w przeprowadzanych raz na trzy lata ocenach okresowych)[236][237].
Chociaż na początku panowania dynastii Ming następowało przekazywanie uprawnień do prowincji, już w latach dwudziestych XV wieku rozpoczął się proces delegowania urzędników administracji centralnej, aby koordynowali w prowincjach działania administracji wojskowej, cywilnej i kontrolnej, czyli działali jako faktyczni ich gubernatorzy. Później oddelegowanym urzędnikom administracji centralnej podlegały, jako naczelnym dowódcom lub wicekrólom, dwie lub więcej prowincje, co miało na celu ograniczenie zakresu władzy wojskowych na rzecz władz cywilnych[238].
Instytucje rządowe w cesarskich Chinach były zorganizowane w podobny sposób, jednak każda dynastia tworzyła specjalne urzędy i biura, dostosowane do ich szczególnych potrzeb. Za panowania dynastii Ming był to Wielki Sekretariat z Wielkimi Sekretarzami wspierającymi cesarza, zajmującymi się pracą administracyjną i mającymi stosunkowo niską rangę urzędniczą za panowania cesarza Yongle; za panowania cesarza Hongxi (1424-25), aby wzmocnić ich prestiż w kontaktach z innymi urzędnikami, zostały im nadane najwyższe, niefunkcyjne tytuły urzędnicze np. Wielkiego Nauczyciela[239]. Wielcy Sekretarze wywodzili się z Akademii Hanlin i byli uważani za przedstawicieli cesarza, a nie władzy ministerialnej czy administracyjnej (z tego powodu czasami mieli inne zdanie zarówno od cesarza jak i ministrów)[240]. Wielki Sekretariat koordynował pracę administracji i ministerstw[c], podczas gdy sześć ministerstw – Kadr (Służby Państwowej), Finansów, Ceremonii, Wojny (Wojska), Sprawiedliwości i Robót Publicznych – było organami administracji państwa[241]. Ministerstwo Kadr (Służby Państwowej) było odpowiedzialne za nominacje, oceny okresowe, awanse służbowe i degradacje urzędników, jak również za przyznawanie tytułów honorowych (z którymi związane było najczęściej odpowiednie wynagrodzenie)[242]. Ministerstwo Finansów było odpowiedzialne za spisy ludności i podatkowe, zbieranie podatków i innych dochodów państwowych, podlegały mu też dwa biura zarządzające mennicami[243]. Ministerstwo Ceremonii było odpowiedzialne za organizację uroczystości państwowych i religijnych, składanie ofiar; nadzorowało również duchowieństwo buddyjskie i taoistyczne oraz odpowiadało za przyjmowanie posłów z państw trybutarnych[244]. Ministerstwo Wojny (Wojska) było odpowiedzialne za nominacje, awanse i degradacje wojskowych, utrzymanie instalacji wojskowych, dostawy broni i zaopatrzenia, a także za utrzymanie systemu kurierskiego[245]. Ministerstwo Sprawiedliwości było odpowiedzialne za nadzór nad sprawami sądowymi i wymierzaniem kar, ale nie nadzorowało Cenzoratu i Wielkiego Trybunału Apelacyjnego[246]. Ministerstwo Robót Publicznych było odpowiedzialne za państwowe projekty budowlane, zatrudnianie rzemieślników i pracowników tymczasowych, produkcję materiałów dla administracji państwowej, utrzymanie dróg i kanałów, standaryzację miar i wag, a także za gromadzenie zapasów żywności[246].
Przy obsłudze dworu cesarskiego zatrudnieni byli prawie wyłącznie eunuchowie i kobiety[247]. Służące pracowały dla biur: służby pałacowej, ceremonii, odzieży, żywności, sypialni, rękodzieła i nadzoru nad służbą[247]. Począwszy od lat dwudziestych XV wieku eunuchowie zaczęli przejmować stanowiska zajmowane dotychczas przez kobiety, aż pozostało przez nie obsadzone tylko biuro ds. odzieży wraz z czterema biurami pomocniczymi[247]. Za panowania cesarza Hongwu eunuchowie byli zatrudnieni w Dyrekcji Służby Pałacowej, lecz wraz ze wzrostem ich znaczenia, zostały utworzone dodatkowe urzędy[247]. Ostatecznie było to: dwanaście dyrekcji, cztery urzędy i osiem biur[247]. Przy obsłudze dworu cesarskiego było zatrudnionych tysiące eunuchów, którymi zarządzała Dyrekcja Służby Pałacowej. Eunuchowie podlegali różnym dyrekcjom (chiń. upr. 监; chiń. trad. 監; pinyin Jiān), które były odpowiedzialne za nadzór nad personelem, ceremonie i rytuały, żywność, przedmioty użytku domowego, dokumenty, stajnie, pieczęcie, odzież, itp.[48]. Urzędy (chiń. 司; pinyin Sī) były odpowiedzialne za zapewnienie opału, papieru, muzyki i za kąpiele[48]. Biura (chiń. 局; pinyin Jú) odpowiadały za broń, wyroby ze srebra, wyroby z brązu, pranie, nakrycia głowy, produkcję odzieży, winnice i ogrody[48]. Czasami najbardziej wpływowy eunuch w Dyrekcji Ceremonii de facto rządził cesarstwem[248].
Istniały również urzędy administracji cesarskiej ściśle współpracujące ze służbą pałacową np. Urząd Pieczęci, który zajmował się wytwarzaniem i konserwacją cesarskich pieczęci, datowników i różnego rodzaju pieczątek[249]. Istniały również biura zajmujące się sprawami członków rodziny cesarskiej[250].
Po okresie panowania cesarza Hongwu – który w latach 1373-84 obsadzał urzędy urzędnikami zatrudnianymi wyłącznie na podstawie rekomendacji – urzędników-uczonych (mandarynów), którzy zasilali szeregi biurokracji, wyłaniano w drodze systemu egzaminów, który ukształtował się za panowania dynastii Sui (581–618)[252][253][254]. Teoretycznie dzięki temu systemowi każdy mógł znaleźć się w szeregach urzędników-uczonych (chociaż początkowo nie mogły w nich uczestniczyć osoby wywodzące się z klasy kupców); w rzeczywistości czas i pieniądze potrzebne do przygotowania się do egzaminów powodowały, że kandydaci najczęściej pochodzili z klasy właścicieli ziemskich (gentry). Rząd zatrudniając urzędników, określał ilu spośród nich musi pochodzić z poszczególnych prowincji[255]. Miało to przeciwdziałać koncentracji władzy przez gentry pochodzącą z zamożniejszych regionów, gdzie szkolnictwo było na wyższym poziomie[256]. Rozwój techniki drukarskiej w czasach dynastii Song pozwolił na upowszechnienie wykształcenia i tym samym zwiększenie liczby kandydatów przystępujących do egzaminów ze wszystkich prowincji[257]. Dla młodych uczniów były drukowane tabliczki mnożenia i elementarze; dla dorosłych kandydatów do egzaminów były produkowane masowo, niedrogie tomy klasyków konfucjańskich i zbiory prawidłowych odpowiedzi egzaminacyjnych[258].
Podobnie jak we wcześniejszych okresach, egzaminy skupiały się na znajomości klasycznych tekstów konfucjańskich[252], a podstawą programową był czteroksiąg konfucjański opracowany wraz z komentarzem przez Zhu Xi w XII wieku[259]. Za panowania dynastii Ming zdanie egzaminów stało się jeszcze trudniejsze, gdy od 1487 wymagano napisania ośmioczęściowego eseju o sztywnej strukturze baguwen (chiń. 八股文; pinyin bāgǔwén), który odbiegał od ówczesnych trendów językowych i literackich[19][259]. Poziom trudności egzaminów wzrastał na każdym kolejnym szczeblu, a po zdaniu kolejnego egzaminu był przyznawany odpowiedni tytuł. Mandaryni byli podzieleni na dziewięć rang, z których każda dzieliła się na dwa stopnie (najwyższa 1a, najniższa 9b)[260]. Każdej randze i stopniowi odpowiadało określone wynagrodzenie (teoretycznie wypłacane w ryżu)[260]. Osobom, które zdały egzaminy na szczeblu prowincji juren (chiń. upr. 举人; chiń. trad. 舉人; pinyin Jǔrén) i zostały zatrudnione w administracji, przydzielano niższe rangi, tak jak i osobom, które zdały egzaminy na najniższym szczeblu (powiatu i prefektury), czyli shengyuan (chiń. upr. 生员; chiń. trad. 生員; pinyin Shēngyuán) lub xiucai (chiń. 秀才; pinyin Xiùcái); natomiast osoby, które zdały egzaminy centralne otrzymywały tytuł jinshi (chiń. upr. 进士; chiń. trad. 進士; pinyin Jìnshì) i miały zapewnioną wysoką rangę urzędniczą[261][262]. Przez 276 lat panowania dynastii Ming i 90 egzaminów na szczeblu centralnym, przyznano 24 874 tytuły jinshi[261]. Ebrey zauważyła, że „było tylko od 2 do 4 tysięcy osób mających tytuł jinshi jednocześnie w danym okresie, co oznaczało, że 1 osoba z tym tytułem przypadała na 10 tysięcy dorosłych mężczyzn.”[255]. Dla porównania pod koniec XVI wieku było około 100 tysięcy osób z tytułem shengyuan[255].
Maksymalny okres sprawowania urzędu wynosił dziewięć lat, a co trzy lata przełożeni dokonywali okresowej oceny pracy urzędników[263]. W zależności od tej oceny byli oni awansowani o jedną rangę, utrzymywali tą samą rangę lub byli degradowani o jedną rangę. W wyjątkowych przypadkach mogli zostać zwolnieni lub nawet skazani. Tylko urzędnicy administracji centralnej posiadający rangę czwartą lub wyższą nie podlegali obowiązkowej ocenie okresowej, chociaż oczekiwano, że wyznają oni popełnione przez siebie błędy[237]. Poza tym w szkołach powiatowych i prefekturalnych było zatrudnionych ponad 4 tysiące nauczycieli, którzy podlegali ocenie okresowej raz na dziewięć lat. Główny nauczyciel na szczeblu prefektury posiadał rangę 9b[264]. Istniał Urząd nauczania cesarskiego, który nadzorował edukację następcy tronu; na jego czele stał urzędnik mający rangę 3a[250].
Urzędnicy-uczeni (mandaryni), którzy objęli urzędy po zdaniu egzaminów, kierowali znacznie większą rzeszą urzędników, którzy nie posiadali żadnej rangi urzędniczej – niższymi urzędnikami. Na jednego urzędnika posiadającego rangę urzędniczą przypadało czterech urzędników nie posiadających żadnej rangi; Charles Hucker ocenia, że było ich ponad 100 tysięcy w całym cesarstwie. Wykonywali oni prace biurowe w urzędach cesarskich. Nie byli zaliczani do nich strażnicy, posłańcy i tragarze-nosiciele lektyk; niżsi urzędnicy również podlegali ocenom okresowym jak urzędnicy-uczeni i po 9 latach mogli otrzymać niższą rangę urzędniczą[265]. Urzędnicy-uczeni co pewien czas zmieniali stanowisko i przeważnie nie otrzymywali stanowiska w swojej rodzinnej prowincji; tym samym musieli polegać na znajomości lokalnych uwarunkowań oraz współpracy niższych urzędników, którzy wywodzili się z danego terenu[266].
Za panowania dynastii Ming eunuchowie uzyskali bezprecedensową władzę nad sprawami państwa. Jednym z najbardziej efektywnych środków kontroli była tajna policja nazywana od jej siedziby Wschodnim Zakładem na początku panowania dynastii Ming, a później Zachodnim Zakładem. Ta tajna służba była nadzorowana przez Dyrekcję Ceremonii, stąd stojący na jej czele eunuchowie faktycznie kontrolowali sprawy państwa[48]. Eunuchowie również mieli system rang, będący odpowiednikiem rang w administracji cesarskiej, jednak zamiast dziewięciu rang były cztery[267].
Książęta i potomkowie pierwszych cesarzy z dynastii Ming otrzymywali duże posiadłości i tytularne dowództwa wojskowe. Te posiadłości nie były lennami, książęta nie pełnili żadnych funkcji administracyjnych i tylko za panowania dwóch pierwszych cesarzy sprawowali faktyczne dowództwo nad wojskami[268]. Dla porównania książęta z dynastii Han i Jin byli lennikami cesarza. Chociaż książęta nie służyli w administracji państwowej, książęta, małżonkowie księżniczek i osoby spokrewnione z cesarzem byli zatrudnieni w Urzędzie Rodziny Cesarskiej, który zajmował się sprawami dotyczącymi tej rodziny[250].
Podobnie jak urzędnicy-uczeni posiadający system rang urzędniczych, wojskowi również posiadali hierarchiczny system stopni wojskowych i podlegali co pięć lat ocenie okresowej[269]. Jednak wojskowi cieszyli się mniejszym prestiżem niż urzędnicy-uczeni. Wynikało to z tego, że służba w wojsku była dziedziczna (a nie opierała się tylko na kryterium merytorycznym), a także z zasad konfucjanizmu, który wyżej cenił osoby, które wybrały drogę wiedzy (wen), niż osoby, które wybrały drogę przemocy (wu)[269][270]. Chociaż służba wojskowa była uważana za mniej prestiżową, od 1478 były przeprowadzane egzaminy dla oficerów, sprawdzające ich umiejętności wojskowe[271]. Na początku panowania dynastii Ming wyższe stanowiska wojskowe były obsadzane przez przedstawicieli arystokracji; stopniowo zaczęli ich zastępować ludzie wywodzący się z niższych warstw społeczeństwa[272].
Podobnie jak za wcześniejszych dynastii, panowanie dynastii Ming było okresem rozkwitu sztuki, zarówno malarstwa, poezji, muzyki, literatury, jak i przedstawień scenicznych. Okres ten charakteryzowało odwoływanie się do przeszłości, co powodowało, że nie czyniono wysiłków wykroczenia poza istniejące dotychczas wzorce (rozwój bez postępu). Szczególnie było to widoczne w malarstwie i poezji[273][274].
Wzory rzeźbione w lace i wzory z glazury na porcelanie dorównywały stopniem szczegółowości i złożoności obrazom tworzonym przez artystów. Znajdowały się one w domach ludzi zamożnych wraz z haftowanymi tkaninami z jedwabiu, wyrobami z nefrytu, jadeitu, kości słoniowej i emalii komórkowej (cloisonné). Znajdowały się w nich również meble z hebanu i kompozycje ażurowe. Materiały pisarskie znajdujące się w gabinecie osoby wykształconej, w tym misternie rzeźbione trzonki pędzelków pisarskich wykonane z kamienia lub drewna, były projektowane i rozmieszczone z uwzględnieniem ich walorów estetycznych[275].
Pod koniec panowania dynastii Ming kolekcjonowano głównie rzeczy charakteryzujące się wyrafinowanym smakiem artystycznym, co dawało zatrudnienie handlarzom sztuki, a nawet fałszerzom, którzy wytwarzali falsyfikaty i przypisywali autorstwo stworzonych przez siebie dzieł sztuki, sławnym artystom[275]. Zauważył to nawet jezuita Matteo Ricci podczas swojego pobytu w Nankinie, pisząc, że chińscy fałszerze dzieł sztuki są bardzo pomysłowi i osiągają ogromne zyski[276]. Istniały również poradniki dla nowych, przezornych kolekcjonerów; Liu Tong w książce wydanej w 1635, przekazał czytelnikom sposoby na rozróżnienie oryginałów od falsyfikatów[277]. Ujawnił, że np. brązy z okresu panowania cesarza Xuande (1425-35) posiadają charakterystyczny połysk; natomiast porcelanę z ery Yongle (1402-25) można rozpoznać po grubości ścianek[278].
Panowanie dynastii Ming było okresem rozwoju chińskiej literatury. Xu Xiake (1587–1641) publikował swoje Dzienniki z podróży (mające 404 tysiące znaków pisma chińskiego) zawierające informacje na temat odwiedzanych miejsc, od opisu okolicy do mineralogii[279][280]. Pierwsze wzmianki o wydawanych w Pekinie przez osoby prywatne gazetach pochodzą z 1582; do 1638 gazety zaczęły być drukowane czcionką ruchomą, zamiast techniką drzeworytniczą[281]. W odpowiedzi na zapotrzebowanie ze strony kupców pod koniec panowania dynastii Ming powstawały przewodniki z zakresu etyki biznesu[282]. Chociaż krótkie opowiadania były popularne już w czasach dynastii Tang (618–907)[283], a tacy autorzy jak Xu Guangqi, Xu Xiake i Song Yingxing tworzyli dzieła o charakterze encyklopedycznym i technicznym, panowanie dynastii Ming było okresem rozwoju chińskiej powieści. Podczas gdy członkowie gentry posiadali wystarczające wykształcenie by w pełni rozumieć dzieła w klasycznym języku chińskim, osoby nie posiadające klasycznego wykształcenia – kobiety z zamożnych domów, kupcy, sprzedawcy – stały się głównymi odbiorcami twórczości w języku potocznym[284]. Opublikowana w 1610 powieść Jin Ping Mei (pol.: Kwiaty śliwy w złotym wazonie) jest uważana przez niektórych za piątą wielką powieść cesarskich Chin, w nawiązaniu do czterech klasycznych powieści chińskich. Trzy z tych powieści, Opowieści znad brzegów rzek, Opowieści o Trzech Królestwach i Wędrówka na Zachód, zostały ostatecznie zredagowane w czasach dynastii Ming[285]. Sztuki teatralne tego okresu dorównywały powieściom. Jednym z najbardziej znanych chińskich utworów teatralnych jest mająca 55 aktów Altana peoniowa, autorstwa Tang Xianzu (1550–1616)[286].
W przeciwieństwie do Xu Xiake, który w swoich Dziennikach z podróży koncentrował się na opisywaniu odwiedzanych miejsc, chiński poeta i urzędnik Yuan Hongdao (1568–1610) traktował zapiski z podróży jako sposób wyrażenia swojego indywidualizmu, jak również niezadowolenia z polityki dworskiej[287]. Yuan Hongdao pragnął uwolnić się od moralnych kompromisów, które były nierozerwalnie związane z jego karierą urzędniczą. W tym zakresie był kontynuatorem twórczości poety i urzędnika Su Shi (1037–1101) z czasów dynastii Song[288]. Yuan Hongdao i jego bracia, Yuan Zongdao (1560–1600) i Yuan Zhongdao (1570–1623), byli założycielami szkoły listów Gong’an[289]. Ta wysoce indywidualistyczna szkoła poezji i prozy była krytykowana za koncentrację na zmysłowych przeżyciach wewnętrznych, co łączyło ją z powieściami w języku potocznym z tego okresu, takimi jak Jin Ping Mei[289]. Jednak nowa romantyczna literatura oddziaływała również na gentry i urzędników-uczonych, szukających w kochankach również romantycznej miłości, której najczęściej nie mogło im zapewnić aranżowane małżeństwo[290].
Najbardziej znanymi artystami z okresu dynastii Ming w dziedzinie sztuk plastycznych byli Ni Zan, Shen Zhou, Tang Yin, Wen Zhengming, Qiu Ying i Dong Qichang. Czerpali oni z technik, wzorów i wyrafinowania w malarstwie osiągniętego przez ich poprzedników z czasów poprzednich dynastii, nie wykraczając poza nie, zarówno pod względem tematów, jak i technik malarskich. Znani artyści z tej epoki mogli utrzymać się wyłącznie z malowania obrazów, dzięki wysokim cenom i dużemu popytowi na ich twórczość. Np. Qiu Ying otrzymał 2,8 kg (100 uncji) srebra za obraz na 80. urodziny matki bogatego nabywcy. Znani artyści mieli wielu naśladowców, z których część była amatorami, którzy malowanie traktowali jako hobby, a część zarabiała na życie dzięki malarstwu[291][292].
Poza malarzami również niektórzy inni twórcy byli cenieni dzięki swojej twórczości, jak He Chaozong z początku XVII wieku, znany dzięki swoim rzeźbom z białej porcelany. Głównymi ośrodkami produkcji porcelany w czasach dynastii Ming były Jingdezhen w prowincji Jiangxi i Dehua w prowincji Fujian. Od XVI wieku porcelana wytwarzana w Dehua, przeznaczona na rynek europejski, była dostosowywana do gustów europejskich nabywców. Chuimei Ho w The Ceramic Trade in Asia szacuje, że pod koniec panowania dynastii Ming około 16% chińskiego eksportu porcelany było przeznaczone na rynek europejski, a reszta na rynek japoński i do Azji Południowo-Wschodniej[187].
W czasach dynastii Ming Chińczycy czcili wiele bóstw należących do ludowego panteonu chińskiego, jak również byli buddystami, taoistami i konfucjanistami[293][294].
Pod koniec panowania dynastii Ming do Chin przybyli pierwsi jezuiccy misjonarze z Europy, w tym Matteo Ricci i Nicolas Trigault. Swoich misjonarzy wysyłali również dominikanie i franciszkanie[295][296].
Matteo Ricci pracował wraz z chińskim matematykiem, astronomem i agronomem Xu Guangqi nad przekładem na język chiński greckiego traktatu matematycznego Elementy Euklidesa. Chińczycy byli pod wrażeniem znajomości przez Europejczyków astronomii, pomiaru czasu, matematyki, mechaniki i geografii. Aby zdobyć zaufanie i poważanie wśród Chińczyków, większość europejskich misjonarzy prezentowało się raczej jako uczeni niż kapłani[295]. Jednak większość Chińczyków była nieufna, a nawet otwarcie wroga wobec chrześcijaństwa, które było odmienne od chińskich wierzeń i praktyk religijnych[295]. Punktem kulminacyjnym tej niechęci był Nankiński Incydent Religijny z lat 1616–22, chwilowe zwycięstwo konfucjańskich tradycjonalistów, którzy doprowadzili do odrzucenia europejskiej nauki i misjonarzy, na rzecz górującego nad nimi modelu chińskiego; wkrótce jednak europejscy misjonarze zostali zatrudnieni w cesarskim obserwatorium astronomicznym[297].
W Chinach mieszkali również Żydzi, których głównym ośrodkiem był Kaifeng; Matteo Ricci poznał historię Żydów w Chinach, gdy spotkał jednego z nich w Pekinie[298]. Początki islamu w Chinach sięgają VII wieku; za panowania dynastii Ming muzułmaninem był między innymi słynny chiński podróżnik – admirał Zheng He. Muzułmanami byli również dowódcy w armii cesarza Hongwu: Chang Yuchun, Lan Yu, Ding Dexing i Mu Ying[299].
Za panowania dynastii Ming, poglądy Zhu Xi (1130–1200) i neokonfucjanizm były powszechnie akceptowane przez dwór cesarski i ogół wykształconych Chińczyków. Jednak wśród intelektualistów nigdy nie występuje całkowita akceptacja jednego sposobu myślenia. Również za panowania dynastii Ming były jednostki, które – jak Su Shi (1037–1101) za panowania dynastii Song – nie zgadzały się z powszechnie przyjmowanymi poglądami i nie obawiały się ich kwestionować. Czołowym przedstawicielem nowych trendów w konfucjanizmie był urzędnik-uczony Wang Yangming (1472–1529), którego krytycy oskarżali o to, że jego poglądy zostały skażone przez poglądy buddyjskiej szkoły chan[300].
Analizując koncepcję Zhu Xi „samodoskonalenia” (czyli zdobywania wiedzy i poszerzania poznania poprzez staranne i racjonalne badanie rzeczy i zdarzeń (chiń. upr. 理学; chiń. trad. 理學; pinyin Lǐxué; chiń. 格物致知; pinyin Géwù zhìzhī)), Wang Yangming doszedł do wniosku, że uniwersalne „zasady” są czymś, co może znajdować się w umyśle każdego człowieka[301]. Kwestionując dotychczasowe poglądy, Wang Yangming twierdził, że każdy, niezależnie od statusu społecznego czy wykształcenia, może stać się równie mądry jak starożytni mędrcy Konfucjusz i Mencjusz, a ich pisma nie są źródłem prawdy, lecz tylko przewodnikami, które mogą mieć nieścisłości[302]. Według Wang Yangminga rolnik, który posiada bogate doświadczenie życiowe i wyciąga z niego wnioski jest mądrzejszy niż urzędnik, który uważnie studiował dzieła klasyków, a nie ma doświadczenia życiowego[302].
Konserwatywni urzędnicy obawiali się dokonanej przez Wang Yangminga interpretacji zasad konfucjanizmu, rosnącej liczby jego zwolenników i buntowniczego przesłania jego idei[300]. Aby ograniczyć jego wpływy był on często wysyłany daleko poza stolicę, by zajmował się zwalczaniem powstań i sprawami wojskowymi[300]. Pomimo to jego idee przeniknęły do głównego nurtu myśli chińskiej i wywołały zainteresowanie buddyzmem i taoizmem[300]. Ponadto ludzie zaczęli kwestionować istniejącą hierarchię społeczną i zasadę, że urzędnicy-uczeni stoją na jej szczycie[300]. Zwolennik Wang Yangminga i pracownik kopalni soli Wang Gen przekonywał zwykłych ludzi o znaczeniu edukacji dla poprawy ich życia, a He Xinyin kwestionował znaczenie i rolę rodziny w chińskim społeczeństwie[300]. Współczesny mu Li Zhi (1527–1602) nauczał nawet, że kobiety mają takie same możliwości intelektualne jak mężczyźni i mają prawo do wykształcenia; zarówno Li Zhi i He Xinyin zmarli w więzieniu, uwięzieni pod zarzutem szerzenia „niebezpiecznych idei”[303]. Jednak te „niebezpieczne idee” edukacji kobiet od dawna były wdrażane przez matki dające swoim dzieciom podstawy edukacji[304], jak również przez kurtyzany, które miały podobne wykształcenie jak ich kochankowie[305].
Przeciwnikami liberalnych poglądów Wang Yangminga byli konserwatywni urzędnicy Cenzoratu – urzędu odpowiedzialnego za zwalczanie wykroczeń i nadużyć władzy urzędników – i wyżsi urzędnicy z Akademii Donglin, która została ponownie otwarta w 1604[306]. Chcieli oni odrodzenia ortodoksyjnej etyki konfucjańskiej[306]. Konserwatyści tacy jak Gu Xiancheng (1550–1612) nie zgadzali się z koncepcją wrodzonej wiedzy moralnej Wang Yangminga, twierdząc, że byłaby to prosta droga do uzasadnienia niemoralnych zachowań, takich jak chciwość i dbanie wyłącznie o osobiste korzyści[306]. Te dwie przeciwstawne tendencje powodowały podziały frakcyjne wśród wyższych urzędników, którzy – jak za Wang Anshi i Sima Guanga za panowania dynastii Song – korzystali z każdej okazji do pozbycia się z dworu członków przeciwnej frakcji[306].
Wang Gen mógł propagować swoje idee wśród ludzi z różnych regionów, ponieważ – zgodnie z tendencją widoczną już w czasach dynastii Song – lokalne społeczności stawały się coraz mniej odizolowane od siebie w wyniku wzrostu wymiany handlowej i powstania gęstej sieci małych miasteczek[307]. Zwiększała się liczba szkół, poszerzały kręgi rodzinne, powstawały różnego rodzaju stowarzyszenia, w tym religijne, co powodowało intensyfikację kontaktów międzyludzkich, w tym między ludźmi wykształconymi a chłopami[307]. Jonathan Spence pisze, że za panowania dynastii Ming zacierały się różnice pomiędzy miastami i terenami wiejskimi, ponieważ tereny podmiejskie wraz z gospodarstwami rolnymi znajdowały się tuż poza, a niekiedy nawet wewnątrz murów miejskich[308]. Zacierały się granice nie tylko pomiędzy miastem a wsią, lecz także w ramach tradycyjnego podziału społeczeństwa na cztery grupy społeczne: urzędników, rolników, rzemieślników i kupców (chiń. upr. 士农工商; chiń. trad. 士農工商; pinyin Shìnónggōngshāng); rzemieślnicy czasem pracowali w gospodarstwach rolnych w czasach dobrej koniunktury, a chłopi często wędrowali do miast, żeby znaleźć tam pracę w okresach niedostatku[308].
Wiele zajęć i zawodów przechodziło z ojca na syna lub było dziedziczonych. Należeli do nich m.in. ślusarze, kowale, krawcy, szewcy, kucharze; wytwórcy makaronu, pieczęci, trumien; karczmarze, właściciele herbaciarni, winiarni, domów publicznych, lombardów; drobni handlarze oraz handlarze-bankierzy zaangażowani w proto-bankowy system obracający wekslami[190][309]. W prawie każdym mieście był dom publiczny z żeńskimi i męskimi prostytutkami[310]. Mężczyźni otrzymywali większe pieniądze niż kobiety, ponieważ związki homoseksualne z nastoletnimi chłopcami były uważane za oznakę statusu, pomimo że było to niezgodne z obowiązującymi w tym okresie normami seksualnymi[311]. Łaźnie publiczne były o wiele bardziej rozpowszechnione niż w poprzednich epokach[312]. W miastach sprzedawano różnorodne towary, w tym np. specjalny papier spalany w ofierze przodkom, specjalistyczne towary luksusowe, nakrycia głowy, delikatne tkaniny, różne rodzaje i gatunki herbat[313]. Ludzie z mniejszych miejscowości, które były zbyt biedne lub miały zbyt mało ludności, by zapewnić popyt na towary oferowane przez wyspecjalizowane sklepy lub rzemieślników, nabywali je od domokrążców lub na jarmarkach[308]. Małe miasteczka były także miejscem podstawowej edukacji, nowości i plotek, swatów, uroczystości religijnych, występów wędrownych trup teatralnych, poboru podatków, a także dystrybucji żywności w okresach głodu[308].
Rolnicy na północy kraju uprawiali przede wszystkim pszenicę i proso, podczas gdy rolnicy z terenów położonych na południe od rzeki Huai He zajmowali się uprawą ryżu, a także hodowlą kaczek i ryb w jeziorach i stawach. Uprawa drzew morwy, której liście stanowią jedyny pokarm dla jedwabników morwowych i krzewów herbaty miała miejsce na terenach na południe od Jangcy; dalej na południe uprawiano trzcinę cukrową i owoce cytrusowe[308]. Z górzystych rejonów na południu Chin pozyskiwano drewno bambusowe. Poza pozyskiwaniem drewna bambusowego, biedacy utrzymywali się z wypalania węgla drzewnego, prażenia muszli w celu pozyskania wapna, wypalania ceramiki oraz wyplatania koszy i mat[314]. Na północy kraju do transportu wykorzystywano zazwyczaj konie lub zaprzęgi, podczas gdy na południu, ze względu na obfitość rzek, kanałów i jezior, tańszy i łatwiejszy był transport wodny. Chociaż na południu najczęściej ziemię posiadali bogaci właściciele ziemscy, którzy dzierżawili ją dzierżawcom, mieszkający na północy kraju niezależni chłopi, ze względu na gorszy klimat, żyli na niższym poziomie niż dzierżawcy z południa kraju[315].
W porównaniu do rozkwitu nauki, techniki i technologii w czasach dynastii Song, postęp w tych dziedzinach za panowania dynastii Ming był wolniejszy, szczególnie w porównaniu do tempa rozwoju w krajach cywilizacji zachodniej. Najważniejsze osiągnięcia z drugiej połowy panowania dynastii Ming były zainspirowane kontaktami z Europą. W 1626 Johann Adam Schall von Bell napisał pierwsze opracowanie naukowe w języku chińskim na temat teleskopu Yuanjingshuo; w 1634 cesarz Chongzhen otrzymał teleskop zmarłego jezuity Johanna Schrecka (1576–1630)[316]. Heliocentryczny model budowy wszechświata był odrzucany przez misjonarzy katolickich, ale poglądy Johannesa Keplera i Galileusza powoli przenikały do Chin[317]. Jezuici w Chinach głosili na dworze cesarskim teorię heliocentryczną, chociaż oficjalnie jako zgodną z Biblią Kościół katolicki głosił teorię geocentryczną; dopiero od roku 1865 misjonarze katoliccy w Chinach oficjalnie wspierali model heliocentryczny[318]. Chociaż Shen Kuo (1031–95) i Guo Shoujing (1231–1316) stworzyli podstawy trygonometrii w Chinach, następną ważną pracą z tej dziedziny była publikacja w 1607 tłumaczenia Elementów Euklidesa dokonana przez chińskiego urzędnika i astronoma Xu Guangqi (1562–1633) i włoskiego jezuitę Matteo Ricciego (1552–1610)[319]. Paradoksalnie, niektóre wynalazki, które miały swoje korzenie w starożytnych Chinach, zostały ponownie sprowadzone do Chin z Europy, np. młyn polowy[320].
Kalendarz chiński wymagał reformy, ponieważ długość roku zwrotnikowego określał on na 365 ¼ dnia (jak w kalendarzu juliańskim), co powodowało opóźnienie względem roku zwrotnikowego o 10 minut i 14 sekund rocznie, czyli 1 dzień na około 128 lat[321]. Chociaż dynastia Ming przyjęła kalendarz Shoushi Guo Shoujinga z 1281, który był równie dokładny jak kalendarz gregoriański, Cesarski Urząd Astronomii nie wprowadzał okresowych poprawek; wynikało to prawdopodobnie z braku dostatecznej wiedzy, odkąd urząd ten stał się dziedziczny, a prawo zabraniało osobom prywatnym zajmowania się astronomią[322]. Książę Zhu Zaiyu (1536–1611), potomek w szóstym pokoleniu cesarza Hongxi, przedłożył propozycję skorygowania opóźnienia w 1595, lecz ultrakonserwatywna komisja odrzuciła ją[321][322]. Zhu Zaiyu opisał również system znany jako system równomiernie temperowany, opisany w tym samym czasie w Europie przez Simona Stevina (1548–1620)[323]. Oprócz swoich prac na temat muzyki, opublikował również odkrycia dotyczące kalendarza w 1597[322]. Rok wcześniej, wniosek Xing Yunlu sugerujący ulepszenie kalendarza został odrzucony przez zwierzchnika Cesarskiego Urzędu Astronomii, ze względu na prawo zakazujące osobom prywatnym zajmowania się astronomią; Xing Yunlu pracował później wraz z Xu Guangqi w 1629 nad reformą kalendarza chińskiego i dostosowaniem go do zachodnich standardów[322].
Kiedy założyciel dynastii Ming, cesarz Hongwu zobaczył urządzenia mechaniczne znajdujące się w pałacu dynastii Yuan w Dadu – m.in. fontanny z kulkami tańczącymi na strumieniach wody, automat w kształcie tygrysa, urządzenia w kształcie głowy smoka rozpylające perfumy i zegary mechaniczne nawiązujące do tradycji Yi Xinga (683–727) i Su Songa (1020–1101)  – uznał je za jeden z przejawów dekadencji mongolskich władców i nakazał ich zniszczenie[324]. Zostały one szczegółowo opisane przez urzędnika Ministerstwa Robót Publicznych, Xiao Xuna, który także starannie opisał położenie i wygląd pałacu[325]. Europejscy jezuici Matteo Ricci i Nicolas Trigault pokrótce opisali rodzime chińskie zegary z napędem mechanicznym[326]. Jednak zwrócili uwagę, że mechanizmy europejskich XVI-wiecznych zegarów były o wiele bardziej zaawansowane niż ówczesne zegary chińskie, które podzielili na zegary wodne, zegary ogniowe i „inne mechanizmy... z kołami obracanymi przez piasek, jak gdyby była to woda”[327]. Chińskie źródła – a mianowicie „Historia dynastii Ming” (Ming Shi) – opisują 'pięciokołowy zegar piaskowy', mechanizm wprowadzony przez Zhan Xiyuana (tworzącego w latach 1360–80), który wykorzystał w nim koło wodne (w tym przypadku poruszane siłą naporu piasku, a nie wody) zastosowane w zegarze Su Songa i tarczę zagarową z przesuwającą się nad nią wskazówką, podobnie jak w europejskich zegarach z tego okresu[328]. Ten napędzany piaskiem zegar mechaniczny został ulepszony przez Zhou Shuxue (tworzącego w latach 1530–58), który dodał czwarte duże koło zębate (tym samym zegar miał sześć kół), zmienił przekładnię zębatą i poszerzył otwór przez który przesypywały się ziarna piasku, ponieważ wcześniejszy model był krytykowany za zbyt częste zatykanie się[328].
Chińczycy byli zaciekawieni europejskimi wynalazkami i technologiami, podobnie jak Europejczycy chińskimi. W 1584 Abraham Ortelius (1527–98) przedstawił w swoim atlasie Theatrum orbis terrarum chińską innowację polegającą na zamontowaniu na wozie masztów i żagli, takich jak na dżonkach – czyli żaglowóz[330]. Juan González de Mendoza także wspomniał o tym rok później – opisując nawet wzory na jedwabiu – podczas gdy Gerard Merkator (1512–94) przedstawił je w swoim atlasie, John Milton (1608–74) w jednym z najbardziej sławnych poematów, a Andreas Everardus van Braam Houckgeest (1739–1801) w swoim dzienniku z podróży do Chin[331].
Encyklopedysta Song Yingxing (1587–1666) opisał szeroki zakres procesów technologicznych i przemysłowych w swojej encyklopedii Tiangong Kaiwu z 1637. Opisywał w niej mechaniczne i hydrauliczne urządzenia stosowane w rolnictwie i nawadnianiu[332], typy statków[333][334] i sprzęt do nurkowania dla poławiaczy pereł[335], proces produkcji jedwabiu i tkanin[336], procesy metalurgiczne takie jak techniki wytopu metali i ich stopów oraz hartowanie[337], procesy produkcyjne takie jak prażenie pirytu (ruda – siarczek żelaza) w celu otrzymania siarki, używanej do produkcji prochu – pokazując jak ruda wraz z węglem jest wsypywana do pieca posiadającego głowicę, przez którą siarka w stanie gazowym była przekazywana do odbieralnika, gdzie krzepła i ulegała krystalizacji[338] – a także użycie prochu m.in. w minie morskiej połączonej z brzegiem linką pełniącą funkcję zapłonnika i której wybuch powodowało pociągnięcie linki uruchamiające zamontowany w niej zamek kołowy[339].
W swojej pracy poświęconej rolnictwu, Xu Guangqi (1562–1633) opisywał nawadnianie, nawożenie, zapobieganie klęskom głodu, uprawę roślin[340].
Na początku panowania dynastii Ming istniało wiele różnego rodzaju broni prochowej, ale od połowy panowania dynastii Chińczycy zaczęli często używać europejskich wzorów artylerii i broni palnej[341]. Huolongjing (Podręcznik ognistego smoka) opracowany przez Jiao Yu i Liu Ji (zm. 1375), krótko przed 1375 (z wstępem dodanym przez Jiao Yu w 1412)[342], opisuje wiele rodzajów najnowocześniejszych broni tego okresu. Były to m.in. wydrążona, wypełniona prochem, wybuchająca kula armatnia[343], mina lądowa używająca skomplikowanego mechanizmu zapadek, iglic i zamków kołowych do inicjowania wybuchu, która uruchamiała się pod wpływem nacisku[344], mina morska[339], rakieta z zamocowanymi statecznikami służącymi do stabilizowania jej lotu[345], rakieta wielostopniowa napędzana przez rakietę nośną, przed odpaleniem wielu mniejszych rakiet[346] i armatki mające do dziesięciu luf[347].
Li Shizhen (1518–93) – jeden z najbardziej znanych lekarzy i farmaceutów w historii Chin – działał pod koniec panowania dynastii Ming[348]. W 1578 ukończył pierwszą wersję Encyklopedii korzeni i roślin (chiń. upr. 本草纲目; chiń. trad. 本草綱目; pinyin Běncǎo Gāngmù; Wade-Giles Pen3-ts’ao3 Kang1-mu4), w której opisał 1892 leki stosowane w tradycyjnej medycynie chińskiej[348]. Chociaż podobno wynalezione przez taoistycznych pustelników z Emei Shan pod koniec X wieku, szczepienia pacjentów przeciwko ospie były stosowane na szeroką skalę w Chinach za panowania cesarza Longqinga (1567–72), na długo przedtem niż to miało miejsce w innych krajach[349]. Do higieny jamy ustnej, starożytni Egipcjanie używali prymitywnych szczoteczek z włosiem zwierzęcym na jej końcu, ale w Chinach w 1498 wynaleziono nowoczesną szczoteczkę do zębów, a do jej produkcji używano włosia świni[350].
Historycy nadal spierają się o to ilu faktycznie mieszkańców liczyły Chiny za panowania dynastii Ming. Timothy Brook zwraca uwagę, że dane ze spisów powszechnych są niedoszacowane, ponieważ wynikające z nich zobowiązania podatkowe skłaniały wiele rodzin do zaniżania liczby osób w gospodarstwie domowym, a wielu lokalnych urzędników zaniżało liczbę gospodarstw domowych na podległym im terenie[351]. Statystyki ludności z tego okresu wskazują, że często zaniżano liczbę dzieci, szczególnie dziewczynek[352]. Również liczba kobiet była zaniżana[353]; np. władze prefektury Daming (Północne Zhili) raportowały w 1502, że zamieszkuje ją 378 167 mężczyzn i 226 982 kobiet[28]. Rząd usiłował urealnić dane spisowe dokonując oszacowania przewidywanej średniej liczby osób w gospodarstwie domowym, lecz nie rozwiązywało to powszechnego problemu związanego z unikaniem spisów podatkowych[354].
Zgodnie ze spisem z 1381 ludność Chin liczyła 59  873  305 osób; jednak jej liczba znacznie spadła według danych ze spisu z 1391 ze względu na to, że około 3 miliony osób uniknęło rejestracji[356]. Chociaż zaniżanie danych i unikanie rejestracji było od 1381 zagrożone karą śmierci, wielu ludzi, aby przeżyć, unikało rejestracji i opuszczało swoje miejsca zamieszkania, pomimo wysiłków podejmowanych przez Hongwu, by utrudnić przemieszczanie się ludności. Rząd próbował skorygować to, podając własne ostrożne oszacowanie liczby ludności w 1393; jego zdaniem Chiny zamieszkiwało 60  545  812 ludzi[355]. W swojej książce Studies on the Population of China: 1368-1953 (Studia nad ludnością Chin: 1368-1953), Ho Ping-ti (chiń. 何炳棣; pinyin Hé Bǐngdì) proponuje rewizję faktycznej liczby ludności w 1393 do ponad 65 milionów zauważając, że duże obszary północnych Chin i tereny pograniczne nie były uwzględnione w tym spisie[357]. Brook twierdzi, że według oficjalnych danych po 1393 liczba ludności wynosiła od 51 do 62 milionów, podczas gdy w rzeczywistości liczba ludności wzrastała[355]. Nawet cesarz Hongzhi (1487-1505) spostrzegł, że liczba jego poddanych wzrasta, a liczba zarejestrowanych cywilów i żołnierzy spada[314]. William Atwell twierdzi, że ludność Chin na przełomie XIV i XV wieku mogła wynosić nawet 90 milionów powołując się na Heijdra i Mote[358].
Historycy badają obecnie lokalne gazetery z czasów dynastii Ming w poszukiwaniu wskazówek ukazujących stały wzrost ludności[352]. Korzystając z gazeterów, Brook szacuje, że całkowita liczba ludności za panowania cesarza Chenghua (1464–87) wynosiła około 75 milionów[354], mimo liczby 62 milionów wynikającej ze spisów[314]. Podczas gdy władze prefektur z całego cesarstwa, w połowie panowania dynastii Ming, donosiły o spadku lub stagnacji liczby ludności, lokalne gazetery donosiły o ogromnych rzeszach napływowych pracowników, dla których nie było wystarczającej ilości ziemi, którą mogliby uprawiać, co powodowało, że wielu z nich stawało się włóczęgami, przestępcami lub zajmowało się wycinaniem lasów, co przyczyniało się do deforestracji okolicy[359]. Cesarze Hongzhi i Zhengde złagodzili kary wobec osób, które uciekły ze swoich stron rodzinnych, a cesarz Jiajing (1521–67) kazał już tylko rejestrować wędrownych kupców i pracowników w celu zwiększenia wpływów podatkowych[28].
Nawet reformy cesarza Jiajinga nakazujące zaniechanie kar i rejestrację wędrownych handlarzy i pracowników nie spowodowały, że pod koniec panowania dynastii Ming spisy ludności pokazywały prawidłowo ogromny wzrost liczby ludności. Odnotowywały to gazetery z całego obszaru cesarstwa i sporządzały własne szacunki liczby ludności. W niektórych przypuszczano, że od 1368 liczba ludności podwoiła się, potroiła lub nawet była pięć razy większa[360]. Fairbank szacuje, że Chiny pod koniec panowania dynastii Ming liczyły prawdopodobnie 160 milionów mieszkańców[361], Brook, że 175 milionów[360], a Ebrey stwierdza, że mogło to być nawet 200 milionów[135]. Jednak wielka epidemia, która rozpoczęła się w północno-zachodnich Chinach w 1641, spustoszyła gęsto zaludnione tereny wzdłuż Wielkiego Kanału; gazeter z północnego Zhejiangu odnotował, że ponad połowa mieszkańców zachorowała w tym roku, a na niektórych obszarach do 1642 zmarło nawet 90% mieszkańców[362].
Lista cesarzy z dynastii Ming[363][364][365][366]:
U góry zostało podane nazwisko rodowe Zhu (朱) oraz imię osobiste, każdego z wymienionych w tabeli członków rodziny cesarskiej. Nazwisko rodowe rodziny cesarskiej to Zhu (朱), lecz założyciel dynastii jako jej nazwę przyjął Ming (明)[8].
W Chinach cesarze z dynastii Ming nazywani są najczęściej ich imieniem świątynnym[367], które zostały podane jako drugie od góry (cesarz Jianwen nie otrzymał imienia świątynnego, zamiast tego zostało podane jego imię pośmiertne: Huidi (惠帝)). Zhu Biao i Zhu Youyuan nigdy nie panowali, zostali pośmiertnie podniesieni do rangi cesarzy przez ich synów oraz zostały im nadane imiona świątynne, które są podane jako drugie od góry. Członkowie rodziny cesarskiej, którzy zostali cesarzami, zostali oznaczeni tekstem pogrubionym oraz zostały podane ich lata panowania (jako trzecie od góry). Nazwy oznaczone tekstem pogrubionym są przyjętymi przez nich nazwami er panowania, pod którymi są określani najczęściej w literaturze europejskiej. Wstępując na tron cesarz nadawał swoim rządom nazwę (np. Hongwu), która nie zmieniała się w okresie jego panowania, dlatego odnosi się ona do okresu jego panowania, a nie do osoby (cesarz Zhu Qizhen panujący w latach 1435-49 i 1457-64, miał dla każdego z tych okresów inną nazwę ery panowania i dlatego w literaturze jest nazywany swoim nazwiskiem rodowym (Zhu) i imieniem osobistym (Qizhen)[368][369]. Po upadku dynastii Ming w 1644 i samobójstwie cesarza Chongzhena, szereg członków rodziny cesarskiej z południa Chin zgłaszało pretensje do korony cesarskiej, ich rządy określane są pod nazwą Południowa dynastia Ming[226]. Dla tych pretendentów podano nazwy ich er panowania wraz z horyzontem czasowym ich rzekomego panowania jako cesarzy Chin podanym w nawiasach. Ostatni z nich Yongli został stracony w 1662[370]. W 1662 Zheng Chenggong (Koksinga) zdobył, znajdujący się wówczas pod kontrolą holenderską, Tajwan i założył na wyspie Królestwo Dongning lojalne wobec dynastii Ming[226]. Królestwo przetrwało do 1683, kiedy zostało podbite przez wojska dynastii Qing[228].
Źródła: [371][372][365][366]
Wielki kryzys (ang. Great Depression) – największy kryzys gospodarczy w historii kapitalizmu, który miał miejsce w latach 1929–1933 i objął praktycznie wszystkie kraje (oprócz ZSRR) oraz praktycznie wszystkie dziedziny gospodarki[1].
Tradycyjnie przyjmuje się, iż kryzys ujawnił się w Stanach Zjednoczonych, po tzw. czarnym czwartku (a według niektórych źródeł już kilka miesięcy wcześniej), czyli po tzw. panice na giełdzie nowojorskiej na Wall Street 24 października 1929, kiedy to, zgodnie z zachowanymi informacjami, gwałtownie spadły ceny praktycznie wszystkich akcji, pociągając za sobą łańcuch bankructw i zadłużenia, które rozprzestrzeniły się stopniowo na prawie wszystkie kraje (oprócz wspomnianego ZSRR). Skutkiem kryzysu była też utrata pracy przez miliony ludzi – w USA bezrobocie sięgnęło, według dostępnych danych, 1/3 siły roboczej. Spadek produkcji przemysłowej sięgnął w niektórych krajach 50% (Polska, USA), a szczególnie silnie odczuło kryzys rolnictwo. Wolumen handlu światowego zmalał z 3 mld (ówczesnych) dolarów w 1929 do mniej niż 1 mld w 1933[2]. Poprawa sytuacji gospodarczej nastąpiła w 1933 roku.
Kryzys został pogłębiony i przedłużony przez wymienialność walut na złoto (standard złota). W przypadku waluty wymienialnej na złoto i wystąpienia deficytu handlowego, niezrównoważonego napływem kapitału z zagranicy, następował odpływ złota z rezerw banku centralnego. Aby zapobiec utracie rezerw bank centralny podnosił stopę procentową, co powodowało obniżenie podaży kredytu. W rezultacie kraje, które zdewaluowały swoje waluty w 1931, miały wyraźnie wyższą produkcję przemysłową w 1934 niż w 1929 – w ostatnim roku przed kryzysem. Kraje złotego bloku, do którego wchodziła Polska, miały produkcję przemysłową niższą o 22% niż przed kryzysem. W 1936 różnica ta wynosiła 41 punktów procentowych: odpowiednio 127% poziomu z 1929 w krajach po dewaluacji w 1931 i 86% w krajach złotego bloku[3].
Skutkiem wielkiego kryzysu, według oceny większości historyków[4], było m.in. dojście Hitlera do władzy w Niemczech, gdzie poziom bezrobocia przekroczył 30%[3]. Kraje, które porzuciły wymienialność na złoto mogły prowadzić ekspansywną politykę monetarną i fiskalną, w sytuacji kryzysu bez niebezpieczeństwa inflacji, zwiększający popyt krajowy uruchamiający niewykorzystywane w kryzysie moce produkcyjne. W Stanach Zjednoczonych skutki kryzysu zostały złagodzone przez tzw. Nowy Ład Gospodarczy (ang. New Deal), czyli program reform ekonomiczno-społecznych (polegających na zatrudnianiu bezrobotnych w inwestycjach prowadzonych przez państwo) wprowadzonych w USA przez prezydenta Roosevelta w latach 1933–1939 – popularność zyskała wtedy doktryna ekonomiczna keynesizmu zakładająca interwencję państwa w gospodarce, która przez wiele lat dominowała potem w teorii i praktyce gospodarczej krajów kapitalistycznych.
Wybuch kryzysu był poprzedzony okresem niezwykle intensywnego wzrostu gospodarczego w USA i pozbawioną precedensu hossą na giełdzie[5]. Po zakończeniu I wojny światowej kraje Europy były mocno zadłużone wobec USA, spłata zadłużenia generowała duże napływy kapitału do USA[5]. Z drugiej strony USA inwestowały mocno w krajach europejskich (około 1 mld rocznie w latach 1919-1930), co sprawiło, że gospodarka Europy była bardzo mocno uzależniona od kondycji sektora finansowego w USA[6]. W latach 20. mocno wzrósł poziom życia w USA. W okresie prezydentury Calvina Coolidge'a (1921-1929) pensje w Stanach rosły średnio o 3,4%[5]. Amerykanie na masową skalę kupowali dobra luksusowe, takie jak samochody. W 1928 roku na USA przypadało 78% wszystkich samochodów na świecie, obywatele posiadali aż 21,6 mln samochodów osobowych i 3,6 mln ciężarówek[5]. 
Rozwojowi gospodarki towarzyszyła hossa na giełdzie. Akcje popularnych przedsiębiorstw np. producentów samochodów mocno zyskiwały na wartości, przykładowo akcje spółki Chrysler zyskały 1300% w ciągu kilku lat, akcje producenta odbiorników radiowych Radio Corporation wzrosły z 1.50$ w 1921 do 570$ w 1929 roku[5]. Indeks spółek notowanych na giełdzie w Nowym Jorku podwoił swoją wartość w przeciągu trzech lat przed wybuchem kryzysu. Ogromne wzrosty wartości akcji wzbudzały w inwestorach oczekiwanie, że każda spółka będzie rosła o kilkaset procent w krótkim czasie[5]. Część inwestorów stosowała mechanizm dźwigni finansowej, czyli kupowała akcje na kredyt. Dawało to możliwość większych zysków, ale jednocześnie zwiększało ryzyko w razie spadków, zmuszało inwestorów do sprzedaży akcji w razie, gdy ich kurs spadł poniżej pewnego poziomu. W 1929 Amerykanie pożyczyli aż 9 mld dolarów na zakup akcji[5].
W październiku 1929 roku niektórzy ekonomiści krytycznie oceniali gorączkę spekulacyjną na rynkach finansowych. Brytyjski minister finansów Philip Snowden 2 października stwierdził, że Amerykanie oddają się "orgii spekulacyjnej", wzmianka ta była szeroko cytowana w amerykańskiej prasie i zdaniem niektórych przyczyniła się do wybuchu paniki[5]. Tego samego dnia wypowiedział się sekretarz skarbu USA Andrew Mellon, który również skrytykował inwestorów giełdowych twierdząc, że są zbyt optymistyczni i naiwnie zakładają, że ceny akcji będą rosły w nieskończoność[5]. Następnego dnia po tych wzmiankach indeksy giełdowe w USA znacząco spadły. Przez pierwszą połowę października kursy wielu znanych spółek nieznacznie spadały. 23 października całkowita kapitalizacja rynku akcji w USA spadła aż o 4,6%, dzień później miał miejsce tzw. czarny czwartek, nastąpił krach, indeks akcji Dow Jones spadł aż o 11%[7]. Na kolejnych sesjach giełdowych indeksy giełdowe kontynuowały spadki, w czarny wtorek indeks stracił 12%[7]. Spadki na giełdach trwały aż trzy lata. Indeks Dow Jones spadł z poziomu 300 punktów 23 października 1929 do 41 punktów w czerwcu 1932, czyli stracił 90% wartości[7].
Załamanie na giełdzie pociągnęło za sobą kryzys finansowy i kryzys zaufania, inwestorzy niechętnie podejmowali się nowych inwestycji i gromadzili gotówkę. To spowodowało załamanie na rynkach europejskich, które były mocno uzależnione od napływów kapitału z USA[6].
W Stanach Zjednoczonych krach na giełdzie spowodował ograniczenie akcji kredytowej przez banki. To z kolei spowodowało drastyczny spadek konsumpcji[5]. Spadek konsumpcji pociągnął za sobą spadek cen (deflacja), co doprowadziło do fali bankructw przedsiębiorstw[8]. Bankrutujące przedsiębiorstwa nie były w stanie spłacać swoich długów, co w dalszym ciągu ograniczało podaż pieniądza[8]. Kryzys katastrofalnie wpływał na sytuację finansową banków, w latach 1929-1932 upadło 20% wszystkich amerykańskich banków[9].
W latach 1929-1933 roczne PKB kraju spadło o 45%, z czego 26% to spadek produkcji a 19% spadek cen[8]. Drastycznie spadły inwestycje, gdy przyjąć wskaźnik inwestycji za 100 w 1929 roku, w 1932 roku wynosił zaledwie 16[8]. 
Szczególnie dotkliwy dla większości ludzi był drastyczny wzrost bezrobocia (z poniżej 5% w 1929 roku do 30% w 1932 roku)[8]. Aż 1/4 Amerykanów cierpiało na niedostatek żywności[9].
Administracja prezydenta Herberta Hoovera początkowo nie podejmowała żadnych działań dla przeciwdziałania kryzysowi. Hoover uznawał, że zbyt duża pomoc rządu sprawiłaby, że Stany Zjednoczone popadną w socjalizm i kolektywizm[9]. Podobne poglądy głosił Sekretarz Skarbu Andrew Mellon, który zapewniał, że kryzys jest zjawiskiem naturalnym i przejściowym[5]. Mellon koncentrował się przede wszystkim na zapewnieniu zrównoważonego budżetu, bez deficytu, w warunkach kryzysu oznaczało to podwyżki podatków i obniżki świadczeń[10]. W 1930 roku Hoover zdecydował się podjąć pewne działania, było to wprowadzenie restrykcyjnych ceł na import do USA[9]. Cła uderzyły mocno w Europę, import do USA spadł z 4.4 mld dolarów w 1929 roku do 3.1 mld w 1930[9]. Cło na niektóre towary wzrosły nawet o 50%[9]. W odpowiedzi na amerykańskie cła kraje europejskie również wprowadziły restrykcje na amerykański import, w konsekwencji spadła wartość eksportu z USA do Europy[9].
W Polsce kryzys był mocno odczuwalny, w latach 1929 - 1935 polski produkt krajowy zmniejszył się o 52%[11]. Jednak interwencjonizm państwowy wprowadzono na szerszą skalę dopiero w 1935 po śmierci Józefa Piłsudskiego, kiedy to Eugeniusz Kwiatkowski został wicepremierem odpowiedzialnym za gospodarkę[12]. Co prawda Kwiatkowski był już uprzednio ministrem przemysłu i handlu w latach 1926–1930, ale ze względu na jego nieortodoksyjne („etatystyczne”) poglądy został on w latach 1931–1935 odsunięty na „boczny tor” (na stosunkowo podrzędne stanowisko dyrektora Państwowych Fabryk Związków Azotowych w Chorzowie i Mościcach koło Tarnowa). Do 1935 polityka gospodarcza Polski polegała głównie na utrzymywaniu wymienialności złotego na złoto po wysokim (przedkryzysowym) parytecie. Powodowało to, zgodnie z opinią ekonomistów, malejącą konkurencyjność polskich wyrobów za granicą, pogłębioną dewaluację walut w innych krajach. Innym skutkiem było zmniejszanie podaży pieniądza w miarę spadku produkcji (PKB), co powodowało dalsze tendencje deflacyjne, pogłębiające jeszcze bardziej kryzys na skutek spadku popytu. Podczas deflacji konsumenci oczekują dalszych obniżek cen, więc wstrzymują się z zakupami, powodując tym samym dalsze ograniczenie podaży i PKB oraz wyższe bezrobocie. Podczas gdy Polska dbała głównie o silną walutę, Niemcy hitlerowskie i ZSRR, gdzie wprowadzono wcześniej etatyzm (w ZSRR w jego skrajnej postaci tzw. socjalistycznej gospodarki planowej), rozbudowywały szybko swój przemysł i silnie się zbroiły.
Carl Menger, jeden z głównych przedstawicieli szkoły austriackiej[13] uważał, że cykle koniunkturalne nie są nierozerwalnie związane z kapitalistyczną gospodarką wolnorynkową, i że są one jedynie i wyłącznie skutkiem interwencji państwa w podaż pieniądza. Ekonomiści tej szkoły, tacy jak Ludwig von Mises, wskazują na rolę stóp procentowych jako ceny kapitału w kierowaniu decyzjami inwestycyjnymi. Kanonicznej dla szkoły austriackiej analizy kryzysu dokonał Murray Rothbard w książce Wielki kryzys w Ameryce[14].
Według niektórych ekonomistów podejście takie jest niezgodne z praktyką biznesu, który podejmując decyzje inwestycyjne, kieruje się nie tyle bieżącymi stopami procentowymi, a wysokością spodziewanych zysków[15]. Przedstawiciele szkoły austriackiej twierdzą, że argument ten nie ma znaczenia, ponieważ gracze rynkowi nigdy nie posiadają doskonałej wiedzy, w związku z czym nie mogą zasadniczo w racjonalny sposób przewidzieć zmian stóp procentowych, a co za tym idzie (według postulatów tej szkoły ekonomicznej) cyklicznych zmian w gospodarce, które one powodują przez odkształcenie struktury alokacji kapitału[16][17]. Co więcej, typową praktykę biznesu stanowi korzystanie z kalkulacji wartości bieżącej netto do alokowania kapitału, w której wartość inwestycji jest z perspektywy przedsiębiorcy zależna od stóp procentowych[18].
Innego zdania co do cykli koniunkturalnych jest keynesizm, którego zwolennicy uważają, że jeśli gospodarka znajduje się w stanie recesji (kryzysu), to operuje na poziomie niepełnego zatrudnienia. Wtedy interwencja państwa ma sens i odpowiednio przeprowadzona powinna doprowadzić do zwiększenia PKB oraz zatrudnienia, to jest do likwidacji cyklicznego (koniunkturalnego) bezrobocia, nawet za cenę podwyżki podatków i zwiększenia deficytu co ogranicza inwestycje sektora prywatnego[19].
Według Keynesa w prawidłowo zarządzanej gospodarce istnieją tzw. automatyczne stabilizatory, które powodują, iż dochody są bardziej stabilne niż PKB, a więc tym samym moderują wpływ cykli koniunkturalnych, zarówno w fazie wzrostu gospodarczego (niedopuszczenie do „przegrzania” gospodarki), jak i w fazie recesji (niedopuszczenie do sytuacji, w której recesja przechodzi w depresję – patrz cykl koniunkturalny). Do tych automatycznych stabilizatorów zaliczamy[20][21]:
1. Progresywny podatek od wynagrodzeń:
Powoduje on wolniejsze zmiany w dochodzie (dokładnie dochodzie będącym do dyspozycji) niż zmiany w PKB, poprzez automatyczne „przechodzenie” podatników do wyższych stawek podatku (podczas fazy wzrostu) bądź też niższych (podczas recesji). Tak więc progresywny podatek od wynagrodzeń działa „wbrew kierunkowi cyklu koniunkturalnego” (countercyclical), moderując tym samym jego przebieg.
2. Podatek od zysków osiąganych przez spółki:
Działa on podobnie jak progresywny podatek od wynagrodzeń, jako iż zyski spółek zmieniają się szybciej niż PKB. Podczas fazy wzrostu gospodarki owe zyski rosną szybciej niż PKB, a podczas recesji spadają szybciej niż PKB. Tak więc podatek od tych zysków rośnie szybko w fazie ekspansji gospodarki, pozwalając tym samym na zmniejszenie deficytu budżetu i jednocześnie zwalniając tempo wzrostu PKB (tzw. fiscal drag, czyli „fiskalne zwolnienie” wzrostu gospodarczego). Z kolei podczas fazy recesji wpływy budżetu państwa z tego tytułu szybko maleją, przez co rośnie deficyt budżetu (albo maleje jego nadwyżka), co zmusza rząd do prowadzenia ekspansywnej polityki gospodarczej, wiodącej (przynajmniej w teorii) do przezwyciężenia recesji.
3. Zasiłki dla bezrobotnych:
Stymulują one gospodarkę podczas fazy recesji (stabilizując popyt poprzez stabilizację dochodów ludności), a podczas fazy wzrostu, kiedy na skutek zwiększonego zatrudnienia rząd wypłaca ich mniej, zwalniają one niejako tempo wzrostu dochodów ludności, zapobiegając tym samym przegrzaniu gospodarki. Jednocześnie powodują one wzrost deficytu budżetu podczas fazy recesji, zmuszając tym samym rząd do podjęcia ekspansywnej polityki fiskalnej, a podczas fazy wzrostu zmniejszają one deficyt budżetu (albo zwiększają jego nadwyżkę), powodując tym samym bardziej restryktywną politykę fiskalną, niedopuszczającą do „przegrzania” gospodarki.
Szkoła marksowska uważa za Karolem Marksem, iż cykle koniunkturalne są zjawiskiem, które musi zachodzić w wolnorynkowej (to jest czysto kapitalistycznej) gospodarce ze względu na skłonność kapitalistów do nadinwestowania (w celu maksymalizacji zysków) przy jednoczesnym ograniczaniu płac realnych, a więc przy ograniczaniu popytu[22][23]. Stąd nadmiernie zwiększona (w celu maksymalizacji zysku) produkcja nie może znaleźć zbytu, co jest początkiem kryzysu. Kiedy kapitaliści zaakceptują, iż zwiększona produkcja nie znajduje zbytu, to zmniejszają oni produkcję, pogłębiając jednocześnie kryzys. Wyjście z kryzysu następuje dopiero na skutek zwiększonych inwestycji, ale z opóźnieniem, jako iż park maszynowy (ogólnie środki produkcji) jest wymieniany dopiero po jego zupełnym zużyciu (fizycznym bądź moralnym).
Teoria marksowska nie uwzględnia jednak naturalnych procesów rynkowych i konkurencji między podmiotami[potrzebny przypis]. Ważną rolę w krytyce tego podejścia odgrywa prawo Saya, które mówi, że podaż w wolnej gospodarce zawsze równa się popytowi, a więc niemożliwe jest wytworzenie się ujemnego sprzężenia zwrotnego.
Michał Kalecki uważał, iż we współczesnym kapitalizmie rządy nie mogą sobie pozwolić na tolerowanie wysokiego bezrobocia[24]. Kalecki obarczał winą za recesje rządy: miały one być spowodowane naciskiem ze strony kapitału, upatrującym w niskim bezrobociu, a więc i relatywnie wysokich płacach realnych, przyczyny niskiego zysku. Jednakże wyższe bezrobocie i niższe płace realne oznaczały w praktyce niższy popyt (aggregate demand), a więc i recesję[25][26][27].
Neoklasyczny i monetarny liberalizm nie zgadza się z Keynesem, szczególnie jeśli chodzi o zdolność państwa (rządu) do kierowania gospodarką w taki sposób, aby uniknąć cyklicznych kryzysów, a nawet aby tylko osłabić ich negatywne skutki („złagodzić ich przebieg”)[28]. Przykładowo tacy laureaci Nagrody Nobla w dziedzinie ekonomii jak Milton Friedman oraz Edmund Phelps uważają, iż krzywa Phillipsa ma w dłuższym czasie postać linii pionowej, czyli że w dłuższym czasie zwiększona inflacja nie wpłynie na spadek bezrobocia, które na skutek manipulacji ze zwiększonym popytem wróci do jego „naturalnego” poziomu, ale przy wyższym poziomie cen. Jednakże ich argumentacja oparta jest głównie na analizie stagflacji, to jest jednoczesnej wysokiej inflacji i wysokiego bezrobocia z początku lat 70. Stagflacja z początku lat 70. wywołana została przez inflacyjne (to jest na kredyt) finansowanie wojny w Wietnamie (a dopuszczanie istnienia deficytu w celu zwiększenia inwestycji publicznej jest zdaniem Keynesa dopuszczalne) i kolejne kryzysy („szoki”) energetyczne (spowodowane poparciem Zachodu dla Izraela) oraz próbą przegrzania gospodarki (zwiększenia popytu, zatrudnienia oraz PKB w sytuacji, gdy PKB był niemożliwy do zwiększenia w krótkim czasie, gdyż gospodarka USA pracowała wówczas „pełną parą”). Taka próba zwiększenia PKB w fazie ekspansji musiała skończyć się więc wysoką inflacją przy niezmienionym realnym PKB (to jest PKB liczonym w cenach stałych) i zwiększeniem stopy bezrobocia[29].
Wielu historyków i ekonomistów podjęło się badania podobieństw między wielkim kryzysem i załamaniem gospodarki globalnej z 2007 roku. Przykładowo John Paul Rossi, historyk z The Behrend College, filii Pennsylvania State University, uważa, że sytuacja z XXI wieku była efektem powtórzenia błędów, które doprowadziły do załamania z lat 20. XX wieku.[30]
Ekonomiści szkoły austriackiej tacy jak Peter Schiff wskazują, że mechanizmy powstania kryzysu były analogiczne do tych z lat 20. Poprzez zwiększanie się podaży pieniądza rosły bańki spekulacyjne na rynku, najpierw NASDAQ, potem rynek nieruchomości subprime, następnie ze względu na niemożność dalszych stymulacji (kłopoty z monetyzacją długu, wspieraniem akcji kredytowej przy i tak bliskich zeru stopach procentowych) sytuacja musiała się pogorszyć. Twierdzą, że kolejne rundy pompowania pieniędzy w rynek (Quantitative Easing), które zgodnie z polityką keynesowską powinny pomóc gospodarce, w dłuższym okresie mogą jej jedynie zaszkodzić.
Barry Eichengreen﻿(inne języki), a w ślad za nim Stefan Kawalec i Ernest Pytlarczyk porównują kryzys w strefie euro do wielkiego kryzysu, a samo euro do wymienialności walut na złoto (standard złota), co pogłębiło i przedłużyło kryzys. Kraje, które przeprowadziły dewaluację waluty poradziły sobie znacznie lepiej[3].
Miasto (od psł. *město „miejsce”) – historycznie ukształtowana jednostka osadnicza charakteryzująca się dużą intensywnością zabudowy, małą ilością terenów rolniczych, ludnością pracującą poza rolnictwem (w przemyśle lub w usługach) prowadzącą miejski styl życia.
W różnych państwach kryteria miejskości są różne, najczęściej są to kryteria ludnościowe bądź administracyjnoprawne (prawa miejskie).
Miasta charakteryzują się większym zagęszczeniem ludzi, co sprzyja większej ilości i różnorodności kontaktów między ludźmi. Przekłada się to na większą produktywność i kreatywność, która rośnie szybciej niż liczba mieszkańców – im większe miasto tym większy (na głowę), np. dochód (w USA w x razy większym mieście suma płac jest średnio większa o x1,12) oraz większa liczba wynalazków i patentów, ale także większa przestępczość (w USA w x razy większym mieście jest średnio x1,16 więcej poważnych przestępstw), czy większa ilość zachorowań na AIDS (w USA w x razy większym mieście jest średnio o x1,23 więcej zachorowań na AIDS). Także szybkość chodzenia jest większa w większych miastach. Jednocześnie większa koncentracja ludzi sprzyja intensywniejszemu wykorzystaniu infrastruktury, która rośnie wolniej niż liczba mieszkańców – im większe miasto tym mniej na głowę np. dróg (w USA w x razy większym mieście jest średnio x0,83 więcej dróg), stacji benzynowych czy kabli elektrycznych[1].
Wśród czynników miastotwórczych można wymienić następujące:
Najstarsze znane założenia urbanistyczne pochodzą z obszarów basenu Morza Egejskiego z epoki brązu, gdzie rozwijały się kultury trojańska, minojska i mykeńska. Ich cechami wspólnymi była przede wszystkim nieregularność ulic, istnienie centralnego placu oraz pałacu, zwanego też domem książęcym. W przypadku kultury minojskiej miasta nie miały fortyfikacji, w celach obronnych wykorzystywano ukształtowanie terenu (miasta lokowano najczęściej na wysokich brzegach morza lub na stokach gór). Cechy odrębnych miast wykazywały także wielkie założenia pałacowe (jak np. w Knossos). Wszystkie trzy kultury przestały istnieć około X-XI w. p.n.e. – na jej miejsce wkroczyła cywilizacja grecka.
W wiekach ciemnych i okresie archaicznym głównym czynnikiem powstawania miast było porzucanie przez ludzi i osiedlanie pod ochroną znaczącej świątyni (okręgu kultowego) lub twierdzy. Miasta takie cechuje nieplanowy rozwój, przez co nie mają jasno wytyczonych granic i umocnień, oddzielenie i niepowiązanie planistycznie agory, zabudowań świątynnych (często umieszczonych poza obrębem właściwego miasta, na akropolach) i domostw, których lokalizacja zależała od korzystnych warunków topograficznych, oraz prostota, wręcz prymitywizm, budynków mieszkalnych.
W 1. poł. V w. p.n.e. Hippodamos z Miletu (prawdopodobnie; jego autorstwo nie jest pewne) opracował nowy układ urbanistyczny, zwany systemem hippodamejskim. System uwzględniał warunki naturalne, perspektywiczny rozwój i funkcjonalność miasta. Obszar przyszłego miasta o wyraźnie zakreślonych granicach dzielono głównymi arteriami wytyczonymi wzdłuż osi północ-południe i wschód-zachód na kwartały. W centralnie usytuowanej części miasta lokowano ośrodek administracyjno-handlowy (agora) i kultowy (akropol), pozostałe kwartały zapełniała zabudowa mieszkalna. Miasto uzupełniał m.in. teatr lokowany w kotlinie na zboczu. Główne i boczne ulice krzyżowały się pod kątem prostym, tworząc geometryczną regularną siatkę. System ten, opierający się na demokratycznej zasadzie równości, był podstawą greckiej urbanistyki aż do pocz. naszej ery, powrócono do niego także w czasach nowożytnych – większość nowo zakładanych miast w Ameryce Północnej od XVII do XX wieku także posiada regularną, hippodamejską siatkę ulic.
Urbanistyka podobna do systemu hippodamejskiego – oparta na regularnej siatce ulic – występowała także w miastach etruskich. Miały one kształt kwadratu lub prostokąta podzielone w krzyż głównymi ulicami, cardo (oś N-S) i decumanus (oś E-W).
W starożytnym Rzymie, z połączenia wzorców etruskich i greckich, wykształciła się forma tzw. castrum Romanum – warownego obozu wojskowego wznoszonego według ściśle określonego planu, mającego kształt kwadratu lub prostokąta, otoczonego zawsze fosą i wałem, często też murem lub palisadą, z czterema bramami. Przy tego typu obozach powstawały osiedla canabae, które często przeradzały się stopniowo w miasta. Castra Romana dały początek takim m.in. Kolonii, Wiedniowi, Paryżowi czy Reims, będąc po rozbiórce wszelkich fortyfikacji jądrem miast średniowiecznych. Zgoła odmiennie sytuacja przedstawiała się w już istniejących miastach, których struktura narastała przez lata, jak np. w Rzymie, gdzie mimo różnych przekształceń układ przestrzenny pozostał nieregularny. Centrum każdego rzymskiego miasta stanowiło tzw. forum, które w odróżnieniu od greckiej agory było nie tyle placem targowym, ile pełniło funkcje reprezentacyjne – przy nim lokowano najważniejsze gmachy publiczne, główne świątynie itp.
Po zakończeniu wielkiej wędrówki ludów wiele miast rzymskich zostało zniszczonych, wiele znacząco podupadło. Wraz ze zmniejszaniem się liczby ludności skracano mury miejskie, niejednokrotnie także jedynie dawne centra miast, zdolne do obrony, otaczano umocnieniami, a pozostałe obszary zamieniano na pola uprawne bądź dopuszczano do zarośnięcia. Przykładowo, średniowieczny Trewir powstał na 4-metrowej warstwie gruzów miasta rzymskiego.
Jednocześnie Germanie nie mieli żadnej tradycji urbanistycznej, toteż nowe ośrodki miejskie powstawały z wykorzystaniem tradycji rzymskich. Jądrami nowych miast były zazwyczaj castra Romana, odpowiednikiem rzymskiego canabae były tzw. burgum – przedmieścia składające się z kalenicowo ustawionych domów rzemieślnicznych bronione ciągiem zewnętrznych ścian domów i zamknięte bramą. Liczni książęta budowali swoje rezydencje w dawnych miastach rzymskich, legitymując się w ten sposób jako następcy cesarstwa zachodniorzymskiego, np. władcy ostrogoccy w Rawennie, longobardzcy w Pawii, merowińscy w Kolonii.
Nowszym typem miast, powstającym od IV wieku, były tzw. miasta biskupie. Początek dał im Sobór nicejski I w 325 r., który zobowiązał biskupów do stałego przebywania w miastach diecezjalnych. Najważniejszymi elementami takich miast była katedra i plac targowy. Przykładem tego typu ośrodka może być Wormacja. Podobny charakter do miast biskupich miały miasta kolegiackie, z tą różnicą, że tereny kościelne były wyraźnie oddzielone od „części mieszczańskiej”.
We wczesnym średniowieczu samowystarczalnymi ośrodkami – de facto samodzielnymi organizmami miejskimi – były często klasztory. Do najsłynniejszych należą m.in. opactwa w Reims i Sankt Gallen. Pochodzący z 820 r. plan tego drugiego uznawany jest za wzorzec idealnego klasztoru-miasta. Wszystkie tego typu organizmy z czasem stały się integralnymi częściami miast, które wokół klasztorów się rozwinęły.
Kolonizacją wschodnią nazywa się planowe akcje osadnicze organizowane od VIII do XIV wieku na terenie Europy Środkowej. Wyróżnia się cztery jej etapy:
Miasta powstające w okresie kolonizacji wschodniej mają podobny kształt przestrzenny: narys zbliżony do prostokąta lub owalu, siatka prostopadłych ulic, prostokątny plac targowy (rynek) z blokiem zabudowy śródrynkowej (w którym był m.in. ratusz), kościół na własnym placu oraz, jeżeli istnieje, zamek na obrzeżach miasta posiadający własne umocnienia.
Kształt przestrzenny nowo lokowanych miejscowości we Francji całkowicie różnił się od miejscowości z kręgu kultury niemieckiej. Wyróżnia się trzy typy jednostek osadniczych:
W przeciwieństwie do rzymskiego castrum Romanum, w którym niemożliwa była żadna rozbudowa (chyba że poprzez powstawanie canabae), miasta średniowieczne, zaplanowane mniej rygorystycznie pozwalały na dalszy rozwój przestrzenny. Rozbudowa miasta mogła przebiegać na kilka sposobów:
Między XIII a XIV w. nastąpił olbrzymi rozwój urbanistyki w północnych i środkowych Włoszech. Przyczynił się do tego rozpad Włoch na samodzielne księstwa i miasta-państwa, z których największe znaczenie miały: Mediolan, Werona, Florencja, Siena, Ferrara, Genua, Wenecja, Piza i Lukka. Nowe założenia przestrzenne i wielkie dzieła architektury powstawały przede wszystkim dzięki przekazaniu przez rody arystokratyczne władzy mieszczanom i chęci demonstrowania swojej siły politycznej poprzez kształtowanie wyglądu miast. Podobna sytuacja miała miejsce w Niemczech i Niderlandach, gdzie samodzielność uzyskały największe potęgi handlowe basenu Morza Bałtyckiego i Północnego, m.in. Lubeka, Amsterdam, Brugia, Antwerpia, Gdańsk, Toruń czy Rostock. Wspólnym elementem układu urbanistycznego miast nadmorskich, tak włoskich, jak i niemieckich i niderlandzkich, była (mniejsza lub większa) sieć kanałów utworzona w celu bezpośredniego udostępnienia statkom miejskich magazynów i kantorów.
Podobny charakter do włoskich miast-państw miały wolne miasta Rzeszy w Niemczech. Były to dawne miasta biskupie, które w XIII-XIV wieku zrzuciły władzę duchowną, podlegając bezpośrednio cesarzowi i posiadające własną reprezentacją w Reichstagu.
W renesansie pojawiła się koncepcja tzw. miasta idealnego. Głównym źródłem ich projektantów był traktat „O architekturze ksiąg dziesięć” autorstwa rzymskiego architekta Witruwiusza. Twórcą nowych założeń dotyczących ideału miasta był Leonardo da Vinci. Według niego miasto:
Ponieważ zarówno we Włoszech, jak i Niemczech wielkie miasta skoncentrowane były przede wszystkim na przeprowadzeniu wewnętrznych reform i przekształceń, do realizacji utopijnych projektów dochodziło dość rzadko. Do najsłynniejszych należą:
Prawdziwą popularność koncepcja miasta idealnego zdobyła dopiero w okresie baroku. Ich budowa była przez absolutystycznych władców pożądana i wspierana. Główne założenia barokowych miast idealnych:
Najsłynniejsze przykłady barokowych miast idealnych to:
Rozwój techniki wojskowej, który szczególnie uwidocznił się podczas wojen włoskich (1494–1559), spowodował, że średniowieczne mury miejskie, wieże i baszty przestały być skutecznym systemem umocnień. Zaczęto więc budować twierdze, zastępując mury wałami ziemnymi a baszty – bastionami. Wyróżnia się trzy typy twierdz, tzw. narysy fortyfikacyjne:
Będące z początkami w miarę prostymi systemami składającymi się z pojedynczego wału i pięciokątnych bastionów z czasem przekształcały się, poprzez dodawanie kolejnych elementów, w olbrzymie fortyfikacje zajmujące niejednokrotnie więcej przestrzeni niż położone wewnątrz nich miasto.
Podstawy teoretyczne budowy i zdobywania twierdz obowiązujące do poł. XIX wieku stworzył Sebastian Vauban, przez którego zaprojektowany zespół dwunastu grup warownych na granicach Francji znajduje się obecnie na liście UNESCO.
Wiek XIX wywarł największy wpływ na większość współczesnych miast europejskich. To właśnie wtedy ukształtowały się śródmieścia znakomitej części ośrodków miejskich. Główną przyczyną niczym nieskrępowanego rozwoju przestrzennego miast w tym stuleciu było błyskawiczne uprzemysłowienie tychże miast, a możliwy on był dzięki likwidacji przestarzałych fortyfikacji.
Urbanistyka klasycyzmu, czyli końca wieku XVIII i pocz. XIX, przezwyciężyła palladiańską koncentrację motywów antycznych w wyizolowanej, pojedynczej budowli i tworzyła szeroko zakrojone, przejrzyste osie, z ich symetrią i wyznaczeniem dominant. Założenia klasycystyczne są dobrze widoczne w układzie przestrzennym centrum Monachium. W tym drugim zastosowano podział na nieregularne miasto mieszczan na południu i miasto królewskie na północy składające się z trzech arterii odchodzących od zamku miejskiego na zachód, północ i wschód, których dominantami były odpowiednio: Propyleje, gmach uniwersytetu i pałac Maximilianeum.
Klasycyzm nie miał jednak takiego znaczenia dla historii urbanistyki jak druga połowa stulecia, czyli okres tzw. drugiej rewolucji przemysłowej. Jak już wspomniano, rozwój miast determinował rozwój przemysłu, który pociągał za sobą masowy napływ ludności. Niektóre miasta, jak np. polska Łódź, w ciągu kilkudziesięciu lat przekształciły się z prowincjonalnych miasteczek czy nawet wsi w wielkie metropolie o międzynarodowym znaczeniu. Aby zapanować nad boomem budowlanym i nie dopuścić do powstania chaosu przestrzennego, zaczęto prowadzać surowe przepisy budowlane regulujące kształt, wysokość i styl architektoniczny budowli (było to możliwe tym bardziej, że bardzo często w całych dzielnicach właściciele działek korzystali z usług tych samych architektów) oraz tworzyć tzw. plany regulacyjne.
Podstawowym rodzajem zabudowy były kamienice, układające się we w miarę ujednolicone pierzeje, które tworzyły mniej lub bardziej regularne kwartały, niejednokrotnie przypominające starożytny system hippodamejski (za wzorcowy przykład XIX urbanistyki kwartałowej uchodzi barcelońska dzielnica Eixample zbudowana w oparciu o ściśle geometryczne zasady – ulice przecinają się pod kątem prostym, tworząc czworoboczne kwartały pod zabudowę mieszkalną).
Stare części miast stopniowo traciły swoje znaczenie na rzecz nowych dzielnic, gdzie powstawały nowe gmachy publiczne, nowe place publiczne zastępujące staromiejskie rynki itd.; w niektórych przypadkach, np. Warszawa, starówki stawały się dzielnicami biedoty.
Ważnym obiektem, determinującym kierunek rozwoju przestrzennego miasta, był – położony zazwyczaj poza centrum – dworzec kolejowy – cechą wspólną praktycznie wszystkich miast europejskich, które nie zostały zniszczone w czasie wojny bądź okresie późniejszym, jest to, że najbardziej reprezentacyjnymi wielkomiejskimi arteriami są te prowadzące od dworca do ścisłego centrum (centralnego punktu miasta).
W wielu miastach dochodziło do tzw. wielkich przebudów, które poprzedzały, kontrowersyjne do dziś, masowe akcje wyburzeniowe. Do najsłynniejszych należą: wielka przebudowa Paryża w latach 1852–1870 kierowana przez Georges’a Haussmanna, podczas której zrównano z ziemią ponad 20 tys. budynków, asanacja praska w latach 1895–1914, w której czasie zupełnie zmieniono oblicze dzielnic Podskalí i żydowskiego Josefova, wspomniana wyżej budowa dzielnicy Eixample w Barcelonie i budowa Ringu wiedeńskiego w latach 1860–1890.
Zakłady przemysłowe, których powstawanie było przyczyną napływu ludności i rozwoju miast, początkowo powstawały w centrach, wtapiały się one resztę w zabudowy – fabryka, kościół i park obok siebie nie były niczym dziwnym. Z czasem jednak uciążliwość przemysłu i pochodzące z fabryk zanieczyszczenia doprowadziły do realizacji koncepcji „rozdziału funkcji miasta”. Zakłady zaczęto przenosić na obrzeża, tworząc odrębne dzielnice przemysłowe. W ich pobliżu budowano tzw. osiedla robotnicze składające się z tanich (a przez to posiadających niski standard) mieszkań dla robotników, którzy nie musieli dzięki temu codziennie dojeżdżać do pracy czy szukać na własną rękę noclegu. W architekturze takich osiedli wyróżnia się dwa typy budynków: wielorodzinne przypominające XX-wieczne bloki (w Polsce nazywane najczęściej familokami) oraz szeregowe z ogródkami przydomowymi. Charakterystyczną cechą zarówno fabryk, jak i domów dla robotników w całej Europie były nietynkowane elewacje z czerwonej cegły. W wielu przypadkach osiedla robotnicze, zamieszkiwane przez najbiedniejszą ludność, przeradzały się w slumsy; złe warunki na nich panujące były jedną z głównych przyczyn krytyki XIX-wiecznej urbanistyki w czasach modernizmu. Niekiedy osiedla robotnicze powstawały jako samodzielne osady przekształcające się później w miasta.
Pod koniec stulecia rozwinęło się, na obrzeżach miast (choć nie tylko), budownictwo willowe dające początek współczesnym osiedlom domów jednorodzinnych na przedmieściach. Jeszcze przed II wojną światową wille/domy jednorodzinne stały się dominującą formą budownictwa mieszkalnego, zastępując kamienice. W Wielkiej Brytanii, za sprawą Ebenezera Howarda, forma przedmieść wyewoluowała w tzw. przedmieścia-ogrody (różniły się od zwykłych przedmieść znacznym, często przekraczającym połowę całej powierzchni, udziałem terenów zielonych) oraz miasta-ogrody, które były formalnie samodzielnymi, satelickimi ośrodkami miejskimi powstającymi wokół wielkich miast. Ideą miast-ogrodów było połączenie zalet miasta i wsi, „humanitaryzm” (ciasną zabudowę miejską już wtedy wielu uważało za „niehumanitarną”) oraz współżycie człowieka z naturą. Pierwszym takim miastem było podlondyńskie Letchworth Garden City, które według projektu Howarda zaczęto budować w 1902 r.
Na terenach szczególnie uprzemysłowionych rozwijało się więcej ośrodków miejskich (zarówno już istniejących, jak i nowo zakładanych, np. w formie osiedla robotniczego), tworzących okręgi przemysłowe. W wielu przypadkach miasta takich okręgów zaczęły tworzyć jedną funkcjonalną całość (albo, jak w Zagłębiu Ruhry czy na Górnym Śląsku, wręcz zlewały w jeden ośrodek) – powstawały pierwsze aglomeracje.
W dwudziestoleciu międzywojennym XIX-wieczna urbanistyka zaczęła być powszechnie krytykowana. Czołowi urbaniści tego okresu głosili potrzebę zaprzestania tworzenia zabudowy kwartałowej i radykalnej zmiany kształtu przestrzennego miast – opartego na blokach mieszkalnych budowane pośród kwartałów zieleni przeciętych drogami szybkiego ruchu. Charakterystykę „stanu obecnego” i postulatów, jak mają wyglądać miasta w przyszłości, zawiera Karta Ateńska – uchwała końcowego posiedzenia 4. Międzynarodowego Kongresu Architektury Nowoczesnej w 1933 r.
W części poświęconej opisowi cech ówczesnych miast jako negatywne skutki zwartej zabudowy miejskiej wymieniono m.in. zbyt małą powierzchnię mieszkalną na osobę, niewystarczające nasłonecznienie, występowanie zarodków chorób, zbyt małe ilości zieleni, „nieprzystosowanie do mieszkania budowli położonych wzdłuż dróg” i niedobór urządzeń sanitarnych. W rzeczywistości charakteryzowało to najbiedniejsze dzielnice – slumsy powstające na osiedlach robotniczych.
Jako alternatywę dla tego stanu sygnatariusze Karty Ateńskiej postulowali:
(...)§ 23. Dzielnice mieszkaniowe muszą w przyszłości zajmować najlepsze obszary w mieście, gdzie czerpiane będą pożytki z topografii i położenia, a także dysponujące najkorzystniejszym położeniem względem słońca i dogodnie położoną zielenią.§ 24. Wybór terenów dla mieszkalnictwa musi być przeprowadzany w oparciu o kryterium higieny.§ 25. Rozsądna gęstość zaludnienia, odpowiadająca formom osadniczym, określonym przez naturę terenu.
§ 26. Dla każdego mieszkania ustalić trzeba godzinne minimum nasłonecznienia.§ 27. Linia zabudowy wzdłuż tras komunikacyjnych musi być zabroniona.§ 28. Wykonywanie wysokich budynków, korzystając z nowoczesnej techniki.§ 29. Wysokie budynki muszą stać w odpowiednio szerokich odstępach, tak, by było między nimi możliwie wiele zieleni.(...)§ 35. Każda dzielnica mieszkalna musi w przyszłości dysponować terenami zielonymi dla rozsądnych urządzeń sportowych i rekreacyjnych dla dzieci, młodzieży i dorosłych.§ 36. Niezdrowe kwartały muszą być wyburzone i zastąpione terenami zielonymi: wzrośnie wartość graniczących z nimi bloków!§ 46. Miasta przemysłowe muszą być rozmieszczone nie koncentrycznie, a linearnie.§ 47. Leżące wzdłuż osi zaopatrzenia okręgi przemysłowe będą oddzielone pasem zieleni od równoległych osiedli dla pracowników.(...)§ 60. Drogi komunikacyjne muszą być klasyfikowane zgodnie z ich charakterem, i budowane odpowiednio do poruszających się nimi pojazdów i ich szybkości.§ 62. Piesi muszą używać innych ulic niż samochody.§ 64. Drogi dalekobieżne powinny być zasadniczo izolowane przez tereny zielone.§ 66. Zachowanie starej substancji budowlanej nie może jednak oznaczać warunków mieszkalnych, które byłyby niewytrzymalne dla ludności.§ 67. Likwidacja mizernych kwartałów wokół zabytków historycznych umożliwi stworzenie obszarów zieleni.(...)
Kierunek urbanistyczny, obowiązujący na świecie do połowy lat 70. XX wieku, którego założenia zawarto w Karcie Ateńskiej, nazywa się modernizmem. W Europie, w przeciwieństwie do Ameryk czy Azji, początkowo nie znalazł on większego uznania – założenia Karty Ateńskiej zaczęto wypełniać praktycznie dopiero w latach 50. podczas tzw. modernizacji miast, która jednak w pełni przebiegała tylko na Wyspach Brytyjskich. W Europie kontynentalnej ograniczano się głównie do budowy wielkich zespołów mieszkaniowych. Praktycznie jedynym przykładem miasta modernistycznego jest Hawr zbudowany od podstaw po zniszczeniu starego Hawru w czasie wojny. Ponadto znane są przykłady nigdy niezrealizowanych projektów przebudów miast europejskich. Najsłynniejszym z nich jest pomysł wyburzenia paryskiego śródmieścia i zastąpienia go modelowym miastem „nowoczesnym” składającym się z wieżowców ustawionych w równych kwartałach parkowych autorstwa Le Corbusiera.
Zniszczenia II wojny światowej nie miały sobie równych w historii nowoczesnej Europy. Żaden inny konflikt zbrojny w tak wielkim stopniu nie wpłynął na wygląd współczesnych miast, na terenie Francji, Niemiec, Polski, Wielkiej Brytanii, ZSRR. Naloty lotnicze, podobnie jak fanatyczne walki o miasta doprowadziły nie tylko do śmierci dziesiątek tysięcy osób, ale doszczętnych zniszczeń zabudowy, przekraczających w wielu przypadkach 90%.
Urbanistyka europejska stanęła przed nowym zadaniem – odbudowy zniszczonych miast. Można wyróżnić kilka „typów” odbudów (po średniku przykład miasta):
W żadnym przypadku nie miało miejsce całkowite przywrócenie stanu przedwojennego. Było to sprzeczne z obowiązującą wówczas Kartą Wenecką (dokument traktujący o zasadach konserwacji i restauracji zabytków), która sprzeciwiała się rekonstrukcjom całych zespołów architektonicznych.
Gwałtowny rozwój motoryzacji w latach 50. i 60. spowodował, że układy komunikacyjne miast pochodzący często jeszcze w XIX wieku stawał się niewydolny. Istnienie wąskich, krętych, brukowanych uliczek z torowiskami tramwajowymi połączone z brakiem obwodnic i parkingów powodowały tworzenie ogromnych korków i „duszenie się” zarówno wielkich metropolii, jak i mniejszych ośrodków. Podjęto więc radykalne środki mające na celu stworzenie „miasta dla samochodów” – rozpoczęto wielkie akcje wyburzeniowe w centrach miast, aby na miejscu burzonych budynków (często zabytkowych) budować wielopasmowe drogi szybkiego ruchu z bezkolizyjnymi skrzyżowaniami niejednokrotnie dzielące miasto na kilka części. Budowę nowych arterii połączono z realizacją założeń Karty Ateńskiej – zastępowaniem „niezdrowych” kwartałów zabudową modernistyczną. Tego typu działania nazywane są modernizacjami miast i obecnie powszechnie krytykowane ze względu na niszczenie tkanki miejskiej w celu, który można było osiągnąć w o wiele mniej szkodliwy sposób (obwodnice biegnące poza obszarami zabudowanymi, tunele, rozwój komunikacji zbiorowej itd.).
Nie ma w Europie kraju, gdzie w II poł. XX wieku nie dochodziłoby do działań modernizacyjnych, jednak na największą skalę prowadzone były w Wielkiej Brytanii, będącej pod silnym wpływem trendów amerykańskich (właśnie w USA zapoczątkowano modernizacje miast), oraz w krajach bloku wschodniego. W tych drugich często chodziło jednak nie tyle o wypełnianie postulatów Karty Ateńskiej, ile o realizację celów ideologicznych, np. „zastępowanie miast burżuazyjnych socjalistycznymi” czy budowa potężnych kompleksów architektonicznych, ewentualnie realizację planów gospodarczych np. poprzez burzenie całych dzielnic, a nawet miast, pod którymi znajdowały się złoża surowców lub chciano postawić wielkie zakłady przemysłowe. Przykładem może być wyburzenie w latach 80. 7 km² śródmieścia Bukaresztu i wybudowanie na jej miejscu monumentalnej dzielnicy rządowej z Pałacem Parlamentu w centrum czy stuprocentowe zniszczenie w latach 60. i 70. 50-tysięcznego Mostu w zachodnich Czechach w celu eksploatacji znajdujących się pod nim złóż węgla kamiennego.
Jednym z „produktów” urbanistyki modernistycznej są wielkie zespoły mieszkaniowe, zwane potocznie blokowiskami – osiedla, której zabudowę mieszkalną tworzą bloki wielorodzinne.
Ojcem blokowisk jest czołowy urbanista modernizmu, Le Corbusier. Chcąc zapewnić ubogim robotnikom godziwe miejsce do życia, wśród zieleni, światła słonecznego i porządku stworzył tzw. maszyny do mieszkania, czyli właśnie wielkie zespoły mieszkaniowe. Pierwszym blokiem wybudowanym według planów Le Corbusiera była oddana w 1952 r. Jednostka Mieszkaniowa (Unité d’Habitation) w Marsylii. Miała 23 rodzaje mieszkań od jednopokojowych po wielkie, dwupoziomowe. W chwili oddania był to największy budynek świata, na 8 piętrze zlokalizowano dwukondygnacyjną ulicę handlową. Na dachu jest m.in. basen, plac zabaw dla dzieci, solarium i klub.
Koncepcje Le Corbusiera trafiły na szczególnie podatny grunt w obozie komunistycznym. Potrzeba nowych mieszkań stale rosła. Bloki wydawały się więc rozwiązaniem idealnym. Budowano więc budynki wielopiętrowe i wieloklatkowe, wykonywane zazwyczaj w technice wielkiej płyty. Bloki stawiano w zespołach, czyli większych grupach i zwykle występujących w zabudowie rzędowej lub w swobodnej kompozycji.
W Europie Wschodniej powstanie charakterystycznych, podobnych do siebie bloków mieszkalnych datuje się na lata 60., 70. i 80. XX wieku. Wówczas osiedla bloków przez lokalne władze traktowane były jako wizytówka nowoczesności. Bloki mieszkaniowe powstawały więc nie tylko w dużych miastach, gdzie ich obecność wydawała się uzasadniona, ale i w małych miasteczkach, a nawet wsiach burząc ład kompozycyjny okolicy.
W wielu przypadkach wzm-y były budowane jako samodzielne miasta-sypialnie przy wielkich kompleksach przemysłowych lub jako przedmieścia dużych miast. Cechą wspólną takich „miast socjalistycznych” był podział na sektory zamieszkiwane przez kilka tysięcy osób oraz istnienie centralnego placu z gmachami użyteczności publicznej (siedziba władz miejskich, partii, centra kultury itp.) i przebiegającej przez niego „magistrali” – głównej ulicy handlowej i kręgosłupa komunikacyjnego przystosowanego do organizowania wielkich manifestacji.
Urbanistykę modernistyczną odrzucono w Europie Zachodniej w latach 80. Czasy współczesne, czyli okres postmodernizmu cechuje powrót do klasycznych definicji przestrzeni publicznej, wnętrza ulicy i placu, tkanki miejskiej. Przywołuje się wypróbowane historyczne wzorce, zakorzenione w kulturze europejskiej, modyfikując jednak ich znaczenie, przystosowując do ery samochodu. Dąży się do stworzenia przestrzeni przyjaznej człowiekowi, lecz jednocześnie do nierozdzielania funkcji dróg pieszych i kołowych.
Przywraca się istniejącą w XIX w. rangę elewacjom i krawędziom przestrzeni, różnicując jej skalę, wprowadzając przewężenia, przejścia itp. Budynkom nadaje się indywidualny rys, nastawiając się jednak przede wszystkim na ich współudział w kreowaniu przestrzeni. Urbanistyka opiera się na grze przestrzeni pustych i tkanki miejskiej, które są wyraźnie rozgraniczone.
Postmodernizm rehabilituje dawne i tradycyjne miasto, przeciwstawiając się modernistycznemu modelowi osiedla i zastępując go kwartałami i dzielnicami. Podkreśla się znaczenie mieszanej struktury miasta, negując większość postanowień Karty Ateńskiej. W zabudowie mieszkaniowej powszechnym modelem jest zabudowa obrzeżna, odgraniczająca wnętrze kwartału od publicznej ulicy.
Zabudowę wielorodzinną realizuje się na dwa sposoby:
Najpowszechniejszym typem zabudowy jest jednak budownictwo jednorodzinne tworzące wokół miast rozległe przedmieścia. Może to prowadzić do negatywnego zjawiska, jakim jest suburbanizacja.
Odrzucenie modernizmu w rzadkich przypadkach doprowadziło nawet do burzenia osiedli modernistycznych i zastępowania ich dzielnicami postmodernistycznymi. Najsłynniejszym przykładem jest amsterdamska dzielnica Bijlmermeer – wybudowany w latach 70. wielki zespół mieszkaniowy, który na pocz. XXI wieku został zlikwidowany poprzez wyburzenie 3/4 bloków (resztę pozostawiono jako „muzeum modernizmu Bijlmer”) i budowę na ich miejscu zabudowy pierzejowej, szeregowej i niskiej wolnostojącej oraz likwidację wszystkich autostrad i dróg szybkiego ruchu wraz ze skrzyżowaniami bezkolizyjnymi i zamienienie ich w zwykłe, jednopasmowe miejskie ulice i pasaże piesze.
Systemy prawne i administracyjne wielu krajów podchodzą do kwestii nazewnictwa obszarów zurbanizowanych na różne sposoby. W poszczególnych państwach przyjmuje się różne kryteria uznawania osiedli za miasta. Rozróżnia się tu wielkościowe (statystyczne), prawno-administracyjne kryteria identyfikacji miast[2].
W Polsce przyjmuje się następującą definicję legalną: miasto to jednostka osadnicza o przewadze zwartej zabudowy i funkcjach nierolniczych posiadającą prawa miejskie, bądź status miasta nadany w trybie określonym przepisami[3]. Zgodnie z ustawą o samorządzie gminnym z 1990 o nadaniu lub zniesieniu statusu miasta decyduje Rada Ministrów w drodze rozporządzenia.
Według ustawy aby miejscowość mogła otrzymać prawa miejskie, musi w niej być zameldowanych ponad 2000 osób, posiadać zabudowę miejską (nie zagrodową) oraz przynajmniej 2/3 mieszkańców musi być zatrudniona poza rolnictwem.
W podziale administracyjnym miasta mają status samodzielnej gminy (gmina miejska) lub – zazwyczaj w przypadku mniejszych miast – wchodzą w skład gminy miejsko-wiejskiej jako jednostka pomocnicza gminy miejsko-wiejskiej. Miasta będące samodzielną gminą mogą z kolei tworzyć jednostki pomocnicze: dzielnice lub osiedla, a także sołectwa. W Polsce miasta zarządzane są przez prezydenta miasta lub burmistrza; w wypadku gminy miejsko-wiejskiej burmistrz jest także organem wykonawczym gminy miejsko-wiejskiej.
66 miast w Polsce ma status miasta na prawach powiatu. Miasta te posiadają wszystkie funkcje i kompetencje przynależne powiatom i realizują zadania powiatów. Pozostają jednak gminami, dlatego zasady ich funkcjonowania określa ustawa o samorządzie gminnym, a zadania określa wiele innych ustaw, w tym ustawa o samorządzie powiatowym.
W Polsce jest 107 miast, na których czele stoi prezydent miasta (historycznie to wszystkie miasta pozostałe po II wojnie światowej w Polsce, a wydzielone z otaczających je powiatów reformą samorządową z roku 1933 oraz miasta, w których prezydent miasta jest organem zarządzającym na mocy przepisów powojennych i reformy ustroju miast z lat 1973–1974). Największym miastem będącym samodzielną gminą miejską w Polsce, którym zarządza burmistrz, są 60-tysięczne Tarnowskie Góry, największym miastem w obrębie gminy miejsko-wiejskiej 46-tysięczna Nysa, zaś najmniejszym miastem prezydenckim i zarazem najmniejszym ludnościowo miastem na prawach powiatu 38-tysięczny Sopot.
Według przywołanej ustawy miastem prezydenckim może być:
O zmianie granic miast, samodzielnie lub na wniosek zainteresowanych gmin decyduje Rada Ministrów. Zgodnie ze schematem aplikacyjnym załączonym do rozporządzenia z dnia 21 lipca 2021 r. w sprawie ewidencji miejscowości, ulic i adresów[4].
Urbanizacja charakteryzuje się szybszym tempem wzrostu zaludnienia obszarów centralnych miasta aniżeli obszarów zewnętrznych. Liczba ludności całej aglomeracji wzrasta na skutek dodatniego przyrostu naturalnego i migracji ludności. Napływ ludności do miasta jest wysoki i związany jest ze wzrostem zatrudnienia, głównie w przemyśle.
Suburbanizacja charakteryzuje się szybszym przyrostem ludności na obszarach zewnętrznych aniżeli w centrum miasta. Przyrost naturalny na obszarach zewnętrznych jest wyższy, a niektóre centralne obszary wykazują przyrost ujemny. Okres ten jest okresem rozwoju aglomeracji. Główne miasto traci na znaczeniu i zaczyna chylić się ku upadkowi. O zmianie ludności w dalszym ciągu decydują migracje, zatrudnienie ogółem wzrasta, ale w niektórych centralnych dzielnicach zaczyna występować spadek tej wielkości. Podstawowa rola i zatrudnienie w sferze usług.
Dezurbanizacja charakteryzuje się spadkiem liczby ludności na obszarze centralnym, a następnie również i zewnętrznym. Spadek ten powoduje zmniejszanie się aglomeracji jako całości, tempo migracji ludności z obszarów węzłowych (centralnych) na obszary zewnętrzne i do mniejszych miast przewyższa jej przyrost na obszarach zewnętrznych aglomeracji. W fazie tej funkcje centrów miast ulegają bardzo silnej erozji. Funkcja mieszkaniowa zanika, działalność usługowa nastawiona na zaspokojenie potrzeb ludności też się zmniejsza, funkcja administracyjna i zatrudnienie wzrastają, infrastruktura przeznaczona dla transportu zajmuje coraz większe obszary, co koliduje z funkcją mieszkaniową. Funkcje społeczne i kulturalne spełniane przez centra miast ulegają zmniejszeniu.
Reurbanizacja występuje wówczas, gdy udział ludności obszaru centralnego w ogólnej liczbie ludności rośnie początkowo na skutek zahamowania tempa ubytku, a następnie wzrostu ludności tego obszaru. W procesie odradzania się miasta przeważają siły dośrodkowe, w przeciwieństwie do fazy dezurbanizacji, w której dominowały siły odśrodkowe.
Rozróżnia się funkcje egzogeniczne i funkcje endogeniczne miasta. Funkcje endogeniczne to funkcje miasta skierowane do wewnątrz (administracja miejska, część handlu). Funkcje egzogeniczne to funkcje skierowane na zewnątrz miasta (przemysł, turystyka, handel morski itp.).
Funkcje egzogeniczne dzielą się na:
Układy lokalizacyjne miast według funkcji miasta:
Funkcje miast zależą od przeważającej funkcji i infrastruktury z tym związanej.
Wyróżnia się miasta o funkcji:
Miasto stołeczne to nazwa zarezerwowana dla stolicy kraju, jest ona również stosowana w nazwach miast pełniących niegdyś tę funkcję (np. Kraków), albo miast posiadających formalnie, de iure status stołeczny, lecz de facto niewypełniających w praktyce administracyjnych funkcji stolicy.
W wielu przypadkach miasta stołeczne posiadają odrębny status administracyjny, często są wydzielone z podziału administracyjnego państwa na specjalnych warunkach.
Warszawa posiadała specjalny status administracyjny w latach 1946–1975, i jako jedno z kilku miast w Polsce (stanowiła wówczas miasto wydzielone, tzw. województwo miejskie). Obecnie ustrój Warszawy reguluje ustawa „Ustrój miasta stołecznego Warszawy” nadając stolicy dodatkowe podziały administracyjne i kompetencje organów (Rady dzielnic).
Zestawienia:
Połączenie sąsiednich miast na wspólnym obszarze
Bliski Wschód (arab. ‏الشرق الأوسط‎; hebr. המזרח התיכון; pers. ‏خاورمیانه‎, Xâvar-e Miyâne; przestarzałe Lewant) – region geograficzny obejmujący głównie Azję Zachodnią, ale w pewnych kontekstach także obszary Europy i Afryki.
Obejmuje on głównie państwa położone w południowo-zachodniej Azji. Najczęściej do krajów Bliskiego Wschodu zalicza się: Arabię Saudyjską, Bahrajn, Egipt, Irak, Iran, Izrael, Palestynę, Jemen, Jordanię, Katar, Kuwejt, Liban, Oman, Syrię, Turcję, Zjednoczone Emiraty Arabskie oraz w niektórych źródłach też Cypr. W sumie zajmują one powierzchnię ok. 9,7 mln km² z liczbą ludności wynoszącą 456 mln mieszkańców (2021)[1].
Bliski Wschód jest regionem, gdzie prawdopodobnie rozpoczęły się procesy przejścia z paleolitu do neolitu. Region ten (Żyzny Półksiężyc) jest również kolebką wielkich cywilizacji starożytnych (Mezopotamia, starożytny Egipt) oraz trzech wielkich religii abrahamowych: judaizmu, chrześcijaństwa i islamu. Przez prawie trzydzieści wieków było to centrum starożytnego świata.
Było to też wielkie pole bitwy wielkich imperiów:
Po I wojnie światowej na Bliskim Wschodzie ścierały się interesy Francji i Wielkiej Brytanii, po II wojnie zaczęły stopniowo tracić swe wpływy na rzecz Stanów Zjednoczonych.
Współcześnie pod względem kulturowym zdecydowaną większość stanowi ludność arabskojęzyczna wyznająca islam (oprócz Arabów wyznają go również Turcy oraz ludy irańskie).
Region ten był i nadal jest niestabilny pod względem politycznym, można wymienić chociażby konflikt izraelsko-arabski i związane z nim wojny izraelsko-arabskie, wojnę iracko-irańską czy wojnę w Zatoce Perskiej.
Spójność geopolityczna regionu Bliskiego Wschodu jest związana z szeregiem czynników, które identyfikują ten obszar jako całość. Do czynników spajających region należą:
Bliski Wschód jest bardzo ważnym regionem gospodarczym, gdzie koncentruje się znaczna część światowego wydobycia ropy naftowej. Ponadto krzyżują się tu ważne międzynarodowe linie komunikacji lotniczej i morskiej (Kanał Sueski).
Określenia Bliski, Środkowy i Daleki Wschód to typowy przykład nazwy etnocentrycznej, a w tym wypadku europocentrycznej, czyli z punktu widzenia Europejczyków. Nazwa „Bliski Wschód” oznacza południowo-zachodnią Azję i północno-wschodnią Afrykę. Obecnie w języku angielskim nazwa Near East jest wypierana przez Middle East (Środkowy Wschód).
Badaniem starożytnych i nowożytnych, żywych i wymarłych kultur Bliskiego Wschodu zajmują się odpowiednie działy orientalistyki.
Bliski Wschód odgrywa duże znaczenie gospodarcze i strategiczne. Leży na skrzyżowaniu trzech kontynentów: Europy, Azji i Afryki, przechodzą tędy ważne szlaki komunikacyjne z Europy i północy Afryki na Daleki Wschód. Drogi morskie przebiegają przez cieśniny czarnomorskie, Morze Śródziemne, Kanał Sueski, Morze Czerwone i zatokę Perską, krzyżują się linie powietrzne łączące najważniejsze porty lotnicze Europy z południową Afryką, Azją, Dalekim Wschodem i Australią[2].
Kuba (hiszp. Cuba), Republika Kuby (República de Cuba) – państwo wyspiarskie w Ameryce Północnej, położone w archipelagu Wielkich Antyli, na wyspie Kuba oraz szeregu otaczających ją mniejszych wysepek. Największy pod względem powierzchni kraj na Karaibach (110 860 km²), zamieszkany przez 11 032 343 osób (2021). Stolicą i największym miastem jest Hawana[4].
Kuba jest unitarną republiką socjalistyczną[5]. Językiem urzędowym jest hiszpański. W XVI wieku skolonizowana została przez Hiszpanów. Pozostawała hiszpańską kolonią do 1898 roku, a niepodległość uzyskała w 1902 roku, po czteroletniej okupacji przez Stany Zjednoczone. W 1959 roku w wyniku rewolucji kubańskiej władzę w kraju objął Fidel Castro, a kraj wkroczył na drogę socjalizmu. Nastąpiło jednoczesne zbliżenie w stosunkach ze Związkiem Radzieckim oraz zaognienie relacji ze Stanami Zjednoczonymi, które w 1961 roku nałożyły na Kubę embargo, pozostające w mocy do dnia dzisiejszego. Po upadku Związku Radzieckiego w 1991 roku sytuacja gospodarcza kraju uległa pogorszeniu. Od tego czasu podjęte zostały umiarkowane reformy wolnorynkowe, wzmożone po odstąpieniu Castro od władzy w 2008 roku[4][5].
Kuba to największa z wysp karaibskich, a na liście największych wysp świata zajmuje 17. miejsce.
Otoczona jest 1600 wysepkami, skałami i rafami (największa wyspa Isla de la Juventud, do 1978 r. nazywająca się Pinos – 3056 km²). Otaczające Kubę głębiny morskie sięgają 7000 m. Od Florydy oddziela ją Cieśnina Florydzka, od Meksyku – Cieśnina Jukatańska, od San Domingo – Cieśnina Zawietrzna.
Brzegi dobrze rozwinięte obfitują w liczne zatoki. Okoliczne wyspy tworzą archipelagi takie jak: Jardines de la Reina, Los Canarreos, Los Colorados, Sabana-Camagüey.
Większość wyspy zajmuje nizina. W południowo-zachodniej części wznoszą się wapienne góry Sierra de los Organos. Wzdłuż południowych wybrzeży ciągnie się łańcuch gór Sierra Maestra z najwyższym szczytem Kuby – Pico Turquino (1974 m). Klimat podrównikowy, z przeciętną sumą opadów 1000–1500 mm. Pora deszczowa występuje między majem a październikiem. Średnie temperatury wynoszą w styczniu 20–24 °C, w gorącym sierpniu 25–28 °C. Jesienią dość często występują huragany.
Główne miasta: Hawana, Santiago de Cuba, Guantánamo, Bayamo, Santa Clara, Cienfuegos, Holguín, Camagüey, Pinar del Río, Matanzas, Las Tunas, Trinidad.
Na największej wyspie, Kubie, płynie ponad 200 rzek, jednak ich łączna długość nie przekracza 2500 km. Od czasu masowych wyrębów lasów przez Hiszpanów i wprowadzenia na równinach monokultur rolniczych, ilość wody w rzekach gwałtownie zmalała. Największa rzeka Kuby, Rio Cauto, która była żeglowna jeszcze przez dziesiątki lat po osiedleniu się tu Europejczyków, jest dzisiaj płytka i ledwie co płynie.
Prekolumbijskimi mieszkańcami wyspy byli Karaibowie, Tainowie i Sibonejowie. Dla Europy odkrył ją w 1492 Krzysztof Kolumb. W latach 1511–1514 została opanowana przez Hiszpanię. Stała się bazą dla hiszpańskiej konkwisty w tak zwanym Nowym Świecie. Wprowadzony przez kolonialne władze system encomienda przyniósł zagładę ludności indiańskiej. W pierwszej połowie XVI wieku rozpoczęto sprowadzanie na wyspy niewolników czarnoskórych 
którzy wykorzystywani byli na uprawach plantacyjnych. Od 1797 ważny punkt handlu niewolnikami. W pierwszej połowie XIX wieku Kuba była głównym producentem cukru w regionie, jednym z najbardziej rozwiniętych miejsc w Ameryce Łacińskiej, a także jedną z ostatnich kolonii hiszpańskich. Od końca XVIII wieku utrzymywała bliskie relacje handlowe ze Stanami Zjednoczonymi i w XIX wieku stała się obiektem ekspansjonizmu tego mocarstwa[6][7].
W pierwszej połowie XIX wieku na Kubie zaczęły się kształtować ugrupowania: reformistów (rzecznicy reform i rządów hiszpańskich), aneksjonistów (zwolenników włączenia Kuby do USA) i independentów (niepodległościowcy). W drugiej połowie XIX wieku obóz niepodległościowy zyskał szerokie poparcie a jego członkowie byli jednocześnie zwolennikami zniesienia niewolnictwa i demokracji. W 1868 roku wybuchło antykolonialne powstanie, które przerodziło się w trwającą dziesięć lat wojnę. Proklamowany przez rebeliantów rząd proklamował zniesienie niewolnictwa, a w 1869 roku ogłosił niepodległość wyspy jako republiki. Powstanie w kolejnych latach zostało stłamszone, niemniej jednak Hiszpania poszła na ustępstwa i przyznała Kubańczykom miejsce w hiszpańskim parlamencie. W 1880 roku rząd metropolii zniósł niewolnictwo na wyspie. W 1895 miało miejsce kolejne nieudane powstanie. W 1898 roku Stany Zjednoczone wywołały wojnę z Hiszpanią i zbrojnie przejęły kontrolę nad Kubą. Wojska amerykańskie okupowały kraj do 1902 roku i wymusiły na rządzie tego kraju przyjęcie w konstytucji tzw. poprawki Platta. Poprawka zapewniła Amerykanom prawo do ingerencji w wewnętrzne sprawy Kuby i założenia bazy morskiej w Guantánamo[6][7].
W 1906 roku doszło do drugiej interwencji wojsk amerykańskich, zakończonej dwuipółletnią okupacją. Po wycofaniu się zagranicznych wojsk władza w kraju spoczywała w rękach szeregu dyktatorów. W trakcie I wojny światowej rząd Kuby wziął udział w walkach po stronie państw Ententy. W latach 1925–1933 rządził generał Gerardo Machado. Machado został obalony podczas tzw. rewolty sierżantów a rzeczywistą władzę w kraju przejął jej lider, Fulgencio Batista. W 1934 roku Kuba zawarła z USA układ anulujący poprawkę Platta. W 1940 roku, kierujący dotąd państwem zza kulis (jako szef sztabu) Batista, został wybrany na prezydenta (do 1944). W 1941 roku Kuba symbolicznie wzięła udział w II wojnie światowej po stronie aliantów. Zaostrzenie wewnętrznego konfliktu politycznego w czasie zimnej wojny i kryzys gospodarczy skłonił Batistę do przeprowadzenia w 1952 roku zamachu stanu. Dyktator zalegalizował władzę w zbojkotowanych przez opozycjonistów wyborach z 1954. Rządy dyktatora obaliła rewolucja kubańska, w wyniku której władzę w 1959 objęła zbrojna opozycja zgrupowana w Ruchu 26 Lipca i Armii Powstańczej[6][7].
W 1959 roku premierem Kuby został przywódca rewolucji Fidel Castro a prezydentem Osvaldo Dorticós Torrado (do 1976). Rozwiązaniu uległy kongres, partie i organizacje społeczne. W 1960 roku powołano Komitety Obrony Rewolucji. Rząd zapoczątkował nacjonalizację gospodarki, zwłaszcza własności zagranicznej i realizację reformy rolnej. W proteście przeciwko takim działaniom rząd USA wprowadził w 1960 embargo handlowe (utrzymywane do dziś) a rok później zerwał stosunki dyplomatyczne z wyspą. Po takich działaniach USA rewolucjoniści zbliżyli się do Związku Radzieckiego. W wyniku rewolucji doszło do emigracji, którą szacuje się na około dwa miliony osób. Większość emigrantów udało się do Stanów Zjednoczonych. W kwietniu 1961 roku emigranci zorganizowani przez Central Intelligence Agency dokonali nieudanej inwazji w Zatoce Świń. Castro w następstwie inwazji (licząc na radzieckie wsparcie) proklamował „socjalistyczny” charakter rewolucji. W tym samym roku połączono organizacje rewolucyjne i wprowadzono system jednopartyjny (od 1965 roku rządzi ugrupowanie o nazwie Komunistyczna Partia Kuby). Rozmieszczenie na wyspie radzieckich rakiet doprowadziło w 1962 roku do kryzysu kubańskiego[6][7].
W 1976 roku dokonano tzw. instytucjonalizacji rewolucji: odbyły się pierwsze porewolucyjne wybory do parlamentu i uchwalono nową konstytucję. Rząd Kuby w latach zimnej wojny prowadził politykę zagraniczną, angażując się w sprawy państw takich jak Nikaragua, Grenada, Angola i Etiopia. Na początku lat 90. miejsce miał kryzys gospodarczy związany głównie z ograniczeniem importu ropy naftowej z byłego ZSRR. Przyczynił się on do zmniejszenia produkcji rolniczej i przemysłowej. W 2003 roku miejsce miały zorganizowane aresztowania opozycjonistów. W roku 2006 Boliwia, Wenezuela i Kuba utworzyły sojusz przeciw kapitalizmowi i imperializmowi, który nazwano „osią dobra przeciw osi zła”[6][7].
W sierpniu tego samego roku Fidel Castro przekazał władzę swojemu młodszemu bratu, Raúlowi Castro. Raúl w 2008 roku stanął na czele Rady Państwa, a w 2011 objął ster w partii. Jego rządy związane są z umiarkowaną liberalizacją gospodarczą i systemową[6][7]. Fidel Castro zmarł 25 listopada 2016 roku.
W 2021 r. władze kubańskie rozpoczęły reformę walutową w celu wyeliminowania istniejącego od połowy lat 1990. równoległego obiegu dwóch oficjalnych walut (peso i peso wymienialnego) oraz zapowiedziały reformy gospodarcze, które mają na celu zapobieżenie skutkom rozszerzenia embarga amerykańskiego oraz pobudzenie inwestycji na wyspie. 10 lutego 2021 Ministerstwo Pracy i Polityki Społecznej opublikowało listę branż niedostępnych dla Kubańczyków zainteresowanych samozatrudnieniem. Dotychczas obywatele mogli prowadzić jednoosobowe działalności gospodarcze w 127 branżach wymienionych w Narodowym Klasyfikatorze Aktywności Ekonomicznych[8], natomiast zapowiedziana zmiana rozszerzyła tę listę do 2100. Obecnie jedynie 124 branże są zarezerwowane dla aktywności spółdzielczej oraz państwowej[9].
Wyspa jest w większości zamieszkiwana przez Kubańczyków. W 2012 roku biali stanowili 64,1% mieszkańców, Metysi 26,6%, a czarnoskórzy 9,3%[10].
Struktura religijna w 2015 roku[11]:
Silnie widoczny jest synkretyzm religijny[12]. Według niektórych szacunków nawet 80% społeczeństwa (głównie katolików) praktykuje obrzędy kultu Santeria[13].
Kuba jest klasyfikowana jako republika socjalistyczna. Kierowniczą rolę w państwie sprawuje partia komunistyczna[7][14]. Władza ustawodawcza należy do jednoizbowego parlamentu, Zgromadzenia Narodowego Władzy Ludowej. To liczące 609 osób zgromadzenie, wybierane jest raz na pięć lat w wyborach powszechnych. Organ kierownictwa państwowego stanowi 31-osobowa Rada Państwa, wybierana przez parlament spośród jego członków. Przewodniczący Rady Państwa jest głową państwa i szefem rządu. Władza wykonawcza należy do rządu (powoływanego przez parlament na wniosek przewodniczącego Rady Państwa). Obecnie obowiązuje konstytucja z 1976 roku, która to została znowelizowana w 1992[15].
Na wyspie obowiązuje system jednopartyjny, w którym funkcjonuje Komunistyczna Partia Kuby (istniejąca od 1925 roku). Na emigracji działają ugrupowania dysydenckie[7][14].
Kuba jest podzielona na piętnaście prowincji i jeden specjalny okręg administracyjny.
Dochód narodowy na jednego mieszkańca wyniósł w 2004 roku 3 tysiące dolarów amerykańskich. Inflacja utrzymuje się na poziomie 3,1%. Pod względem struktury zatrudnienia dominuje sektor usług (51%) kolejne są przemysł (25%) i rolnictwo (24%). Dobrze rozwinięty przemysł hutniczy, elektrotechniczny, metalowy i chemiczny. Najważniejszymi partnerami handlowymi kraju są Holandia, Rosja, Niemcy i Chiny[7].
Kuba jest jednym z największych na świecie eksporterów cygar, są one czwartym towarem eksportowym tego kraju.
Najważniejszą instytucją naukową jest założona w 1962 roku Kubańska Akademia Nauk w Hawanie. Stanowi ona instytucję państwową do kierowania nauką i badaniami naukowymi oraz korporację uczonych. Oprócz niej działa Kubańska Akademia Języka związana z Hiszpańską Akademią Królewską oraz Akademia Nauk Medycznych, Fizyki i Przyrody (jedno z najstarszych towarzystw naukowych w regionie). Na Kubie działa pięć uniwersytetów (w Hawanie, Santiago de Cuba, Santa Clara, Pinar del Río i Camagüey)[16].
Prawa człowieka były powszechnie łamane za czasów dyktatury Batisty. Są one również łamane od czasu przejęcia władzy przez Castro. Dokumentowaniem morderstw politycznych oraz innych przypadków łamania praw człowieka zajmują się organizacje pozarządowe – Cuba Archive[17] oraz Amnesty International[18] Kubański kodeks karny (art. 91) przewiduje kary więzienia do 20 lat za czyny, które „zagrażają integralności państwa”. Wprowadzona w 1999 roku ustawa 88 przewiduje z kolei kary więzienia za „stwarzanie zagrożenia dla socjalistycznego państwa”[19].
Łączna liczba wojskowych wyniosła w 2015 roku około 90 tysięcy osób[20]. Obowiązuje zasadnicza służba wojskowa. Oprócz regularnej armii istnieją siły paramilitarne. W południowo-wschodniej części wyspy, nad zatoką Guantánamo znajduje się baza wojsk Stanów Zjednoczonych Naval Station Guantanamo Bay – stacjonuje w niej 2,3 tysiąca żołnierzy amerykańskich[21].
Meksyk (hiszp. México, nah. Mēxihco), oficjalnie Meksykańskie Stany Zjednoczone (hiszp. Estados Unidos Mexicanos) – państwo w Ameryce Północnej. Sąsiaduje ze Stanami Zjednoczonymi (na północy), z Oceanem Spokojnym (na zachodzie i południu), Zatoką Meksykańską i Morzem Karaibskim (na wschodzie) oraz z Gwatemalą i Belize (na południowym wschodzie).
Meksyk zajmuje powierzchnię prawie 2 mln km², co daje mu pod tym względem 14. miejsce na świecie[5]. Z prawie 130 mln mieszkańców jest 11. państwem pod względem liczby ludności[6]. Meksyk jest federacją 31 stanów i Dystryktu Federalnego, w którego skład wchodzi stolica – miasto Meksyk – jedno z najgęściej zaludnionych miast na świecie.
Wraz z Chile, Kolumbią i Kostaryką jest przedstawicielem Ameryki Łacińskiej w OECD, co było możliwe dzięki silnej pozycji w regionie i ustabilizowanej sytuacji gospodarczej[7][8]. Od momentu wejścia do porozumienia o wolnym handlu ze Stanami Zjednoczonymi Ameryki i Kanadą (NAFTA) w 1994 gospodarka Meksyku wkroczyła na ścieżkę dynamicznego rozwoju gospodarczego. W 2000 roku po raz pierwszy od kilkudziesięciu lat władzę utraciła PRI. Jej hegemonia została przerwana dopiero po wygranej Vicente Foxa w wyborach prezydenckich. Meksyk należy do grupy krajów nowo uprzemysłowionych[9] oraz należy do państw określanych jako wschodzące potęgi tzw. emerging powers[10][11]. Według przewidywań banku Goldman Sachs z 2012 roku, do 2020 roku Meksykańskie Stany Zjednoczone miały się stać siódmą pod względem wielkości gospodarką na świecie[12] i miały należeć do grupy najszybciej rozwijających się gospodarek. Domniema się, że do 2050 roku państwo ma się stać piątą co do wielkości gospodarką na świecie[13][14][15].
Meksyk jest państwem bogatym w surowce mineralne, jednakże z ograniczonym areałem ziemi uprawnej i z gwałtownie rosnącą populacją. Ponad połowa mieszkańców żyje w środkowej części kraju, podczas gdy rozległe rejony na suchej północy i tropikalnym południu są rzadko zaludnione. W ostatnich latach gospodarka kraju w dużej części opiera się na dochodach z turystyki i eksportu ropy naftowej. Wewnętrzna migracja ludności z rejonów wiejskich spowodowała gwałtowny rozrost ośrodków miejskich, w których mieszka obecnie ponad 2/3 ludności. Mimo poprawiających się wskaźników ekonomicznych duża część społeczeństwa w dalszym ciągu pozostaje biedna, co można przypisać częściowo kryzysowi z lat 80 XX w., który przejawiał się wysoką inflacją i ogromnym zadłużeniem zagranicznym.
Modernizacja i uprzemysłowienie pozostają w sprzeczności z tradycyjnym stylem życia, który przetrwał głównie na odizolowanych terenach wiejskich. Jego częścią są małe społeczności, złożone z indiańskich chłopów, którzy żyją podobnie jak ich przodkowie. Kulturalne pozostałości po wielkich cywilizacjach indiańskich (np. Chichén Itzá czy Tulum) współistnieją obok kolonialnych miast hiszpańskich, takich jak Taxco czy Querétaro. Różnice pomiędzy poszczególnymi regionami, wyrażające się w kulturze, architekturze, kuchni i sposobie gospodarowania są cechą charakterystyczną dla współczesnego Meksyku.
Meksyk położony jest w jednym z najaktywniejszych tektonicznie obszarów na świecie. Z tego też powodu nie są tu rzadkością trzęsienia ziemi i erupcje wulkaniczne. Citlaltépetl (5636 m n.p.m.) i Popocatépetl są przykładami aktywności wulkanicznej, która stosunkowo niedawno doprowadziła do ukształtowania powierzchni południowej części kraju. Meksyk leży na zachodnim krańcu płyty północnoamerykańskiej, która styka się z płytami pacyficzną i karaibską[16]. Na styku tych wielkich form geologicznych doszło do znacznego wypiętrzenia się terenu, którego efektem są liczne pasma górskie.
Pod względem geologicznym w Meksyku można wyróżnić osiem głównych regionów. Najważniejszym jest Wyżyna Meksykańska (północna i środkowa część kraju). Swój początek bierze ona na przesmyku Tehuantepec i dochodzi do granicy ze Stanami Zjednoczonymi. Wyżyna składa się z zasadniczej środkowej części, od której odchodzą liczne odgałęzienia. Z północy na południe zwiększa się jej średnia wysokość n.p.m. (na północy jest to średnio 1200 m, a na południu przy mieście Meksyk już 2400 m)[17].
Środkową partię Wyżyny Meksykańskiej można podzielić na dwie części. Pierwszą stanowi Mesa Północna (zaczyna się przy granicy amerykańskiej i kończy niedaleko stanu San Luis Potosí). Jest to stosunkowo suchy i bezodpływowy obszar, istnieje tu kilka stałych cieków wodnych. Z kolei Mesa Centralna (zaczyna się w stanie San Luis Potosí i dochodzi do miasta Meksyk) została uformowana przez działalność wulkaniczną. Mesa Centralna jest bardziej zasobna w wilgoć niż Mesa Północna, wyżej położona i bardziej płaska. Znajduje się tu szereg wąwozów i zagłębień, oddzielonych od siebie wulkanami. Niziny znajdujące się na Mesie Centralnej są z reguły niewielkie powierzchniowo (największe z nich nie przekraczają 250 km²). Są one stosunkowo żyzne i stanowią tradycyjny spichlerz kraju. W niższych częściach Mesy Centralnej istniały dawniej jeziora, które w większości zostały osuszone po pojawieniu się hiszpańskich kolonistów.
Wyżyna Meksykańska jest otoczona przez trzy pasma górskie: Sierra Madre Zachodnią[18] (na zachodzie), Sierra Madre Wschodnią[19] (na wschodzie) oraz Kordylierę Wulkaniczną[20] (na południu). Najwyższym szczytem Sierra Madre Zachodniej jest Cerro Agua Caliente (3315 m n.p.m.). Pasmo to zbudowane jest głównie z prekambryjskich łupków krystalicznych oraz z wapieni i piaskowca. Sierra Madre Zachodnia opada stopniowo ku Zatoce Kalifornijskiej. Granice Sierra Madre Wschodniej wyznaczają bieg rzeki Rio Grande (na północy) i Kordyliera Wulkaniczna (na południu). Najwyższym szczytem tego pasma jest Pena Nevada (4054 m n.p.m.). Zbudowane jest głównie z mezozoicznych i trzeciorzędowych wapieni i piaskowców, miejscami przykrytych skałami pochodzenia wulkanicznego. Sierra Madre Oriental opada stromo ku Zatoce Meksykańskiej. Na obszarze Kordyliery Wulkanicznej znajdują się potężne wulkany: Popocatépetl, Iztaccíhuatl (5286 m n.p.m.) i Toluca (4558 m n.p.m.).
Na wschód i zachód od Wyżyny Meksykańskiej rozciągają się nadbrzeżne równiny. Równina nad Zatoką Meksykańską rozpościera się na długości ponad 1400 km, począwszy od amerykańskiego stanu Teksas aż po wybrzeża Jukatanu. Charakterystyczne dla niej są laguny i rozległe nisko położone bagna. Równina nad Pacyfikiem jest dużo węższa, swój początek bierze przy Dolinie Mexicali a kończy się na południu, niedaleko Tuxpan. Większość jej obszaru przylega do Zatoki Kalifornijskiej. Rejon ten jest dosyć ubogi w wodę i dlatego uprawa roli jest tu możliwa głównie w oparciu o nawadnianie.
Odizolowany od reszty kraju pozostaje półpustynny Półwysep Kalifornijski, o długości prawie 1300 km i szerokości ok. 160 km.
Na południe od Wyżyny Meksykańskiej leży depresja Balsas, która swoją nazwę wzięła od największej rzeki tego regionu. Charakterystyczne dla niej są niewielkie wzgórza, które są poprzecinane wąwozami i parowami. Klimat w tym regionie jest suchy i gorący.
Na południu rozciągają się wysoko położone wyżyny i pasma górskie. Najważniejszym z nich jest stosunkowo niewysoka (2000–2400 m n.p.m.) Sierra Madre Południowa. Graniczy ona z malowniczą „Riwierą Meksykańską” (m.in. Acapulco), która jest chętnie odwiedzana przez turystów. Sierra Madre del Sur położona jest między Kordylierą Wulkaniczną a przesmykiem Tehuantepec. Jej najwyższym szczytem jest Teotepec (3703 m n.p.m.). W jej skład wchodzą głównie granity i skały wulkaniczne.
Następnym ważnym pasmem na południu jest Sierra Madre de Chiapas – zbudowana z krystalicznych bloków skalnych i biegnąca wzdłuż Pacyfiku od przesmyku Tehuantepec do terytorium Gwatemali. Na terytorium stanu Oaxaca (południowa część kraju) znajduje się Mesa Południowa, składająca się z płaskowyżów z licznymi dolinami. Średnia wysokość n.p.m. sięga tu 1200–1500 m.
Przesmyk Tehuantepec, oddzielający Ocean Spokojny od wód Zatoki Meksykańskiej wznosi się na wysokość 270 m n.p.m. Jego środkowa część składająca się z niewysokich wzgórz po obydwu stronach styka się z nadbrzeżnymi równinami. Wyżyny w stanie Chiapas, będące przedłużeniem pasm górskich z Ameryki Środkowej, mają formę uskoku otaczającego wysoko położoną dolinę. W kierunku północno-zachodnim leży dolina rzeki Grijalva, pomiędzy nią a Równiną Tabasco znajduje się kilka mniejszych pasm z charakterystycznymi uskokami.
Na północny wschód od Równiny Tabasco znajduje się półwysep Jukatan. Tworzą go głównie skały wapienne, teren jest tu płaski i rzadko przekracza 150 m n.p.m. Z uwagi na węglanowe podłoże wykształcił się tu kras, m.in. głębokie jaskinie podziemne, powstałe w wyniku działalności wody oraz tzw. cenotas (l.p. cenote) – studnie krasowe, które były dla prekolumbijskich Majów podstawowym źródłem wody.
Największe rzeki i zbiorniki wodne w Meksyku znajdują się w środkowej części kraju. Lerma bierze swój początek w basenie Toluca, a następnie kieruje się na zachód, gdzie formuje największe naturalne jezioro kraju – Chapala. Z Chapali wypływa z kolei Santiago, która kieruje się na północny zachód poprzez Sierra Madre Zachodnią i wpada następnie do Pacyfiku. We wschodniej części Mesy Centralnej znajduje się grupa rzek Moctezuma-Pánuco, która wydrążyła głębokie wąwozy w masywach Sierra Madre Wschodniej, ich bieg kończy się w wodach Zatoki Meksykańskiej. Z dawniej charakterystycznych dla Mesy Centralnej licznych jezior ocalały jedynie niewielkie pozostałości, takie jak m.in. Pátzcuaro i Cuitzeo.
Z depresji Balsas odpływa rzeka o tej samej nazwie, z którą łączą się również mniejsze dopływy z Mesy Centralnej. Rzeka Balsas została w wielu miejscach przecięta zaporami wodnymi (rejon Sierra Madre Południowej), które stanowią ważne źródło energii elektrycznej. Na południowym wschodzie Usumacinta i Grijalva wypływają z wilgotnych wyżyn stanu Chiapas. Razem z rzeką Papaloapan (która wpada do Zatoki Meksykańskiej na południe od Veracruz) Usumacinta i Grijalva stanowią przeszło 40% systemu rzecznego Meksyku.
Północna część Meksyku ze swoim suchym klimatem jest uboga w duże cieki wodne. Największą rzeką jest tutaj Río Grande, która tworzy naturalną granicę ze Stanami Zjednoczonymi Ameryki. Dopływ Rio Grande – Conchos bierze swój początek na Mesie Północnej. Ze względu na bliskość Sierra Madre Zachodniej i Wschodniej rzeki na wschodnim i zachodnim wybrzeżu są krótkie i płyną wartkim nurtem. Na nizinach rozciągających się nad Pacyfikiem – Yaqui, Fuerte i Culiacán posłużyły do stworzenia rozbudowanego systemu nawadniania terenów rolnych. Skąpe opady w rejonie Półwyspu Kalifornijskiego oraz porowate wapienne skały wchodzące w skład Jukatanu sprawiły, że w regionach tych praktycznie brak jest większych powierzchniowych rzek.
Południowa część kraju ze względu na swój tropikalny charakter w przeważającej części składa się z gleb laterytowych. Posuwając się na południowy wschód można zauważyć, że ziemia staje się bardziej czerwonawa i żółta, co jest spowodowane występowaniem dużych ilości wodorotlenków żelaza i aluminium i wypłukiwaniem cennych składników przez wodę co czyni ją słabo przydatną dla rolnictwa. Z kolei Mesa Centralna obfituje w żyzne gleby wulkaniczne, które już w epoce prekolumbijskiej były intensywnie uprawiane. Intensywne rolnictwo wywołało jednak przyśpieszoną erozję tych terenów. Sucha północ charakteryzuje się brązowo-szarymi ziemiami, bogate w wapień po odpowiednim nawodnieniu dają one wysokie plony, przy czym duże zasolenie stwarza problemy w ich wykorzystaniu.
Meksyk ze względu na dużą rozciągłość szerokości geograficznej i zróżnicowanie topograficzne cechuje się dużą różnorodnością klimatyczną. Ponad połowa kraju znajduje się pod zwrotnikiem Raka. W rejonach tych wilgotne i morskie masy powietrza znad Zatoki Meksykańskiej, Morza Karaibskiego i Pacyfiku są ściągane przez niskie ciśnienie atmosferyczne. Są one głównym źródłem opadów, których szczyt przypada na okres od maja do sierpnia. Zarówno wschodnie, jak i zachodnie wybrzeże jest narażone na często występujące tam huragany (w szczególności w sierpniu i w październiku). Północna część kraju, ze swoimi pustyniami i półpustyniami, nie obfituje w opady, co jest spowodowane głównie przez wyż baryczny panujący nad Pacyfikiem.
Różnice temperatur w wilgotniejszych i tropikalnych częściach Meksyku są niewielkie (rzadko przekraczają 5 °C pomiędzy najzimniejszym i najcieplejszym miesiącem). Okres zimowy można tu raczej zdefiniować jako porę deszczową. Na klimat w tym regionie największy wpływ ma położenie nad poziomem morza a poszczególne strefy klimatyczne ułożone są południkowo. Na wysokościach od 0 do 900 m temperatury są wysokie, obszary te są określane nazwą tierra caliente (hiszp. – gorąca ziemia). Przykładowo Veracruz położone nad Zatoką Meksykańską ma średnią dzienną temperatur ok. 25 °C. Tierra templada (hiszp. – ziemia umiarkowana) obejmuje regiony gdzie wysokość nie przekracza 1800 m. W Jalapie położonej 1400 m n.p.m. średnia temperatur wynosi ok. 19 °C. Tierra fria (hiszp. – zimna ziemia) to obszary sięgające 3300 m n.p.m. Pachuca znajdująca się na wysokości 2400 m cechuje się średnią roczną temperatur ok. 15 °C. Nad terra fria znajdują się jeszcze tzw. paramos (górskie pastwiska, na których zalega niekiedy śnieg), występują one głównie w środkowej części kraju na terenach od 3900 do 4200 m n.p.m.
Im bardziej na północ od rejonów tropikalnych, tym większa staje się temperatura, która najwyższe poziomy osiąga w środkowej części Mesy Północnej. Amplitudy temperatur pomiędzy zimą i latem są tutaj znaczące. W Kalifornii Dolnej oraz w północnej części pustyni Sonora nierzadko w lipcu i sierpniu powietrze rozgrzewa się do nawet 43 °C. Poza wysokimi masywami górskimi i północną częścią Mesy Północnej temperatura rzadko spada poniżej 0 °C.
Meksyk jest krajem, w którym opady są w większości rejonów przeważnie niewystarczające. Z wyjątkiem wyżynnych części Sierra Madre Wschodniej i Zachodniej oraz nadbrzeżnej równiny nad Zatoką Meksykańską na tereny powyżej Zwrotnika Raka spada mniej niż 500 mm deszczu rocznie. Prawie cały stan Kalifornia Dolna oraz większość Sonory i Chihuahua otrzymują nie więcej jak 250 mm opadów rocznie. Duża część środkowych i południowych regionów kraju cechuje się poziomem opadów poniżej 1000 mm rocznie (które występują głównie w okresie od maja do sierpnia). Jedynie obszary obejmujące równinę nad Zatoką Meksykańską oraz przyległe tereny od Tampico po Villahermosa oraz Chiapas i półwysep Jukatan dysponują obfitymi całorocznymi opadami.
Upalne regiony stanów Kalifornia Dolna, Sonora oraz przeważająca część północnego i środkowego Meksyku porośnięte są niską i sucholubną roślinnością. W wyżej położonych regionach Sonory, Chihuahua oraz Coahuila i Tamaulipas występują trawy, krzewy oraz wiele gatunków kaktusów i innych sukulentów. Charakterystyczne dla tego obszaru jest rzadko występujące drzewo boojum (środkowa Kalifornia Dolna). Półsucha część depresji Balsas porośnięta jest niskimi krzewiastymi zaroślami, karłowatymi drzewami oraz kaktusami.
Większość Sierra Madre Zachodniej i Mesy Centralnej była pierwotnie pokryta wiecznie zielonymi lasami iglastymi oraz drzewami liściastymi. Podobna roślinność rozciąga się również dalej na południe w kierunku wyżyn. Z uwagi na długą obecność człowieka w tym regionie doszło do zubożenia bogactwa przyrodniczego, co przejawia się w ograniczeniu pasma lasów tylko do wyższych partii Sierra Madre Zachodniej. W ich skład wchodzą przeważnie sosny oraz jodły. Wystarczy wspomnieć, że w okresie 1990-2005 trwający proces wycinki lasów spowodował średni roczny ubytek w areale o ok. 0,5%, łącznie zniknęło więc 260 tys. ha lasów.
Bogate w duże opady deszczu równiny nadbrzeżne, wyżyny Chiapas oraz południową część półwyspu Jukatan pokrywa gęsty las równikowy (selva). W jego skład wchodzą m.in. szerokolistne drzewa o różnych wysokościach, w tym palmy, cenione ze względu na twarde drewno odmiany Bocote, Chechen i Machichi, a na wybrzeżu mangry. W mniej wilgotnych rejonach występują gatunki roślin charakterystyczne dla stepów i sawann.
Na wschodnim wybrzeżu występują lasy kolczaste, zrzucające okresowo liście, i wiecznie zielone lasy podzwrotnikowe. Południowo-zachodnie wybrzeże porastają również suche lasy kolczaste.
Takie zwierzęta jak: małpy, tapiry, jaguary, które są charakterystyczne dla Ameryk Środkowej i Południowej, posiadają również swoje siedliska w wilgotnych lasach południowego Meksyku. Ponadto południowa część kraju ze względu na niewielką gęstość zaludnienia pozostaje ostoją dla wielu rzadkich odmian zwierząt[21].
Środkowy i północny Meksyk ze względu na silne oddziaływanie człowieka na środowisko utraciły swoją pierwotną różnorodność biologiczną. Jednakże można tu spotkać głównie w parkach i rezerwatach m.in. niedźwiedzie, jelenie, pumy czy kojoty. W zimie do Sierra Madre Occidental przybywają liczne grupy kaczek i gęsi. Na pustynnych i półpustynnych częściach Meksyku występują króliki, węże i pancerniki. Mimo podejmowanych przez obrońców środowiska prób w 2004 r. zagrożonych wyginięciem było 57 gatunków ptaków, 72 gatunki ssaków, 21 gatunków gadów i 106 gatunków ryb.
Ocelot
Żarłacz biały
Papuga
Iguana
Wydobycie surowców mineralnych w nowożytnej historii Meksyku zawsze było ważną częścią gospodarki kraju. Srebro przez długi czas stanowiło najistotniejszą pozycję eksportową; także w chwili obecnej Meksyk jest czołowym producentem tego kruszcu na świecie. Główne ośrodki jego wydobycia koncentrują się na obszarze tzw. „srebrnego pasa”, rozciągającego się od Guanajuato i Zacatecas na Mesie Centralnej aż po Chihuahua na Mesie Północnej i San Luis Potosi na wschodzie. Oprócz srebra pozyskuje się w tym rejonie również: cynk, ołów, rtęć, antymon, mangan i kadm.
Złoża żelaza znajdują się na terenie stanu Durango, ich eksploatacja została zapoczątkowana w pierwszej połowie XX w. Dzięki nim możliwe było uruchomienie licznych hut, które przyczyniły się do rozwoju przemysłu w Monterrey. Największym zagłębiem węgla kamiennego jest Sabinas (na północ od Monterrey), tutejszy węgiel ze względu na dużą kaloryczność jest wykorzystywany do produkcji koksu. Niedaleko Santa Rosalía (Kalifornia Dolna) pod koniec XIX w. odkryto bogate pokłady miedzi, jednakże zostały one już praktycznie wyczerpane. W chwili obecnej największe ilości miedzi wydobywa się niedaleko Cananea i La Caridad w północnej części stanu Sonora.
Od połowy lat 70 XX w. głównym surowcem eksportowym Meksyku jest ropa naftowa. Prawie 70% dochodów z eksportu kraju pochodzi właśnie ze sprzedaży ropy naftowej, która jest dostarczana głównie do Stanów Zjednoczonych Ameryki. Pierwsze szyby naftowe w Meksyku powstały na początku XX w. koło Tampico nad Zatoką Meksykańską. Z biegiem czasu odkrywano kolejne pola naftowe, w szczególności koło przesmyku Tehuantepec. Do momentu nacjonalizacji przedsiębiorstw naftowych w 1938 ogromna większość wydobycia była kierowana za granicę. W 1938 utworzony został Pemex (Petróleos Mexicanos) – spółka państwowa, której zadaniem jest poszukiwanie, produkcja i handel ropą naftową i gazem ziemnym.
Najwięcej działających platform wiertniczych jest w Zatoce Meksykańskiej. Główne pola naftowe to: Poza Rica (niedaleko Tuxpan), Tampico-Misantla oraz wybrzeże w stanach Chiapas i Tabasco. Główne rejony wydobycia gazu ziemnego są położone koło Reynosy w północno-wschodniej części kraju oraz w okolicach regionu Chiapas-Tabasco. Meksyk w dalszym ciągu posiada jeszcze duże rezerwy nieeksploatowanych złóż ropy i gazu.
Ze względu na zróżnicowanie geograficzne i klimatyczne Meksyk jest producentem wielu różnorodnych produktów rolnych. Pomimo tego, że hodowla i rolnictwo były od zawsze podstawą gospodarki, to Meksyk nie posiada zbyt dużego areału ziemi rolnej. Większość kraju jest zbyt uboga w opady albo zbyt skalista. W wielu rejonach konieczne jest nawadnianie. Szacuje się, że tylko 20% powierzchni nadaje się pod produkcję rolną. W rzeczywistości tylko 10–12% ziemi jest przygotowywane pod uprawę, a później tylko z połowy ze względu na trudne warunki klimatyczne zbiera się plony. Ponadto 20% gruntów jest nawadnianych.
Meksyk tylko w 1/5 części jest porośnięty lasami, ocenia się, że w XVI w. stosunek zalesienia wynosił 2/3. Ze względu na gwałtowną eksploatację lasów ich powierzchnia ulegała radykalnemu zmniejszeniu. Z jednej strony w niektórych rejonach Sierra Madre Zachodniej rozpoczęto program systematycznego zalesienia, ale z drugiej na południu prowadzi się nadal wzmożoną wycinkę lasów deszczowych, które są zamieniane następnie w pastwiska.
Meksyk dysponuje bogatymi łowiskami rybnymi, jednakże ryby nie stanowią głównego składnika diety przeciętnego Meksykanina. Szczególnie bogata w owoce morza jest Zatoka Kalifornijska (m.in. marliny, krewetki). Na południe od Półwyspu Kalifornijskiego łowi się tuńczyka. W południowych rejonach Zatoki Meksykańskiej, niedaleko Jukatanu wody obfitują w różne odmiany ryb tropikalnych. Północna część Zatoki jest wykorzystywana do odławiania krewetek
Kraj dysponuje korzystnym układem rzek, które umożliwiają produkcję „czystej” energii elektrycznej. Od lat 40 XX w. i 50 XX w. rozpoczęto szereg programów mających na celu budowę hydroelektrowni. Większość z nich zlokalizowana została we wschodniej i południowej części Mesy Centralnej. Rozwój technologii umożliwił budowę tam w bardziej niedostępnych regionach na rzece Grijalva na obrzeżach wyżyn Chiapas.
Historia Meksyku jako niepodległego państwa ma swój początek 24 sierpnia 1821 r., kiedy to po trwającej ponad 10 lat wojnie o niepodległość wojska hiszpańskie zostały pokonane przez oddziały kreolskie pod dowództwem Augustyna I, a wicekról Juan Ruiz de Apodaca musiał abdykować. Na terenach współczesnego Meksyku istniało kilka cywilizacji prekolumbijskich, m.in. Olmekowie, Zapotekowie, Majowie, Toltekowie i najbardziej znani – Aztekowie. Ci ostatni stworzyli potężne imperium ze stolicą w Tenochtitlán (czyt. tenocztitlan), położoną w miejscu obecnego Miasta Meksyk. Państwo Azteków istniało od ok. połowy XIII w. do 1521 r. i obejmowało w szczytowym okresie potęgi znaczną część tzw. Mezoameryki.
Cywilizacja i imperium Azteków zostały zniszczone przez hiszpańskich konkwistadorów pod wodzą Hernána Cortésa w latach 1518–1521. Dzięki przebiegłości, wykorzystaniu wewnętrznych słabości państwa Azteków, przewadze uzbrojenia i dużej dozie szczęścia, stosunkowo niewielkie siły konkwistadorów, liczące ok. 600 osób, zdołały opanować imperium posiadające początkowo kilkusettysięczną[22] armię.
Po podboju Azteków na terenie współczesnego Meksyku utworzone zostało hiszpańskie wicekrólestwo Nowa Hiszpania. Ogromną rolę w tym okresie odgrywał Kościół katolicki, a zwłaszcza jezuici, którzy stworzyli zręby systemu edukacji, pomocy społecznej i położyli podwaliny pod rozwój kulturalny kraju. W tym czasie doszło do wymieszania się ludności pochodzącej z Hiszpanii z miejscowymi Indianami i czarnymi niewolnikami sprowadzanymi z Afryki. Podstawami gospodarki były wówczas wydobycie srebra i rolnictwo. Władzę w państwie sprawowali rodowici Hiszpanie, z którymi silnie konkurowali urodzeni na miejscu biali, zwani Kreolami. Ludność mieszana rasowo (Metysi) i Indianie stanowili podstawową siłę roboczą i tworzyli dolne warstwy społeczeństwa.
Po wojnie o niepodległość, początkowo utrzymano monarchię, tworząc Cesarstwo Meksyku. Jednak w 1823 roku zaprowadzono ustrój republikański. Młoda republika straciła Teksas w wyniku secesji przeprowadzonej przez amerykańskich osadników. W 1845 roku wybuchła wojna amerykańsko-meksykańska, która doprowadziła kraj do ruiny. Kilka lat później, w czasie rządów Benito Juáreza, Meksyk został zaatakowany przez Francję. Francuzom udało się na pewien czas opanować państwo i przekształcić je w Drugie Cesarstwo z Maksymilianem I jako władcą. Armię republikańską, pod wodzą Juáreza, poparły, po zakończeniu wojny secesyjnej, Stany Zjednoczone. W 1867 r. republikanie pojmali Maksymiliana, który został rozstrzelany, a samo cesarstwo legło w gruzach.
W 1877 roku, już po śmierci Juáreza, władzę przejął Porfirio Diaz, który sprawował dyktatorskie rządy aż do 1911 roku, kiedy stracił władzę i udał się na wygnanie na skutek wzrastającego oporu społeczeństwa i wzrostu znaczenia opozycji. Władzę po nim objął Francisco Madero. Od tego momentu zaczęły się destabilizacja kraju, chaos polityczny i przewroty wojskowe. W międzyczasie w 1917 przyjęto nową, liberalną konstytucję, która jednak nie pomogła krajowi wyjść z kryzysu.
Po wyborze Plutarco Elíasa Callesa na prezydenta w 1923 r. rozpoczęła się długa era rządów Partido Revolucionario Institucional (PRI) – Partii Rewolucyjno-Instytucjonalnej, która rządziła metodami autorytarnymi praktycznie aż do wyborów prezydenckich w 2000 roku. Partia ta początkowo miała charakter lewicowo-populistyczny. Opowiadała się za reformą rolną i nacjonalizacją przemysłu, później jednak jej program coraz bardziej ewoluował w kierunku rządów technokratów i zwolenników gospodarki wolnorynkowej. W latach 30. XX wieku Meksyk zaczął eksploatować duże zasoby ropy naftowej i, w czasach prezydentury Lázaro Cárdenasa, stał się ważnym eksporterem tego surowca. W czasie II wojny światowej, pod wpływem USA, Meksyk dołączył do aliantów i wypowiedział wojnę państwom Osi. Wielu Meksykanów walczyło wówczas w szeregach United States Army.
Po II wojnie światowej Meksyk przeżył kilka kryzysów gospodarczych, cały czas pozostając pod rządami PRI. Po kryzysie z 1976 roku PRI radykalnie zmieniła politykę gospodarczą, otwierając kraj na kapitał zagraniczny. Do władzy zaczęli dochodzić młodzi, żądni sukcesu działacze partyjni, którzy w większości zostali wykształceni w USA. W 1988 roku nastąpił rozłam w PRI. W wyborach prezydenckich startowało dwóch kandydatów wywodzących się z tej partii. Wybory „wygrał” w niezbyt jasnych okolicznościach oficjalny kandydat PRI Carlos Salinas de Gortari, który okazał się zręcznym politykiem. Dzięki niemu Meksyk odzyskał równowagę ekonomiczną, a w 1992 roku podpisał ze Stanami Zjednoczonymi i Kanadą umowę dającą początek Północnoamerykańskiemu Porozumieniu Wolnego Handlu (NAFTA).
W 1994 roku wybuchło powstanie kierowane przez Zapatystowską Armię Wyzwolenia Narodowego. Równolegle pojawiło się ryzyko głębokiego kryzysu gospodarczego, które zostało zażegnane szybką ekonomiczną interwencją USA i Międzynarodowego Funduszu Walutowego. Pod koniec 1997 roku miało miejsce drugie powstanie zapatystów, połączone z aferami korupcyjnymi, które łącznie podkopały zaufanie społeczne do rządu. PRI po raz pierwszy utraciła większość w parlamencie. W wyborach w 2000, po raz pierwszy od 1929 roku, wygrał kandydat opozycyjnej partii, Vicente Fox. Jego następcą został Felipe Calderón. W 2012 wybory wygrał Enrique Peña Nieto z PRI.
Na przełomie marca i kwietnia 2009 roku w Meksyku wybuchła epidemia grypy H1N1.
W 2014 roku w kraju pojawiły się kontrowersje, kiedy rząd Enrique Peña Nieto nakazał porwanie, torturowanie i zamordowanie 43 studentów z miasta Ayotzinapa.[23]
Ze względu na znaczące różnice w środowisku geograficznym, strukturę rasową i sieć osadniczą, w Meksyku doszło do wytworzenia się zwartych regionów kulturowych. Kraj można podzielić na hiszpańsko-metyską północ i indiańskie południe. Podział ten przypomina podobną sytuację w czasach prekolumbijskich, gdy również na obszarze Mesy Centralnej i na południu istniały rozwinięte cywilizacje, natomiast na północy funkcjonowała bardziej prymitywna kultura grup zbieracko-myśliwskich.
Północny Meksyk jest terenem rzadko zaludnionym z nielicznymi skupiskami ludzkimi. W jego ramach można wyróżnić cztery części. Największa z nich pokrywa się z grubsza z Mesą Północną. Górnictwo i rolnictwo wprowadzone tu przez Hiszpanów w XVI stuleciu nadają mu specyficzny charakter, w ostatnim okresie realizowane programy nawadniania ziemi oraz duże inwestycje przedsiębiorstw amerykańskich przyczyniły się do zróżnicowania źródeł utrzymania mieszkańców. Z kolei część północno-wschodnia rozciąga się od Tampico do granicy ze Stanami Zjednoczonymi, a w głębi lądu do Sierra Madre Wschodniej. Pierwotna ludność indiańska została wyparta przez hiszpańskich osadników, którzy stworzyli wielkoobszarowe rancza i farmy. Przez długi czas był to jeden z najuboższych rejonów Meksyku. Jednak z uwagi na rozwijające się wydobycie ropy naftowej, powstanie przemysłu hutniczego i przeprowadzenie licznych projektów irygacyjnych wzdłuż Río Bravo del Norte doszło do znacznej poprawy sytuacji gospodarczej.
Północny zachód zajmuje część Sierra Madre Zachodniej i rozciąga się na południe od granicy z USA do stanu Nayarit. Przed podbojem hiszpańskim znajdowały się tu liczne grupy Indian. W chwili obecnej pozostały po nich w odizolowanych regionach tylko plemiona Seri i Tarahumara. W początkowym okresie kolonizacji przeważało tu górnictwo jednak współcześnie najważniejszą rolę odgrywa rolnictwo (a w szczególności hodowla bydła).
Kalifornia Dolna – historycznie jeden z najtrudniej dostępnych rejonów Meksyku pozostaje nadal rzadko zaludniony, mieszkańcy koncentrują się głównie w niewielkich miasteczkach położonych na przeciwnych końcach Półwyspu Kalifornijskiego. Nieliczni Indianie w XVIII w. padli ofiarą przyniesionych tu przez misjonarzy chorób. Kolonizacja europejska koncentrowała się głównie w wilgotniejszych miejscach (np. San Ignacio czy Mulejé), które dawały możliwość uprawy roli.
W południowym Meksyku bardziej widoczne są ślady wielkich cywilizacji prekolumbijskich. Najprężniejszym ich ośrodkiem był środkowy Meksyk (Mesa Centralna i sąsiednie wyżyny). Prym wiedli Aztekowie, chociaż istniało tu wiele innych mniejszych plemion. Po przybyciu Hiszpanów sytuacja nie uległa istotnej zmianie – było to nadal centrum Wicekrólestwa Nowej Hiszpanii. Region ten stał się ośrodkiem gospodarczym Nowej Hiszpanii. Duże zagęszczenie ludności sprawiło, że mogła tu rozkwitnąć uprawa roli wymagająca dużej liczby siły roboczej. Doszło do integracji miejscowych Indian z białymi przybyszami oraz z czarnymi niewolnikami. Pierwotni mieszkańcy przetrwali tylko w nielicznych enklawach (np. Michoacán, Mezquital czy Toluca), ich obecność silnie kontrastuje z przeludnionym i silnie zurbanizowanym miastem Meksyk.
Region zachodni ma swój ośrodek w mieście Guadalajara i obejmuje stany: Jalisco, Colima, Nayarit, Zacatecas i Guanajuato. Przeważa tu ludność wiejska, której głównym zajęciem pozostaje rolnictwo. Od najbardziej zamierzchłych czasów był to „spichlerz” kraju, co jest spowodowane urodzajnością gleb oraz korzystnymi warunkami wodnymi i klimatycznymi. W ostatnich latach rozwija się tu również intensywnie przemysł, w szczególności w Querétaro, Salamanca i Irapuato. Nadmorskie Manzanillo pozostaje jednym z największych portów pacyficznych. Takie charakterystyczne rzeczy dla kultury meksykańskiej jak: tequila, sombrero czy muzyka mariachi wzięły swój początek właśnie na zachodzie.
Rejon Balsas jest rzadko zaludniony z uwagi na gorący i suchy klimat. Głównymi zajęciami ludności pozostają tu hodowla oraz ekstensywna uprawa roli.
Południowe wyżyny obejmujące większość stanów: Michoacán, Guerrero i Oaxaca są słabo rozwinięte gospodarczo, co przekłada się na niski stopień zamożności. Tutaj też w największej liczbie koncentruje się ludność indiańska (Zapotekowie, Mistekowie), która zajmuje niewielkie gospodarstwa rolne i posługuje się tradycyjnymi metodami uprawy roli. Jedynie nadbrzeżne kurorty turystyczne (Acapulco, Puerto Escondido) wyróżniają się na tle powszechnie panującego tu ubóstwa.
Kolejnym regionem jest wybrzeże nad Zatoką Meksykańską, ze stanami Tabasco i Veracruz i przyległymi do niego zboczami Sierra Madre Wschodniej. Przeważają tu Metysi, jedynie w wyżej położonych częściach znajdują się niewielkie społeczności indiańskie. Przez długi czas Veracruz było głównym „oknem na świat” tego regionu i także współcześnie odgrywa dużą rolę, zarówno pod względem ekonomicznym, jak i kulturowym. Na prowincji rolnictwo towarowe i hodowla są najważniejszym źródłem utrzymania. Południowa część regionu do niedawna była niemal niedostępna dla osadnictwa ze względu na bagnisty i malaryczny charakter, jednakże po przeprowadzeniu licznych inwestycji mających na celu uregulowanie stosunków wodnych tereny stały się bardziej przydatne gospodarczo.
Większość stanu Chiapas jest odizolowana od reszty kraju. Rolnictwo towarowe (w szczególności uprawa bawełny) ma szanse rozwoju na wybrzeżu pacyficznym, w reszcie stanu dominuje pasterstwo i uprawa roli na własne potrzeby. Indianie stanowią większość na obszarze północnych wyżyn wokół San Cristóbal de las Casas, Metysi przeważają na południu.
Centrum dawnej cywilizacji Majów na Jukatanie nadal pozostaje „indiańskie”. Region ten słynie z licznych stanowisk archeologicznych (Chichén Itzá i Uxmal i Tulum). Pod koniec XIX w. w Méridzie – jedynym większym mieście w tej części kraju rozkwitła produkcja agawy (henequen) co doprowadziło do rozkwitu gospodarczego. W tropikalnych lasach deszczowych prymitywne rolnictwo i zbieractwo dają utrzymanie niewielkim społecznościom zamieszkującym ten rejon Jukatanu.
W epoce prekolumbijskiej ludność koncentrowała się w zachodniej, środkowej i południowej części regionu wyżyn. Pierwsi Hiszpanie osiedlali się w dotychczas istniejących miastach indiańskich – w celu łatwiejszego wykorzystania siły roboczej i sprawowania nadzoru nad tubylcami. Poza centralną częścią kraju osadnictwo było rozproszone i rzadkie, ograniczało się głównie do takich placówek jak: kopalnie, misje zakonne czy forty. Początkowo górnictwo odgrywało największą rolę przy koncentracji nowych osadników. W XVI w. powstały liczne osady górnicze, w których wydobywano głównie srebro, były to m.in. Durango, San Luis Potosí, Aguascalientes i Pachuca. Dopiero w połowie XIX w. pojawili się w większej liczbie koloniści w północnej części kraju, którzy zajęli się hodowlą bydła. Powyższe procesy spowodowały, że obecnie Meksyk jest krajem nierównomiernie zaludnionym, występują zarówno rejony o bardzo dużej liczbie mieszkańców (np. miasto Meksyk), jak i tereny praktycznie niezamieszkane (część Kalifornii Dolnej, południowy Jukatan).
Współcześnie Meksyk ulega gwałtownej urbanizacji. W liczbach bezwzględnych przodują wprawdzie największe zespoły miejskie, jednak najwyższy przyrost procentowy ludności dotyczy szeregu małych i średnich miast. W połowie lat 80. kraj posiadał ponad sto ośrodków miejskich, w których liczba mieszkańców wynosiła ponad 50 tys. Główną osią procesu urbanizacji pozostaje wąski pas obejmujący środkowy Meksyk od Puebli do Guadalajary. Najbardziej zauważalny ostatnimi czasy stał się jednak rozrost miast położonych przy granicy ze Stanami Zjednoczonymi.
Wśród meksykańskich miast niekwestionowanym liderem jest miasto Meksyk, które jest głównym ośrodkiem politycznym, gospodarczym, społecznym i oświatowym kraju. Liczące ponad 20 mln mieszkańców miasto Meksyk jest największym zespołem miejskim na świecie.
Guadalajara jest drugim pod względem poziomu zurbanizowania rejonem państwa. Ma ona bardziej tradycyjny niż stolica charakter, zarówno pod względem zabudowy, jak i wyglądu. Guadalajara jest stolicą stanu Jalisco, a jej wpływ gospodarczy rozciąga się na cały region zachodni. Guadalajara jest ważnym ośrodkiem handlowym i przemysłowym. Znajdują się tu m.in. uniwersytet, akademia medyczna oraz wiele szkół, co sprawia, że jest ważnym centrum kulturalnym i oświatowym.
Monterrey z kolei rozwinęło się jako ośrodek przemysłowy (głównie hutniczy). Miasto jest stosunkowo młode (gwałtowna rozbudowa sięga początków XX w.) i pozbawione cennych zabytków architektonicznych. Przyczyną powstania licznych tu hut stali było istnienie blisko położonych złóż węgla kamiennego (zagłębie Sabinas), a oprócz hut istnieje szereg innych zakładów z gałęzi przemysłu ciężkiego. Monterrey jest ostoją Partii Akcji Narodowej i twierdzą politycznego konserwatyzmu.
Ludność Meksyku tworzą głównie Metysi, którzy stanowią ok. 60% społeczeństwa, reszta to Indianie (30%) i ludność pochodzenia europejskiego (przeważnie Hiszpanie) stanowiąca ok. 9%. Dodatkowo w latach 1519 do 1650 sprowadzonych zostało 120 tys. czarnych niewolników. Pod koniec epoki kolonialnej Nową Hiszpanię zamieszkiwało ok. 200 tys. niewolników. Czarni zawierali mieszane małżeństwa zarówno z Indianami, jak i Metysami, w końcu rozpłynęli się wśród liczniejszej pierwotnej ludności, pewne ich ślady w wyglądzie mieszkańców są bardziej zauważalne na zachodnim i wschodnim wybrzeżu (głównie rejon Veracruz).
Ludność indiańska wyróżnia się z reszty społeczności szeregiem cech, takich jak chociażby: język, zwyczaje, strój, jedzenie czy domostwa. Głównym kryterium, stosowanym m.in. na szczeblu administracyjnym, które służy identyfikacji Indian pozostaje język. Ocenia się, że ok. 8% wszystkich Meksykanów włada językami indiańskimi. W roku 2000 jedynie 1% ludności posługiwało się tylko i wyłącznie językami tubylczymi.
Największe skupiska Indian znajdują się w regionach gdzie w momencie przybycia Hiszpanów rozkwitały największe cywilizacje. Są to przede wszystkim środkowa, południowa i południowo-wschodnia część kraju. Przykładowo na terenach dawnej cywilizacji Majów znajduje się najwyższy odsetek mieszkańców posługujących się językami indiańskimi (37%), na drugim miejscu pod tym względem jest Oaxaca (36%). Innymi stanami z licznymi populacjami Indian są: Jukatan, Chiapas, Quintana Roo, Hidalgo i Campeche.
Pierwszymi Polakami w Meksyku byli przybyli tu w XVII wieku misjonarze. W latach 1863–1865 prawie 2 tysiące Polaków służyło we francuskiej armii interwencyjnej, która miała na celu osadzenie na tronie cesarza Maksymiliana. Obecna Polonia meksykańska ukształtowała się głównie w pierwszej połowie XX w., a zwłaszcza w okresie międzywojennym gdy do Meksyku przybyło kilka tysięcy Polaków. Polacy zamieszkują głównie rejon Dystryktu Federalnego, środkową część kraju i stanowią nieliczną mniejszość.
Z uwagi na długoletnie panowanie Hiszpanii nad Meksykiem język hiszpański jest obecnie językiem dominującym w życiu publicznym, mimo że nigdy nie zostało to uregulowane ustawowo.
Oprócz hiszpańskiego w użyciu są 63 języki indiańskie (nie licząc wielu ich wariantów, często bardzo odległych), które posiadają status „języka narodowego” zagwarantowany przez Ogólną Ustawę Praw Językowych. 80% Meksykanów, którzy posługują się językami indiańskimi zna również język hiszpański. Najpowszechniejszym językiem tubylczym jest nahuatl (ok. 1,6 mln użytkowników, 1/4 wszystkich Indian). Kolejnym językiem jest język maya (używany przez 13% populacji indiańskiej), później są: zapotecki (7%) i mixtec (7%). Żaden z pozostałych języków nie jest używany przez więcej niż 5% Indian.
Hiszpanie po przybyciu do Meksyku, oprócz gwałtownej eksploatacji Indian, prowadzili również szeroko zakrojoną akcję misyjną wśród tubylców. Oprócz ewangelizacji władze kolonialne nałożyły zakaz wyznawania jakichkolwiek innych religii niż katolicyzm. Wszystko to sprawiło, że Meksykanie są współcześnie społeczeństwem bardzo jednolitym pod względem religijnym, z dominującym Kościołem Rzymskokatolickim. W praktyce jednak, w szczególności w rejonach wiejskich praktykowanie katolicyzmu odbiega znacząco od jego odmian europejskiej i miejskiej. Na prowincji przetrwał szereg zwyczajów, które były tolerowane przez misjonarzy i zostały niejako włączone do praktyk chrześcijańskich.
Konstytucja meksykańska z 1917 gwarantuje swobodę wyznania. Nowelizacja konstytucji z 1992 zniosła wiele poważnych ograniczeń nałożonych wcześniej na Kościół katolicki. Reformy dotyczyły m.in. zniesienia zakazu głosowania przez duchowieństwo, pozostawiono jednak ścisły zakaz bezpośredniego uczestniczenia Kościoła w życiu politycznym. W ostatnich latach można zauważyć coraz poważniejsze włączanie się hierarchii kościelnej w krytykę polityki ekonomicznej i społecznej rządu federalnego.
Spis powszechny przeprowadzony w 2020 r. przez Narodowy Instytut Statystyki i Geografii (INEGI) wskazał rzymskokatolicyzm jako główną religię obejmującą 77,7% populacji (82,7% w 2010), podczas gdy 11,2% zadeklarowało należność do innych kościołów chrześcijańskich – w tym bezdenominacyjnych (5,4%), ewangelikalnych i zielonoświątkowych (2,8%), świadków Jehowy (1,2%), adwentystów dnia siódmego (0,63%), mormonów (0,27%) i innych protestanckich (0,87%). 2,5% osób zadeklarowało się jako wierzący, lecz bez przynależności religijnej, 8,5% wskazało brak religii lub się nie określiło (7,4% w 2010) i 0,2% wyznawało inną religię[27].
W ostatnich latach w Meksyku nasila się obecność chrześcijaństwa postdenominacyjnego silnie zakorzenionego w przebudzeniach ewangelicznych i zielonoświątkowych ostatniego stulecia[28]. Wzrasta także odsetek osób bez przynależności religijnej.
Podział wyznaniowy według Operation World w 2010 roku[29]:
Jedną z najbardziej charakterystycznych cech społeczeństwa meksykańskiego od lat 40. XX w. jest gwałtowny przyrost naturalny. Nawet pomimo jego spowolnienia w latach 80. liczba ludności rośnie o 50% szybciej niż wynosi średnia światowa. Taki rozwój sytuacji jest pochodną wielu czynników, na które składają się przede wszystkim rozwój opieki medycznej oraz rolnictwa. Drastycznie spadł odsetek śmiertelności wśród niemowląt oraz wydłużyła się średnia długość życia, która jednak i tak jeszcze w dużym stopniu odstaje od krajów rozwiniętych. Średnia długość życia niemalże podwoiła się do tej z lat 30. i wynosi obecnie ok. 75 lat.
Kolejną cechą populacji kraju jest duży odsetek ludności, która nie ukończyła nawet 15 lat (stanowi ona 40% społeczeństwa). Od 1915 liczba mieszkańców kraju wzrosła o 500%. Taka gwałtowna eksplozja demograficzna stworzyła szereg napięć społecznych, przede wszystkim na tle gospodarczym i społecznym. Praktycznie wszystkie rządy próbowały prowadzić politykę ograniczającą liczbę urodzin, jednakże bez większego powodzenia.
Wewnętrzna migracja ludności doprowadziła do nierównomiernego zaludnienia kraju. Duże liczby małorolnych chłopów z prowincji i małych miasteczek przeniosły się do miast. Szacuje się, że już 70% wszystkich Meksykanów żyje w miastach, z tego 50% w miastach, których liczba mieszkańców wynosi więcej niż 50 tys. Gwałtownie z drugiej strony spadła liczba ludności na terenach wiejskich. Przy czym jest to spadek tylko procentowy natomiast w liczbach bezwzględnych z uwagi na wysoką migrację obecnie liczba ludności na wsi utrzymuje się na mniej więcej tym samym poziomie. Przyczynami takiego stanu rzeczy są niedostatek ziemi rolnej i brak pozarolniczych miejsc pracy.
Większe szanse na awans społeczny w dużych miastach sprawiają, że mieszkańcy wsi cechują się dużą mobilnością i gotowością zmiany miejsca zamieszkania. Dużo migrujących wybiera Guadalajarę, Monterrey czy Pueblę, jednak od wielu lat to stolica jest głównym celem chłopów z prowincji. Drugim miejscem migracji są tereny przygraniczne na północy, które skorzystały na dużych inwestycjach amerykańskich. Ze względu na duży napływ przybyszów z południa rozrosły się takie miasta jak: Ciudad Juárez, Mexicali i Tijuana.
Oprócz przemieszczania się ludności wewnątrz kraju od lat 70. znacząco rośnie emigracja zewnętrzna, w przeważającej części do Stanów Zjednoczonych. Ocenia się, że w latach 1970–1985 od 4 do 8 mln Meksykanów nielegalnie przekroczyło granicę z USA. Większość z tych emigrantów stanowią słabo wykształceni oraz niewykwalifikowani mieszkańcy prowincji, chociaż daje się też zauważyć rosnący odsetek lepiej wykształconych Meksykanów co przekłada się na proces tzw. „drenażu mózgów”.
Nielegalna emigracja pełni rolę zaworu bezpieczeństwa, który pomaga rozładować narastające problemy gospodarcze wywołane przez przeludnienie. Ponadto pieniądze przesyłane przez emigrantów z zagranicy również nie pozostają bez znaczenia dla rodzin, które pozostały w kraju.
Przestępczość, oprócz spraw gospodarczych, jest jednym z największych problemów we współczesnym Meksyku. Meksyk jest głównym krajem tranzytowym w szlaku narkotykowym (kokaina, marihuana i heroina) biegnącym z Ameryki Południowej do Stanów Zjednoczonych. Przemyt narkotyków doprowadził do wzrostu zjawisk korupcyjnych w administracji rządowej oraz w policji. Oprócz tego na terenie krajów powstały silne struktury przestępcze, których metody działania są bardzo brutalne[30].
Poziom przestępczości jest szczególnie wysoki w dużych miastach[31]. Duże różnice społeczne i polaryzacja pod względem ekonomicznym są jednym z czynników odgrywających dużą rolę we wzroście liczby popełnianych przestępstw. Ponadto wymiar sprawiedliwości działa opieszale i nieskutecznie, co skutkuje stosunkowo niewielkim odsetkiem osób aresztowanych, a następnie skazanych. W celu walki z gangami narkotykowymi Stany Zjednoczone nawiązały z Meksykiem szeroko zakrojoną współpracę. Negatywny wpływ na zwalczanie przestępczości ma fakt, iż niektórzy policjanci pozostają na usługach karteli narkotykowych. Nawet wprowadzenie wojska do walki z gangami nie okazało się skuteczną metodą na ich wyeliminowanie. W ostatnich latach coraz bardziej wzrasta liczba zabójstw na tle porachunków mafijnych, wzmaga się także liczba ataków na policjantów. Oprócz narkotyków kolejnymi sferami działalności przestępczej pozostają handel ludźmi i przerzucanie emigrantów do USA[32].
Najsilniejszym kartelem narkotykowym w Meksyku jest Sinaloa Cartel[33]. Kartelem tym kieruje Joaquín Guzmán Loera, który dzięki handlowi narkotykami dorobił się znacznej fortuny, miesięcznik Forbes w 2011 wymienił go na 10. miejscu wśród najbogatszych ludzi Meksyku[34]. Sinaloa Cartel działa głównie w centrum i na północy Meksyku. Inne silne kartele narkotykowe to między innymi: El Golfo Cartel, Juárez Cartel, Tijuana Cartel, Los Zetas Cartel, La Familia Michoacana Cartel. Kartele narkotykowe prowadzą między sobą wojny narkotykowe, które co roku pochłaniają tysiące ofiar[33].
Od czasów rewolucji z 1910 w Meksyku zauważalne stało się osłabienie obecności obcego kapitału przy jednoczesnym utrzymaniu stosunkowo wysokiego wzrostu gospodarczego. Kolejną cechą gospodarki meksykańskiej jest duży udział sektorów publicznego oraz mieszanego (partnerstwo sektora prywatnego z państwowym). Państwo posiada liczne mechanizmy regulacyjne, przez które może oddziaływać na rynek, są to m.in. różne licencje, pozwolenia, kwoty produkcyjne czy chociażby ceny urzędowe. Ponadto istnieją ograniczenia w inwestowaniu kapitału prywatnego w pewne branże. Przykładem takich restrykcji może być przede wszystkim działalność naftowa i rafineryjna, telekomunikacyjna czy energetyczna, gdzie państwo pozostaje praktycznie monopolistą. Rząd federalny stara się także uniemożliwiać przedsiębiorstwom zagranicznym przejęcie kontroli nad ubezpieczeniami, leśnictwem i górnictwem i wieloma innymi strategicznymi sektorami gospodarki.
Meksyk należy do grupy krajów rozwijających się. W latach 1960–1980 PKB wzrósł o ponad 150%. Przy równoczesnej eksplozji demograficznej wzrost gospodarczy w tym okresie wynosił średnio 7%, co jest bardzo dobrym wynikiem na tle innych państw Ameryki Łacińskiej. Usługi odpowiadają za około 50% dochodu narodowego, przemysł stanowi 25%, a rolnictwo ok. 10%. Poziom PKB względem siły nabywczej stawia Meksyk na 12. miejscu na świecie[35]. W ostatnich latach Meksyk przoduje wśród państw Ameryki Łacińskiej pod względem wartości nominalnej PKB (8,340 $), zajmując w tym rankingu drugie miejsce[36]. Wydaje się, że kraj podźwignął się z kryzysu ekonomicznego z 1994 i nie opiera się wyłącznie na eksporcie surowców.
Ludność w wieku produkcyjnym stanowi jedną trzecią społeczeństwa. Największa grupa ludności (ok. 30%) zatrudniona jest w sektorze usług. Ponad 25% osób pracuje w rolnictwie, a 12% – w przemyśle. Prawie połowa nierolniczej siły roboczej jest zrzeszona w związkach zawodowych. Najliczniejszy z nich – Konfederacja Pracowników Meksykańskich – jest blisko związany z dominującą w meksykańskim życiu politycznym Partią Rewolucyjno-Instytucjonalną.
Gwałtowny „boom” gospodarczy, który rozpoczął się w latach 70. był podtrzymywany przez rosnące dochody z drożejącej ropy, które umożliwiały duże inwestycje w sektorach publicznym i prywatnym. Napływ kapitału przełożył się na nowe miejsca pracy i rozszerzenie bazy dla rynku towarów i usług. Drugą stroną szybkiego rozwoju kraju było rosnące zadłużenie w prywatnych bankach i instytucjach finansowych z zagranicy. Dodatkowo na początku lat 80. doszło do spadku cen ropy naftowej co wywołało głęboki kryzys ekonomiczny.
Rząd został zmuszony do rozluźnienia polityki celnej i wprowadzenia większej swobody gospodarczej w celu pobudzenia wymiany handlowej, przyciągnięcia inwestorów zagranicznych i pobudzenia przedsiębiorczości. W pierwszej połowie lat 90. miał miejsce umiarkowany wzrost PKB. Jednakże poważne zagrożenie zaczęło stanowić przewartościowanie peso w stosunku do dolara[37]. Dewaluacja peso przeprowadzona w 1994 doprowadziła z kolei do gwałtownego spadku wartości krajowej waluty. W reakcji na to z Meksyku szerokim strumieniem zaczął odpływać kapitał. Tym samym gospodarka meksykańska znalazła się na skraju głębokiego załamania ekonomicznego. W reakcji na to Stany Zjednoczone zdecydowały się udzielić pomocy w postaci pożyczki w celu ratowania słabnącego peso. Jak się później okazało, kryzys gospodarczy z lat 1994–1996 był najpoważniejszy w kraju od czasów wielkiej depresji z lat 30. Doprowadził do wzrostu ubóstwa i spadku poziomu życia dużej części Meksykanów. PKB w latach 1994–1995 spadł o 6,2%[38].
W nowe stulecie Meksyk wkroczył z nieco mocniejszą ekonomią, którą napędzają w dużej części przemysł oraz górnictwo. Nie bez znaczenia dla poprawy sytuacji gospodarczej było wstąpienie Meksyku do NAFTA – Północnoamerykańskiego Porozumienia o Wolnym Handlu. Meksyk pozostaje nadal bardzo uzależniony od sytuacji ekonomicznej Stanów Zjednoczonych. W 2005 PKB Meksyku wyniósł 768,4 mld dolarów[39].
Rolnictwo ma 10-procentowy udział w wytwarzanym dochodzie narodowym i daje zatrudnienie ok. jednej czwartej ludności w wieku produkcyjnym. Znacząca część sektora rolniczego opiera się na tradycyjnych metodach gospodarowania, w szczególności w biedniejszych regionach zamieszkanych przez Indian. Produkcja rolna w takich rejonach bazuje głównie na kukurydzy, fasoli, uprawianych często na ziemi należącej do wspólnot gminnych. Taki system cechuje się wysokim zapotrzebowaniem na siłę roboczą i niską wydajnością. Z ogólnego areału ziemi rolnej grunty orne i sady zajmują 12% powierzchni kraju a łąki i pastwiska – 37%.
Jednym ze skutków rewolucji 1910 r. była reforma ziemska, która zaowocowała wprowadzeniem tzw. systemu ejido. W momencie wybuchu rewolucji ogromna część chłopów praktycznie nie posiadała własnej ziemi i pracowała u bogatych posiadaczy ziemskich na hacjendach. Konstytucja z 1917 zawierała postanowienia nakładające ograniczenia na maksymalną wielkość jednego gospodarstwa rolnego. Przepisy te dawały możliwość wywłaszczania i podziału uzyskanej w ten sposób ziemi pomiędzy bezrolnych chłopów. Małe obszarowo działki zostały oddane we wspólne użytkowanie niewielkim społecznościom, z których członkowie korzystali indywidualnie (ziemia uprawna) lub wspólnie (pastwiska, lasy). Późniejsze zmiany przyniosły powstanie lokalnych wspólnot pod nazwą ejidos. Pod koniec lat 30. XX w. większość z hacjend upadła (utrzymały się głównie na pasterskiej północy). Reforma rolna doprowadziła do powstania licznych małych gospodarstw, których powierzchnia najczęściej wynosiła od 4 do 8 ha. Mimo że duża część ich produkcji jest wykorzystywana na własne potrzeby to znaczna ilość trafia również do mieszkańców miast i osad nierolniczych. Kukurydza pozostaje tam podstawową rośliną uprawną, ale istnieje również na niewielką skalę chów zwierząt[40].
Gospodarstwa rolne nastawione na produkcję towarową są najliczniejsze w rejonie Zatoki Meksykańskiej, wyżyn Chiapas, nawadnianych części na północy i północnym zachodzie kraju oraz Guanajuato (Mesa Centralna). Rośliny tropikalne już od czasów kolonialnych uprawia się na nadbrzeżnej równinie przy Zatoce Meksykańskiej i przyległych terenach. Obecnie uprawy rozciągają się również bardziej na południowy wschód od Tampico do Chiapas aż po wschodnie zbocza Sierra Madre Wschodniej. Kawa i trzcina cukrowa pod względem areału i dochodowości mają największe znaczenie. Produkty te wystarczają na zaspokojenie krajowych potrzeb a duże nadwyżki są przeznaczane na eksport.
Kawa jest jednym z najcenniejszych artykułów eksportowych, jednakże wzrost zapotrzebowania na cukier sprawił, że straciła nieco na znaczeniu. Banany, mango, kakao i ryż znajdują zbyt w dużej części na rynku krajowym. Meksyk jest również wiodącym producentem wanilii, która jest zbierana w tym rejonie. Mniejsze areały upraw kakao i trzciny cukrowej znajdują się w zachodnim Chiapas. Natomiast bawełna dominuje na polach położonych przy granicy z Gwatemalą i nad Pacyfikiem.
Części północna i północno-zachodnia kraju cechują się dużą skalą wykorzystania irygacji w rolnictwie. Nawadniane są przede wszystkim uprawy bawełny. Duże projekty irygacyjne zaczęły być realizowane w regionie już od lat 30. Plan „Laguna” przeprowadzony niedaleko miasta Torreón był jednym z pierwszych, mających na celu doprowadzenie wody na te skąpe w opady tereny. Pozyskane dla rolnictwa ziemie były oddawane ejidos, które uprawiały bawełnę za pomocą zmechanizowanych metod. Z kolei w okolicach Chihuahua zrealizowano plan „Las Delicias”, który umożliwił powiększenie areału pszenicy. Pomimo powstania nowych terenów rolnych gwałtowny przyrost ludności sprawia, że Meksyk pozostaje importerem zboża od lat 80.
W dolinach rzek Fuerte i Yaqui w latach 40. przeprowadzono program odzyskania zasolonych gleb na potrzeby nowych upraw. Podobne prace prowadzono również na terenach sięgających do Hermosillo na północy i do Culiacán na południu. Większość z tych obszarów została powierzona spółdzielniom – ejido, część natomiast znajduje się w prywatnych rękach. W regionie stanu Sinaloa pszenica odgrywa najistotniejszą rolę, jest to jeden z największych ośrodków uprawy tego zboża. Oprócz tego północno-zachodnie rejony Meksyku są wykorzystywane do zbiorów bawełny, warzyw i roślin oleistych. Duża część warzyw takich jak pomidory czy sałata jest eksportowana do Stanów Zjednoczonych. Dolina Mexicali stanowi natomiast ważny ośrodek upraw roślin włókienniczych.
Region Mesy Centralnej od dawna uważany jest za „spichlerz” kraju. Pszenica, kukurydza, warzywa, orzeszki ziemne i fasola są podstawowymi produktami tutejszych małych gospodarstw. W ostatnich latach coraz większe znaczenie na rynku produktów rolnych zdobywa region Guanajuato, co jest spowodowane sąsiedztwem z gęsto zaludnionymi miastami.
Hodowla koncentruje się w północnej części kraju. Mimo przeprowadzonej reformy rolnej w dalszym ciągu istnieją tu duże gospodarstwa, które dobrze sprawdzają się przy chowie dużych stad. Ze względu na suchy klimat i ograniczoną wegetację występują tu duże trudności w rozwijaniu produkcji. Duża część mięsa trafia na eksport na rynek amerykański. W celu poprawienia wyników tutejszych farmerów zastąpiono stare hiszpańskie rasy bydła nowymi – bardziej mięsnymi. Niektóre pastwiska dzięki nawożeniu i nawadnianiu stały się dostępne do użytkowania przez cały rok. Coraz powszechniejsze staje się również karmienie krów paszami.
Oprócz północy chów bydła jest również popularny w tropikalnych rejonach Chiapas i północno-wschodniego wybrzeża Zatoki Meksykańskiej. Farmerzy uzyskują tutaj zdecydowanie lepsze wyniki w hodowli niż na północy ze względu na większą wilgotność i lepsze warunki wegetacyjne. Lasy deszczowe są wycinane, a na ich miejscu sieje się trawy w celu szybszej możliwości wykorzystania nowych terenów. Proces ten wywołuje niekorzystne skutki dla środowiska, ze względu na ważną rolę lasów w regulacji stosunków wodnych i powstrzymywaniu procesów erozyjnych.
Meksyk jest producentem rzadkich roślin przemysłowych. Są to m.in. henequen (gatunek agawy) – roślina wykorzystywana w meblarstwie. Henequen została wprowadzona na Jukatanie w latach 80. XIX w. i przez długi czas to właśnie Jukatan był głównym źródłem tego surowca. Reforma z lat 30. XX w. przyczyniła się jednak do zmniejszenia areału z uwagi na wcześniejszą specjalizację dużych hacjend w tym zakresie. Maquey (kolejny gatunek agawy) porasta niektóre pola na Mesie Centralnej. Początkowo maquey było wykorzystywane do produkcji pulque – niedrogiego napoju alkoholowego. Roślina ta cieszy się dużym powodzeniem w niewielkich gospodarstwach ze względu na możliwość uprawy na mało żyznych i skalistych ziemiach. Jeden z symboli kraju – tequila jest również wyrabiana z maquey, ośrodkiem jej produkcji jest miasto Tequila położone w stanie Jalisco. Z agawy wyrabiany jest również mescal – jeden z popularnych alkoholi meksykańskich.
Meksyk dysponuje znaczącymi zasobami leśnymi, nawet pomimo tego, że dużą część kraju stanowią półpustynie i duża część lasów istniejących przed przybyciem Europejczyków została wycięta, a ogołocona ziemia wystawiona na działanie wiatru. Szacuje się, że ok. 2/3 obecnego Meksyku na początku XVI w. okrywały lasy. W 2005 obszary leśne stanowiły 34% powierzchni kraju. Gwałtowna wycinka spowodowała, że pozyskiwanie drewna zostało objęte ścisłą kontrolą. Jednak nawet nadzór administracji rządowej nie jest w stanie powstrzymać szybko postępującej deforestacji. Pomiędzy 1970 a 1985 rokiem Meksyk stracił 1/6 swoich lasów.
Największe obszary leśne znajdują się na południowym wschodzie i południu kraju. Składa się na nie wiele cennych drzew użytkowych (m.in. mahoniowiec, dąb i sosna). Ponadto lasy deszczowe stanu Chiapas są ważnym źródłem gumy chicle uzyskiwanej z drzew pigwicy właściwej, która jest wykorzystywana do wyrobu gum do żucia. Wyższe partie Sierra Madre Wschodniej i Zachodniej porastają z kolei lasy iglaste (sosna i jodła) oraz niektóre gatunki drzew liściastych (dąb). Produkcja drzewa nie jest jednak wystarczająca aby sprostać potrzebom kraju, dlatego duża część papieru i tektury musi być importowana.
Od lat 40. XX w. Zatoka Meksykańska na szerokości od Tampico do Veracruz jest wykorzystywana do przemysłowego połowu krewetek. Do tego samego celu służą też wody Zatoki Kalifornijskiej. Głębokie wody pacyficzne niedaleko Półwyspu Kalifornijskiego obfitują w wiele gatunków cennych ryb oceanicznych. Rejon ten stał się głównym obszarem połowowym kraju od momentu pojawienia się nowoczesnej floty rybackiej. Poławia się tu głównie sardynki i tuńczyki. Ważne łowiska znajdują się także przy Zatoce Campeche i Półwyspie Jukatańskim.
Meksyk w ostatnich latach odchodzi od modelu typowego kraju surowcowego, którego główną pozycję eksportową stanowiła ropa naftowa[41] do bardziej zróżnicowanego modelu gospodarczego. Rozwinięte są m.in. przemysł samochodowy, metalowy, chemiczny, papierniczy, tekstylny, spożywczy, elektroniczny.
Meksyk jest jednym z najbardziej uprzemysłowionych państw Ameryki Łacińskiej. Produkcja przemysłowa odpowiada za prawie jedną czwartą PKB i daje zatrudnienie jednej dziesiątej siły roboczej. Zdecydowana większość zakładów znajduje się w obszarze metropolitarnym stolicy, ze względu na duży rynek zbytu i dobrze rozwiniętą infrastrukturę. Profil produkcji jest tutaj bardzo szeroki i obejmuje przetwórstwo spożywcze, przemysł samochodowy, maszynowy, elektroniczny, hutniczy i wiele innych. Wysiłki rządu zmierzające do zdekoncentrowania produkcji przemysłowej jak na razie nie przynoszą efektów. Jedyne co daje się zauważyć to przenoszenie się fabryk na obrzeża aglomeracji, ale dalej w ramach środkowej części kraju. Odstępstwem od koncentracji produkcji w środkowym Meksyku jest szybki rozwój tzw. maquiladoras (montowni), położonych przy granicy ze Stanami Zjednoczonymi. W ich ramach odbywa się bezcłowy transport surowców i materiałów pomiędzy Meksykiem a USA w wąskim pasie granicznym. Przeważająca większość maquiladoras należy do zagranicznych koncernów, które zdecydowały się na ich założenie ze względu na niskie koszty pracy.
Oprócz Dystryktu Federalnego duże zagęszczenie fabryk występuje w rejonach miast Monterrey i Guadalajara. Szczególny skok można zauważyć w przemyśle ciężkim, który zaczął się szybko rozwijać po II wojnie światowej. Głównym jego ośrodkiem pozostaje miasto Monterrey. W latach 1970–1980 produkcja stali w Meksyku praktycznie się podwoiła. W ostatnich latach do branży trafiły nowoczesne technologie, które zwiększyły konkurencyjność meksykańskich stalowni. W latach 90. niektóre zakłady należące do przedsiębiorstw zagranicznych zostały przejęte przez rodzimy kapitał.
Branże maszynowa i samochodowa koncentrują się w okolicach Puebla, Toluca i Hermosillo. Fabryki włókiennicze i odzieżowe są natomiast rozproszone. Niemniej jednak w niektórych regionach istnieją ośrodki z dużymi tradycjami we włókiennictwie (m.in. Guadalajara), cały czas powstają też nowe zakłady np. w Torreón czy Ciudad Juárez. Powstające jak „grzyby po deszczu” maquiladoras przyczyniły się do powstania bardziej zaawansowanych technologicznie sektorów gospodarki, takich jak: branże elektroniczna i komputerowa (np. Tijuana).
Komisja Federalna ds. Elektryczności zarządza siecią energetyczną kraju. Głównymi źródłami energii w Meksyku są obecnie ropa i gaz, które wyprzedziły pod tym względem hydroelektrownie[42].
Przed spowolnieniem gospodarczym w latach 80. roboty publiczne (mieszkania, drogi, koleje) odpowiadały prawie połowie całej branży budowlanej. Z biegiem czasu państwo ze względu na brak środków zaczęło ograniczać swoją rolę a inicjatywę w tym zakresie przejął kapitał prywatny. Pewnym bodźcem dla rozwoju tego sektora stało się trzęsienie ziemi, które nawiedziło kraj w 1985.
Brak wystarczających zasobów mieszkaniowych jest jednym z największych problemów współczesnego Meksyku. Ponadto wiele z istniejących mieszkań nie odpowiada podstawowym standardom, sytuacja wygląda szczególnie źle na terenach wiejskich. W miastach rząd federalny prowadzi programy budowy mieszkań komunalnych. Jednakże mimo ingerencji państwa problem narasta cały czas. Ponadto konieczność cięć budżetowych sprawia, że funduszy na budowę mieszkań jest coraz mniej.
W społeczeństwie meksykańskim daje się zauważyć duże rozwarstwienie zarówno pod względem majątkowym, jak i dostępu do edukacji czy świadczeń opieki zdrowotnej. Mimo że cały czas rozwija się klasa średnia to w dalszym ciągu jest ona nieliczna. Podstawowymi grupami społecznymi są dobrze wykształcona i bogata elita oraz miejska i wiejska biedota.
Najbardziej widoczne jest ubóstwo na prowincji. Losu chłopstwa nie poprawiła reforma rolna. Dzięki niej chłopi dostali wprawdzie ziemię na własność, ale ich gospodarstwa są zbyt małe aby konkurować na rynku i z reguły ledwie wystarczają na utrzymanie swoich właścicieli. Mały areał powoduje, że mieszkańcy wsi nie są w stanie wytwarzać żywności w większych ilościach na potrzeby handlu. Chłopi nie mają żadnych perspektyw na polepszenie swojego losu czy awans społeczny.
Jedyną szansą dla nich pozostaje ucieczka do przeludnionych miast gdzie nowo przybyli tworzą prymitywne osiedla zamieszkane przez biedotę. Duży przyrost naturalny powoduje również wytworzenie się niekorzystnej struktury własności, gdzie coraz większą część stanowią bezrolni chłopi. Są oni zmuszeni do podejmowania pracy w bogatszych gospodarstwach, często za minimalną stawkę. W wielu rejonach (głównie na północy kraju) duzi posiadacze ziemscy stanowią nieliczną elitę. Pracując na wielkoobszarowych farmach mogą wykorzystywać na szeroką skalę zmechanizowane środki produkcji. Szacuje się, że do takich właśnie osób należy prawie połowa dochodu wypracowywanego w meksykańskim rolnictwie. Istnieje też nieliczna klasa średnia wśród farmerów, ale ma ona niewielkie znaczenie.
W miastach największą liczbę ludności stanowią najbiedniejsi mieszkańcy. Szacuje się, że 40% mieszkańców meksykańskich aglomeracji osiąga dochody poniżej oficjalnego progu uważanego za poziom ubóstwa. Do grupy tej należą także pracownicy sfery budżetowej. Ogromna większość z nich zajmuje mieszkania, które nie posiadają podstawowych udogodnień, takich jak: woda, kanalizacja itp. Przykładowo Nezahualcóyotl – wschodnią część miasta Meksyk zamieszkuje ponad 1 mln osób, żyjących w warunkach urągających podstawowym standardom sanitarnym. Ich przeciwieństwo stanowi nieliczna klasa przedsiębiorców, polityków, lekarzy, prawników itp., która ze względu na wysokie dochody ma decydujący wpływ na życie gospodarcze i polityczne kraju.
Od 1934 w Meksyku obowiązuje płaca minimalna. Wynagrodzenia są zróżnicowane w zależności od kosztów życia w danym regionie i rodzaju pracy. Z reguły płace w miastach są wyższe od tych na prowincji. Najwyższe zarobki osiągają pracownicy w miastach Meksyk, Tijuana, Mexicali i w okolicach Ciudad Juárez. Od lat 70. występuje duży wzrost wynagrodzeń.
Aż do 1980 inflacja nie była zbyt wysoka jak dla kraju o tak wysokiej stopie wzrostu gospodarczego. W okresie pomiędzy 1960 a 1970 koszty życia rosły przeciętnie o ok. 3%. W latach 70. inflacja sięgała już 10–20%, w latach 80. osiągnęła poziom trzycyfrowy. W 2005 ceny wzrosły o 3,3%.
Turystyka jest najszybciej rozwijającym się sektorem gospodarki meksykańskiej, po wydobyciu ropy naftowej stanowi najistotniejsze źródło dochodów[43]. Dawniej turyści odwiedzali głównie stolicę i przyległe kolonialne miasteczka na Mesie Centralnej, a także starożytne miasta Majów na Jukatanie. W ostatnich latach w siłę rosną ośrodki położone nad piaszczystymi plażami Zatoki Meksykańskiej i Oceanu Spokojnego. Szczególną popularnością cieszą się takie kurorty jak: Puerto Vallarta, Acapulco, Cancún, Cozumel, Ixtapa-Zihuatanejo, Mazatlán czy Cabo San Lucas. Od lat 60. buduje się wiele hoteli, lotnisk i innych elementów infrastruktury turystycznej po to aby przyciągnąć nowych gości z zagranicy. Wśród turystów największą liczbę stanowią Amerykanie, którzy upodobali sobie Meksyk ze względu na jego bliskość, niskie ceny i kulturową różnorodność. Zdecydowanie najbezpieczniejszą, ale i najdroższą częścią Meksyku jest Półwysep Jukatan[44].
Prawie dwie trzecie obecnego zadłużenia zagranicznego Meksyku powstało w drugiej połowie lat 70. i w latach 80. Było to związane z załamaniem się cen ropy naftowej. W związku z gwałtownie narastającym deficytem konieczne okazało się przeprowadzenie negocjacji z wierzycielami odnośnie do zmiany sposobu spłaty zadłużenia. Nieuniknione okazało się także zaciągnięcie dodatkowych pożyczek w celu spłaty bieżących odsetek i zmiany struktury długu. Wiele z problemów ekonomicznych kraju zostało wywołanych przez rosnące zadłużenie zagraniczne. Rząd federalny został zmuszony do wdrożenia szeroko zakrojonego programu oszczędności budżetowych, na który złożyło się również sztuczne ograniczanie cen i płac.
W wyniku kłopotów finansowych administracji rządowej z Meksyku zaczął odpływać kapitał na inne rynki finansowe, w szczególności do Stanów Zjednoczonych. Nieumiejętność poradzenia sobie z deficytem budżetowym doprowadziła do zahamowania wzrostu gospodarczego i ograniczenia inwestycji zagranicznych.
Do 1982 r. system bankowy miał dualistyczny charakter. Składały się na niego komercyjne banki prywatne oraz instytucje finansowe należące do sektora publicznego. Prywatny sektor bankowy został w tym okresie znacjonalizowany w celu zatrzymania „manipulacji” na rynkach finansowych. Jednakże podczas kadencji prezydenta Carlosa Salinasa Gortariego pod naciskiem niezadowolonych przedsiębiorców i firm zagranicznych zdecydowano się na ponowną prywatyzację części banków. Wraz z przystąpieniem Meksyku do NAFT-y rząd został zmuszony do otwarcia sektora bankowego na banki z innych krajów[45].
Głównym zadaniem Banku Meksyku jest emisja waluty narodowej, nadzór nad udzielaniem kredytów, ustalanie poziomu rezerw walutowych oraz sprzedaż złota prywatnym podmiotom. Oprócz banku centralnego istnieje szereg innych publicznych instytucji. Najważniejszą jest Narodowy Bank Rozwoju, za jego pośrednictwem obca pomoc zagraniczna jest kierowana na poszczególne programy infrastrukturalne.
W latach 70. rząd wielokrotnie przeprowadzał dewaluację peso. Celem tych operacji była redukcja kursu, po którym peso było wymieniane na obce waluty. Spowodowane to było przede wszystkim sytuacją ekonomiczną, w której ceny towarów i usług w Meksyku wyrażone w dolarach amerykańskich przewyższały ich rzeczywistą wartość, co zniechęcało turystów do przyjazdu, a importerów do zakupu meksykańskich dóbr eksportowych. Niekorzystnym skutkiem dewaluacji jest zawsze podwyższenie cen dla Meksykanów, z drugiej strony z takiej sytuacji korzystają eksporterzy. W 1994 rząd obniżył wartość peso o 45% w stosunku do innych walut światowych. Tak wysoka obniżka zniechęciła inwestorów zagranicznych, którzy zaczęli wycofywać swój kapitał, a to z kolei doprowadziło do poważnego kryzysu gospodarczego w 1995.
Obrót papierami wartościowymi odbywa się na giełdzie w mieście Meksyk Bolsa de Valores, niektóre przedsiębiorstwa meksykańskie są notowane na giełdzie nowojorskiej.
Od lat 70. XX w. meksykański import rośnie w bardzo szybkim tempie, szczególnie duży wzrost daje się zauważyć w sektorze dóbr konsumpcyjnych i produktów rolnych. Z drugiej strony wzrósł również i to znacznie eksport, co jest głównie zasługą ropy naftowej. Głównym partnerem handlowym są Stany Zjednoczone. Prawie 2/3 wartości całego eksportu i ponad 2/3 importu stanowi wymiana handlowa z sąsiadem z północy. Innymi dużymi partnerami handlowymi są: Kanada, Hiszpania, Chile i Japonia[6].
Budowa zintegrowanej sieci transportowej ze względu na ukształtowanie geograficzne napotykała w Meksyku od zawsze na duże trudności. Mimo to Meksyk stał się jednym z pierwszych krajów Ameryki Łacińskiej, który rozpoczął budowę linii kolejowych. Koleje państwowe są jednakże niewydajne, a poważniejszą rolę odgrywały jedynie w XIX w. i na początku XX w. Obecnie nie są w stanie zaspokoić wszystkich potrzeb rozwijającej się gospodarki. Pod koniec lat 90. koleje zostały sprywatyzowane a podmioty, które je przejęły otrzymały 50-letnie koncesje na użytkowanie linii. Długość linii kolejowych wynosi 26,7 tysiąca kilometrów[46].
Duży przyrost naturalny trwający od lat 50. wywiera dużą presję na rząd federalny w celu rozbudowy infrastruktury transportowej. Realizacja tego zadania stwarza problemy w niektórych trudno dostępnych regionach, takich jak chociażby tereny pomiędzy zachodnimi równinami nadbrzeżnymi a centralnie położoną wyżyną.
Obecnie najistotniejszą rolę odgrywa transport samochodowy. Większość dróg meksykańskich jest dwupasmowa. W czasie rządów prezydenta Carlosa Salinasa Gortariego (1988-1994) zachęcano prywatnych przedsiębiorców do budowy płatnych autostrad. Inwestycje te nie spełniły jednak swojej roli ze względu na dużo wyższe koszty budowy niż w przypadku podobnych dróg budowanych przez sektor publiczny. Łączna długość dróg kołowych sięga 352 tysiące km z czego Droga Panamerykańska stanowi 3,5 tysiąca km[46]. Większość szlaków transportowych ma układ południkowy.
Meksyk dysponuje stosunkowo dobrze funkcjonującą siecią lotnisk obsługującą ruch krajowy i międzynarodowy. Największymi i najruchliwszymi lotniskami są: miasto Meksyk, Guadalajara, Monterrey, Puerto Vallarta, Cancún i Tijuana.
Flota handlowa liczy ponad 600 statków i odpowiada za wywóz 85% towarów eksportowych[47]. Meksyk dysponuje 108 portami morskimi, największe z nich to: Veracruz, Tampico, Coatzacoalcos położone nad Zatoką Meksykańską oraz Manzanillo, Mazatlán i Guaymas nad Pacyfikiem. Meksyk dysponuje też rozwiniętą siecią rurociągów z czego na rurociągi naftowe przypada 28,2 tysiąca km, a gazociągi – 13,3 tysiąca km.
Głównymi problemami miejskich sieci transportowych są korki oraz duże zanieczyszczenie powietrza przez pojazdy spalinowe. Pomimo istnienia w wielu miastach publicznych środków transportu (metro, autobusy) wielu Meksykanów preferuje poruszanie się prywatnymi samochodami. Rosnący ruch samochodowy zmusił rząd do wprowadzenia pewnych ograniczeń w celu zmniejszenia zanieczyszczenia środowiska.
System polityczny Meksyku został stworzony na podobieństwo amerykańskiego modelu władzy. Tak samo jak w USA, można wyróżnić podział władz na: sądowniczą, wykonawczą i ustawodawczą. W odróżnieniu jednak od Stanów Zjednoczonych, w Meksyku dużą przewagę posiada władza wykonawcza z prezydentem na czele. Przez większą część XX w. tylko jedna partia odgrywała istotną rolę – była to Partia Rewolucyjno-Instytucjonalna (PRI). PRI założona w 1929, z biegiem czasu zdołała skupić w swoich rękach władzę na wiele lat. Praktycznie do 1988 PRI zdobywała wszystkie miejsca w senacie, a do 1989 nie przegrała żadnych wyborów gubernatorskich. W 2000 prezydentem został Vicente Fox (Partia Akcji Narodowej) – pierwszy raz od wielu lat osoba spoza kręgów PRI. W 2012 PRI powróciła do władzy, kiedy w wyborach prezydenckich zwyciężył Enrique Peña Nieto.
Prezydent jest wybierany w głosowaniu powszechnym na 6-letnią kadencję, istnieje zakaz ponownego kandydowania tej samej osoby. Prezydent posiada szerokie uprawnienia, a oprócz tego jest często szefem swojej partii, przez co ma wpływ na obsadę wielu stanowisk na szczeblu federalnym. Nawet członkowie Kongresu zawdzięczają w dużej części swoje stanowiska prezydentowi. Od prezydenta pochodzi prawie 90% wszystkich projektów legislacyjnych. Członkowie gabinetu prezydenckiego (sekretarze) stoją na czele poszczególnych działów administracji rządowej (m.in. sekretariaty ds. wewnętrznych, zagranicznych, bezpieczeństwa publicznego, gospodarki itp.)[48].
Parlament meksykański składa się z dwóch izb: Izby Deputowanych i Senatu. Członkowie 500-osobowej Izby Deputowanych wybierani są na trzyletnią kadencję. 300 z nich pochodzi z jednomandatowych okręgów wyborczych, natomiast reszta jest wyłaniana na podstawie ogólnej puli głosów oddanych w całym kraju na poszczególnych kandydatów. 128-osobowy Senat jest wybierany wraz z prezydentem co 6 lat. Od roku 2000 wszyscy senatorowie są wyłaniani w jednym głosowaniu. 64 senatorów reprezentuje poszczególne części kraju (po dwóch z każdego stanu i Dystryktu Federalnego), a pozostałe 64 miejsca są obsadzane według ogólnej liczby głosów oddanej na daną partię. Senatorowie i deputowani mogą się starać o reelekcję, ale dopiero po upływie kolejnej kadencji[49].
Izba Deputowanych uchwala ustawy, nakłada podatki i pełni funkcję kontrolną. Senat z kolei ratyfikuje traktaty i zatwierdza niektóre nominacje prezydenckie. Może również upoważnić rząd federalny do wkroczenia w uprawnienia władz stanowych, jeżeli uzna, że został naruszony porządek konstytucyjny[48].
Judykatywa odgrywa niewielką rolę w systemie władzy Meksyku. Najwyższą instancję stanowi Sąd Najwyższy[50], którego członków powołuje prezydent za zgodą dwóch trzecich senatorów. Meksykański Sąd Najwyższy rzadko dokonuje wykładni prawa, daje się pod tym względem zauważyć jego uległość wobec administracji prezydenckiej. Od 1995 Sąd Najwyższy ma możliwość analizowania nowo uchwalanych ustaw pod względem zgodności z aktami prawnymi wyższego rzędu, jeżeli z taką inicjatywą wystąpi co najmniej jedna trzecia deputowanych i senatorów.
Meksyk jest państwem federacyjnym, dzieli się na 31 stanów i Dystrykt Federalny (obejmujący stolicę). Na czele każdego stanu stoi gubernator, wybierany w wyborach powszechnych na 6-letnią kadencję. Również burmistrz miasta Meksyk – Dystryktu Federalnego powoływany jest w głosowaniu. Przed 1997 szef Dystryktu Federalnego był jedynie członkiem rządu federalnego mianowanym przez prezydenta. W Dystrykcie Federalnym swoją siedzibę mają władze federalne. Każdy stan ma własną konstytucję i parlament – Izbę Deputowanych, wybieraną na 3 lata. Najniższym szczeblem podziału terytorialnego są gminy (municypia), na czele których stoją burmistrzowie[51].
Centrami administracji w gminach są większe miasta, które zajmują się podziałem dochodów uzyskanych przez gminę i zbieraniem podatków. Samorząd terytorialny w Meksyku ma niewielki zakres autonomii, głównie ze względu na ograniczone źródła dochodów. Większość podatków jest pobierana przez scentralizowane agencje państwowe.
Poszanowanie praw człowieka oraz wolności obywatelskich stanowi poważny problem w Meksyku. Nieprawidłowości dotyczą przede wszystkim sposobu przeprowadzania wyborów (liczne nadużycia prawa wyborczego) oraz nierówności wobec prawa. Szczególnie głośna stała się sprawa kobiet z przygranicznego miasta Ciudad Juárez, z których wiele zostało zamordowanych lub zaginęło, w wyniku przemocy seksualnej i handlu „żywym towarem”. Przemoc wobec kobiet jest charakterystyczna dla całego kraju, przy czym nie podejmuje się większych prób zwalczenia tego zjawiska. Policja często używa przymusu bezpośredniego do rozpraszania demonstracji, zdarzają się przypadki śmiertelne wśród demonstrantów (szczególnie przy protestach, które tłumi wojsko). Osobom, które są rzecznikami ochrony praw człowieka, często grozi się, nierzadkie są także przypadki zabójstw „osób niewygodnych politycznie”[52]. Nagminna jest dyskryminacja Indian pod względem dostępu do szkolnictwa czy opieki zdrowotnej. Rząd nie konsultuje z Indianami inwestycji na terenach przez nich zamieszkanych (reakcją na to jest m.in. ruch zapatystów). Dużo przypadków naruszenia prawa istnieje w zakresie działania organów wymiaru sprawiedliwości. Częste są bezpodstawnie dokonywane zatrzymania. Oskarżeni nie mogą być pewni prawa do uczciwego procesu. Duże uchybienia występują w zakresie przestrzegania wolności słowa.
Mimo że Meksyk przez wiele lat był zdominowany przez rządzącą metodami autorytarnymi PRI, to w okresie tym istniały konkurencyjne wobec niej ruchy polityczne. W 2000 jedna z partii opozycyjnych – PAN (Partia Akcji Narodowej) przejęła po raz pierwszy od wielu lat stery władzy. Jej kandydat – Vicente Fox został prezydentem, a PRI przegrała oprócz tego również w wyborach parlamentarnych. PAN została założona w 1939 przez część członków, która zdecydowała się opuścić PRI. Jest to partia centrowa opowiadająca się za prywatyzacją przedsiębiorstw państwowych oraz za ograniczeniem wydatków socjalnych. Inną partią opozycyjną wobec PRI jest PRD (Partia Rewolucji Demokratycznej). PRD powstała w wyniku rozłamu PRI w 1989. Pod względem programowym jest bardziej lewicowa od PAN i mniej pojednawcza wobec PRI. Sprzeciwia się liberalizacji gospodarki oraz pewnym aspektom członkostwa Meksyku w NAFTA, które obwinia o utratę przez państwo kontroli nad pewnymi sektorami ekonomii.
W latach 80. PRI utraciła wpływy polityczne, głównie przez nieudolną politykę gospodarczą, która doprowadziła do pogorszenia się sytuacji ekonomicznej społeczeństwa meksykańskiego. W 1987 grupa jej działaczy z Cuauhtémociem Cárdenasem Solórzano została wyrzucona z partii. Solórzano wystartował w wyborach prezydenckich w 1988 na czele lewicowej koalicji. Wybory wygrał kandydat PRI – Carlos Salinas de Gortari, przy czym w opinii wielu obserwatorów w ich trakcie dochodziło do wielu nadużyć. Mimo wygranej Gortariego opozycja zdołała uzyskać 240 miejsc w Izbie Deputowanych, co położyło kres 60-letnim jednopartyjnym rządom PRI. Dzięki przeprowadzonej następnie reformie prawa wyborczego partie opozycyjne urosły w siłę, co przyczyniło się do pluralizacji życia politycznego Meksyku[61].
Meksyk dysponuje trzema rodzajami sił zbrojnych: siłami lądowymi, marynarką wojenną oraz siłami powietrznymi[62]. Uzbrojenie sił lądowych Meksyku składało się w 2021 z: 3435 opancerzonych pojazdów bojowych oraz 200 zestawów artylerii holowanej[62]. Marynarka wojenna Meksyku (Armada de México) dysponowała w 2021: 133 okrętami obrony wybrzeża oraz 5 fregatami[62].
Wojska meksykańskie w 2021 roku liczyły 275 tys. żołnierzy służby czynnej oraz 82 tys. rezerwistów. Według rankingu Global Firepower (2021) meksykańskie siły zbrojne stanowią 46. siłę militarną na świecie, z rocznym budżetem na cele obronne w wysokości 4,38 mld dolarów (USD)[62].
Biorąc pod uwagę poziom PKB przeznaczanego na obronę, Meksyk jest jednym z państw, które przeznaczają na nią bardzo małą część swoich zasobów, gdyż wynosi to tylko około 0,5% PKB[63].
Teoretycznie służba wojskowa jest obowiązkowa dla wszystkich mężczyzn powyżej 18 roku życia, w praktyce jednak obowiązek ten obejmuje tylko niewielką liczbę rekrutów. Wojsko podlega nadzorowi władz cywilnych, jednakże ma ono też pewien wpływ na bezpieczeństwo publiczne ze względu na wykonywanie przez nie niektórych zadań policyjnych. Przykładowo odpowiada za walkę z przemytnikami narkotyków czy ruchem zapatystów.
Proporzec marynarki wojennej
Flaga meksykańskiej piechoty morskiej
Flaga meksykańskich wojsk lądowych
Żołnierze meksykańscy
Okręty Marynarki Wojennej
Śmigłowiec Mi-8 meksykańskiej marynarki
Meksyk jest członkiem ONZ oraz wielu jej agend (np. FAO), a także Międzynarodowej Organizacji Pracy. Należy również do OPA (Organizacji Państw Amerykańskich), a także do wielu organizacji regionalnych. Będąc członkiem Grupy Rio uczestniczy w rozwiązywaniu lokalnych konfliktów w Ameryce Środkowej od lat 80.
Przez cały okres kolonialny za edukację w Meksyku odpowiadał prawie w całości Kościół katolicki. Po uzyskaniu niepodległości zaczęto formować pierwsze podstawy publicznego systemu edukacji. Obecnie szkoły podstawowe (1-6 klasa) są całkowicie laickie. Rząd federalny odpowiada za ustalanie programu zajęć i dostarczanie podręczników.
Konstytucja z 1917 wprowadziła obowiązkową edukację do poziomu 6 klasy szkoły podstawowej. Z obowiązku szkolnego wywiązuje się ok. 90% dzieci i młodzieży w wieku 6–14 lat. Dużo gorzej wygląda sytuacja w wieku powyżej 15 lat, w tym przedziale wiekowym tylko 50% osób uczęszcza do szkół. Meksyk uczynił duże postępy w zakresie likwidacji analfabetyzmu. Przykładowo w 1970 74% osób w wieku powyżej 15 roku życia posiadało umiejętność czytania i pisania. W 2005 odsetek ten wynosił już 93%. Nie ma znaczących różnic w poziomie analfabetyzmu pomiędzy płciami. Analfabetyzm jest natomiast poważniejszym problemem w najuboższych stanach (Chiapas, Guerrero)[64].
Szkolnictwo wyższe jest zdominowane przez uczelnie państwowe. Większość z nich mieści się w mieście Meksyk. Największym uniwersytetem jest UNAM (Narodowy Autonomiczny Uniwersytet Miasta Meksyk, założony w 1551). Inne większe uczelnie położone w stolicy to m.in. Narodowy Instytut Politechniczny (założony w 1937), Kolegium Meksykańskie (Colegio de México), Autonomiczny Instytut Technologiczny (założony w 1946), Uniwersytet Ibero-Amerykański (1943). Oprócz tego należy wymienić: Uniwersytet w Guadalajarze (1792), Autonomiczny Uniwersytet w Puebli Benemérita (1937), Uniwersytet w Veracruz (1944) czy Instytut Zaawansowanych Badań Technologicznych w Monterrey (1943).
Do bardziej znanych meksykańskich naukowców i wynalazców należą: Luis E. Miramontes (twórca pigułki antykoncepcyjnej), Manuel Mondragon (pierwszy karabin automatyczny), Guillermo González Camarena (pionier w dziedzinie telewizji kolorowej), Mario Molina (zdobywca Nagrody Nobla w dziedzinie chemii). Rodolfo Neri – absolwent UNAM został pierwszym meksykańskim kosmonautą (w ramach misji STS-61-B w 1985).
W ostatnich latach z większych przedsięwzięć badawczych realizowanych w Meksyku można wymienić budowę Wielkiego Teleskopu Milimetrowego, służącego do obserwacji części kosmosu przysłoniętych pyłem gwiezdnym. Cieniem na osiągnięciach naukowców meksykańskich kładą się niewielkie środki przeznaczane przez rząd na rozwój naukowy (obecnie jest to ok. 0,4% PKB[65]) , co jest wynikiem niskim w porównaniu do innych państw. Niekorzystnym zjawiskiem jest również koncentracja prawie całego potencjału naukowego w stolicy (75% wszystkich przewodów doktorskich ma miejsce w mieście Meksyk). W 1962 powołana została Narodowa Komisja Kosmiczna, która została jednak później zlikwidowana. W ostatnich latach pojawiły się plany jej odtworzenia[66].
Meksyk jako pierwszy kraj w historii w swojej konstytucji z 1917 zawarł pojęcie „bezpieczeństwa socjalnego”, które pozostawało jednak martwym pojęciem praktycznie do 1943. Problemem zabezpieczenia społecznego zajmuje się Meksykański Instytut Zabezpieczenia Społecznego. Ubezpieczenia są finansowane ze składek pracodawców, pracowników oraz przez rząd i obejmują ok. 50% obywateli. W Meksyku nie ma natomiast zasiłków dla bezrobotnych. Wydatki na zabezpieczenie społeczne w 1997 stanowiły 18,1% wydatków budżetowych[67].
Na system opieki społecznej składa się m.in. dotowana przez państwo opieka zdrowotna, która przynajmniej w teorii ma być dostępna dla wszystkich obywateli. W praktyce jednak w wielu mniejszych miejscowościach mieszkańcy nie mają do niej żadnego lub tylko ograniczony dostęp. Prywatne przychodnie i szpitale oferują dużo wyższy standard od publicznych, ale ogromnej większości społeczeństwa nie stać na usługi przez nie oferowane. Wydatki na ochronę zdrowia stanowiły w 1997 3,4% wydatków budżetowych.
Średnia długość życia w Meksyku wynosi 73 lata dla mężczyzn i 79 lat dla kobiet. Śmiertelność niemowląt to ok. 20 zgonów na 1000 urodzeń.
Ze względu na zróżnicowanie etniczne i podziały społeczno-ekonomiczne kultura meksykańska nie jest jednorodna. Na prowincji chłopi w dalszym ciągu czują silny związek ze swoimi małymi ojczyznami (patria chica) co pomaga podtrzymać różnorodność kulturową. Do obecnych czasów przetrwała duża liczba języków oraz zwyczajów indiańskich (w szczególności na południu). Rząd federalny wspiera rękodzieło oraz sztukę ludową, a także sztukę klasyczną pochodzenia europejskiego. Wysiłki te mają na celu wytworzenie jednolitego wzorca kultury meksykańskiej. Od lat 30. indigenismo – duma z dorobku dawnych kultur indiańskich jest jednym z głównych motywów w sztuce meksykańskiej.
W celu promowania kultury meksykańskiej powołany został Narodowy Instytut Sztuk Pięknych. Pod jego auspicjami odbywają się pokazy międzynarodowe, jak i krajowe Narodowej Orkiestry Symfonicznej, tańca ludowego i baletu klasycznego. Oprócz tego działają różne agencje, których celem jest zachowanie w nieskażonej formie pierwotnej kultury i rzemiosła indiańskiego.
Największą liczbą instytucji kulturalnych dysponuje stolica, znajdują się tu: Narodowe Muzeum Antropologii i Muzeum Sztuki Ludowej. Dużą rolę odgrywają również instytuty kulturalne funkcjonujące przy ośrodkach akademickich.
W Meksyku każdego roku, obok oficjalnych kościelnych i państwowych świąt (tzw. dias festivos), odbywa się niezliczona liczba uroczystości regionalnych. Obchody świąteczne świętowane są na wesoło, z dużą ilością dobrego jedzenia i picia, w tradycyjnych strojach, przy tańcach, pochodach i sztucznych ogniach. W całym kraju obchodzone są 17 stycznia – Dzień Świętego Antoniego, błogosławieństwo dla zwierząt domowych, następnie karnawał w lutym oraz 21 marca, kiedy jest wiosenne zrównanie dnia z nocą. W czasie Tygodnia Wielkanocnego (tzw. Semana Santa) w wielu miejscach odbywają się procesje, przypominające drogą krzyżową Jezusa. Ważnymi świętami są również 10 maja, kiedy obchodzony jest Dzień Matki czy 15 maja, dzień św. Izydora, kiedy to uroczyście rozpoczyna się okres siewów. Cały maj stoi pod znakiem kulturalnych festiwali w Acapulco i Cancún. Dniem Marynarza jest 1 czerwca, kiedy to w każdym porcie wzdłuż całego wybrzeża obchodzony jest Dzień Marynarki (tzw. Dia de la Marine). W dniu 15 sierpnia obchodzone jest Día de la Assunción inaczej Wniebowstąpienie Najświętszej Marii Panny, a w całym kraju organizowane są procesje. Świętem dużego znaczenia w Meksyku jest Día de los Muertos, czyli Święto Zmarłych. Zgodnie z tradycją Meksykanie celebrują to święto w trzech miejscach – w domu, w kościele oraz na cmentarzu. Przygotowania do święta zmarłych zaczynają się już 27 i 28 października. 31 października w pokoju lub na patio wznosi się tzw. ofiarę przeznaczoną dla krewnych rodziny, którzy odeszli już z tego świata. Najczęściej jest to odpowiednio zaadaptowany ołtarzyk domowy, zwany ołtarzem ofiarnym. Na ścianie pokoju zawiesza się obrazki świętych, a pod nimi ustawia się stół przykryty obrusem, na którym umieszcza się różne przedmioty, np. kadzielnice z copal (kadzidło z żywicy sosny ocote), lichtarze ze świecami, figurki świętych i aniołków, wazony i słoiki z kwiatami oraz inne ozdoby. Najbardziej spektakularna część świąt odbywa się 2 listopada i ma miejsce na cmentarzu. Zwykle po północy lub po godzinie 2, ale w wielu rejonach Meksyku ludzie przychodzą tam dopiero o świcie.
Boże Narodzenie, podobnie jak inne święta katolickie, zaczęto w Meksyku obchodzić zaraz po przybyciu konkwistadorów hiszpańskich i wprowadzeniu chrześcijaństwa. Tydzień przed Bożym Narodzeniem traktowany jest jak czas gościnności, przypominając szukanie miejsca schronienia świętej rodziny wędrującej do Betlejem. Głównym daniem wigilijnym jest wędzony dorsz z papryką i oliwą oraz małymi grzankami. Podaje się również pieczonego indyka, owoce, słodycze. Ponadto przy każdej okazji, w Meksyku organizowane są tzw. fiesty.
Najbardziej znani na świecie pisarze meksykańscy podejmują ponadczasowe i uniwersalne tematy w swoich utworach. Duży wpływ na twórczość literacką w Meksyku po 1945 wywarł Samuel Ramos, który w swoich książkach zajmuje się rozważaniami na temat człowieka i kultury. Przodującym poetą w całej Ameryce Łacińskiej jest z kolei Octavio Paz. Uznaniem na całym świecie cieszą się nowele Carlosa Fuentesa, w świecie hiszpańskojęzycznym bardzo popularni są Gustavo Sainz oraz Juan José Arreola. Wśród dramatopisarzy prym wiodą Rodolfo Usigli, Luisa Josefina Hernández i Emilio Carballido.
Juana Inés de la Cruz
Juan Ruiz de Alarcón
José Joaquín Fernández de Lizardi
Octavio Paz
Carlos Fuentes
Elena Poniatowska
Najbardziej rozpoznawalną formą malarstwa meksykańskiego są bez wątpienia murale. Mówi się nawet o istnieniu szkoły muralistów meksykańskich. Legendarne stały się murale opisujące rewolucję meksykańską, modernizację i walkę klas tworzone przez Diego Riverę czy Davida Alfaro Siqueirosa. Z innych znanych malarzy można wymienić Fridę Kahlo, Rufino Tamayo i Juana Soriano. Dużą popularność zyskały też wykonywane przez ludowego artystę José Clemente Orozco gipsowe figury.
Na współczesną architekturę meksykańską duży wpływ wywarły funkcjonalizm i ekspresjonizm. Widać w niej również pewne elementy architektury amerykańskiej i europejskiej. Jednym z bardziej znanych przykładów oryginalnej meksykańskiej myśli architektonicznej jest kompleks uniwersytecki UNAM w mieście Meksyk, a w szczególności biblioteka uniwersytecka. Wśród architektów meksykańskich szczególną sławę zdobyli Luis Barragán, Juan O’Gorman i Felix Candela.
Jose Maria Velasco
Diego Rivera
Frida Kahlo
Meksykańska muzyka, w szczególności ta wywodząca się z kręgów ludowych (ranchero, mariachi) zyskała dużą popularność w całej Ameryce Łacińskiej, a miasto Meksyk stało się siedzibą wielu studiów nagraniowych i wytwórni fonograficznych. Krajowy przemysł filmowy należy do największych na zachodniej półkuli. Za złotą erę meksykańskiego filmu uważa się lata 40. i 50., był to okres szczytowy pod względem liczby kręconych filmów, jak i budżetu przeznaczanego na kino. Z Meksyku wywodzi się wielu znanych reżyserów, są to m.in. Alejandro González Iñárritu („Amores Perros”, „Babel”, „Zjawa”), Alfonso Cuarón („Children of Men”) i Guillermo del Toro („Labirynt Fauna”). Światową sławę osiągnęła również aktorka Salma Hayek znana z filmów Desperado z 1995 oraz Frida z 2002.
Niewątpliwie najbardziej znaną meksykańską gwiazdą muzyki początku XXI w. był (rozpadł się w 2009 roku) młodzieżowy zespół RBD, który na całym świecie zdobył mnóstwo fanów i sprzedał miliony płyt. Z udziałem członków tej grupy powstały również bardzo popularny na świecie serial dla młodzieży „Rebelde” (Zbuntowani) oraz sitcom „RBD:La Familia” opowiadający o fikcyjnym życiu członków RBD. Znanymi piosenkarzami muzyki pop są również Cristiano Castro, Ariadna Thalía Sodi Miranda – Thalía, Julieta Venegas, Paulina Rubio oraz Alejandro Fernández.
Aniceto Ortega
Silvestre Revueltas
Carlos Chávez
Salma Hayek
Guillermo del Toro
Mariachi
Thalía
Julieta Venegas
Największymi stacjami telewizyjnymi w Meksyku są „Televisa” oraz „TV Azteca”. Z ważniejszych tytułów prasowych należy wymienić: „Esto” (nakład 400 tys. egz.), „El Heraldo de México” (nakład 209 tys. egz.), „La Prensa” (nakład 208 tys. egz.), „Excélsior” (nakład 200 tys. egz.). Wśród czasopism najwyższy nakład osiągają miesięcznik „Selecciones del Reader’s Digest” (nakład 610 tys. egz.) i dwutygodnik „Fama” (nakład 350 tys. egz.). W Meksyku działają agencje prasowe: Agencia Mexicana de Información (AMI) oraz Agencia de Informatión Integral Periodistíca[68].
Meksykanie dużą wagę przykładają do wartości rodzinnych i tradycji. Mimo że kobiety stanowią znaczącą część siły roboczej (35,2% w 2005) to wiele z nich pozostaje w domach i zajmuje się dziećmi oraz gospodarstwem. Dzieci, w szczególności te z wyższych klas pozostają w domach rodzinnych dosyć długo. W lepiej uposażonych rodzinach kobiety często dysponują pomocą w opiece nad dziećmi, przygotowaniu posiłków czy sprzątaniu. Przeważająca część meksykańskich kobiet musi dzielić czas pomiędzy pracą w domu i na zewnątrz, nierzadko pracując w kilku miejscach.
Tortilla, salsa i suszona wołowina z jajkiem
Chilaquiles
Gulasz z ciemnej fasoli
Guacamole
Stadion piłki nożnej Azteca, miasto Meksyk
Charreada
La Catrina – figurka na Święto Zmarłych
Mieszkańcy miast ubierają się podobnie do Europejczyków. Natomiast na terenach wiejskich da się zauważyć bardziej tradycyjne ubiory. Indian w przeciwieństwie do metysów cechuje większe przywiązanie do rodzimej kultury.
Meksykańska kuchnia jest bardzo różnorodna i zmienia się wraz z klasami społecznymi i zwyczajami kulinarnymi. Podstawę diety przeciętnego Meksykanina stanowią tortille z kukurydzy albo pszenicy, fasola, papryka i pomidory. Kolejną tradycyjną potrawą bez której „nie byłoby Meksyku” jest pozole, potrawa przypominająca lokalny bigos - Meksykanie wrzucają do garnka wszystko co jest pod ręką. Dlatego też istnieje kilka rodzajów tej potrawy: rojo, blanco, verde[69]. W bogatszych domach kuchnia jest bardziej urozmaicona z większymi wpływami kuchni amerykańskiej. Charakterystyczną cechą kuchni meksykańskiej jest również duże spożycie piwa, chociaż w ostatnich latach bardzo poprawiła się jakość win meksykańskich. Symbolem Meksyku pozostaje też wyrabiana z agawy tequila.
Dużą popularnością w Meksyku cieszą się piłka nożna, boks, wyścigi samochodowe, a także walki byków. Oprócz tego duży wpływ na spędzanie wolnego czasu w Meksyku mają wzorce przychodzące ze Stanów Zjednoczonych. W szczególności w północnych stanach daje się zauważyć mieszanie się kultur amerykańskiej i meksykańskiej.
Szwajcaria, Konfederacja Szwajcarska (niem. Schweiz, Schweizerische Eidgenossenschaft, fr. Suisse, Confédération suisse, wł. Svizzera, Confederazione Svizzera, romansz Svizra, Confederaziun Svizra) – państwo federacyjne w Europie Zachodniej. Jest jednym z niewielu państw, w których obowiązują szeroko stosowane formy demokracji bezpośredniej. Szwajcaria od kongresu wiedeńskiego w 1815 roku jest państwem neutralnym. Do Organizacji Narodów Zjednoczonych przystąpiła dopiero 10 września 2002 po przegłosowaniu tej decyzji w referendum minimalną większością 52% głosów.
Nie jest częścią Unii Europejskiej ani Europejskiego Obszaru Gospodarczego – w 2001 roku Szwajcarzy w referendum zdecydowanie (77% głosów przeciw) odrzucili nawet samą koncepcję rozpoczęcia rozmów o akcesji[6][7]. Posiada jednak specjalne dwustronne stosunki z UE, dzięki czemu uczestniczy w wybranych inicjatywach (np. układ z Schengen) oraz dopłaca do budżetu unijnego[8]. Szwajcaria formalnie nie ma stolicy, jej funkcję de facto pełni jednak Berno, będące siedzibą rządu[9].
Szwajcaria jest jednym z krajów alpejskich. Graniczy z Niemcami, Austrią, Liechtensteinem, Włochami i Francją. Dominującymi religiami w Szwajcarii są katolicyzm (41,8% ludności) i protestantyzm (35,3%)[10].
Szwajcaria jest górzystym państwem położonym w Europie Zachodniej. Nie posiada dostępu do morza. Graniczy z pięcioma państwami: Austrią i Liechtensteinem na wschodzie, Francją na zachodzie, Włochami na południu oraz z Niemcami na północy. Jest jednym z najmniejszych państw Europy – rozciągłość z północy na południe wynosi 220 km, a ze wschodu na zachód 350 km[11].
Alpy zajmują południową, południowo-zachodnią i wschodnią część kraju. Na północ od Alp znajduje się Wyżyna Szwajcarska. Na północnym zachodzie wyżyna ograniczona jest przez góry Jura. Większość północnej granicy z Niemcami biegnie wzdłuż Renu, który wpływa do Szwajcarii w pobliżu Szafuzy. Część granicy z Niemcami i Austrią przebiega przez Jezioro Bodeńskie, a Jezioro Genewskie stanowi część granicy z Francją.
Szwajcaria podzielona jest na 26 kantonów. Kantony leżące na Wyżynie Szwajcarskiej są bardziej zaludnione[12], uprzemysłowione i z reguły zamieszkane przez protestantów[13]. Kantony alpejskie natomiast są słabiej zaludnione, bardziej nastawione na rolnictwo i turystykę; przeważają tu też katolicy[13].
W Szwajcarii obowiązują cztery języki urzędowe: niemiecki, którym posługuje się 63,7% populacji, francuski, którym mówi 20,4% populacji, włoski używany przez 6,5% populacji oraz język romansz, którym mówi 0,5% populacji[14]. Na wschód od Berna (z wyjątkiem Ticino) przeważa język niemiecki. Na zachód od Berna przeważa język francuski. Język włoski jest najpowszechniejszym językiem w kantonie Ticino. Romansz, który wywodzi się z łaciny ludowej, najczęściej używany jest w kantonie Gryzonia.
Powierzchnia:
Długość granic:
Brak dostępu do morza.
Poniższa lista zawiera ważniejsze jeziora Szwajcarii o powierzchni powyżej 10 km², w nawiasach podano nazwy oryginalne:
Globalne ocieplenie jest szczególnie widoczne w Szwajcarii. Wynika to z położenia w umiarkowanych szerokościach geograficznych i klimatu umiarkowanego ciepłego przejściowego[15]. Pomiędzy początkiem zapisów pogodowych w 1864 a 2019 r.[15] temperatura w Szwajcarii wzrosła średnio o 1,9 °C[15]. Co oznacza zmianę dwukrotnie szybszą od średniej światowej[15]. W ciągu ostatnich 30 lat ocieplenie przyspieszyło[15]. Każdy rok od 1991 do 2019 był cieplejszy niż średnia dla lat 1961–1990[15]. Spośród dziesięciu najwyższych średnich temperatur w czerwcu od początku zapisów pogodowych, siedem zmierzono po 2002 roku[15]. W 1890 roku Davos było 231 dni przymrozków (dni, w których temperatura przy gruncie spadła poniżej 0 °C, przy jednoczesnej dodatniej wartości średniej dobowej temperatury powietrza) natomiast w 2018 r. już tylko 161. Powierzchnia lodowców szwajcarskich zmniejszyła się prawie o połowę między 1850 r. (1621 km²) i 2019 r. (944 km²)[16]. Z badań naukowych wynika, że jeśli nie zostanie osiągnięty cel ograniczenia globalnego wzrostu temperatury powietrza do 2 °C, wynikający z podpisanego w 2015 roku porozumienia paryskiego, to od około 2050 r. nie będzie możliwe uprawianie w Szwajcarii sportów zimowych[17].
Szwajcaria jest konfederacją państw, wchodzących w jej skład jako kantony o bardzo dużej autonomii. Związek niektórych z nich trwa nieprzerwanie 700 lat, co stawia Szwajcarię na drugim miejscu wśród najstarszych republik Europy (po San Marino). Szwajcaria jest także jedną z najstarszych federacji i demokracji na świecie[18]. W 1291 roku kanton Schwyz (od którego wywodzi się nazwa państwa), Uri i Unterwalden sygnowały akt utworzenia „związku wieczystego” (Akt Konfederacji Szwajcarskiej). Głównym celem była chęć uwolnienia się spod wpływu Habsburgów. Przełomem było zwycięstwo nad armią Habsburgów w bitwie pod Morgarten 15 listopada 1315. Zwycięstwo to przyczyniło się do późniejszych akcesji.
Do 1353 trzy założycielskie kantony zostały połączone z kantonami Glarus i Zug oraz miastami Lucerna, Zurych i Berno, tworząc tak zwaną Starą Konfederację, która rosła w siłę i bogactwo przez cały XV wiek (chociaż Zurych został wykluczony z federacji w latach 40. XV wieku na skutek zatargu terytorialnego). Dzięki pokonaniu Karola Zuchwałego w latach 70. XV wieku i najemnym wojskom szwajcarskim została utrzymana niezależność federacji. Pogrom armii Habsburgów oraz śmierć księcia Leopolda w bitwie pod Sempach 9 lipca 1386 zapewnił Szwajcarii faktyczną niezależność. W bitwie tej ponoć legendarnego czynu dokonał Arnold Winkelried, który rzucając się na lance habsburskiej piechoty, umożliwił przełamanie ich szyków i osiągnięcie zwycięstwa[19].
W 1506 roku papież Juliusz II najął wojska szwajcarskie do ochrony osobistej, tworząc Gwardię Szwajcarską, która po dziś dzień pełni tę funkcję (choć obecnie bardziej w roli reprezentacyjnej).
W XVI wieku Szwajcaria stała się jednym z głównych ośrodków reformacji w Europie. Objęła ona jednak głównie bogatsze i bardziej rozwinięte kantony miejskie. W 1531, po wojnie kantonów protestanckich z katolickimi (wiejskimi), w wyniku tzw. drugiego pokoju kappelskiego nastąpiło ustalenie podziału wyznaniowego kantonów.
Na mocy postanowień pokoju westfalskiego z 1648 roku, kończącego wojnę trzydziestoletnią, cesarz rzymsko-niemiecki uznał oficjalnie niepodległość Szwajcarii i jej formalne wyodrębnienie z Rzeszy Niemieckiej. Od tego czasu Szwajcarię zaczęto nazywać Związkiem Szwajcarskim.
W 1798 armie francuskie podbiły Szwajcarię, narzucając nową ujednoliconą konstytucję, osłabiającą kantony, a wzmacniającą rząd centralny. Okres ten (1798-1803) nazywany jest Republiką Helwecką. Jednakże system ten zniszczył wielowiekowe tradycje kulturowe i był bardzo niepopularny wśród Szwajcarów. Kiedy wybuchła wojna, Szwajcaria stała się areną walk pomiędzy Francją a Austrią i Rosją. Pojawiły się również dwie frakcje wśród Szwajcarów: „republikanów”, będących zwolennikami nowego porządku, oraz „federalistów”, chcących powrotu do systemu federacyjnego, opartego na szerokiej autonomii kantonów. Po spotkaniu zorganizowanym przez Napoleona w 1803 w Paryżu oba stronnictwa doszły do porozumienia, co znalazło wyraz w podpisaniu tzw. Aktu Mediacyjnego, przywracającego w znacznym stopniu system federacyjny. Kongres wiedeński potwierdził neutralność Szwajcarii, do której przyłączono ostatnie trzy kantony: Valais, Neuchâtel i Genewę.
W 1847 wybuchła wojna domowa pomiędzy kantonami katolickimi i protestanckimi (tzw. szwajcarska wojna domowa, niem. Sonderbundskrieg). Katolicy starali się nie dopuścić do wzmocnienia władzy centralnej, do czego dążyli rządzący wówczas przedstawiciele Partii Radykalnej. W wyniku miesięcznych walk zginęło około 100 osób i był to ostatni poważniejszy konflikt zbrojny na terytorium Szwajcarii. W wyniku tego w 1848 stworzono konstytucję federalną oraz system oparty na referendach, pozostawiając kwestie lokalne w gestii kantonów. W 1874 wniesiono poprawki, uwzględniające wprowadzenie wspólnej waluty oraz zmiany wymuszone przez rozwój populacji i rewolucję przemysłową. W 1891 konstytucja została ponownie poprawiona, utworzono unikatowy system, silnie oparty na demokracji bezpośredniej.
W 1920 r. Szwajcaria została członkiem Ligi Narodów, a w 1963 Rady Europy. Podczas I wojny światowej proklamowała neutralność, podobnie zresztą w czasie II wojny światowej. Mimo tego Niemcy planowali zajęcie Szwajcarii (operacja Tannenbaum), co wydawało się wówczas nieuniknione. W zaistniałej sytuacji Szwajcaria mimo oficjalnej neutralności przyjęła postawę antyniemiecką, przeprowadziła mobilizację armii pod dowództwem gen. Henriego Guisana i przygotowała się do długotrwałej obrony (Reduta Centralna), przyjmując zarazem 51219 uchodźców[20].
Od 12 grudnia 2008 Szwajcaria jest członkiem układu z Schengen.
Szwajcaria jest federacją demokratyczną oraz parlamentarną, gdzie na szeroką skalę wykorzystywana jest instytucja referendum (demokracja bezpośrednia). Szczególnie silna jest pozycja parlamentu i władz kantonalnych. Ustrój ten nazywa się parlamentarno-komitetowym; najbardziej typową cechą jest dla niego brak rozdziału pomiędzy władzę ustawodawczą, wykonawczą i sądowniczą.
Szwajcaria jest państwem federalnym, podzielonym na kantony posiadające charakter organizmów państwowych. Konstytucja określa podział kompetencji między federacją a kantonami.
Władza ustawodawcza należy do Zgromadzenia Związkowego (parlamentu), składającego się z dwóch izb: Rady Kantonów (izba wyższa) i Rady Narodowej (izba niższa). Głową państwa i szefem rządu jest prezydent, wybierany na okres 1. roku przez Zgromadzenie Związkowe spośród członków 7-osobowej Rady Związkowej (rządu).
Nie ma żadnych tak zwanych hamulców – nie można rozwiązać jej izb przed upływem kadencji, nie może być zwoływana na sesje przez żaden inny organ, nie ma instytucji sądownictwa konstytucyjnego (brak trybunału konstytucyjnego). Ponadto wszelkie konflikty kompetencyjne rozstrzygane są przez sam parlament. Wybiera on też siedmiu członków wykonawczej Rady Federalnej (rządu) i Trybunału Federalnego.
Szwajcarski konserwatyzm podkreśla fakt, że mimo tak solidnych tradycji demokratycznych kobiety uzyskały prawa wyborcze dopiero w lutym 1971 roku, a i to nie wszędzie. Najdłużej opierał się kanton Appenzell Innerrhoden, który ustąpił przed wyrokiem sądu najwyższego w roku 1990.
Szwajcaria jest państwem federacyjnym, składającym się z 26 kantonów[21], 143 okręgów[21] i 2222 gmin[21].
Szwajcaria dysponuje dwoma rodzajami sił zbrojnych. Są to siły powietrzne oraz wojska lądowe (200 czołgów, 200 dział samobieżnych oraz 1600 opancerzonych pojazdów bojowych). Pomimo braku dostępu do morza, Szwajcaria ma marynarkę wojenną (7 okrętów pływa po jeziorze Bodeńskim).
Wojska szwajcarskie liczą 135 tys. żołnierzy zawodowych oraz 77 tys. rezerwistów. Według rankingu Global Firepower (2020) szwajcarskie siły zbrojne stanowią 30. siłę militarną na świecie, z rocznym budżetem na cele obronne w wysokości 5 mld dolarów (USD)[22].
Spośród państw europejskich gospodarka Szwajcarii ma najlepszy wskaźnik wolności. W rankingu ogólnoświatowym zajmuje miejsce 4, ustępując miejsca jedynie Hongkongowi, Singapurowi i Nowej Zelandii[23]. Kraj jest bardzo wysoko rozwinięty, jeden z najbogatszych na świecie. Długotrwała neutralność (od 1815) i zasada nienaruszalności tajemnicy bankowej ugruntowały zaufanie do Szwajcarii jako finansowego centrum Europy i świata. Napływ obcych zasobów pieniężnych umożliwił stopniowy rozwój rodzimego, wysoko wyspecjalizowanego przemysłu, intensywnego rolnictwa i turystyki. W latach 80. niewielkie tempo wzrostu gospodarczego (przeciętnie 2,1% rocznie), od początku lat 90. – recesje, spadek produktu krajowego brutto o 0,3% w 1992 i 0,9% w 1993. Usługi wytwarzają 62% produktu krajowego brutto, w tym sektor bankowo-ubezpieczeniowy – ok. 16%, przemysł i budownictwo – powyżej 34%, rolnictwo – ok. 4%; produkt krajowy brutto na 1 mieszkańca – 42 003 USD.
Szwajcarskie rezerwy złota wynoszą 83 mln uncji jubilerskich, dewizowe są szacowane na ok. 6•1012 USD. Działa tu ponad 630 banków, z których 5 należy do największych w świecie: Schweizerischer Bankverein (z siedzibą w Bazylei, wartość depozytów 124 mld dolarów), Schweizerische Bankgesellschaft (Zurych, 114 mld dolarów), Crédit Suisse (Zurych, 89 mld dolarów), Schweizerische Volksbank (Berno) i Bank Leu. Łączna wartość depozytów w szwajcarskich bankach wynosi ok. 6 bln dolarów (2007 r.), w tym ok. 50% od klientów zagranicznych.
Działają liczne, znane w świecie przedsiębiorstwa ubezpieczeniowe (Zürich Insurance, Swiss Life, Reassurances), 6 giełd papierów wartościowych, w tym w Zurychu. Same podatki stanowią 30% PKB Szwajcarii, dodatkowo 7% PKB stanowią obowiązkowe ubezpieczenia społeczne (emerytalne, inwalidzkie, składki na służbę zdrowia), co łącznie daje 37% PKB (dla porównania: Szwecja 52%, Wielka Brytania 36%, Polska 34,5%, Stany Zjednoczone 27%; z uwzględnieniem ubezpieczeń społecznych). Służby zdrowia w Szwajcarii nie finansuje się z podatków, tylko z obowiązkowych ubezpieczeń, które są parapodatkami. PKB per capita wynosi nominalnie 51 771 dolarów, poziom porównywalny do Danii, a po zmierzeniu parytetem siły nabywczej 37 369 dolarów, również porównywalny do Danii. Wskaźnik Giniego dla Szwajcarii wynosi 33, co jest poziomem porównywalnym do Holandii, wyższym niż we Francji, Niemczech i krajach skandynawskich, ale niższym niż w wielu innych krajach.
Szwajcarski eksport charakteryzuje się relatywnie wysoką odpornością na szoki walutowe[24]. Odporność ta osiągana jest dzięki znacznemu eksportowi leków, które cechują się niską elastycznością cenową popytu, a także instrumentów precyzyjnych (w tym zegarków), które postrzegane są jako dobra wyższego rzędu.
Szwajcaria posiada bardzo nieliczne złoża surowców mineralnych. Wydobywa się głównie kruszywa budowlane oraz surowce do produkcji takich materiałów budowlanych jak cement i wapno (wapienie, margle), a także sól kamienną (Jura). Podstawą elektroenergetyki jest energia odnawialna wytwarzana w elektrowniach wodnych na rzekach Alp (58% produkowanej energii elektrycznej) i energia jądrowa z elektrowni jądrowych (40% energii elektrycznej). W czerwcu 2011 roku parlament postanowił o niebudowaniu nowych reaktorów, a tym samym o wycofaniu się z energetyki jądrowej do roku 2034, na kiedy przewidziane jest zamknięcie ostatniego reaktora jądrowego (w Leibstadt)[25]. Produkcja energii elektrycznej na 1 mieszkańca Szwajcarii wynosi 7928 kWh (2011)[26]. Przemysł przetwórczy wytwarza z surowców importowanych produkty wysoko przetworzone (głównie na eksport), wymagające dużego nakładu pracy oraz myśli technicznej. Największe szwajcarskie przedsiębiorstwa przemysłowe to: Nestlé (branża spożywcza, obroty 36 mld dolarów, 1992), ABB (maszynowa, 29 mld dolarów), Novartis i Roche (chemiczna), Sulzer Brothers (maszynowa).
Rozwinięty jest głównie przemysł elektromaszynowy, chemiczny i spożywczy. Przemysł elektromaszynowy dostarcza urządzenia sterownicze dla różnych gałęzi przemysłu, turbiny, generatory, silniki, między innymi okrętowe. Większość zakładów tego przemysłu jest skupiona w Zurychu, Winterthur, Baden. Tradycyjnie bardzo duże znaczenie ma branża zegarmistrzowska i jubilerska. Wyrób zegarków znanych firm (Swatch, Rolex, Montres Epos, Patek Philippe, Tissot) koncentruje się w La Chaux-de-Fonds, Neuchâtel, Genewie. Przemysł chemiczny wyspecjalizowany jest w produkcji leków (główny ośrodek to Bazylea), spożywczy – serów, przetworów mięsnych i czekolady. Ponadto przemysł papierniczy, cementowy i włókienniczy. W Szafuzie i Neuhausen, w pobliżu hydroelektrowni znajdują się huty aluminium (importowany tlenek glinu).
Przemysł jest źródłem zanieczyszczenia wody i powietrza. W dziesięcioleciu 2007–2016 poziom zanieczyszczenia przemysłowego wód różnymi parametrami był podobny, natomiast nastąpił wzrost zanieczyszczenia niklem, a spadł ołowiem. Zanieczyszczenie powietrza w tym okresie spadło[27].
Łączna emisja równoważnika dwutlenku węgla ze Szwajcarii i Liechtensteinu wyniosła w 1990 roku 55,238 Mt, z czego 44,955 Mt stanowił dwutlenek węgla. W przeliczeniu na mieszkańca emisja wyniosła wówczas 6,735 t dwutlenku węgla, a w przeliczeniu na 1 dolar PKB 139 kg. Emisje metanu odpowiadają za większość pozostałych emisji, ale emisje podtlenku azotu i gazów fluorowanych są również zauważalne, a te ostatnie w drugiej dekadzie XXI w. zaczęły się zbliżać do poziomu emisji metanu. Po roku 1990 całkowita emisja wahała się i dotyczy to też emisji dwutlenku węgla pochodzenia kopalnego, która ostatecznie na przestrzeni kilkudziesięciu lat wykazuje trend lekko spadkowy. Główne jego źródła to emisje z transportu i budynków. Ten pierwszy w tym okresie nieznacznie rósł, a drugi malał. Emisje z energetyki są stosunkowo małe, ale z czasem rosną. W 2018 emisja dwutlenku węgla pochodzenia kopalnego wyniosła 40,94 Mt, a w przeliczeniu na mieszkańca 4,792 t i w przeliczeniu na 1 dolar PKB 81 kg[28].
Rolnictwo wysokotowarowe. Użytki rolne zajmują ok. 36% całej powierzchni kraju (2012), w tym łąki i pastwiska prawie 27% ogółu powierzchni kraju (73% użytków rolnych)[29]. Przeważają gospodarstwa o powierzchni 10–20 ha (średnio 16,2 ha). 1 ciągnik przypada na 14 ha użytków rolnych. Poziom nawożenia (349 kg nawozów sztucznych na 1 ha gruntów ornych) ma tendencję spadkową w związku z powszechną produkcją tzw. zdrowej żywności. Chów i hodowla dostarcza ponad 80% wartości produkcji rolnej. Rozwinięte są głównie chów i hodowla bydła domowego o kierunku mlecznym (przeciętny roczny udój od 1 krowy – 5000 litrów, 1994) i trzody chlewnej, w strefach podmiejskich – drobiu, w wyższych piętrach Alp i Jury – owiec. Na Wyżynie Szwajcarskiej przeważa uprawa pszenicy, jęczmienia, ziemniaków i buraków cukrowych, w dolinie Rodanu i nad Jeziorem Genewskim – warzyw i owoców (głównie jabłonie), na południowych stokach Jury – winorośli. Lasy zajmują ok. 31% powierzchni kraju, ale ich znaczenie przemysłowe maleje w związku z rozwiniętą ochroną.
W Szwajcarii funkcjonuje około dwustu kąpielisk wyznaczonych zgodnie z europejską dyrektywą kąpieliskową. W 2015 50,5% z nich osiągnęło najwyższą klasę jakości, tj. doskonałą, a 1,4% najniższą, tj. niedostateczną. W 2018 było to odpowiednio 75% i 1%. Ocena ta oparta jest głównie o kryterium zanieczyszczenia bakteriami kałowymi. W 2018 kilkanaście procent nie zostało ocenionych. Ocenę niższą niż doskonała w 2018 miały niektóre kąpieliska umiejscowione na jeziorach Genewskim, Neuchâtel i Bodeńskim, przy czym na tych jeziorach były również kąpieliska spełniające kryteria najwyższej klasy[30].
Pod koniec 2017 roku ludność Szwajcarii liczyła 8,48 mln osób, w tym 2,13 mln (25,1 proc.) to osoby nie będące obywatelami tego kraju. Są to głównie Włosi, Niemcy, Portugalczycy, Francuzi, Kosowianie, Hiszpanie, Turcy i Serbowie. W 2017 roku do Szwajcarii przybyło 171 tys. imigrantów, o 20 tysięcy mniej niż rok wcześniej. Obywatelstwo otrzymało 45 tys. osób. Łącznie populacja kraju wzrosła o 0,8 proc.[32]
     niemiecki     francuski     włoski     romanszW szwajcarskich instytucjach federalnych urzędowymi są cztery języki: niemiecki, francuski, włoski oraz romansz (retoromański)[33]. Konstytucje kantonów określają jaki język lub języki są urzędowe na obszarze kantonu. Nauczanie w szkołach podstawowych, gimnazjach i liceach odbywa się w jednym z tych języków, w zależności od kantonu. Na uniwersytecie studiuje się po francusku bądź niemiecku (i włosku w kantonie Ticino). Absolwent wyższej uczelni powinien móc porozumiewać się przynajmniej w trzech językach. Język angielski, mimo że nie należy do języków urzędowych, staje się coraz popularniejszy w środowisku przemysłowym i świecie reklamy.
Język codzienny Szwajcarów w kantonach niemieckojęzycznych to dialekt alemański (Schwyzerdütsch). Dialekty używane są tam przez wszystkie warstwy społeczne, nawet w dużych miastach, chociaż pisze się prawie zawsze po niemiecku[34]. W Szwajcarii języki francuski i włoski, mimo drobnych odmienności, np. w niektórych liczebnikach oraz w wymowie, nie różnią się od języka standardowego. Język romansz jest używany tylko w niektórych gminach kantonu Gryzonia, gdzie dominuje Schwyzerdütsch.
Według spisu powszechnego ludności z 2000 roku struktura ludności ze względu na język ojczysty przedstawia się następująco:
Szwajcarzy nie stanowią jedności ani językowo, ani religijnie, mają jednak wspólną historię.
Najludniejsze miasta Szwajcarii (stan na 31 grudnia 2021)[35]:
Szwajcaria od wielu lat utrzymuje się w ścisłej czołówce rankingu państw najbardziej przyjaznych człowiekowi. Genewa oraz Zurych znajdują się w pierwszej dziesiątce miast indeksu GLCI[36].
Szwajcaria była jednym z pierwszych na świecie państw, które wprowadziło powszechne nauczanie elementarne. Szkoły podstawowe i średnie w Szwajcarii są bezpłatne; w wielu kantonach bezpłatne są także obiady oraz podręczniki. Za państwowe szkoły wyższe wnoszona jest opłata za każdy semestr. Studentom z rodzin ubogich przyznawane są jednak stypendia.
W Szwajcarii znajduje się wiele szkół prywatnych, które słyną z wysokiego standardu kształcenia. Ich dyplomy nie są uznawane przez państwo, mogą być jednak uznawane przez firmy prywatne.
Literatura szwajcarska powstaje w języku niemieckim, francuskim, włoskim i retoromańskim, lecz największe znaczenie mają dzieła literackie w pierwszych dwóch językach. Najbardziej znani pisarze szwajcarscy to Max Frisch i Friedrich Dürrenmatt. Bardzo ważnymi postaciami byli także filozof i psycholog Carl Gustav Jung oraz pochodzący z Genewy i tworzący m.in. w Szwajcarii pisarz i filozof Jean-Jacques Rousseau. W Szwajcarii żył i tworzył, chociaż częściowo także we Francji, w Niemczech i we Włoszech, niemiecki filozof i filolog klasyczny Friedrich Nietzsche, który przez kilka lat był profesorem Uniwersytetu w Bazylei i jednocześnie nauczycielem w miejscowym gimnazjum klasycznym.
Znani pisarze szwajcarscy:
W Szwajcarii dominują wyznania chrześcijańskie, z podziałem pomiędzy katolicyzmem, który wyznawało 35,2% całej populacji w 2018 roku (46,7% w 1970) a Szwajcarskim Kościołem Reformowanym z 23,1% populacji (48,8% w 1970), z których najwięcej wyznawców liczy kalwinizm. W wyniku imigracji żyje tam również pewna liczba wyznawców islamu (wzrost do 5,3% w 2018 z 0,2% w 1970) i innych wyznań chrześcijańskich (5,6%), w tym prawosławia. Ponadto w Szwajcarii żyje sporo wyznawców judaizmu, buddyzmu i hinduizmu, którzy łącznie stanowili 1,5% populacji kraju. 29,4% Szwajcarów nie identyfikowało się z żadną religią, z czego 28,0% zdeklarowało brak związków z jakąkolwiek religią (wzrost z 1,2% w 1970 i 11,4% w 2000), a pozostałe 1,4% to nieokreśleni[37]. Sondaż przeprowadzony przez Eurobarometer w 2010 pokazał, że tylko 44% Szwajcarów deklaruje wiarę w Boga, dalsze 39% dopuszcza możliwość istnienia nieokreślonej siły wyższej, a 11% deklaruje ateizm. Pozostałe 6% nie potrafiło lub nie chciało się określić.
Mniejsze ważniejsze wyznania i kościoły w Szwajcarii to: prawosławni (158 tys. wiernych), zielonoświątkowcy (68 tys.), hinduizm (42,5 tys.), buddyzm (36,5 tys.), Kościół Nowoapostolski (34,3 tys.), Świadkowie Jehowy (19,5 tys.), Bracia plymuccy (17,7 tys.), judaizm (17,5 tys.), Stowarzyszenie Wolnych Kongregacji Ewangelicznych (14 tys.), Kościół Metodystyczny (14 tys.), Pilgrim Mission of St. Chrischona (14 tys.), Kościół Anglikański (13,8 tys.), Kościół Chrześcijańskokatolicki (9,8 tys.), mormoni (8,9 tys.) i Kościół Adwentystów Dnia Siódmego (4,7 tys.)[38][39][40].
Historycznie kraj jest dość równo podzielony pomiędzy rzymskich katolików i wyznawców kalwinizmu, ze skomplikowaną mozaiką mniejszości na obszarze niemal całego kraju. Niektóre kantony, np. Appenzell, są nawet oficjalnie podzielone na sekcje katolickie i protestanckie. W większych miastach (Berno, Zurych, Bazylea) dominują protestanci. Centralna Szwajcaria jest tradycyjnie katolicka. Istnieje też Kościół Chrześcijańskokatolicki w Szwajcarii, zaliczany do nurtu starokatolickiego. Jest jednym z oficjalnych Kościołów w kraju, choć liczba jego członków bardzo szybko się zmniejsza i obecnie należy do niego jedynie 0,2% Szwajcarów. Konstytucja szwajcarska z roku 1848 – w dużym stopniu pod wpływem ówczesnych starć pomiędzy kantonami katolickimi a protestanckimi (tzw. szwajcarska wojna domowa, niem. Sonderbundskrieg) – świadomie wprowadziła zasady, które umożliwiają pokojową koegzystencję pomiędzy różnymi wyznaniami. W 1980 roku odbyło się referendum w sprawie całkowitego rozdziału państwa od Kościołów. Zostało to odrzucone (80% przeciw).
We wsi Dornach koło Bazylei swoją siedzibę ma chrześcijańskie Towarzystwo Antropozoficzne, znajduje się tam antropozoficzne Goetheanum, w którym bardzo często wystawiany jest Faust Goethego. Budynek jest ogólnodostępny dla zwiedzających. Z kolei w Bernie powstała w latach pięćdziesiątych pierwsza świątynia Kościoła Jezusa Chrystusa Świętych w Dniach Ostatnich w Europie.
Szwajcarzy pod względem religijności nie odróżniają się od większości narodów Europy. Udział w praktykach religijnych, głównie z okazji wielkich uroczystości i najważniejszych świąt, ogranicza się do mniejszości. W zależności od kantonu i wyznania, systematyczny udział w nabożeństwach niedzielnych wynosi 6–9% wśród protestantów i 10–14% wśród katolików. Papiestwo, watykański centralizm i dogmatyczność Kościoła są poddawane krytyce nie tylko przez wiernych Kościoła rzymskokatolickiego, ale także przez hierarchię katolicką, która żąda większej niezależności Kościoła Szwajcarii od Watykanu. Większość istniejących w Szwajcarii parafii katolickich jest bardzo liberalna i egalitarna: w wielu kościołach, podobnie jak w kościołach reformowanych, nie ma obrazów i figur, najwyżej wiszące na ścianach krzyże; księża wbrew nauce Kościoła nierzadko łamią celibat i zakładają rodziny, jak również dokonują wielu zmian w tradycyjnej liturgii; nabożeństwa nieraz są prowadzone przez świeckich katechetów lub pastorów czy nawet pastorki z Kościoła reformowanego, gdy brakuje księży katolickich[41].
Funkcję telewizji publicznej pełni SRG SSR, która dzieli się według języków na Télévision Suisse Romande (TSR), Televisione svizzera di lingua italiana (TSI) i Schweizer Fernsehen (SF). Istnieją także prywatne telewizje regionalne TeleZüri, TeleBärn i Telebasel. Największe niemieckojęzyczne dzienniki to Blick, Neue Zürcher Zeitung i Tages-Anzeiger.
Tradycyjnym specjałem szwajcarskim jest fondue, które robione jest z roztopionego sera. Sery szwajcarskie, takie jak gruyère czy ementaler, podgrzewa się w specjalnym kociołku z dodatkiem pieprzu, czosnku, białego wina i kirschu. Do nabierania roztopionego sera używa się świeżego chleba. Szwajcaria oprócz serów może pochwalić się doskonałą czekoladą oraz piwem i winem.
Alpy w Szwajcarii
Jezioro Czterech Kantonów
Wiosna w Szwajcarii
Góry na północy
Jezioro Genewskie
Genewa
Berno
Kaplica św. Meinrada na przełęczy Etzel
Kanton Schwyz
Termy w Vals, arch. Peter Zumthor, 1996
Bagdad (arab. ‏بغداد‎ [bɐʁˈd̪ɑːd̪]) – stolica Iraku; liczba mieszkańców wynosi ponad 7 mln. Położony nad Tygrysem jest jednym z największych miast na Bliskim Wschodzie.
Bagdad został założony w 762 roku przez drugiego kalifa z dynastii Abbasydów, Al-Mansura. Zgodnie z intencją założyciela miał być stolicą dynastii Abbasydów i zastąpić dotychczasową siedzibę kalifa, Al-Kufę. Przenosiny były podyktowane zagrożeniem ze strony ugrupowań szyickich i podbitej ludności. Z tego też powodu został usytuowany w zakolu Tygrysu.
Pierwotna nazwa Madinat as-Salam (czyli „Miasto Pokoju”) utrzymała się aż do podboju Mongołów w 1258, po czym miasto przejęło nazwę po okolicznej wiosce. Miasto zbudowano na planie koła o średnicy ok. 3 km, co zapewniało mu funkcjonalność.[potrzebny przypis]
Miasto było otoczone z czterech stron murami nazwanymi odpowiednio: Al-Kufa, Basra, Khuraan i Damaszek. Nazwy miały związek z tym, że poszczególne bramy były zwrócone w kierunkach wymienionych miast[1]. Odległość pomiędzy poszczególnymi bramami wynosiła około 2400 m. Każda z bram posiadała podwójne drzwi wykonane z żelaza. W związku z tym, że były bardzo ciężkie, ich otwarcie i zamknięcie wymagało współdziałania kilku dorosłych mężczyzn. Grubość muru przy bramach wynosiła 44 m u podstawy oraz około 12 m u szczytu. Reszta muru posiadała wysokość 30 m, na którą składały się blanki. Był on dodatkowo otoczony innym potężnym murem o grubości 50 m. Wyżej wymieniony – zewnętrzny mur – był wyposażony w wieżyczki oraz zaokrąglone blanki. Dodatkową ochronę zapewniała solidna pochyłość, którą uzyskano stosując zaprawę murarską (cegły z palonym wapnem). Na zewnątrz drugiego muru znajdowała się fosa wypełniona wodą[1].
W ciągu jednego pokolenia miasto stało się głównym centrum nauczania i handlu. Niektóre źródła podają, że liczyło wtedy ponad milion mieszkańców, jednak bardziej prawdopodobne są mniejsze liczby.
W Bagdadzie założono m.in. bibliotekę Chizanat al-Hikma, która przeistoczyła się w najważniejszą akademię muzułmańskiego świata nazwaną Bajt al-Hikma.
Początkowy rozrost Bagdadu został spowolniony w obliczu kłopotów jakie zaczęły spotykać kalifat, m.in. tymczasowe przenosiny stolicy do Samarry (w latach 808–819 oraz 836–892), utrata najodleglejszych zachodnich i wschodnich prowincji, okresy politycznej dominacji lokalnej perskiej dynastii Bujjidów 945–1055 oraz Turków seldżuckich (1055–1135), a także spory na łonie islamu o schedę po Proroku i o tytuł kalifa (Fatymidzi, Umajjadzi).
Pomimo to miasto pozostało jednym z kulturowych i handlowych ośrodków imperium, aż do 10 lutego 1258, gdy zostało złupione przez Mongołów pod wodzą Hulagu-chana. Mongołowie zmasakrowali wtedy 800 tys. mieszkańców miasta, w tym także kalifa Al-Mustasima, jak się okazało ostatniego z linii Abbasydów, i zniszczyli znaczną część miasta. Napaść na Bagdad zakończyła schyłek kalifatu abbasydzkiego i przyspieszyła upadek cywilizacji arabskiej.
W 1401 roku Bagdad został ponownie splądrowany przez Mongołów pod wodzą Timura (Tamerlana). Stał się stolicą prowincji kontrolowanej przez Dżalajirydów (1400–1411), stowarzyszone plemiona mongolskie: Kara Kojunlu (1412–1469), Ak Kojunlu (1469–1508) i dynastię Safawidów (1508–1534). W 1534 został podbity przez Imperium Osmańskie.
Pozostał pod panowaniem tureckim do czasu zajęcia Iraku przez Brytyjczyków w 1917 roku i ustanowienia królestwa Iraku w 1921. W 1932 roku Irak formalnie uzyskał niepodległość, a w 1946 pełną. Przez cały ten czas pozostawał jego stolicą.
Populacja miasta wzrosła z 145 tys. w 1900 (szacunkowa liczba) do 580 tys. w 1950. Bagdad bardzo ucierpiał w trakcie i po wojnie w Zatoce Perskiej w 1991 – zniszczenia infrastruktury komunikacyjnej, energetycznej i wodno-kanalizacyjnej.
Bagdad znajduje się w centralnej części Iraku, w środkowym biegu rzeki Tygrys, która jest żeglowna od wybrzeża do Bagdadu. Centrum miasta jest położone na wysokości 44 m n.p.m.
Jeśli chodzi o klimat, Bagdad jest jednym z największych miast z klimatem gorącej pustyni. Pod względem temperatury maksymalnej jest to jedno z najgorętszych miast świata. Temperatury w lipcu i sierpniu z łatwością przekraczają 44 °C. W lipcu ta wartość może wrosnąć nawet do +49 °C. Opady, prawie zerowe w lato, i bardzo małe zachmurzenie dają długie godziny nasłonecznienia. Temperatury rzędu 30 °C i więcej trafiają się od kwietnia do późnego października/wczesnego listopada i prawie 90% opadów przypada zimowemu półroczu.
W Bagdadzie znajduje się największa na świecie placówka dyplomatyczna Stanów Zjednoczonych, której oficjalne otwarcie miało miejsce 5 stycznia 2009[2]. Położona jest w sercu tzw. „Zielonej strefy”. Jej powierzchnia, obejmująca kompleks budynków, odpowiada wielkością powierzchni Watykanu[3].
Stambuł[2] (tur. İstanbul) – największe i najludniejsze miasto w Turcji oraz jej centrum kulturalne, handlowe i finansowe. Rozciąga się po obu stronach cieśniny morskiej Bosfor, od północnego wybrzeża morza Marmara do południowego wybrzeża Morza Czarnego. Położenie miejscowości zarówno w europejskiej Tracji, jak i azjatyckiej Bitynii sprawia, że jest ona jedyną metropolią świata znajdującą się na dwóch kontynentach[3].
Jedno z największych miast świata, najludniejsze miasto Europy, stolica prowincji Stambuł. Rozwinięty ośrodek przemysłowy, wytwarzający około ¼ produkcji kraju. Główne centrum finansowe Turcji, siedziba kilkudziesięciu banków i giełdy papierów wartościowych. Siedziba pięciu uniwersytetów, z których najstarszy – Uniwersytet Stambulski – został założony w 1863 roku.
W 2018 roku Stambuł odwiedziło 12,8 mln turystów z całego świata – był ósmym najczęściej odwiedzanym miastem na świecie[4].
Stambuł położony jest nad cieśniną Bosfor i morzem Marmara. Zachodnia część miasta znajduje się w Europie, wschodnia natomiast w Azji. Powierzchnia Stambułu wynosi 1539 km², zaś całej prowincji – 6220 km²[potrzebny przypis].
Miasto tworzą trzy części: część azjatycka, półwysep po stronie europejskiej na południe od Złotego Rogu oraz dzielnica Galata z tzw. Nowym Miastem. W części europejskiej znajdują się instytucje handlowe, zaś część azjatycka ma charakter dzielnicy mieszkaniowej.
Obszar półwyspu odpowiada położeniu XV-wiecznego Konstantynopola, dzisiaj znajdują się tu dzielnice Eminönü oraz Fatih. Półwysep otacza od południa Morze Marmara, a od wschodu cieśnina Bosfor. Na Półwyspie znajdują się największe kościoły, najwspanialsze meczety i największe muzea. Na dwóch z siedmiu wzgórz, na których leży Stambuł, zbudowano Pałac Topkapı i Meczet Sultanahmet. Tu znajduje się również Hipodrom, Hagia Sofia. Stare miasto jest otoczone murami miejskimi pochodzącymi z IV i V n.e. Mają kilka części: część nadmorska leży nad Morzem Marmara, lądowa ciągnie się od Yedikule aż do Złotego Rogu.
Na północ od Złotego Rogu znajdują się historyczne dzielnice Beyoğlu i Beşiktaş, z pałacami sułtańskimi, willami: Ortaköy, Bebek położonymi nad brzegiem cieśniny. Na brzegu Bosforu, zarówno po stronie europejskiej, jak i azjatyckiej, bogaci Stambulczycy budowali luksusowe wille, zwane yalı, służące za letnie rezydencje.
Dystrykty Üsküdar i Kadıköy znajdujące się w azjatyckiej części miasta, początkowo były niezależnymi miastami (podobnie jak Beyoğlu po europejskiej stronie).
Miasto znajduje się w strefie klimatu śródziemnomorskiego z pewnymi wpływami klimatu kontynentalnego[5]; ciepłe i wilgotne lato, chłodną i deszczową zimę. Roczna suma opadów wynosi średnio 844 mm[6]. Zimą często występują opady śniegu. Okres letni trwa od czerwca do września. Najcieplejszym miesiącem jest sierpień, a najchłodniejszym styczeń. Miasto jest wietrzne, średnia prędkość wiatru wynosi 17 km/h. Najwyższą zarejestrowaną w mieście temperaturę 40,5 °C odnotowywano 12 lipca 2000, najniższą zaś, −16,1 °C, 9 lutego 1927[7].
Stambuł jest usytuowany w pobliżu uskoku północnoanatolijskiego, który biegnie z północnej Anatolii do Morza Marmara. Dwie płyty tektoniczne: Afrykańska i Eurazjatycka stykają się tutaj ze sobą. Uskok ten był przyczyną wielu dużych trzęsień ziemi: w 1509 roku trzęsienie ziemi spowodowało tsunami, które przedarło się za mury miejskie, niszcząc ponad 100 meczetów i zabijając 10 000 ludzi. W 1894 roku trzęsienie ziemi spowodowało zawalenie się części Krytego Bazaru. Trzęsienie ziemi 17 sierpnia 1999 roku z epicentrum w Izmicie zabiło 18 000 ludzi.
Pierwsza nazwa miasta – Bizancjum (stgr. Βυζάντιον Byzantion) – pochodziła od imienia jego założyciela – Byzasa. W III wieku n.e. miasto nosiło nazwę Augusta Antonina, na cześć syna Septymiusza Sewera – Antoniusza. Cesarz Konstantyn I Wielki zmienił nazwę miasta na Nova Roma, czyli Nowy Rzym, jednocześnie gruntownie przebudowując miasto. Inne nazwy z tego okresu to: Wschodni Rzym, Drugi Rzym, Roma Constantinopolitana.
Nazwa Konstantynopol (gr. Κωνσταντινούπολις Konstantinupolis, łac. Constantinopolis, nowogr. Κωνσταντινούπολη Konstantinupoli), czyli „Miasto Konstantyna”, została po raz pierwszy użyta przez cesarza Teodozjusza II i stała się oficjalną nazwą miasta przez cały okres bizantyński, używaną również w czasach Imperium Osmańskiego.
W 1930 roku rząd turecki wystąpił z apelem do państw zachodnich o zaprzestanie używania formy Konstantynopol, a używanie tylko nazwy Istanbul, jednocześnie wymuszając stosowanie tej nazwy nowym prawem pocztowym[10]. W Imperium Osmańskim oraz w świecie arabskim miasto to istniało również pod nazwą Kostantiniyye.
Nazwa Stambuł (İstanbul) pochodzi od zniekształconego przez Turków greckich słów: εἰς τὴν πόλιν eis ten polin, oznaczających: do miasta. „Miastem” nazywali Bizantyjczycy stolicę cesarstwa, w czasach Konstantyna Wielkiego uważaną za najludniejsze miasto świata. Frazą eis ten polin ludy wschodu (Ormianie, Persowie, a za nimi Turcy) określały kierunek podróży do Konstantynopola od wielu wieków[11][12].
W krajach słowiańskich dawniej miasto było znane także jako Carogród, w tekstach skandynawskich Miklagard[13].
Pierwsze ślady osadnictwa w okolicach Stambułu pochodzą z epoki kamienia i znajdują się w części azjatyckiej miasta. Pierwsza zorganizowana osada istniała w epoce brązu na terenie dzisiejszego pałacu Topkapı. Około 680 p.n.e. uciekinierzy z greckiej Megary założyli kolonię Chalkedon w azjatyckiej części.
Około 660 p.n.e. inna grupa Greków pod przewodnictwem Byzasa założyła po stronie europejskiej pierwszą większą miejscowość – obecnie Sarayburnu, czyli „Cypel Pałacu” na Historycznym Półwyspie[14]. Dzięki korzystnemu położeniu geograficznemu miasto bardzo szybko się rozwijało. W 513 p.n.e. miasto zostało zdobyte przez wojska perskie. Od 407 p.n.e. miasto należało do Aten, a od 405 p.n.e. do Sparty. W 227 p.n.e. po azjatyckiej stronie miasta zaczęli się osiedlać Galowie.
W 146 p.n.e. został zawarty związek z Republiką rzymską[potrzebny przypis]. W 196 roku cesarz Septymiusz Sewer włączył miasto do Cesarstwa. Cesarz Konstantyn I Wielki przebudował miasto i w 330 roku nadał mu nową nazwę – Konstantynopol. Po podziale Cesarstwa rzymskiego po śmierci Teodozjusza I, stolicą Cesarstwa Wschodniorzymskiego został Konstantynopol. W V wieku miasto otoczono nowymi murami. Za rządów cesarza Justyniana wybudowano Kościół Hagia Sofia. W latach 674–678 i 717–718 oblegane przez Arabów. W 1054 roku Konstantynopol stał się centrum kościoła prawosławnego. W 1096 roku miasto odwiedzili krzyżowcy, nie czyniąc szkód. Napływali również handlarze z Wenecji i Genui – powstała wtedy dzielnica Galata. Po zdobyciu i złupieniu 13 kwietnia 1204 przez krzyżowców, w latach 1204–1261 Konstantynopol był stolicą stworzonego przez nich Cesarstwa Łacińskiego. Odzyskany przez Michała VIII Paleologa, stracił jednak dawne znaczenie. Kolonia Galata przejęła zyski z handlu. Konstantynopol pustoszał, liczba ludności malała, budowle ulegały ruinie.
Pod koniec XIV wieku pod mury miasta po raz pierwszy podeszli Osmanowie i zajęli okolice miasta. W 1390 roku wojska Bajazyda I oblegały miasto, następne oblężenie miało miejsce w 1422 roku pod dowództwem Murada II. Miasto zostało zdobyte przez Turków pod wodzą Mehmeda II 29 maja 1453 i stało się stolicą Imperium Osmańskiego. Gdy w 1517 roku Osmanowie podbili Egipt i kalifat został przeniesiony do Stambułu, miasto stało się centrum islamskiego świata. W XVI-XVIII wieku miasto ozdabiano pałacami, meczetami, kompleksami architektonicznymi. W XIX wieku, kiedy państwo czyniło próby europeizacji, zrezygnowano z tradycyjnej osmańskiej architektury, zastępując ją barokiem i rokoko. Po I wojnie światowej 15 marca 1919 roku do miasta wkroczyły wojska alianckie.
W 1923 stolicę Republiki Tureckiej przeniesiono do Ankary. 28 marca 1930 roku miasto zostało oficjalnie nazwane Stambułem dla potrzeb kontaktów międzynarodowych.
Merem Stambułu i równocześnie gubernatorem miasta i prowincji stambulskiej od 2019 jest Ekrem İmamoğlu. Podział administracyjny miasta został ustalony w 1930 roku i przetrwał do 2008. 974,97 km² miasta znajduje się po europejskiej stronie, a 855,95 km² po stronie azjatyckiej (anatolijskiej). W kwietniu 2008 roku do 32 istniejących dystryktów Stambułu zostało dodanych 7 nowych[15].
Metropolia jest podzielona na dystrykty i miasta. W samym Stambule znajduje się dwadzieścia siedem dystryktów, cztery niezależne dystrykty w metropolii (Büyükçekmece, Çatalca, Silivri i Şile) oraz jeden nienależący do metropolii (Sultanbeyli).
Dystrykty w Stambule:
Miasta wchodzące w skład metropolii:
Alemdağ, Arnavutköy, Bahçeköy, Bahçeköy z okolicami, Boğazköy, Bolluca, Çavuşbaşı, Çekmeköy, Göktürk, Haracci, Orhanlı, Ömerlı, Samandıra, Sarıgazi, Sultançiftliği, Taşoluk, Yenidoğan
Mer metropolii. Wybierany jest raz na pięć lat.
Rada Municypalna. Rada dyskutuje i zatwierdza decyzje dotyczące Metropolii i poszczególnych dystryktów. Składa się z 1/2 członków rad dystryktów i niższych organów wchodzących w skład Metropolii, oraz ich merów. Radzie przewodniczy mer metropolii. Kadencja trwa 5 lat.
Metropolitalny Komitet Wykonawczy jest organem ustawodawczym i wykonawczym oraz doradczym. Jedynym wybieralnym członkiem komitetu jest mer. Komitetowi przewodniczy mer lub osoba przez niego desygnowana. Obowiązki komitetu: kontrola budżetu stworzonego przez mera, sprawy finansowe miasta[17].
Każdy dystrykt ma własnego mera i radę której członkowie wybierani są raz na pięć lat. Jedna piąta
członków tych rad reprezentuje swój dystrykt w Radzie Municypalnej. Dystrykty mają własne budżety i dochody.
Ważnymi instytucjami w Metropolii Stambulskiej są ISKI oraz IETT. ISKI zarządza wodą pitną w mieście, IETT zarządza systemem komunikacyjnym.
W 2012 roku populacja Stambułu wynosiła 13 854 740[18]. Jest tym samym najludniejszym miastem w Turcji, jak i w Europie. Liczba mieszkańców nieustannie rośnie wskutek migracji ludności z obszarów wiejskich. Co roku przybywa około 500 000 mieszkańców, dobudowując nowe ulice – około 1000 rocznie[19].
Gęstość zaludnienia wynosi 2481 mieszkańców na km².
Licząca mniej niż 3 tysiące osób mniejszość grecka zamieszkuje głównie dzielnicę Fanar.
W Stambule znajdują się dwa międzynarodowe lotniska:
Stambuł posiada dwa główne dworce kolejowe: po stronie europejskiej jest to Dworzec Kolejowy Şirkeci (tu kończą bieg wszystkie pociągi przyjeżdżające z Europy), po azjatyckiej Dworzec Kolejowy Haydarpaşa (obsługujący pociągi odjeżdżające w kierunku wschodnim i południowym, w tym na Bliski Wschód).
Sieć kolejowa między europejską a anatolijską stroną miasta była połączona za pomocą promów kolejowych. Zostały one zastąpione przez tunel Marmaray pod Bosforem. Tunel Marmaray połączył również metro po obu stronach Bosforu. Tunel ukończono w 2013, natomiast pierwsi pasażerowie pojechali w roku 2015.
W Stambule od 2000 roku działa system metra uruchomiony po europejskiej stronie miasta, obecnie trwają zaawansowane prace nad rozbudową istniejącego odcinka. Istnieją trzy linie metra. Pierwsza biegnie od Kabataş do Eminönü i do Zeytinburnu przez Aksaray, druga z Aksaray do Portu lotniczego Atatürk przez Yenibosna. Trzecia łączy Taksim z 4. Levent poprzez Sisli.
Kolej łączy Kabataş z Taksim.
Tramwaje kursują po ulicy İstilkal pomiędzy Taksim a Tünel. Ulica ta jest poza tym całkowicie zamknięta dla ruchu kołowego. Kolejna linia tramwajowa biegnie od Kabataş do Bağcılar.
Kolej podmiejska, zwana Banliyö Treni kursuje od Sirkeci na europejskiej stronie oraz od Haydarpaşa po anatolijskiej stronie miasta.
W Stambule znajduje się największy port w Turcji. Stary port na Złotym Rogu służy obecnie prywatnym statkom, natomiast porty Karaköy i Eminönü są używane przez duże statki rejsowe kursujące do portów na Morzu Czarnym i Morzu Śródziemnym. Główny port przeładunkowy w Stambule znajduje się w dystrykcie Harem w okręgu Üsküdar po azjatyckiej stronie miasta.
Przez Bosfor kursują IDO (İstanbul Deniz Otobüsleri) – tradycyjne białe promy, oraz prywatne Tur Yol. Kursują regularnie, podróż jest o wiele szybsza i tańsza niż przedostanie się na drugą stronę Bosforu mostami, stąd są bardzo popularne wśród mieszkańców Stambułu. Podróżować można również katamaranem zwanym „morskim autobusem” (między portem promowymYenikapı po europejskiej stronie a portem promowym Pendik). Po Bosforze kursują również araba vapuru(między Sirkeci a Haremem) oraz motorówki (Kadıköy-Eminönü-Kadıköy, Beşiktaş-Üsküdar-Beşiktaş, Karaköy-Üsküdar-Karaköy).
Brzegi Bosforu łączą dwa mosty:
Za przejazd przez mosty z europejskiej strony na anatolijską pobierane są opłaty.
Złoty Róg spinają obecnie trzy mosty:
Imprezy i koncerty odbywają się w wielu miejscach w Stambule, między innymi w Hagia Eirene, Rumeli Hisarı, Yedikule, Pałacu Topkapı, Parku Gülhane, również w Centrum Kultury Atatürka (AKM) na placu Taksim czy w Sali Koncentrowej im. Cemala Reşita. W mieście znajduje się wiele nocnych klubów, restauracji i barów. Rejony w okolicy ulicy Istiklal oraz Nişantaşı są pełne kafejek, restauracji, pubów i klubów, a również galerii sztuki, teatrów i kin.
Międzynarodowy Festiwal Filmowy w Stambule jest jednym z najważniejszych festiwali w Turcji. Innym znanym jest Biennale w Stambule – impreza artystyczna odbywająca się od 1987, na której wystawiane są dzieła sztuki współczesnej. Muzeum Pera oraz Muzeum Sakıp Sabancı wystawiały dzieła takich artystów jak Picasso, Rodin czy Rembrandt. Muzeum Sztuki Nowoczesnej w Stambule nad Bosforem mieści kolekcję dzieł sztuki współczesnej, wystawiane są dzieła czołówki awangardowych twórców tureckich. Znajdują się tu kino i biblioteka. Muzeum Rahmi M. Koça nad Złotym Rogiem jest prywatnym muzeum poświęconym transportowi, przemysłowi i komunikacji. W 1855 w Stambule zmarł Adam Mickiewicz.
Pierwsza turecka gazeta została wydrukowana 1 sierpnia 1831. Większość gazet o zasięgu ogólnokrajowym ma swoją siedzibę w Stambule, z równoległymi wydaniami w Ankarze i Izmirze.
W Stambule znajduje się 2691 czynnych meczetów, 123 czynne kościoły, 26 czynnych synagog. 109 cmentarzy muzułmańskich oraz 57 niemuzułmańskich. Mniejszości religijne obejmują: wyznawców Kościoła Prawosławnego (Stambuł jest siedzibą prawosławnych patriarchów Konstantynopola; katedra patriarsza mieści się w dzielnicy Fener), Kościoła Ormiańskiego, Kościoła katolickiego oraz Żydów sefardyjskich.
Ormianie zamieszkują głównie Kumkapı w dystrykcie Eminönü. W dzielnicy Balat w dystrykcie Fatih mieszka wielu Żydów, w dzielnicy Fener Grecy, a w Nişantaşı i Beyoğlu lewantyńczycy.
Pierwszy meczet w Stambule został wybudowany w Kadıköy po azjatyckiej stronie miasta, która została zdobyta przez Państwo Osmańskie w 1353. Pierwszym meczetem po europejskiej stronie był meczet wybudowany w Rumeli Hisarı w 1452. Pierwszym dużym meczetem wybudowanym już za panowania Turków mieście był Meczet Sultan Eyüp wybudowany w 1458, natomiast pierwszym meczetem w Stambule był Meczet Fatih (1470), wybudowany na miejscu ważnego bizantyńskiego Kościoła Świętych Apostołów.
Stambuł był siedzibą Kalifatu w latach 1517–1924. W Pałacu Topkapı i Meczecie Eyüp Sułtan znajdują się przedmioty należące do Mahometa i pierwszych Kalifów. Sułtan dzierżył klucze do Mekki.
Siedzibą patriarchy Konstantynopola jest dystrykt Fener. W Stambule ma swoją siedzibę również Apostolski Kościół Ormiański. Po zdobyciu Konstantynopola sułtan Mehmed II ustanowił system Milletów, zgodnie z którymi ludność na terenie Konstantynopola i całego Imperium została podporządkowana do grup według przynależności religijnej, a nie etnicznej. I tak Mehmed II założył w 1461 nieistniejący wcześniej Ormiański Patriarchat Konstantynopola. W epoce Bizancjum, Kościół Ormiański był uważany za herezję i nie pozwalano Ormianom posiadać swojego kościoła wewnątrz murów miejskich. Kilku ormiańskich świętych, takich jak Święty Nerses, było wygnanych i uwięzionych na Adalar w pobliżu Konstantynopola na Morzu Marmara.
W momencie wprowadzenia systemu milletów wiele spraw wewnątrz społeczności przeszło pod zarząd zwierzchników religijnych danych społeczności: patriarchy Konstantynopola dla wyznawców prawosławia, ormiańskiego patriarchy Konstantynopola dla Ormian (oraz przejściowo dla chrześcijan syryjskich) oraz naczelnego rabina dla Żydów.
Od końca XIX w. liczebność mniejszości ormiańskiej i greckiej w Stambule zmniejszała się. W 1923 doszło do wymiany ludności między Grecją a Turcją. Jednakże Greckim mieszkańcom Stambułu oraz wysp Imbros i Tenedos pozwolono pozostać w swoich domach w zamian za co w greckiej Tracji Wschodniej mogła pozostać miejscowa społeczność turecka. Pomimo tego w latach 30. XX w. wprowadzono dyskryminujące ustawodawstwo, które zabraniało Grekom wykonywania około 30 zawodów od krawca czy cieśli po zawody medyczne i prawnicze. W 1942 wprowadzono dodatkowe podatki dochodowe, które miały służyć zebraniu funduszy na obronę kraju w razie ewentualnego przystąpienia Turcji do II wojny światowej. Szczególnie boleśnie dotknęły one ludność niemuzułmańską (chrześcijan, Żydów), wciąż kontrolującą znaczną część tureckiej gospodarki. Dodatkowo w 1955 Pogrom Stambulski spowodował dalszą, liczną emigrację Greków. W 1964 roku wszyscy Grecy nieposiadający tureckiego obywatelstwa (było to około 100 000) zostali deportowani.
Obecnie większość Greków i Ormian w Turcji oraz Lewantyńczyków zamieszkuje okolice Stambułu. Znajduje się tu również pewna liczba Niemców Bosforskich. W okolicach Stambułu znajdują się osiedla zamieszkane przez ludność obcego pochodzenia. Najbardziej znane to:Arnavutköy (wioska albańska), Polonezköy (polska wioska) oraz Yeni Bosna (Nowa Bośnia).
Na początku XX w. w Stambule znajdowało się ponad 40 000 katolików pochodzenia włoskiego (potomkowie kupców z Genui i Wenecji osiadłych tu w czasach Bizancjum i Imperium Osmańskiego oraz włoskich robotników przybyłych tu w XIX w.). W Stambule przebywał Giuseppe Garibaldi oraz Giuseppe Mazzini. Wpływy włoskie widać w Stambule w architekturze dzielnic Galata, Beyoğlu i Nişantaşı.
Żydzi sefardyjscy zamieszkiwali miasto od ponad 500 lat. Wraz z Arabami uciekli z Półwyspu Iberyjskiego w 1492 przed hiszpańską inkwizycją, która zmuszała ich do przejścia na chrześcijaństwo. Sułtan osmański Bajazyd II wysłał do Hiszpanii flotę pod dowództwem Kemala Reisa, by ocalić torturowanych i zabijanych Arabów i Żydów. Ponad 200 000 hiszpańskich Żydów uciekło do Tangeru, Algierii, Genui i Marsylii, a następnie do Salonik i Stambułu. Sułtan nadał obywatelstwo osmańskie ponad 93 000 sefardyjskim Żydom. Kolejna duża grupa Sefardyjczyków przybyła z południowej Italii, która była wówczas pod kontrolą Hiszpanii. Synagoga Włoska w Stambule w dzielnicy Galata została wybudowana właśnie przez potomków włoskich Żydów. Pierwsza maszyna drukarska w Stambule została utworzona przez sefardyjskiego Żyda w 1493. Żydowska rodzina Camondo była znaną rodziną bankierską. Znane ‘Schody Camondo’ na Bankalar Caddesi, to przykład secesji zostały wybudowane przez bankiera Salomona Camondo. Znajdują się one w Karaköy (Galata). Obecnie ponad 20 000 Żydów sefardyjskich mieszka w Stambule.
Mieszka tu od XIX w. również mniejsza liczebnie grupa Żydów aszkenazyjskich. Synagoga austriacka (Avusturya Sinagogu, Aşkenaz Sinagogu) jest jedna z najbardziej znanych synagog w mieście. W latach 30. i 40. XX w. do Stambułu napłynęła druga fala Aszkenazyjczyków z centralnej i wschodniej Europy.
Obecnie w Stambule znajduje się ponad 26 aktywnych synagog. Najważniejsze z nich to Synagoga Neve Shalom założona w 1951 roku, w dzielnicy Beyoğlu.
Naczelnym rabinem Stambułu jest obecnie Ishak Haleva.
Po ustanowieniu niepodległego Izraela w 1948 wielu Żydów wyjechało do Izraela, przyczyniając się jednocześnie do ustanowienia silnych relacji między oboma krajami. Dawid Ben Gurion, Icchak Ben-Zwi oraz Mosze Szarett studiowali w szkołach stambulskich takich jak liceum Galatasaray czy Uniwersytet Stambulski.
Pomimo zniszczeń miasto obfituje w wiele cennych zabytków z okresów bizantyńskiego i osmańskiego.
Miasta partnerskie Stambułu[27]:
Protokoły współpracy:
Porozumienie / Protokół ustaleń:
Mumbaj (nazwa główna), Bombaj (egzonim wariantowy)[3] (marathi मुंबई, trb.: Mumbaj; ang. Mumbai; do 1995 Bombay) – stolica indyjskiego stanu Maharasztra, położona na wyspie Salsette, na Morzu Arabskim. Wraz z miastami satelitarnymi tworzy najludniejszą po Delhi aglomerację liczącą 23 miliony mieszkańców[4]. Dzięki naturalnemu położeniu jest to największy port morski kraju[5]. Znajdują się tutaj także najsilniejsze giełdy Azji Południowej: National Stock Exchange of India i Bombay Stock Exchange[6]. Mumbaj jest ponadto uznawany za stolicę kinematografii indyjskiej.
Indyjska nazwa pochodzi od miejscowej hinduskiej bogini Mumba. W XVI wieku Portugalczycy nazwali to miasto Bom Bahia („Dobra Zatoka”). Brytyjczycy po przejęciu kolonii w 1661[7] zmienili nazwę miasta na Bombay. Oficjalna zmiana nazwy w językach hindi i angielskim nastąpiła w 1995. Komisja Standaryzacji Nazw Geograficznych jako polską nazwę aprobuje zarówno określenie „Mumbaj”, jak i „Bombaj”[a].
Obszar Bombaju był już prawdopodobnie zasiedlony w epoce kamienia. Od III w. p.n.e. należał on do rządzonego przez buddystów Imperium Maurjów. Od 1343 był w posiadaniu państwa Gujarat. Najstarsze budowle w mieście – słoniowe jaskinie, czy kompleks świątyń Walkeśwary, pochodzą właśnie z tamtego okresu.
W 1534 roku miasto zajęli Portugalczycy. W 1661 królowa Katarzyna Bragança przekazała miasto angielskiemu królowi Karolowi II. W 1668 zostało ono wydzierżawione Brytyjskiej Kompanii Wschodnioindyjskiej za cenę 10 funtów rocznie. Miasto zaczęło się szybko rozwijać jako port. Ludność wzrosła z 10 000 w 1661 do 60 000 w 1675. W 1687 siedziba Brytyjskiej Kompanii Wschodnioindyjskiej została przeniesiona z miasta Surat do Bombaju.
W 1817 powstał plan połączenia kilku wysp przybrzeżnych z lądem, dzięki czemu uzyskano by większą powierzchnię miasta. Projektem pokierował inżynier Hornby Vellard. Prace ukończono w 1845 i otrzymano powierzchnię 435 km². W 1853 Bombaj połączono linią kolejową z miastem Thana. W czasie wojny secesyjnej miasto stało się głównym centrum handlu bawełną na świecie. Przyczyniło się to do wielkiego boomu gospodarczego Bombaju. Kiedy ukończono Kanał Sueski (1869) miasto stało się największym portem na Morzu Arabskim.
Były to lata wielkiego rozwoju Bombaju. Już w 1906 w mieście mieszkało milion osób i było (po Kalkucie) drugim największym miastem Indii pod względem ludności. W późniejszym czasie stało się ono centrum indyjskiej walki o niepodległość. W 1942 Mahatma Gandhi wezwał do manifestacji antybrytyjskich. Rząd w tym samym roku wystąpił przeciwko Brytyjczykom z hasłem Opuśćcie Indie (Quit India). Od 1960 stał się stolicą stanu Maharasztra.
Od 1970 w Bombaju znów nastąpił boom ludnościowy – miasto przyciągało imigrantów z całych Indii. W 1986 Bombaj miał podobną ludność co Kalkuta. W 1992 miały miejsce zamieszki skierowane przeciwko ludności muzułmańskiej, które spowodowały liczne ofiary śmiertelne. Kilka miesięcy później, 12 marca, w licznych zamachach bombowych przeprowadzonych jednego dnia zginęło 300 osób (zamieszki w Bombaju).
W 2005 miasto zostało poważnie podtopione na skutek bardzo obfitych deszczów monsunowych.
11 lipca 2006 roku terroryści zaatakowali mumbajską kolej. W ciągu niespełna kwadransa w różnych pociągach na trasie zachodniej eksplodowało siedem bomb. Zginęło ponad 200 osób, a około 800 zostało rannych. 26 listopada 2008 w mieście miała miejsce seria zamachów terrorystycznych, w których zginęło co najmniej 195 osób, a 295 zostało rannych.
Miasto umieszczone jest na wyspie Salsette, przy ujściu rzeki Ulhas. Średnia wysokość miasta wynosi od 10 do 15 m n.p.m. Północna część metropolii jest natomiast pagórkowata, z najwyższym szczytem miasta sięgającym 450 m n.p.m. Całkowita powierzchnia Mumbaju wynosi 438 km².
Na terenie miasta znajdują się 3 jeziora: Tulsi, Vihar i Powai. Pierwsze dwa należą do Parku Narodowego Borivali i są głównymi źródłami wody słodkiej dla miasta. Linia brzegowa jest urozmaicona licznymi zatoczkami i lagunami. Wschodnią część wybrzeża morskiego zajmują zarośla mangrowe, bogate w faunę i florę.
W mieście ziemia składa się przeważnie z piasku (bliskość morza). Natomiast na przedmieściach ziemia jest żyzna (gleby aluwialne). Osady te pochodzą głównie z wyżyny Dekan. Mumbaj leży w strefie aktywnej sejsmicznie. Mogą się tu zdarzyć trzęsienia ziemi o sile do 6,5 stopnia w skali Richtera.
Mumbaj dzieli się na dwie główne dzielnice: miasto (City) i suburbia, te dzielą się na 6 stref i 24 okręgi[8].
Mumbaj leży w pasie klimatów tropikalnych, więc występują tu tylko 2 pory roku: sucha i wilgotna. Pora wilgotna trwa od marca do października. Charakteryzują ją duża wilgotność i temperatury przekraczające 30 °C. Pora monsunowa trwa od czerwca do września i średnie opady w tym czasie wynoszą 2200 mm. Rekord ilości opadów zanotowano w 1954 roku (3 451,6 mm).
Pora sucha trwa od listopada do lutego. Charakteryzuje się małą wilgotnością powietrza oraz niskimi temperaturami. Za największe chłody w styczniu i lutym odpowiada zimny wiatr z północy. Roczna temperatura w porze wilgotnej wynosi 38 °C, a w porze suchej spada do 11 °C. Najwyższe i najniższe temperatury średnie zanotowano w 1962 roku: 43 °C i 7,4 °C.
W Mumbaju mieszczą się siedziby wielu instytucji finansowych o znaczeniu krajowym, m.in. Bombay Stock Exchange, Reserve Bank of India czy National Stock Exchange. Ponadto znajdują się tu mennica i siedziby licznych firm (m.in. Tata Group, Godrej czy Reliance). Większość z nich mieści się w południowej części miasta, przy Dalal Street, którą nazywa się indyjską Wall Street.
Tradycyjnymi gałęziami przemysłu były tekstylny i spożywczy. Duże dochody przynosił także port morski. Obecnie rozwijają się te gałęzie, w których potrzeba coraz więcej wykwalifikowanych pracowników. Są to takie gałęzie jak techniczny, szlifierniczy i informatyczny. W Mumbaju jest wielu wykwalifikowanych specjalistów z różnych dziedzin. Przeważają jednak pracownicy niewykwalifikowani i półwykwalifikowani, którzy pracują jako domokrążcy, mechanicy itp. Wiele osób znajduje także zatrudnienie w obsłudze portu.
W Mumbaju dobrze rozwinięty jest przemysł rozrywkowy. Mieszczą się tu największe indyjskie stacje radiowe, telewizyjne oraz wydawnictwa. Mumbaj jest największym ośrodkiem kinematografii indyjskiej. Mieszczą się tu liczne studia filmowe i kina.
Większość mieszkańców Mumbaju korzysta z publicznego systemu transportu. W mieście jest niewiele przestrzeni parkingowych, a drogi są wąskie i o bardzo dużym natężeniu ruchu. Taka sytuacja nie sprzyja rozwojowi komunikacji prywatnej. Głównym systemem publicznego transportu są koleje miejskie. Zachodnie linie obejmują zachodnią część miasta, środkowe środkową i północno-wschodnią. Wschodnie linie obejmują port i New Mumbai. Rozwinięta jest komunikacja promowa (promy kursują głównie między licznymi zatoczkami w północnej części miasta).
Sieć publicznych autobusów łączy miasto Mumbaj z dystryktami przedmiejskimi (Thane, New Mumbai). Autobusy często są bardzo zatłoczone. Są one jednak tańsze niż pociągi.
Taksówki wyróżniają się spośród innych pojazdów charakterystycznym malowaniem na czarno i żółto. Nałożenie na taksówkarzy podatków, i ograniczenie maksymalnej liczby osób w samochodzie (4), przyczyniły się do wzrostu bezpieczeństwa na ulicach Mumbaju. Popularnym środkiem transportu są autoriksze. Kursują one jednak tylko w strefie przedmieść. Maksymalna liczba pasażerów zabieranych przez autoriksze to 3 osoby.
Mumbaj obsługiwany jest przez międzynarodowy port lotniczy Króla Śiwadźiego w Mumbaju (port lotniczy Chhatrapati Shivaji), którego terminale zlokalizowane są na przedmieściach Santacruz i Sahar. W pobliżu znajduje się również port lotniczy Juhu, który był pierwszym lotniskiem cywilnym w Indiach.
Mumbaj jest ważnym węzłem komunikacji kolejowej. Od miasta rozchodzą się linie kolejowe we wszystkie strony Indii. Znajduje się tu siedziba dyrekcji kolei indyjskich. Głównym dworcem kolejowym jest Dworzec Króla Śiwadźiego (Chhatrapati Shivaji Terminus, dawniej Dworzec Wiktorii).
Dzięki naturalnemu portowi Mumbaj jest jednym z najważniejszych portów handlowych Indii. Tu także znajduje się baza Indyjskiej Marynarki Wojennej.
Obszar metropolitalny Mumbaju zamieszkuje około 19,7 mln osób (2006). Taka liczba ludności na bardzo małej powierzchni (4 355 km²) daje około 4500 osób na km². Na 100 mężczyzn przypada zaledwie 81,1 kobiety. Wynika to z faktu, że wielu mężczyzn, emigrujących do miasta z regionów wiejskich, pozostawia żony i dzieci w rodzinnej wsi.
Analfabetyzm jest niższy niż średni w Indiach – 23%. Jest on jednak nierównomiernie rozłożony wobec płci: 18% dorosłych mężczyzn nie umie czytać i pisać, a ten sam wskaźnik u kobiet wynosi 28,4%. Wyznawcy hinduizmu stanowią 68% mieszkańców, muzułmanie 17%, chrześcijanie i buddyści po 4%. Ponadto żyją tu spore grupy parsów, dźinistów, sikhów i żydów. Duża grupa mieszkańców to ateiści[potrzebny przypis].
Przestępczość w Mumbaju ciągle spada. W 2001 odnotowano 30 991 przestępstw, a w 2004 27 577. Oznacza to spadek o 11%. Największym więzieniem w mieście jest Arthur Road Jail.
W mieście mieszają się imigranci z całego regionu Azji Południowej. Większość ludności posługuje się językiem hindi albo lokalnym dialektem bambaiya hindi. Ponadto popularne języki to marathi i Indian English (indyjska odmiana angielskiego). Marathi jest językiem urzędowym stanu Maharasztra. Natomiast językiem angielskim posługują się głównie biznesmeni i pracownicy biurowi. Innymi językami używanymi w Mumbaju są gudźarati, bengalski, tamilski, urdu, malajalam, telugu, pańdźabi (pendżabski), konkani, nepalski i kannada.
Mieszkańców Mumbaju nazywa się Mumbaikar lub Bombayite.
W mieście odbywają się liczne, hucznie obchodzone, festiwale, festyny i święta różnych religii.
Mumbaj ma 2 własne sieci barów szybkiej obsługi (tzw. fast-foodów): vada pavs i bhelpuri. Popularne są południowoindyjska i chińska kuchnia. W 2004 roku Mumbaj został wpisany na listę światowego dziedzictwa UNESCO.
Miasto jest miejscem narodzin hinduskiej kinematografii. Pierwszy film powstał tu już w 1896. W Mumbaju znajdują się też największa w Azji liczba kin, oraz największa kopuła Imax. Z tych powodów miasto zawdzięcza swój przydomek Bollywood. Mieszczą się tu także 2 galerie sztuki: Jehangir i Narodowa Galeria Sztuki Współczesnej (National Gallery of Modern Art) oraz liczne muzea (m.in. Muzeum Księcia Walii). Zbudowane w 1833 Asiatic Society of Bombay to najstarsza biblioteka publiczna w mieście.
Siedziba rzymskokatolickiej archidiecezji bombajskiej.
W Mumbaju znajduje się wiele zabytkowych budowli. Do szczególnie znanych należą:
W Mumbaju wydawane są liczne gazety, nadają stacje radiowe i telewizyjne. Do najbardziej znanych anglojęzycznych gazet należą:
Najpopularniejsze gazety wydawane w języku marathi to:
Ponadto gazety wydawane są w wielu innych językach, m.in. hindi, gudźarati, malajalam, po bengalsku, urdu, telugu i po tamilsku.
Doordashar to nadawca telewizji publicznej (w Mumbaju można oglądać dwa kanały). Ponadto w mieście nadaje około 100 innych stacji telewizyjnych, często w różnych językach.
W mieście nadaje 6 stacji radiowych (w FM):
Natomiast stacje radiowe nadające w systemie AM należą do nadawcy All India Radio. Nadają one na częstotliwościach:
W Mumbaju, oprócz szkół państwowych, istnieją także szkoły prywatne. Szkoły prywatne cechuje wyższy poziom nauczania niż publiczne. Po zdaniu egzaminu licealnego otrzymuje się Indyjski Certyfikat Szkoły Średniej. W publicznych szkołach uczą się najczęściej uczniowie z rodzin o najniższym statusie. Często brakuje tam podstawowych pomocy naukowych. Natomiast na naukę w prywatnych szkołach mogą sobie pozwolić dzieci z bogatych domów.
Najpopularniejszym sportem w mieście jest krykiet. Szczególnie popularne są niedzielne wypady na krykieta. W mieście znajdują się 2 międzynarodowe stadiony do gry w krykieta: Wankhede i Brabourne. Miejscowa drużyna jest jedną z najlepszych drużyn na świecie. Odbywają się tutaj zawody o prestiżowe trofeum Ranji.
Kolejnym popularnym sportem w Mumbaju jest piłka nożna. Szczególnie popularna jest gra w czasie pory monsunowej. Miasto posiada własne drużyny piłkarskie i stadiony. Piłkarskie Mistrzostwa Świata są jednymi z najczęściej oglądanych zawodów sportowych w telewizji. Hokej w ostatnim czasie stracił na popularności, głównie na rzecz krykieta. Mimo to wielu zawodników z Mumbaju gra w krajowej reprezentacji.
Innymi popularnymi sportami są: tenis, squash, bilard, badminton, tenis stołowy i golf. Mumbaj ma także klub rugby. Na torze wyścigowym Mahalaxmi odbywają się wyścigi koni. Piłka siatkowa i koszykówka są popularne podczas lekcji wychowania fizycznego w szkołach oraz wśród młodzieży.
Miasto jest tłem wielu filmów w języku hindi wyprodukowanych w jego wytwórniach popularnie zwanych Bollywood. Akcja rozgrywa się w Mumbaju między innymi w Shootout at Lokhandwala, Company, Bhoot, Zakaz palenia, Dil Hi Dil Mein, Podróż kobiety, Mann, Ghulam, Dushman, Chalte Chalte,Chaahat, Chameli, Życie w... metropolii, Saathiya, Being Cyrus, Mere Yaar Ki Shaadi Hai, Nigdy cię nie zapomnę, My Bollywood Bride czy Taxi Number 9211, Bombay, Black Friday, Parinda, Nayak: The Real Hero, Chandni Bar, Aamir, Niesie nas wiatr (Hava Aney Dey), Slumdog. Milioner z ulicy i in.
 Tokio i (jap. 東京都 Tōkyō-to) – stolica i największe miasto Japonii, położone na południowo-wschodnim wybrzeżu Honsiu i zarazem największy obszar metropolitalny na świecie na poziomie 38 305 000 mieszkańców (stan na kwiecień 2018)[3]. Nazwa Tōkyō (jap. 東京) oznacza „Wschodnią Stolicę”[4]. Do 1868 roku miasto nazywało się Edo[4].
Tokio formalnie nie jest miastem, ale prefekturą metropolitalną od 1 lipca 1943. Zostało zniszczone przez trzęsienie ziemi w 1923, a także przez amerykańskie bombardowania w maju 1945 (podczas II wojny światowej). Było również organizatorem letnich igrzysk olimpijskich w 1964 i ponownie w 2021.
Znajdują się tutaj: cesarski kompleks pałacowy (Pałac Cesarski), gmach parlamentu, Tokyo Tower, Tokyo Skytree, zabytkowe świątynie buddyjskie, chramy shintō, muzea, obiekty kultury, parki. Tokio jest siedzibą jednej z głównych giełd świata i rekordowej liczby dużych przedsiębiorstw[5].
W 2018 Tokio odwiedziło 12,12 mln turystów z całego świata – było dziewiątym najczęściej odwiedzanym miastem na świecie[6].
Tōkyō-to jest odrębną jednostką w podziale administracyjnym państwa, co jest zaznaczone znakiem 都 (wymawianym to) oznaczającym „stolicę” lub „metropolię”[7] i w języku angielskim tłumaczone jako Tokyo Metropolis lub Tokyo Metropolitan Prefecture, czyli Tokijska Prefektura Metropolitalna. Władze miasta wyróżniają w niej trzy obszary: 23 dzielnice, region Tama oraz wyspy[8].
Obszar wielkiej aglomeracji (niebędącej formalną jednostką administracyjną), zwany Wielkim Tokio, obejmuje części sąsiednich prefektur i miast, skupiając w 2017 r. 37,9 mln mieszkańców[9]. Wielkie Tokio, czyli Greater Tokyo Area zwane jest także Tokyo Megalopolis Region. W takim ujęciu obejmuje Tōkyō-to oraz trzy sąsiednie prefektury: Saitama, Chiba, Kanagawa. Istnieje także jeszcze szersze pojęcie, również nieformalne, National Capital Region, czyli Tōkyō-to i siedem prefektur: Saitama, Chiba, Kanagawa, Ibaraki, Tochigi, Gunma, Yamanashi[10].
Metropolia stołeczna Tokio leży w regionie Kantō, na największej japońskiej równinie, na wyspie Honsiu (Honshū), u ujścia rzeki Sumida do Zatoki Tokijskiej. Współrzędne geograficzne centralnej części miasta to: 35°41′ szerokości geograficznej północnej i 139°46′ długości geograficznej wschodniej.
Obszar prefektury metropolitalnej Tokio pocięty jest rzekami. Sumida jest historycznym korytem rzeki Ara, która od II ćw. XX w. ma sztucznie wykonane, alternatywne koryto do Zatoki Tokijskiej, omijające obszar centralny miasta. Na wschodzie płyną rzeki Naka i Edo. Ta ostatnia jest wschodnią granicą miasta (na odcinku południowym, gdzie przekopano alternatywne koryto do Zatoki, stary ciek nosi nazwę Kyū-Edo). Głównym ciekiem zachodniej części obszaru jest rzeka Tama, będąca na znacznej długości południową granicą miasta.
Wschodnia część miasta leży na płaskim, a na wschód od Sumidy wręcz podmokłym terenie, podczas gdy zachodnia – na terenie lekko pofalowanym, z widocznymi dolinami i płytkimi wąwozami dawnych rzek. Jest to wschodni skraj wyżyny Musashino, ciągnącej się na zachód, aż po góry oddzielające ją od kotliny dawnej prowincji Kai. Góry zajmują duży obszar prefektury. Ich zbocza są strome i zalesione (lasem pokryte jest 36% obszaru Tōkyō-to). Najwyższym szczytem jest Kumotori o wysokości 2017 m n.p.m. Na terenie stołecznej prefektury znajdują się także inne szczyty, m.in.: Takanosu (1737 m n.p.m.), Ōdake (1267 m n.p.m.), czy otoczone szczególnym kultem Mitake (929 m n.p.m.) i Takao (599 m n.p.m.). Wysoko w górach na rzece Tamie znajduje się największe jezioro na terenie prefektury – sztuczny zbiornik Okutama-ko.
Wielki zespół miejski południowego Kantō, którego stolica Japonii stanowi centrum, tworzy obszar zurbanizowany, zajmujący większą część Tōkyō-to oraz prefektury Kanagawa (gdzie leży Jokohama, drugie co do wielkości miasto Japonii), jak również znaczną część prefektur Chiba i Saitama. W strefie bezpośredniego oddziaływania miasta pozostają również prefektury: Gunma, Ibaraki, Tochigi, Yamanashi i Shizuoka.
Stolica jest centrum politycznym, handlowym, finansowym, edukacyjnym i medialnym kraju. Nie ma ona konkurencji w Japonii pod względem liczby biur, ministerstw, korporacji, uczelni, muzeów, teatrów i parków rozrywki. Aż 47,3% korporacji japońskich o kapitale powyżej miliarda jenów znajduje się w Tokio. 9,8% wszystkich zatrudnionych w kraju pracuje na terenie prefektury Tokio i wytwarza 16,8% dochodu narodowego. Metropolia ma wysoko rozwinięty system komunikacji miejskiej, m.in. najdłuższą na świecie sieć szybkich kolei miejskich oraz jeden z największych systemów miejskich dróg ekspresowych.
Tokio leży w strefie dużej aktywności sejsmicznej, co stwarza specyficzne wymagania dla zabudowy. Istnieje realne zagrożenie kolejnym wielkim trzęsieniem ziemi, które mogłoby ponownie zniszczyć miasto[11]. Mimo to miasto wciąż przyciąga ludzi z całego kraju, a nawet świata. Od dłuższego czasu imigracja lokuje się głównie nie w samym mieście, ale na obszarze szeroko pojętej aglomeracji.
Szacuje się, że już w I połowie XVIII wieku liczba ludności Tokio (ówczesnego Edo) przekroczyła 1 mln, czyniąc to miasto największym na świecie. Przez kolejne 100 lat populacja miasta pozostawała na podobnym poziomie, choć bywały okresy znacznych ubytków wynikających z katastrof naturalnych (pożarów, trzęsień ziemi, tsunami, tajfunów, powodzi), rzadziej w wyniku powstań i epidemii.
Restauracja Meiji w 1868, jak również wielki pożar w 1871, wpłynęły na znaczne zmniejszenie populacji stolicy. Koniec XIX i początek XX wieku charakteryzowały się ponownym przyrostem ludności wynikającym z industrializacji kraju, dalszej urbanizacji i modernizacji w stylu zachodnim.
W 1923 miasto i jego okolice nawiedziło potężne trzęsienie ziemi, co istotnie wpłynęło na populację (większość terenów miasta została doszczętnie zniszczona nie tylko w wyniku ruchów skorupy ziemskiej, ale przede wszystkim następujących po nich pożarów).
W kolejnych latach (zwłaszcza w latach 30. XX wieku) miasto doświadczyło nieznanego w historii wzrostu liczby ludności (głównie powrotu rdzennych tokijczyków oraz migracji nowej ludności z terenów wiejskich) wynikającego z odbudowy zniszczonych dzielnic, przebudowy pozostałych terenów (unowocześnienia infrastruktury, założenia nowych zakładów przemysłowych, powstania nowoczesnych osiedli mieszkaniowych).
Pod koniec II wojny światowej miasto doświadczyło kolejnego znacznego spadku liczby ludności w wyniku amerykańskich nalotów bombowych, które zniszczyły około 70% powierzchni miasta.
Od 1946 następowały odbudowa i ponowny powrót tokijczyków do stolicy oraz napływ nowej ludności z okolicznych regionów. Od 1967 migracje do/z Tokio charakteryzowały się saldem ujemnym, z wyjątkiem 1985. W 2004 zanotowano saldo dodatnie 72 tys. osób (na 795 tys. osób migrujących). Jednakże nie oznacza to, że wzrost miasta został zahamowany, ponieważ urbanizacji uległy tereny położone poza 23 okręgami specjalnymi i prefekturą Tokio. Na przykład sieć transportowa zachodnich dzielnic Kawasaki i Jokohamy, nakierowana raczej na Tokio, niż na centra obu miast, świadczy o tym, że wzrost ich populacji był w gruncie rzeczy wzrostem bezpośrednio związanym z Tokio. 
Według stanu na 1 października 2015 r. populacja Tokio szacowana jest na 13,5 mln, co stanowi około 11% całkowitej liczby ludności Japonii. Ma zatem największą populację spośród wszystkich 47 prefektur, a zajmując powierzchnię 2191 km², co stanowi 0,6% całkowitej powierzchni kraju, ma gęstość zaludnienia 6158 osób/km², a więc jest prefekturą najgęściej zaludnioną. W podziale na trzy wyróżniane regiony liczba ludności wynosi: (1) obszar okręgów specjalnych (23-ku) zamieszkuje 9,2 mln osób, (2) obszar Tama – 4,2 mln, (3) Wysp Tokio – 26 tysięcy. Prefektura metropolitalna Tokio ma 6,9 mln gospodarstw domowych, ze średnią 1,94 osoby na gospodarstwo domowe. Liczba rezydentów zagranicznych według podstawowego rejestru wynosi 440 tys. na dzień 1 października 2015 roku[13].
Proces starzenia się populacji jest jednym z większych problemów demograficznych Japonii. W Tokio w 1985 roku 8,9% ludności znajdowało się w grupie powyżej 65 lat, a w 2000 było to 17,3%. Odsetek osób starszych przekroczył standard ONZ 14% dla „społeczeństwa w podeszłym wieku” w 1998 roku, a obecnie zbliża się do poziomu 21%, co oznacza „społeczeństwo w podeszłym wieku”[13]. 
W 2000 roku liczba osób w wieku produkcyjnym wynosiła 6,47 mln, z czego 312 tys. pozostawało bez zatrudnienia. Spośród zatrudnionych mieszkańców Tokio 46,3% pracowało na stanowiskach urzędniczych, 29,9% – w usługach, 24% – w przemyśle i transporcie, 0,5% – w rolnictwie i rybołówstwie[14].
Pierwsza wzmianka o osadzie rybackiej na przedpolu późniejszego zamku Edo pojawiła się w XII wieku. Ieyasu Tokugawa – założyciel dynastii siogunów, panującej od XVII do XIX wieku, uzyskawszy od Hideyoshi Toyotomi ziemie Kantō jako dzierżawę lenną (lata 90. XVI w.), uczynił z Edo centrum swoich dóbr, a potem faktyczną stolicę kraju. Edo było siedzibą dynastii Tokugawa do restauracji Meiji w XIX wieku.
Na początku okresu Meiji (1868–1912) miasto otrzymało nową nazwę Tōkyō, co znaczy „wschodnia stolica”[15], i oficjalnie stało się siedzibą cesarza i władz kraju. W 1871 utworzono prefekturę Tokio (jap. 東京府 Tōkyō-fu), która rok później uzyskała granice zbliżone do obecnych; obejmowała ona 15 dzielnic (jap. 区 ku) miasta i kilka powiatów. W 1889 dzielnice te weszły w skład nowo utworzonej jednostki – miasta Tokio (jap. 東京市 Tōkyō-shi). Jako jednostka administracji terytorialnej istniała do 1943, w granicach powiększonych w 1932 i składała się z 35 dzielnic.
Ewolucja miasta od stolicy siogunów feudalnego państwa do stolicy rozwijającego się, nowoczesnego kraju przebiegała stopniowo. Dzielnice centrum miasta zajęte przez mieszczan ewoluowały w kierunku centrum handlowego, dzielnice daimyō zajęte zostały przez rezydencje arystokracji, rodzącej się burżuazji, wysokich urzędników państwowych, dzielnice samurajów – przez przedstawicieli klasy średniej. Szczególnie duże przemiany miały miejsce wkrótce na terenach dawnych rezydencji daimyō.
W 1872 pożar strawił część śródmieścia. Odbudowa była pierwszą próbą wprowadzenia architektury zachodniej: powstała wówczas „georgiańska” obudowa głównej ulicy Ginzy – Chūō-dōri. W 1876 otwarto park Ueno, pierwszy wielki kompleks zieleni publicznej, założony obok XVII-wiecznego chramu Ueno Tōshō-gū. W 1882 znalazło w nim lokalizację tokijskie Muzeum Narodowe (założone w 1872 jako pierwsze muzeum w Japonii). W tym samym roku założono ogród zoologiczny. W parku organizowano wielkie wystawy, m.in. w 1877 (I Krajowa Wystawa Przemysłowa), 1890 (III Wystawa), 1907 (Wystawa Przemysłowa Meiji), 1922 (Wystawa Pokoju). Powstawały dla nich okazałe pawilony, będące nieraz manifestacją najmodniejszych form architektury Zachodu. W 1877 otwarto Uniwersytet Tokijski. Jego główny kampus powstał na terenie dawnego majątku rodziny Maeda, na zachód od parku Ueno.
W 1872 otwarto pierwszą w kraju linię kolejową, łączącą tokijską dzielnicę Shimbashi z Jokohamą. Drugą – doprowadzono w 1883 od północy do dworca Ueno. Obie linie połączyła w 1885 obwodnica kolejowa. Linia ta, zwana linią Yamanote (lub Yamate), choć budowana jeszcze w znacznym oddaleniu od zabudowy, wyznaczyła przyszły obszar dzielnic centralnych i miała odegrać wielką rolę w ukształtowaniu współczesnego miasta. W 1882 w mieście uruchomiono pierwszą linię tramwaju konnego. Elektryfikacja sieci następowała od 1903. Mniej więcej w pierwszej dekadzie XX w. zabudowa miasta osiągnęła linię obwodnicy Yamanote, a wraz z nią rozbudowywano linie tramwajowe. Sieć ta znalazła się pod jednym zarządem, a w 1911 została wykupiona przez miasto.
Charakterystyczne drewniane mosty z okresu Edo przetrwały najdłużej na węższych rzekach i kanałach. Na Sumidzie proces zastępowania ich konstrukcjami żelaznymi zaczął się w 1874 (Umaya-bashi), a skończył w 1911 (Shin-ōhashi). Most Nihonbashi, zastąpiony konstrukcją kamienną w 1911, uzyskał atrakcyjną oprawę architektoniczną[16].
Modernizacja miasta na większą skalę nastąpiła poprzez budowę dzielnicy rządowej. Ulokowano ją wokół cesarskiego kompleksu pałacowego, na terenie uzyskanym po zlikwidowanych rezydencjach daimyō. Powstały wówczas liczne budynki urzędów, agencji rządowych i ministerstw w Kasumigaseki oraz siedzib banków i instytucji finansowych w Marunouchi, bezpośrednio na zachód od wzniesionego w 1914 dworca Tōkyō. Kanał, stanowiący południową fosę dawnego pierścienia domów samurajskich, został zasypany, aby umożliwić budowę szerokiej ulicy obwodowej. Pomiędzy Marunouchi i Kasumigaseki urządzono publiczny park Hibiya, otwarty w 1903 roku. Japońską wersję Wersalu zrealizowano w postaci pałacu Akasaka Geihinkan (arch. Tōkuma Katayama, 1909) przeznaczonego pierwotnie na siedzibę następcy tronu, a obecnie pełniącego rolę rezydencji dla oficjalnych gości państwa. Jednocześnie, w 1908, pojawiła się pierwsza w mieście (i kraju) kamienica czynszowa. Zbudowano ją koło parku Ueno i nazwano „Ueno Club”[17].
Wskutek braku dostępnych terenów w śródmieściu, duże kompleksy zielone zakładano na obrzeżach miasta. W 1906 otwarto park Shinjuku (Shinjuku Gyoen), a w latach 20. urządzono kompleks parkowo-architektoniczny chramu Meiji, poświęcony cesarzowi i cesarzowej Meiji (zm. 1912 i 1914).
W drugiej połowie okresu Meiji wzrost liczby ludności Tokio stymulowany był rozwojem przemysłu. W 1897 były 333 fabryki zatrudniające 30 tys. osób, zaś w 1919 roku 7233 fabryki zatrudniały niemal 190 tys. osób. Głównym miejscem, gdzie lokował się przemysł, początkowo głównie w postaci małych i średnich przedsiębiorstw, były wschodnie brzegi Sumidy, gdzie grunty były podmokłe i przez to tanie[18]. Później popularnymi lokalizacjami dla fabryk stały się północne obszary Senju (Adachi) i Ōji (Kita).
W dniu 1 września 1923 region Kantō nawiedziło potężne trzęsienie ziemi o sile 8,3 w skali Richtera. Ogromny pożar strawił dwie trzecie obszaru Tokio: całą zabudowę po wschodniej stronie rzeki Sumida, wszystkie dzielnice na jej zachodnim brzegu od Asakusa na północy, przez śródmieście po Shimbashi oraz bezpośrednie otoczenie kompleksu Pałacu Cesarskiego. Liczbę ofiar obliczano na 142 807 osób[19]. Była to jedna z największych katastrof w historii ludzkości.
Po trzęsieniu ziemi rozpoczęto intensywną odbudowę, której kierunki wyznaczyła Agencja Odbudowy Stolicy Cesarskiej pod kierownictwem wybitnego polityka i „modernizatora” Japonii, Shinpei Gotō. Plan zakładał poważne zmiany ukształtowania najbardziej zniszczonych dzielnic, budowę szerokich arterii wyposażonych w odpowiednią infrastrukturę, m.in. Yasukuni-dōri i Shōwa-dōri, zespołu dużych mostów na Sumidzie, urządzenie wielkich parków (w tym na nabrzeżach rzeki), urządzenie hal targowych (m.in. targu w Tsukiji), budowę wielu szkół elementarnych i towarzyszących im terenów zielonych. Chociaż części tych zamierzeń nigdy nie zrealizowano[20], zakres dokonanych restrukturyzacji układu działek i kwartałów śródmieścia oraz liczba przebić i poszerzeń ulic są imponujące. Już wówczas zasypano wiele kanałów śródmiejskich, traktowanych jak anachronizm, i przeznaczono je pod ulice lub zabudowę. Zupełne zniszczenie miasta ułatwiło też Kolejom Rządowym przeprowadzenie dwóch nadziemnych linii średnicowych łączących dworzec Tōkyō z dworcem Ueno oraz dworzec Ryōgoku z powstałą wcześniej linią Chūō (1932), krzyżujących się w Akihabara. Natomiast prywatne konsorcja budowały kolejne odcinki podziemnej linii metra – pierwszy otwarto między Asakusa i Ueno w 1929, a całą trasę do Shibuya otwarto w 1939. Jednocześnie urządzano planowane wcześniej nowe koryto rzeki Ara, które przebiega poza wschodnimi rubieżami ówczesnego miasta (ukończone przed 1932).
W okresie międzywojennym przemiana obrazu architektonicznego historycznego śródmieścia w kierunku wzorca metropolii Zachodu była już bardzo zaawansowana. Szczególnie wiele okazałych gmachów istniało już w Nihonbashi, Kyōbashi i Ginzy. W Nagata-chō ukończono (1925) monumentalny gmach parlamentu z osiowym przedpolem (arch. F. Watanabe, Sh. Takeuchi), w stylu klasycyzującego modernizmu. Rok później ukończono podobny w formie gmach Galerii Obrazów Meiji (arch. Masatsugu Kobayashi). Stopniowo próbowano też stworzyć budowle bardziej związane z tradycją narodową. Przykładem jest gmach główny Muzeum Narodowego wzniesiony w 1932 w parku Ueno. Jego styl określano jako „koronno-imperialny” (arch. Jin Watanabe). Stosowano też formy bardziej surowe, modernistyczne – np. budynek dworca Ueno (1932).
Także w okresie międzywojennym intensywnej urbanizacji ulegały obszary położone coraz dalej na zewnątrz od kolei obwodowej Yamanote. Spowodowała ona budowę licznych linii elektrycznych kolejek dojazdowych, które łączyły się z linią Yamanote, nie mając prawa penetrować samego miasta (gdzie był monopol tramwajów toden). Charakterystyczna dla Japonii rozbudowa ważniejszych stacji kolejowych do skali wielkich zespołów handlowo-usługowych, została podjęta w Tokio już w okresie międzywojennym, najpierw przez zarządy dentetsu. To właśnie te stacje, z towarzyszącymi im własnymi domami towarowymi, stały się ogniskami, wokół których ukształtowały się subcentra tokijskie położone na linii Yamanote. Koleje dojazdowe zaangażowały się też w rozwój przedmieść mieszkaniowych położonych na trasach swoich linii. Najbardziej znane z nich to przedmieście Den'enchōfu kolei elektrycznej Tōkyū. Planowemu rozwojowi towarzyszyła jednak spontaniczna urbanizacja terenów leżących poza koleją obwodową – ona głównie nadała chaotyczne oblicze zachodnim i południowym dzielnicom zewnętrznym.
W 1943 wprowadzono wspólną administrację miasta i prefektury, tworząc specjalną prefekturę stołeczną Tōkyō-to. Dzielnice tworzące dawne miasto Tokio uzyskały status podobny do innych gmin (miast) istniejących wcześniej na terenie prefektury.
U schyłku II wojny światowej miasto zostało ponownie zniszczone przez bombardowania, zwłaszcza przez wielki pożar wywołany amerykańskim nalotem dywanowym 9 marca 1945. Zginęło około 200 tys. osób, więcej niż w wybuchach bomb atomowych w Hiroszimie i Nagasaki. Ludność prefektury, licząca w 1940 roku 7,35 mln zmalała w 1945 do 3,5 mln. Powojenny wzrost ludności, potęgowany imigracją, postępował szybko: w 1962 prefektura liczyła już ponad 10 mln mieszkańców. Oba kataklizmy i kolejne odbudowy przyczyniły się do tego, że wschodnie i centralne obszary śródmieścia mają bardziej regularny układ kwartałów i więcej szerokich ulic, niż obszary zachodnie. Tokio jest zasadniczo miastem o nowej zabudowie, nawet większość zabytków musiała być odbudowana – zwykle w starych kształtach, ale w nowej konstrukcji (np. Sensō-ji).
Do lat 50. XX wieku Tokio było siedzibą amerykańskich wojsk okupacyjnych. Rezydowały one m.in. na terenie dawnego placu ćwiczeń wojskowych w Yoyogi, przylegającego do świątyni Meiji. W 1964 obszar ten stał się głównym terenem olimpijskim, a w 1967 otwarto tam park Yoyogi.
Widocznym wyrazem modernizacji miasta po wojennych zniszczeniach była zabudowa z niepalnych materiałów, zdecydowanie większej skali, w formach modernistycznego „stylu międzynarodowego”. Organizacja Olimpiady letniej w 1964 spowodowała przyśpieszenie inwestycji, przede wszystkim w urządzenia sportowe – powstał wówczas m.in. kompleks stadionów w Yoyogi (arch. Kenzō Tange) oraz ważne inwestycje komunikacyjne. Symbolem tych ostatnich była sieć miejskich dróg ekspresowych, prowadzonych na estakadach (wyjątkowo w tunelach). Trasy, budowane w korytarzach kanałów i rzek tokijskich, właściwie usunęły ze śródmieścia ten tradycyjny element jego pejzażu. Innym symbolem była kolej jednoszynowa Tokyo Monorail łącząca centrum z lotniskiem Haneda. Jednocześnie znacznie przyśpieszono budowę metra, którym, wraz z rozwijaną siecią autobusową, do 1972 zastąpiono tramwaje.
W okresie boomu gospodarczego trwała urbanizacja terenów zewnętrznych. Na południowym zachodzie od Tokio, teoretycznie na obszarach miast Kawasaki i Jokohama, kolej prywatna Tōkyū rozpoczęła po 1953 zabudowę pasa zurbanizowanego wzdłuż specjalnie zbudowanej linii – tzw. Tama Den’entoshi (Miasto-Ogród Tama), obecnie pas ten wybiega na ok. 20 km od granic Tokio. Również inne koleje prywatne zaangażowały się w działalność deweloperską. Prefektura Tokio podjęła w 1965 wielki projekt budowy miasta satelitarnego Tama New Town.
Proces urbanizacji strefy podmiejskiej Tokio odnosi się obecnie do obszarów położonych poza Tōkyō-to, szczególnie na terenach prefektur: Kanagawa, Saitama i Chiba. Nierównomiernemu rozwojowi tej strefy, szczególnie silnemu na zachodzie i południu, usiłuje się przeciwdziałać przez tworzenie infrastruktury na północy, a ostatnio również na wschodzie. Celowi temu miały służyć „koleje trzeciego sektora”, takie jak Hokuso-Kōdan, Tōyō-Kōsoku’ oraz Tsukuba Express. Wschodnim odpowiednikiem „Tama New Town” ma się stać „Chiba New Town”, zaś rozproszone urbanizacje północy próbuje się konsolidować przez administracyjne utworzenie miasta Saitama i budowę jego centrum.
Osuszania tych terenów dokonuje się od początku okresu Edo i jest ono jednym z głównych kierunków rozwoju przestrzennego miasta. Już przed pierwszą wojną światową zabudowywano zespół sztucznych wysp Tsukishima i Kachidoki u wylotu Sumidy, a rejon Shibaury znajdował się w trakcie osuszania. Na mocy planu odbudowy z 1923 zabudowano rejon Minato-Minami, na wschód od dworca Shinagawa, leżącego pierwotnie na nabrzeżu (prace na przełomie lat 20. i 30. XX wieku) oraz kompleksy wysp: Harumi, Toyosu, Shiomi – wszystkie na nabrzeżu wschodniej części miasta. Po wojnie kontynuowano prace. Powstały wówczas m.in. wyspy: Shin-Kiba, Tatsumi, Heiwajima, Katsushima, Yashio. Na nowych terenach mieszczą się obiekty o różnych funkcjach: mieszkaniowych, biurowych, sportowo-rekreacyjnych (np. tor wyścigów konnych Ōi, Park Kasai Rinkai, tokijski Disneyland[21]). Są tam fabryki, składy i urządzenia portowe (w tym np. lotnisko Haneda, kolejowy dworzec kontenerowy i wagonownia pociągów Shinkansen). Sporządzona w 1960 przez zespół Kenzō Tange wizja rozwoju miasta na wielkich osiach w poprzek Zatoki mogła wydawać się „wielką utopią”; jednak obecnie pomysł ten wydaje się bliski realizacji, chociaż w odmiennej formie. Cezurą w procesie zagospodarowywania terenu zatoki była budowa Tōkyō Rinkai („Tokyo Waterfront”) – kompleksu mieszkalno-usługowego Odaiba-Ariake, z pewną przesadą reklamowanego jako „nowe centrum Tokio”. Poszczególne wyspy, usypane za linią dawnego wału odaiba, połączone są siecią tuneli i mostów drogowych i kolejowych, w tym autostradą. Ekspansja miasta na teren Zatoki jest prowadzona nadal w postaci zagospodarowywania wielu wolnych terenów na już powstałych wyspach oraz tworzenia nowych.
Innym kierunkiem ewolucji miasta jest ciągła intensyfikacja wykorzystania terenu. Tradycyjny model „nisko-gęsto” zmienił się w wielu miejscach w gęstą zabudowę średnio-wysoką (do 10 pięter). Dominującym czynnikiem decydującym o intensywności jest rachunek ekonomiczny, a nie wartość zabytkowa czy architektoniczna (symptomatyczna była rozbiórka w 1968 hotelu Imperial F. L. Wrighta).
Od lat 70. XX wieku buduje się wieżowce. Pierwszą dzielnicą wysokich biurowców stało się zachodnie Shinjuku (Nishi-Shinjuku). W 1991 powstała tam dwuwieżowa siedziba władz miasta (proj. Kenzō Tange) o wysokości 243 m. W śródmieściu, na terenach dawnego dworca towarowego JR Shimbashi-Shiodome, dzielnica wieżowców powstała na przełomie wieków.
Roppongi stało się miejscem lokalizacji dwóch gigantów: Roppongi Hills Mori Tower (2003, 238 m) i Tokyo Midtown (2007, 248 m). Stanowią one kompleksy wielofunkcyjne, godząc biura z kulturą i rozrywką. Dzielnicą coraz wyższej zabudowy staje się też śródmiejskie Marunouchi. W trosce o spójność między wysoką dostępnością a intensywnością wykorzystania terenu, skupiska wieżowców lokalizowane są głównie w sąsiedztwie ważniejszych dworców kolei JR (np. Shinjuku, Ikebukuro, Shiodome, Shinagawa, Akihabara). Mimo to Tokio pozostaje metropolią o stosunkowo niskiej gęstości zaludnienia – także w obszarze centralnym, co jest nie tylko efektem exodusu ludności wypieranej przez funkcje komercyjne, ale również wynika z zachowania się na wielu obszarach tradycyjnej jednorodzinnej zabudowy. Poza obszarem zniszczonym podczas wojny, średnio-wysoka zabudowa powstała często tylko wzdłuż głównych ulic i skrywa dzielnice tradycyjne na tyłach.
Proces różnicowania się i specjalizacji dzielnic doprowadził do powstania układu policentrycznego, w którym historyczne śródmieście (Ginza, Kyōbashi, Nihonbashi) stało się dzielnicą przede wszystkim biurowo-handlową, sąsiednie Marunouchi – wyłącznie biurową. Wydaje się ono przegrywać w konkurencji z bardzo ożywionymi dzielnicami i „podcentrami” położonymi na linii Yamanote, przede wszystkim Shinjuku (subcentrum biznesu, handlu i rozrywki), Shibuya (centrum handlu, mody i młodzieżowej rozrywki), czy także z Ikebukuro (subcentrum wielofunkcyjne). Żadna z wielkich sal koncertowych nie została zbudowana w starym śródmieściu. Nawet w kategorii eleganckiego handlu Ginza ma rosnącą konkurencję w okolicach Shibuya, Harajuku i bulwaru Omotesandō. Specyfikę śródmieścia ilustrują dane demograficzne. Trzy dzielnice centralne: Chiyoda, Chūō i Minato, mieszczą w nocy 268 tys. osób, podczas gdy za dnia przebywa w nich 2341 tys. osób[22]. Tak wielka różnica między dzienną i nocną populacją centrum nie ma odpowiedników w innych metropoliach i przyczynia się do przeciążenia układu transportowego miasta. Miasto popiera więc lokalizację w śródmieściu wielkich kompleksów mieszkaniowych, które, ze względu na szczupłość dostępnego miejsca, kształtowane są jako zabudowa wysoka.
Stołeczna prefektura (Tōkyō-to) graniczy z prefekturami: Chiba od wschodu, Yamanashi od zachodu, Kanagawa od południa, Saitama od północy. Na czele władz stoi gubernator, wybierany w wyborach powszechnych.
Prefekturę stołeczną Tōkyō-to dzieli się nieformalnie na trzy części: 23 okręgi specjalne (dzielnice), obszar Tama i obszar wysp[23]. W nomenklaturze japońskiej jednostki administracyjne na tych trzech obszarach, to: „ku”, „shi”, „machi”, „mura”.
Każde miasto czy miasteczko ma swojego własnego burmistrza i radę miejską.
W rejestracji samochodowej nie ma jednolitego oznaczenia dla Tokio. Używane są nazwy jednostek, w których znajdują się urzędy rejestracji: Adachi, Nerima, Shinagawa, Hachiōji i Tama.
Większość współczesnych dzielnic (-ku, ang.: ward) powstała w 1947 z połączenia mniejszych dzielnic miasta Tokio. Poniżej podano także dawne nazwy ze względu na ich znaczenie historyczne.
Dzielnice obecne, położone głównie na obszarze miasta w granicach sprzed 1932:
Dzielnice obecne, położone na terenach włączonych w 1932:
Dzielnice mają łączną powierzchnię 627,53 km2 [1]. W 2020 r. mieszkały w nich 9 744 534 osoby, w 5 209 997 gospodarstwach domowych[2]  (w 2010 r. 8 949 447 osób, w 4 547 435 gospodarstwach domowych)[12].
Miasta (-shi, -machi, ang.: city, town) wchodzące w skład Tōkyō-to mają podobne prawa, co inne japońskie miasta. Większość z nich służy jako „sypialnie” dla ludzi pracujących w centralnym Tokio. Niektóre z nich rozwinęły przemysł, jeszcze w innych ulokowano uniwersytety, centra rozrywki i kultury.
Miasteczka i wsie (-machi, -mura, ang.: town, village)
Cztery gminy w tej grupie jednostek tworzą powiat Tama-Zachód (Nishi-Tama-gun):
Do prefektury metropolitalnej Tokio należą również wyspy na Pacyfiku, wchodzące w skład archipelagów: Izu (Izu-shotō, administracyjnie dzieli się na trzy podprefektury: Ōshima, Miyake, Hachijō) i Ogasawara (Ogasawara-shotō, administracyjnie jako podprefektura Ogasawara-shichō). Niektóre z wysp są oddalone nawet o 1900 km od Tokio. Większość wysp ma status samorządowych wsi (-mura).
Na wyspach Ōshima (Izu Ōshima) i Miyake-jima znajdują się jedyne, czynne wulkany położone na terytorium Tōkyō-to.
Wyspy należące administracyjnie do stolicy, to m.in.:
W prefekturze Tokio istnieją cztery parki narodowe:
Mimo specjalizacji funkcjonalnej niektórych fragmentów śródmieścia, struktura miejska Tokio – jak wielu innych miast japońskich – pozostaje wielofunkcyjna. Chociaż zdarzają się dzielnice bogate, to nie ma wyraźnie ukształtowanych enklaw biedy, charakterystycznych dla wielu miast krajów rozwiniętych. Nie ukształtowały się one nawet w osiedlach dużych bloków mieszkalnych; buduje się je nadal, nierzadko jako priorytetowe inwestycje w eksponowanych punktach miasta.
Przestrzeni Tokio nie odbiera się poprzez strukturę „okręgów specjalnych” (pozostają one nieco sztucznym tworem administracji[28]), ale przez specyfikę formy i funkcji niektórych „sąsiedztw” – mniejszych dzielnic o wyróżniającym się charakterze. Raczej nie pokrywają się one z podziałem administracyjnym. Mentalną topografię miasta tworzą „sąsiedztwa”, nazwy ważniejszych skrzyżowań, stacje kolejowe, charakterystyczne budynki lub sklepy, a nawet przystanki autobusowe (każdy ma swoją nazwę)[29].
W geograficznym środku centralnej części miasta, tworzącej grupę 23 okręgów/dzielnic specjalnych, w skrócie 23-ku, znajduje się cesarski kompleks parkowo-pałacowy Kōkyo o powierzchni 341 ha. Do restauracji Meiji w miejscu tym znajdował się zamek Edo (Edo-jō), siedziba siogunów rodu Tokugawa, który rządził Japonią w latach 1603–1867. Na terenie tym znajduje się kilka budynków i pawilonów, m.in.: główny pałac cesarski Kyūden, rezydencje niektórych członków rodziny cesarskiej, siedziba Agencji Dworu Cesarskiego (Kunai-chō) odpowiedzialnej za sprawy państwowe dotyczące rodziny cesarskiej, archiwum, muzea i biura administracyjne.
Chūō-dōri prowadzi z Ginzy na północ do dzielnicy
Na północ od Pałacu Cesarskiego znajduje się
Na północny zachód od Kandy znajduje się założony w XVII w. ogród chiński Koishikawa Korakuen, jeden z najcenniejszych ogrodów Japonii. Obok niego wyrósł ostatnio wielki kompleks sali widowiskowej Tokyo Dome, któremu towarzyszą zespół handlowy „Tokyo City” i wesołe miasteczko. Na zachód od Kandy, w parku na dawnym terenie zamku Edo – znana sala sportowo-koncertowa Budōkan. Tuż za fosą natomiast znajduje się duży kompleks wzbudzającego kontrowersje chramu Yasukuni, świątyni założonej na początku ery Meiji, w której czci się bohaterów narodowych, ale pochowani są tam także zbrodniarze z okresu II wojny światowej.
Bezpośrednio na południe od Pałacu Cesarskiego znajduje się założony w 1903 park Hibiya, a obok zaczyna się kompleks urzędów państwowych, zlokalizowanych w:
Parkowy kompleks Yoyogi wyraźnie rozdziela dwa tokijskie „podcentra” – Shibuya i Shinjuku.
Na północ od Yoyogi:
Na północ od Shinjuku i mniejszego podcentrum Takadanobaba znajduje się
Ogromne centrum finansowo-handlowe i przemysłowe. Tokio ma największą na świecie gospodarkę wśród innych metropolii: 35 mln mieszkańców aglomeracji generowało w 2005 dochód 1191 mld dolarów USA; wiodąca pozycja miasta ma się utrzymać w najbliższej przyszłości[30].
Tokijska Giełda Papierów Wartościowych jest drugą co do wielkości na świecie pod względem rynkowej wartości zarejestrowanych akcji: 4,99 biliona dolarów amerykańskich. Wielkie banki, przedsiębiorstwa handlowe i ubezpieczeniowe, światowe targi przemysłowe. W okresie japońskiego „boomu” gospodarczego większość central dużych przedsiębiorstw przeniosła się do Tokio, głównie z Osaki, która tym samym utraciła prymat dyspozycyjnego centrum gospodarczego.
W przemyśle wytwórczym największy udział (2004) ma poligrafia (16,2% produkcji w prefekturze), następnie produkcja środków transportu (11,6%), elektronika (11,5%), produkcja maszyn elektrycznych (7,9%) wreszcie produkcja innych maszyn (7,7%)[31]. Na przełomie wieków w Hachiōji był przemysł jedwabniczy. W górach trudniono się hodowlą jedwabników, wydobywaniem wapienia, także tkactwem.
W samym Tokio obecnie nie ma już większych dzielnic przemysłowych (wyjąwszy nabrzeża rzeki Ara w Itabashi-ku). W regionie Tama przemysł występuje na pograniczu miast Hachiōji i Hinō oraz w Fuchū (fabryka firmy Tōshiba). Większe kompleksy przemysłowe istnieją poza Tōkyō-to, na terenach nadbrzeżnych Zatoki Tokijskiej w miastach prefektury Kanagawa (Jokohama, Kawasaki) i prefektury Chiba (Ichikawa, Funabashi, Chiba, Ichihara) oraz nad rzeką Ara w prefekturze Saitama (Toda, Kawaguchi).
Na terenie prefektury znajduje się niecałe 8400 ha użytków rolnych. Najważniejszymi uprawianymi warzywami są szpinak i szpinak musztardowy (komatsuna).
Wydatki konsumpcyjne w Tokio kształtują się na poziomie 109% w stosunku do średniej krajowej. Dorównują tej średniej w zakresie kosztów transportu oraz kosztów gazu, elektryczności i wody, przekraczają ją zaś aż o ok. 50% w mieszkalnictwie.
W tokijskim węźle kolei sieci krajowej (JR) zbiegają się cztery normalnotorowe linie wysokich prędkości Shinkansen: Tōkaidō, Tōhoku, Jōetsu, Nagano. Punktem końcowym wszystkich tych linii jest dworzec Tōkyō. Trzy ostatnie wchodzą do miasta wspólną trasą od dworca Ōmiya w mieście Saitama, pociągi zatrzymują się także na dworcu Ueno. Tōkaidō Shinkansen ma natomiast dodatkowy dworzec Shinagawa. Klasyczna sieć wąskotorowa JR ma linie magistralne: Tōkaidō (obsługuje ją dworzec Tōkyō), Chūō (obsługa przez dworzec Shinjuku), Tōhoku, Jōetsu oraz Jōban-sen (wszystkie trzy obsługiwane przez dworzec Ueno).
Całość sieci kolei krajowej obsługiwana jest przez kolej JR Higashi Nihon, z wyjątkiem linii Tōkaidō Shinkansen, eksploatowanej przez JR Tōkai.
Na węzeł ponadregionalnej komunikacji drogowej składa się przede wszystkim system autostrad. Tokio jest punktem zbiegu następujących autostrad międzyregionalnych: Tōmei (kierunek Nagoja, Kansai), Chūō (Matsumoto, Nagano), Kan’etsu (Nagaoka), Tōhoku (Sendai), Jōban (Mita). Ponadto do autostrad państwowych zalicza się trasę regionalną do Jokohamy, trasę do Chiby i dalej wschodnim wybrzeżem zatoki oraz trasę do lotniska Narita i dalej na wybrzeże Pacyfiku (Kashima). Zalicza się też przeprawę mostowo-tunelową przez Zatokę Tokijską – Tokyo Wan Aqua-Line. Autostrady państwowe docierają do granic „23-ku”, skąd ruch przejmuje sieć miejskich dróg ekspresowych. Tam też znajdują się odcinki nieukończonego zewnętrznego pierścienia autostradowego.
W Tokio zbiega się także 14 dróg krajowych.
Port handlowy, zarządzany przez prefekturę, ma nabrzeża łącznej długości 2,64 km, ok. 1 km² odkrytej powierzchni magazynowej, dostępnej publicznie oraz ok. 0,2 km² hal magazynowych[32]. W 2005 obsłużono 32 180 statków i przeładowano 92 mln ton ładunków, z czego połowę w handlu międzynarodowym (trzecie miejsce w kraju). Specyfiką portu są wielki udział transportu kontenerowego (93% tonażu) oraz przeładunek towarów wysokiej wartości. Instalacje portu występują na wielu sztucznych wyspach: Yashio, Jōnan, Hinode, Aomi, Ariake. Funkcje portowe wymuszają budowę powiązań między wyspami w postaci wysokich mostów lub tuneli (tych drugich na kanałach portowych jest większość). Dla obsługi części portu leżącego w południowych dzielnicach miasta poprowadzono specjalną linię kolei Tōkaidō Kamotsu, od strony dworca towarowego Kawasaki przebiegającą tunelem pod basenami portowymi. Obsługą statków zajmują się ponadto inne porty Zatoki Tokijskiej w Jokohamie, Kawasaki i Chibie.
Miasto obsługują dwa międzynarodowe porty lotnicze: starszy Haneda, nowszy Narita. Haneda znajduje się na terenie miasta, ok. 15 km od centrum, obsługuje ponad 60 mln pasażerów rocznie i jest czwartym pod względem wielkości lotniskiem świata. Narita zaś znajduje się koło miasta o tej samej nazwie (pref. Chiba), ok. 60 km od centrum Tokio. Obsługuje ponad 30 mln osób rocznie. Oba lotniska połączone są z miastem siecią kolejową i autostradową. Poza tym istnieje trzecie lotnisko komunikacyjne w mieście Chōfu, obsługujące loty na wyspy Izu. Duże amerykańskie lotnisko wojskowe (Yokota Air Base) funkcjonuje w mieście Fussa.
Ruch miejski obsługuje przede wszystkim trzynaście linii metra, o łącznej długości 312 km, należących do dwóch zarządów: Tokyo Metro i Toei (czyli prefektury Tōkyō-to).
Poza siecią metra pozostają także linie „małych kolei automatycznych” (Yurikamome i Nippori-Toneri Liner). Działa także linia Rinkai-sen, zbudowana dla połączenia zespołu Odaiba z linią Yamanote oraz duża linia kolei jednoszynowej do lotniska Haneda (Tokyo Monorail), obsługująca też wybrzeża zatoki.
Wielki ruch aglomeracyjny i wewnątrzmiejski obsługuje kolej JR, dzięki kilku liniom miejskim – średnicowym i obwodowym. Ruch pociągów miejsko-aglomeracyjnych jest bardzo gęsty, na liniach czterotorowych sięga 60 par pociągów w godzinie szczytu. System JR jest przeciążony, obciążenia w porannym szczycie sięgają 221% nominalnej pojemności pociągów. Trzy linie JR eksploatowane są z „przechodzeniem” pociągów na metro, jedna – z „przechodzeniem” na Rinkai-sen.
Ruch towarowy ma do dyspozycji dużą obwodnicę Tokio, Musashino-sen, o przebiegu Tsurumi – Fuchū – Nishi-Kokubunji – Minami-Urawa – Shin-Matsue – Nishi-Funabashi, prowadzoną na zachodzie w długich tunelach (trasa liczy ok. 100 km; ukończ. 1976). Na 2/3 długości linia służy też ruchowi aglomeracyjnemu.
Wielką rolę – dowozową, a także lokalną w obrębie przedmieść – spełnia kilkanaście linii kolei prywatnych, rozwiniętych z dawnych kolejek dojazdowych. Są to następujące sieci: Keikyū, Tōkyū, Odakyū, Keiō, Seibu, Keisei, Tōbu. Ich linie mają różny stopień bezkolizyjności w stosunku do sieci ulicznej, jednakże eksploatowane są z intensywnością podobną do metra. Wszystkie kończą się na granicy obszaru centralnego Tokio. Ruch przejmowany jest następnie przez sieć metra albo sieć miejską kolei JR. Część pociągów jadących liniami kolei prywatnych (albo wszystkie – zależnie od linii) może wjeżdżać bezpośrednio na linie metra (tzw. „przechodzenie pociągów”).
Wysoki standard techniczny reprezentuje linia regionalna Tsukuba Express, będąca (podobnie jak Rinkai-sen) koleją „trzeciego sektora” (własność publiczno-prywatna).
Reliktem sieci tramwajowej Toden jest linia Toei-Arakawa-sen, funkcjonująca głównie na wydzielonych torowiskach, w północno-zachodnich dzielnicach. Druga, podobna linia działa w Setagaya-ku jako pozostałość sieci kolejek dojazdowych nadal obsługiwana lekkim taborem (Tōkyū-Setagaya-sen).
Sieć autobusowa podzielona jest na kilka systemów. Większą część obszaru „23-ku” obsługują autobusy Toei. Na peryferiach (również w obrębie „23-ku”) istnieją obszary penetrowane głównie albo wyłącznie przez sieci autobusowe przewoźników prywatnych, którymi najczęściej są firmy eksploatujące też koleje prywatne (Keikyū, Tōkyū, Keiō, Seibu, Keisei). Nie ma powszechnie dostępnej mapy prezentującej linie wszystkich przewoźników. Liczne linie różnych przedsiębiorstw, obsługiwane przez komfortowe autobusy dalekobieżne („limousine bus”), łączą lotniska z różnymi częściami miasta i regionu Tama.
Większość linii autobusowych ma oznaczenia literowo-cyfrowe. Z wyjątkiem niektórych obszarów albo relacji (zwłaszcza peryferyjnych), na których nie działają koleje miejskie, rola autobusów jest drugorzędna.
Podobnie jak w innych miastach kraju, w Tokio właściwie nie ma zbiorowego transportu nocnego. Wszystkie koleje miejskie kończą pracę około 0:30. Nocą jedynym publicznym środkiem transportu są taksówki[33].
Rozwinięta sieć miejskich dróg ekspresowych liczy ok. 280 km i obejmuje miasta Tokio oraz Kawasaki i Jokohamę w sąsiedniej prefekturze Kanagawa. W Tokio sieć ta składa się z małej pętli wokół Nihonbashi i Ginzy, pętli śródmiejskiej, pętli dzielnic wewnętrznych, linii wzdłuż wybrzeża zatoki oraz z kilkunastu tras radialnych. Trasy prowadzone są głównie na estakadach ponad ulicami, kanałami i rzekami (w tym nad nabrzeżem Sumidy). W sąsiedztwie węzłów, a niekiedy także na szlakach, estakady są wielopoziomowe. Cała sieć jest przeciążona. Jest to spowodowane m.in. nieukończeniem autostrad obwodowych na peryferiach (ich zaawansowanie ocenia się tylko na 20%).
Sieć uliczna dysponuje większą liczbą wielopasmowych arterii w śródmieściu niż na peryferiach. Typowa ulica mieszkaniowa, podobnie jak w całej Japonii, jest pozbawiona chodników i z trudem mieści dwa samochody obok siebie. Niektóre gęsto zabudowane przedmieścia mają tak rzadką sieć szerszych ulic, że są na znacznych obszarach pozbawione nawet obsługi autobusowej.
Wskaźnik motoryzacji Tokio jest najwyższy wśród metropolii Azji Wschodniej i wynosi ok. 275 samochodów/1000 mieszkańców[34]. Jest on jednak znacznie niższy niż w metropoliach Zachodu. Jest to prawdopodobnie spowodowane wysokim deficytem terenu, podwyższającym znacznie koszty użytkowania samochodu.
Struktura modalna przemieszczeń na obszarze prefektury kształtowała się w 1998 następująco: pieszo i rowerem – 25% podróży, samochodem – 25%, koleją – 47%, autobusem – 3%. Udział poszczególnych rodzajów kolei w przewozach w całym regionie tokijskim przedstawia się następująco (1995): JR – 40%, koleje prywatne – 38%, metro – 22%[35].
Dla ruchu w aglomeracji charakterystyczne są wielkie potoki dojeżdżających do pracy: do „23-ku” dojeżdża codziennie ok. 2,6 mln osób, w większości do trzech dzielnic centralnych Chūō, Chiyoda i Minato. Codziennie do prefektury Tokio dojeżdża: z prefektury Saitama – 945,9 tys. osób, z Kanagawa – 937,7, z Chiba – 743,9, z Ibaraki – 67,3, z Tochigi – 14,9, z Gunma – 9,1, z Yamanashi – 8,9. Do wydłużania odległości codziennych dojazdów przyczyniają się linie Shinkansen, którymi prowadzi się także szybki ruch regionalny.
Kompleksy wielofunkcyjne
Muzea:
– kompleks parku Ueno
– inne
Teatry, muzyka:
Wybrane uczelnie 
Państwowe
Publiczne
Prywatne
Wybrane biblioteki
Wybrane muzea (spośród ok. stu)
XVIII Igrzyska Olimpijskie w 1964 odbyły się w Tokio. Stały się imprezą łączącą tradycje ze współczesnością (zastosowano m.in. najnowsze techniki pomiarów wyników). Po wykluczeniu przez organizatorów niektórych sportowców z Indonezji oraz Korei Północnej państwa te wycofały swoje reprezentacje. MKOL nie wyraził zgody na występ ekip RPA, co było reakcją na rasistowską (w stosunku do czarnoskórej większości) politykę rządu w Pretorii. Do programu igrzysk wprowadzono kobiecą i męską siatkówkę oraz narodowy sport Japonii – judo. Lekkoatletyka została poszerzona o dwie nowe konkurencje dla kobiet: pięciobój lekkoatletyczny (późniejszy siedmiobój) i bieg na 400 m. Igrzyska olimpijskie w Tokio były rekordowe pod względem osiągniętych wyników – pobito 37 rekordów świata i 77 olimpijskich.
Kompleks Jingū Gai-en:
Inne:
[36]
Ganges (hindi गंगा IPA: [ˈɡəŋɡa:], sanskryt गङ्गा) – rzeka w południowej Azji, przepływająca przez Indie i Bangladesz. Jej długość wynosi 2700 km, a powierzchnia dorzecza 1125 tys. km² (z Brahmaputrą 2060 tys. km²). Ganges uchodzi do Zatoki Bengalskiej będącej częścią Oceanu Indyjskiego.
Główne miasta portowe nad Gangesem:
Źródła Gangesu znajdują się w Himalajach, w lodowcach górskich na zachód od pasma Nanda Dewi. Początkowo płynie w kierunku wschodnim, równolegle do łańcucha Himalajów.
Potem po przełamaniu się przez góry Siwalik, wypływa na Nizinę Gangesu.
Wody Gangesu są wykorzystywane przez szeroko rozbudowaną sieć kanałów i urządzeń nawadniających. Chociaż Ganges leży w Indiach, przeważająca część jego rozległej delty znajduje się w Bangladeszu, gdzie rzeka łączy się z Brahmaputrą, tworząc największą na świecie wieloramienną deltę o powierzchni 80 tys. km². Ganges charakteryzuje się dużymi wahaniami poziomu wód nawet do 15 m. Największe przybory wód zaczynają się w kwietniu, a mają swoją kulminację w sierpniu-wrześniu, powodując powodzie. Wody obu rzek niosą wielkie ilości materiału (rocznie ok. 200 mln t), który osadzając się w ujściu, cały czas powiększa obszar delty. Muł rzeczny, nanoszony na obszar delty przez częste wylewy, jest jednym z czynników utrzymujących żyzność pól uprawnych. Regularne deszcze monsunowe powodują rozległe powodzie.
Ganges jest spławny na całym swoim odcinku nizinnym, a żeglowny na odcinku 1450 km.
Ganges jest uważany przez wyznawców hinduizmu za rzekę świętą i stanowi ucieleśnienie bogini Gangi. Zgodnie z wierzeniami hinduistycznymi król Bhagiratha dzięki tysiącleciom ascezy wyjednał u bogów zgodę, aby Ganga zstąpiła na ziemię i oczyściła z grzechów prochy jego przodków. Ganga była tak potężna, że Śiwa aby uchronić ziemię przed jej impetem uwięził ją we włosach, skąd spłynęła siedmioma nurtami. Dla swych wyznawców Ganges jest rzeką matką, obietnicą zbawienia. Gdy wyznawcy ją obrażą, rzeka wyzwala swe siły i wylewa z brzegów czyniąc wielkie zniszczenia. Wszystkie miasta zbudowane na brzegach Gangesu są święte ze względu na swoje położenie. Każdy religijny hinduista pragnie odbyć pielgrzymkę do źródeł rzeki, znajdujących się w lodowej jaskini u stóp himalajskiego lodowca Gangotri.
W miejscu gdzie do Gangesu wpada Jamuna (inna święta rzeka) jak głosi legenda wpada jeszcze podziemna rzeka Saraswati. Raz na 12 lat w tym miejscu podczas święta Kumbh Mela, w określonym przez astrologów czasie kąpiel ma szczególną moc. Wtedy liczba pielgrzymów liczona jest w milionach[1].
Jej wody zamieszkuje suzu gangesowy[2].
Ocean Spokojny, Pacyfik, Ocean Wielki – największy, najgłębszy i najstarszy, obok Atlantyku, zbiornik wodny na Ziemi. Z powierzchnią równą 155,6 mln km²[1] zajmuje 30% całej powierzchni Ziemi. Oficjalna polska nazwa tego oceanu, zatwierdzona przez Komisję Standaryzacji Nazw Geograficznych, to Ocean Spokojny[3].
Największą rozciągłość południkową osiąga między arktycznym Morzem Beringa a Morzem Rossa (w obrębie Oceanu Południowego) przy brzegach Antarktydy, gdzie wynosi ona aż 15,5 tys. km. Z kolei największą rozciągłość równoleżnikową ma między wybrzeżem Indonezji a Kolumbii i Peru – jest to 19,8 tys. km, czyli prawie połowa obwodu kuli ziemskiej i pięć razy więcej niż średnica Księżyca. Granicami oceanu są wybrzeża: Ameryki Północnej i Południowej na wschodzie, Azji i Australii na zachodzie oraz lody Arktyki i Antarktyki odpowiednio na północy i południu.
Pojemność oceanu szacowana jest na 764,836 mln km³, co stanowi 46% ziemskich zasobów wód. Tradycyjnie dzieli się go wzdłuż równika na części północną i południową, ale wyjątkami są tutaj wyspy Gilberta i Galapagos w całości uznawane za Południowy Pacyfik. Na tym oceanie znajdują się Rów Mariański, który osiąga głębokość 10 924 m[1], oraz druga co do wielkości wyspa świata Nowa Gwinea. Średnia głębokość wynosi 4280 m p.p.m.
Granice oceanów są umowne i niejednokrotnie stanowią przedmiot dyskusji naukowców, ponieważ niełatwo je wyznaczyć – na wodzie nie ma naturalnych granic. Ogólnie przyjmuje się, że Ocean Spokojny oddzielony jest od pozostałych w następujących miejscach:
Liczba mórz na Oceanie Spokojnym w stosunku do jego powierzchni nie jest zbyt wielka, ponieważ większa część jego obszaru to po prostu ocean, bez miejsc, w których można wyznaczyć morza. Jedynie w zachodniej części jest dość gęsta sieć mórz i zatok. Niektóre z mórz to: Tasmana, Koralowe, Banda, Jawajskie, Celebes, Południowochińskie, Wschodniochińskie, Japońskie, Ochockie oraz Beringa. Wśród zatok można wyróżnić: Tajlandzką, Karpentaria oraz na wschodzie Alaska i Kalifornijską.
Morza układu oceanicznego – Ocean Spokojny (Pacyfik)[4][5]:
Na oceanie tym znajduje się około 25 tys. wysp, czyli więcej niż na pozostałych zbiornikach wodnych razem wziętych. Duża ich część nosi nazwę Oceanii. Tradycyjnie składają się na nie:
Prócz tego jest wiele wysp azjatyckich, niezaliczanych do tego podziału, jak np. Wyspy Japońskie, Archipelag Filipiński, Borneo i Jawa.
Do Oceanu Spokojnego wpływa niewiele wielkich rzek. Główną tego przyczyną są nadmorskie łańcuchy górskie na wybrzeżach obu Ameryk oraz Australii, stanowiące dział wodny, który kieruje większość rzek z tych kontynentów do innych oceanów. Jedynie w Azji sieć rzeczna jest bardziej rozbudowana.
Najważniejsze rzeki wpadające do Oceanu Spokojnego to (według kontynentów):
Z pozostałych kontynentów zasilają Pacyfik tylko nieliczne, krótkie rzeki.
Temperatura wód powierzchniowych Oceanu Spokojnego waha się od bliskiej zeru w okolicach podbiegunowych do ok. 30 °C w pobliżu równika. Zasolenie również zmienia się stopniowo. Woda w okolicach równika jest mniej słona (ok. 30‰), z powodu obfitych deszczy nawiedzających te rejony, od tej znajdującej się na zwrotnikach (ponad 35‰). Niższe zasolenie niż w strefie zwrotnikowej cechuje również rejony polarne, gdyż niska temperatura hamuje tam parowanie.
Wraz z głębokością temperatura wody opada do ok. 3–4 °C, przy czym nie dzieje się to stopniowo, ale skokowo (termoklina).
Cyrkulacja powierzchniowa wody w północnej części oceanu jest zgodna z ruchem wskazówek zegara, a w części południowej jest odwrotnie. Główne prądy ciepłe: Północnorównikowy i Południoworównikowy, Równikowy Prąd Wsteczny, Kuro-siwo, Północnopacyficzny, Aleucki oraz Wschodnioaustralijski. Natomiast prądy zimne to m.in.: Prąd Wiatrów Zachodnich (Antarktyczny Prąd Okołobiegunowy), jego odnoga – Prąd Zachodnioaustralijski, Prąd Kalifornijski i Oja Siwo.
Głębiej występują prądy o innej cyrkulacji, często przeciwbieżne. Na ich przebieg wpływ mają różnice temperatur i zasolenia, czyli gęstości wody.
Ukształtowanie dna Oceanu Spokojnego jest bardzo urozmaicone. Występują tam wszystkie wielkie i małe formy dna oceanicznego.
Najbardziej charakterystyczną cechą rzeźby jest obecność najrozleglejszych w oceanie światowym basenów oceanicznych i licznych rowów oceanicznych (2/3 rowów w oceanie światowym), ciągnących się wzdłuż podnóży Azji i obu Ameryk oraz wzdłuż podwodnych grzbietów z łukami wysp w zachodniej części oceanu. Najgłębsze rowy oceaniczne to: Mariański (10 911 m), Tonga (do 10 882 m), Kurylsko-Kamczacki (10 542 m), Filipiński (10 497 m), Izu-Ogasawara (9810 m) oraz Bougainville’a (9140 m).
Światowy system grzbietów śródoceanicznych przechodzi do Oceanu Spokojnego z Oceanu Indyjskiego, od Grzbietu Australijsko-Antarktycznego, którego pacyficznym przedłużeniem jest Grzbiet Pacyficzno-Antarktyczny (Południowopacyficzny), ciągnący się przez południową część dna oceanu do Krawędzi Eltanin, od której z kolei ciągnie się Grzbiet Wschodniopacyficzny, szeroki i rozległy grzbiet skręcający stopniowo na północ w kierunku Zatoki Kalifornijskiej, gdzie kończy się główny pacyficzny ciąg grzbietów śródoceanicznych. W części południowo-wschodniej dna Oceanu Spokojnego leży drugorzędny śródoceaniczny Grzbiet Chilijski, ciągnący się w kierunku południowo-wschodnim, jako przedłużenie poprzecznej do Grzbietu Wschodniopacyficznego Krawędzi Challengera – do podnóża stoku kontynentalnego Ameryki Płd.
Grzbiety śródoceaniczne Oceanu Spokojnego mają wzdłużne doliny ryftowe i są poprzecinane poprzecznie licznymi uskokami transformacyjnymi, tworzącymi długie strefy spękań (krawędzie) ciągnące się daleko w dnie sąsiednich basenów oceanicznych, zwłaszcza na zachód od Grzbietu Wschodniopacyficznego, w wielkim Basenie Północno-Wschodnim, gdzie m.in. krawędzie: Clarion, Clippertona i Galapagos mają po ok. 4 tys. km długości. Główne grzbiety śródoceaniczne dzielą dno Oceanu Spokojnego na 2 nierówne części. Mniejsza rozciąga się wzdłuż stoków kontynentalnych Ameryki i Antarktydy, obejmując kolejno baseny: Gwatemalski, Peruwiański i Chilijski, z rowami oceanicznymi wzdłuż Ameryki oraz rozległy Basen Bellingshausena, rozciągający się do stoków kontynentalnych po południowej części Chile i Antarktydy.
Poza grzbietami śródoceanicznymi, na Pacyfiku występują też inne grzbiety podmorskie. Są to grzbiety powstałe w wyniku przesuwania się płyty oceanicznej ponad tzw. „plamą gorąca”, np. Grzbiet Hawajski, Grzbiet Cesarski, wydłużone wyniesienia podmorskie zbudowane ze skorupy kontynentalnej, np. Grzbiet Carnegie, Grzbiet Norfolski lub o nieznanej genezie.
Na zachodzie i północy od głównych grzbietów rozciąga się większa część dna Oceanu Spokojnego z najgłębszymi rowami i basenami oceanicznymi, z których najrozleglejsze są baseny: Północno-Wschodni, Południowopacyficzny, Północno-Zachodni i Środkowopacyficzny.
W zachodniej części oceanu leżą mniejsze baseny: Filipiński, Zachodniomariański, Melanezyjski, Południowofidżyjski, Tasmana i wiele innych. W basenach tych jest kilka wyraźnych, monotonnych równin abisalnych, największe występują na dnie Basenu Południowopacyficznego (na wschód od Nowej Zelandii) oraz w Basenie Bellingshausena (Równina Amundsena) i Basenie Północno-Wschodnim (m.in. równiny Równina Aleucka i Tuftsa).
Inną, charakterystyczną cechą dna Oceanu Spokojnego jest wielka liczba wzniesień podwodnych, głównie pochodzenia wulkanicznego, w postaci łańcuchów gór wulkanicznych o prostolinijnym przebiegu, lub grzbietów w kształcie łuku, zwieńczonych wyspami, z przyległymi po zewnętrznej stronie głębokimi rowami oceanicznymi. Ponadto na dnie Oceanu Spokojnego występują masowo gujoty – pojedyncze podwodne góry o względnej wysokości do kilku tysięcy metrów i płaskich wierzchołkach, m.in.: Góra Hendersona ze szczytem na głębokości −388 m, Góra Pattona (−662 m), szczyt Ramapo (−73 m), Góra Orne'a (−29 m) oraz góry Wildera (−5 m) i Kammu (−320 m). Na dnie oceanu znajdują się także duże prowincje magmatyczne, w tym Wyniesienie Szackiego, na którym znajduje się rozległy wulkaniczny Masyw Tamu (−1980 m).
W międzyzwrotnikowej części oceanu występują atole i wyspy koralowe oraz bariery koralowe wzdłuż łańcuchów wysp lub na szelfach, m.in. największa tego typu formacja, Wielka Rafa Koralowa, położona na szelfie u północno-wschodnich wybrzeży Australii na Morzu Koralowym.
Stoki kontynentalne w Oceanie Spokojnym są strome, zwłaszcza te, które wznoszą się nad rowami oceanicznymi, a na niektórych odcinkach mają kształt stopni osuwiskowo-tektonicznych i często są pocięte kanionami podmorskimi. Szelfy zajmują niewielką część dna Oceanu Spokojnego, ich szerokość wynosi od kilkudziesięciu kilometrów u wybrzeży Ameryki do 700–800 km w morzach: Beringa, Wschodniochińskim, Południowochińskim, a krawędzie szelfu znajdują się na głębokości 150–200 m, jedynie u wybrzeży Antarktydy 500 m. Geograficznie morza szelfowe należą do oceanu, natomiast pod względem budowy geologicznej dna, stanowią nierozłączną część kontynentu, do którego przylegają.
Wokół Oceanu Spokojnego ciągnie się „pacyficzny pierścień ognia”; jest to najbardziej aktywny sejsmicznie rejon świata. Występują tu liczne erupcje wulkaniczne i trzęsienia ziemi, które powodują fale tsunami. Występuje tam aż 81% wszystkich trzęsień oraz 90% aktywnych wulkanów.
Centralną część Oceanu Spokojnego stanowi oceaniczna skorupa ziemska zbudowana ze skał magmowych o składzie zbliżonym do bazaltu, czyli skorupa oceaniczna. Wiek skorupy oceanicznej rośnie od strefy ryftu w centralnej części grzbietu śródoceanicznego, gdzie ona powstaje, w stronę otaczających kontynentów. Grzbiety śródoceaniczne oraz występujące w nich doliny ryftowe są poprzecinane poprzecznie licznymi strefami rozłamu – uskokami transformacyjnymi, które powodują ich rozsuwanie na wiele setek kilometrów.
Rozległe połacie dna pokryte są głównie czerwonym iłem głębinowym, wapiennym mułem otwornicowym, promienicowym (radiolariowym) lub okrzemkowym. Miąższość tych osadów wynosi średnio 300–400 m, rosnąc od grzbietów śródoceanicznych w stronę lądów, a w rowach oceanicznych dochodzi do 2–3 km. Duże miąższości osadów występują również w strefie równikowej, co jest związane z wysoką produktywnością wód powierzchniowych w tym rejonie[6]. Najstarsze z poznanych dotychczas osadów Pacyfiku pochodzą z okresu górnojurajskiego i występują w zachodnich częściach oceanu.
Bazaltowa skorupa oceaniczna jest odgraniczona od skorupy kontynentalnej tzw. linią andezytową, ciągnącą się wzdłuż rowów oceanicznych i łuków wysp okalających Ocean Spokojny. Z linią tą, poza zmianą składu chemicznego magmy, wiąże się występowanie stref sejsmicznych (stref Benioffa). Teoria tektoniki płyt interpretuje to zjawisko jako efekt subdukcji, czyli podsuwania skorupy oceanicznej wzdłuż tej linii pod skorupę kontynentalną; wielkie naprężenia powodowane przez ten ruch rozładowywane są przez trzęsienia ziemi.
Nazwa (hiszp. pacífico – spokojny) została nadana przez żeglarza portugalskiego w służbie hiszpańskiej Ferdynanda Magellana, gdyż przez większą część jego wyprawy dookoła świata ocean ten faktycznie pozostawał spokojny[potrzebny przypis]. Jednak w rzeczywistości pacyficzne wyspy nękane są przez liczne huragany i tajfuny, a otaczające lądy i wyspy naszpikowane są wulkanami. Często spotykane są trzęsienia ziemi, a te podwodne powodują wielkie fale, zwane tsunami. To właśnie na tym oceanie zanotowano najwyższą w historii falę o wysokości sięgającej nawet 30 metrów w czasie tsunami w 1958 roku w Lituya Bay.
Już w czasach prehistorycznych przodkowie dzisiejszych Polinezyjczyków podróżowali po tym oceanie. Wyruszywszy prawdopodobnie z Azji, dotarli na Tahiti, a następnie na Hawaje i Wyspę Wielkanocną. Najpóźniej została zasiedlona Nowa Zelandia. Według niektórych badaczy podróżowali oni w kierunku odwrotnym, zgodnym z kierunkiem wiatrów i prądów morskich, czyli z Azji wzdłuż brzegów do Ameryki Południowej w kierunku Hawajów i na pozostałe wyspy.
Pierwszym udokumentowanym Europejczykiem, który zobaczył Pacyfik, był Vasco Núñez de Balboa, który dotarł na jego brzeg 23 września 1513. Kolejnym był Ferdynand Magellan w trakcie swojej wyprawy. Potem systematycznie Hiszpanie korzystali z tego akwenu, docierając w 1564 z Meksyku do Filipin pod dowództwem Legazpiego. Przez długi czas żaden Europejczyk nie przepłynął oceanu w kierunku przeciwnym. Do końca XVI wieku wpływy hiszpańskie przeważały w tym rejonie, a statki docierały na Nową Gwineę i Wyspy Salomona. W XVII wieku to Holendrzy zaczęli nadawać prym odkryciom i handlowi, np. Abel Tasman, który dotarł do Tasmanii i Nowej Zelandii w 1642 r. Później wiele wysp odkrył James Cook w służbie brytyjskiej, a w XIX wieku znaczący wkład do wiedzy oceanograficznej został dokonany przez wyprawy HMS Beagle w 1830, HMS Challenger w 1870, USS Tuscarora (1873–1876) i niemiecką Gazelle (1874–1876). W czasie drugiej wojny światowej na Pacyfiku stoczono wiele zaciekłych i krwawych bitew, w których główne role odegrały siły zbrojne Japonii i Stanów Zjednoczonych.
Po wojnie wiele byłych kolonii uzyskało niepodległość.
Obecnie podejmowane są próby eksploatacji dna oceanicznego, lecz największym problemem są tutaj duże głębokości utrudniające prace. Jedynie u wybrzeży Australii i Nowej Zelandii wydobywane są ropa naftowa i gaz ziemny. W Japonii, Papui-Nowej Gwinei, Nikaragui, Panamie i na Filipinach zbiera się dużej wielkości perły, ale ostatnimi czasy notuje się duży spadek ich wielkości.
Największym bogactwem wód Oceanu Spokojnego są ryby. Poławia się tutaj znaczne ilości śledzi, łososi, sardynek, lucjanowatych, miecznika i tuńczyka, jak i skorupiaków.
Tylko wnętrze Australii pozostaje poza wpływem klimatycznym Oceanu Spokojnego, w którego obrębie występuje pięć znacząco różnych stref klimatycznych: okołobiegunowa, umiarkowana, podzwrotnikowa, zwrotnikowa oraz równikowa. Zarówno w północnych, jak i południowych średnich szerokościach geograficznych strumienie powietrza z zachodu powodują spore różnice temperatur w różnych porach roku. Bliżej równika równomiernie wiejące pasaty zapobiegają dużym wahaniom temperatury stabilizując ją na poziomie 21–27 °C przez cały rok.
Region monsunowy leży w zachodniej części oceanu pomiędzy Japonią a Australią. Dla tej strefy klimatycznej charakterystyczne są wiatry wiejące z wnętrza kontynentu do oceanu w zimie, zaś w lecie w kierunku przeciwnym. Konsekwencją jest zauważalna sezonowość zachmurzenia i opadów. Częste tajfuny powodują wiele strat w zachodniej i południowo-zachodniej części oceanu. Z największą częstotliwością występują one w trójkącie między Japonią, centralnymi Filipinami, a wschodnią Mikronezją.
Góra Fudżi w Japonii
Pożary po trzęsieniu ziemi w San Francisco z roku 1904
Tsunami z roku 2004
Wybrzeże Oceanu Spokojnego w pobliżu San Francisco
Sztorm na Pacyfiku
Wybrzeże Tahiti
Morze Ochockie niedaleko Magadanu
Acapulco nocą
Plaża w Sydney
Park Narodowy Baku na Borneo
Anchorage na Alasce
Sahara (z arab. صحراء  ṣaḥrāʼ – „pustynia”) – strefa pustynna położona w północnej Afryce. Jest ona największą gorącą pustynią na Ziemi (ma 9 064 300 km²), rozciągająca się na 5700 km od Oceanu Atlantyckiego na zachodzie po Morze Czerwone na wschodzie; od północy ograniczona jest górami Atlas i wybrzeżem Morza Śródziemnego. Znajduje się na terytoriach 11 państw: Maroka, Algierii, Tunezji, Libii, Egiptu, Sahary Zachodniej, Mauretanii, Mali, Nigru, Czadu i Sudanu.
Sahara obejmuje 14 pustyń:
Występujące na pustyniach szaroziemy mają bardzo słabo rozwiniętą warstwę próchniczą, lub całkowicie jej brak. Powoduje to bardzo rzadkie występowanie roślin. W niektórych miejscach są oazy z dostatkiem wody i bujną roślinnością. Jest tam znacznie więcej zwierząt niż w otwartym terenie.
Pustynia sprawia wrażenie pozbawionej wszelkiego życia, ale nawet w tych wyjątkowo trudnych warunkach żyją rośliny i zwierzęta. Rośliny występujące na pustyniach przystosowane są do oszczędnej gospodarki wodą i do ochrony przed wysokimi temperaturami w ciągu dnia. Środkowa część Sahary jest pozbawiona roślinności.
Występują: dzikie oliwki, oleandry (roślinność pn. Sahary) pistacje, tamaryszek oraz akacja (południowa część Sahary).
Przykładem jest róża jerychońska, która jest bardzo dobrze przystosowana do życia na pustyni. Wiatr pędzi te rośliny w kształcie kuli, które tworzą zaschnięte pędy okrywające owocostan na duże odległości. Wówczas, gdy spadnie trochę deszczu, pędy się prostują, a nasiona rozsiewają.[potrzebny przypis]
Roślinność tych pustyń jest zróżnicowana na kilka typów w zależności od budowy i wilgotności podłoża. Najbujniejsza jest w obniżeniach zwanych na Saharze daya i w dolinach wyschłych rzek – ued (wadi). Rosną w nich miejscami nawet drzewa np. topole. Dość bogata jest również roślinność na skałach, czerpiąca wodę nagromadzoną podczas deszczów ze szczelin skalnych.
Z powodu warunków pustynnych (piaski, susza, ograniczony dostęp do wody) nie ma tam odpowiednich warunków do rozwoju życia. Gęstość zaludnienia jest niska (około jednej osoby na km²). Mieszkańcy Sahary to głównie koczownicy. Najliczniejsze jest pasterskie plemię Tuaregów. Przemierzają oni ze stadami kóz, owiec i wielbłądów rozlegle obrzeża pustyni w poszukiwaniu pastwisk oraz wody. Prowadzą również handel wymienny z mieszkańcami oaz. Tradycyjnie wędrowcy dostarczali mleko, mięso i juczne zwierzęta w zamian za zboże, daktyle, kawę, sól, broń i amunicję. Jednak cywilizacja dotarła także i na Saharę, zmieniając życie jej mieszkańców. Wędrówki Tuaregów zostały znacznie ograniczone. Tam, gdzie były szlaki ich karawan, teraz jeżdżą ciężarówki przewożące rozmaite towary. Karawany poruszające się za pomocą jucznych zwierząt (wielbłądy, muły, osły itp.) były znane już w starożytności, a w średniowieczu stanowiły niemal jedyny sposób podróżowania po Azji Środkowej i Afryce Północnej. Utarte szlaki karawanowe są używane do dziś.[potrzebny przypis]
Występują:
